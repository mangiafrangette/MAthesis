[
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Regional newspapers’ periodicals hold a firm place in the humanities among the sources of studying the First World War and its impact on the state and development of the countries and nations in the early 20 th century. This kind of sources is especially significant at the level of micro-history and history of everyday life. At the same time, it is complicated to put provincial periodicals into scientific circulation due to problems of access, preservation, organization as well as effective search and analysis of information (Kornienko and Gagarina, 2015).  The source-oriented information system “The First World War in Perm Provincial Periodicals” ( http://permnewspapers.ru/) is an integrated solution to identify problems based on approaches, methods and technologies of Digital Humanities in general and Digital History in particular. The project is realized in the laboratory of historical and political information science of Perm State University in cooperation with the Perm Regional Museum (Russia).  The information system provides free access for scholars to images and full-texts of ten newspapers collections published in Perm province during the First World War. There are more than 2500 issues. Publications cover periods of Imperial Russia, the 1917 Revolution and the Civil War and represent different ideological political movements. These editions include:  official periodicals of provincial administration of Imperial Russia: “Пермские губернские ведомости” (“Perm Province Bulletin”); official periodicals of the 1917 Revolution and the Provisional Government period: “Вестник Пермского края” (“Herald of Perm Region”), “Пермский вестник Временного правительства” (“Perm Herald of Provisional Government”); official newspapers of various levels councils:  provincial level: “Известия Пермского губернского комитета” (“News of Perm Provincial Committee”); branch level: “Известия исполнительного комитета Совета железнодорожных депутатов Пермской железной дороги” (“Proceedings of the Executive Committee of the Board of Railway Deputies of Perm Railroad”); district (uyezd) level: “Известия Осинского исполнительного комитета Совета крестьянских, рабочих и солдатских депутатов” (“News of Osa Executive Committee of Peasants, Workers and Soldiers Deputies”);   official gazette of the Perm Province Zemstvo: “Пермская земская неделя” (“Perm Zemstvo Week”).  Meaningful diversity and variety of publication's types are distinguishing features of the provincial newspaper periodicals. Official and unofficial newspapers published full texts of normative acts of different levels authorities, official announcements and telegrams, reference and information materials, articles, notes, satires, and others. There were treatments of government, laws and draft laws, regulations, diplomatic notes, information from the fronts of the First World War, reports on meetings of the Government, the Interim Committee of the State Duma, the State Conference and other agencies, including provincial and district authorities, as well as non-government organization. The newspapers published various materials on local history, geography, statistics, ethnography and cover topical issues of socio-economic, political, scientific and cultural life of the country and the region.  Such breadth of perspective and variety of publications leads to a high level of demand for local periodicals as a source of information for humanitarian studies in political, economic and social history, the history of printing and journalism, literature, linguistics, philology, cultural studies, political science, etc. However, a variety of information submits special claims for its structuring in the information system, the search tools, representation and visualization of electronic versions.  The informational system “The First World War in Perm Provincial Periodicals” is structured based on metadata system that includes thematic fields (rubrics, headings, subheadings of publications), geographical, toponymical fields, personalities. This provides effectiveness of information retrieval and samples formation on various topics, issues, and other criteria at the level of the issue, single newspaper or their combination as well as whole collection. The system also gives possibilities to search keyword and context on titles and full texts for all editions and publications. The informational system allows visualization of information at the level of newspapers, issues and publications. The results of search requests are displayed in the form of various lists and full-text publications. Each issue of newspapers is presented page by page in PDF format (text below the image), which help to preserve the text content and appearance of newspapers as much as possible. In addition, this method of representation allows text verification and editing as well as reading if OCR is found to be impossible. Texts of publications on the First World War theme are presented in HTML format. Both PDF files and texts as well as all other information and metadata on newspapers, issues and publications are stored in MySQL database. Organization and structuring of sources data, search tools and visualization tools allow to obtain data on a wide range of issues related to the First World War, life and activities of the region's population in this period. The information system “The First World War in Perm Provincial Periodicals” provides new possibilities for evaluating potential sources of information, completeness, representativeness and credibility of Perm newspapers periodicals, using computer processing techniques, obtaining new data for scientific humanities research.  Issues that can be studied by the system include attitude towards the war of different social classes and various persons in Perm, the evaluation of forces and the actions of Russia, its allies and opponents. It covers key war events, the role of various commanders, activities of Nicholas II, events in the Perm province and its districts related to the war and their consequences, the creation of images of the war, daily life in the rear and the fronts, etc. Techniques and methods for solving research tasks are developed based on different types of database queries, which allow obtaining quantitative characteristics, samples for various themes and for specific items of publications. The implementation of these types of queries permits to determine the most common types and genres, subject focus of publications and their relation, generate text fragments and interpretation of the results in terms of the completeness and nature of the information source. The study is supported by Russian Foundation for Humanities (grant 14-11-59003). ",
       "article_title":"The First World War in Perm Provincial Periodicals",
       "authors":[
          {
             "given":"Sergei",
             "family":"Kornienko",
             "affiliation":[
                {
                   "original_name":"Perm State University, Russian Federation",
                   "normalized_name":"Perm State University",
                   "country":"Russia",
                   "identifiers":{
                      "ror":"https://ror.org/029njb796",
                      "GRID":"grid.77611.36"
                   }
                }
             ]
          },
          {
             "given":"Dinara",
             "family":"Gagarina",
             "affiliation":[
                {
                   "original_name":"Perm State University, Russian Federation",
                   "normalized_name":"Perm State University",
                   "country":"Russia",
                   "identifiers":{
                      "ror":"https://ror.org/029njb796",
                      "GRID":"grid.77611.36"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016",
       "keywords":[
          "internet / world wide web",
          "databases & dbms",
          "digitisation, resource creation, and discovery",
          "historical studies",
          "digitisation - theory and practice",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" This paper reports on, analyzes, and contextualizes a project I have co-founded and run since 2013, the Stanford Code Poetry Slam ( tinyurl.com/codepoetryslam). This is a series of international contests in which we solicit code poetry, whatever that means to our submitters, and then curate the best works. At each event, the best submissions are then “slammed” by human performers and (often) simultaneously by the computer programs that run them. The project explores the performative potential of computer languages, situates itself within the growing discipline of critical code studies, and has produced some fascinating work. In this short paper, I'll explain the project, analyze several of the poems as code and as poetry, and place the Stanford Code Poetry Slams in the context of recent conversations in critical code studies, particularly with reference to performance. How do humans perform code, and how does that differ from the way computers perform it? The connective aspect, I will argue, is based in the languages code poets use to write their works, each of which afford different performative possibilities. Through a close analysis of the linguistic choices our authors made, I will show in this brief presentation some of the commonalities between critical code studies and translation studies.  Many of our code poets have written and performed works that reference older poetic movements or forms, repurposing them for a digital medium. In Zak Kain’s “Capsized,” written in beautifully descriptive CSS, you can see a clear reference to imagistic works like William Carlos William’s “The Red Wheelbarrow”; utilizing sparse but evocative descriptions, Kain’s poem paints a harsh picture and simultaneously comes off as jokey and whimsical. This duality illustrates a fascinating, secretive aspect to code poetry, where the tone and content of the poem can imply one reading and the very language in which it’s written (and the specific rules of that language) can inspire another reading entirely. In this case, the surrogate performer did a great job of presenting both aspects of the work, by starting sadly and then growing increasingly over-dramatic. The code poem that won CPS 1.1 also played with performance to get across a profound point, in this case crossing seamlessly between aspects of performativity in digital, theatrical, religious, and social media realms. In “21 st Century Prophecies,” Hunter Bacot wrote a poem that calls the most recent tweet from seven “Prophets” (famous twitter users with huge followings) and strings those tweets together into a list of “virtues.” In Keshav Dimri’s performance, each line was intoned with the solemnity of a sermon (“Let KingJames be added to the list of virtues!”) and the resulting poem (the 7 most recent tweets) was spoken like a biblical verse. “21 st Century Prophecies” references movements like bricolage and found poetry by rearranging already extant text in new ways, and it gestures towards “Curation as Creation” and ideas found in the Digital Humanities Manifesto 2.0.  Other kinds of code poems make art out of the strenuous constraints imposed by coding languages, if one’s goal is to write a text that actually compiles. Constrained texts reference much older poetic forms, like the sonnet, which require following a complicated set of rules about syllabic stresses and end rhymes. Many Perl poets write these kinds of texts. One of our best examples was Mike Widner’s “A Pythonic Lament,” which prints out “Alas! Alas!” when run. Finally, in linguistic double coding, a sentence is readable in multiple languages at once. “Jean put dire comment on tape” reads in English (albeit a little ungrammatically) and, in translation from French, says \"Jean is able to say how one types.\" Poems that are readable to humans and readable to computers perform a kind of cyborg double coding, and the ramifications of this possibility for translation studies are one of the themes the CPS series explores. Julian Bliss’ “Polymorphism,” the winning poem from CPS 2.0, took this idea to an incredible extreme, as he created a piece of text that, when compiled in multiple languages, produces a different poem in each. Moreover, each output poem parodies a clichéd English-language poem. This presentation will analyze these and other works of code poetry we’ve slammed, showing how the languages in which they were written greatly affect their performative potential and demonstrate the performative nature of translation. ",
       "article_title":"The Stanford Code Poetry Slam through Critical Code Studies",
       "authors":[
          {
             "given":"Melissa",
             "family":"Kagen",
             "affiliation":[
                {
                   "original_name":"Stanford University, United States of America",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "digital humanities - multilinguality",
          "genre-specific studies: prose, poetry, drama",
          "creative and performing arts, including writing",
          "literary studies",
          "english studies",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Three-dimensional digital visualisations have been used in the last three decades to engage and educate by making complex data more comprehensible. These have been used across a wide range of fields including military, medicine, cultural heritage, archaeology, and history. In cultural heritage contexts, the potential of these technologies as tools in the process of knowledge production has been well demonstrated (see for example Sundstedt et al., 2004; Papadopoulos et al., 2015). Contested Memories: The Battle of Mount Street Bridge project has been exploring how to integrate a 3D visualisation as the primary text of a digital scholarly edition, raising issues of how the phenomenology of place and space can be used to design a new language of scholarly editions, one that has the ability to model experience lost because of technological and evidentiary constraints. This edition, like traditional DSEs, also brings together documentary evidence in the form of apparatus, reimagining digital textuality (see Snyder, 2015).  The project focuses on a battle that took place on Wednesday 26 th April 1916 during the week of the Easter uprising in Dublin. This particular battle, between a small group of Irish rebels (members of the Irish Volunteers) and the British army sent to Dublin to put down the rebellion is used here to investigate to what extent a networked virtual world can be accommodated into a scholarly editing paradigm developed for print (and more recently digital) to enable alternative forms of research, help in the interpretive process, and assist knowledge production for both general audiences and specialists.  The development of the primary text in a gaming platform posed significant challenges, many of which have been ameliorated when creating digital editions of print or manuscript  sources through such methods and standards as XML, The Text Encoding Initiative, and XML-aware databases. This case study will explore these challenges, the project’s accomplishments, and future directions. Contested Memories: The Battle of Mount Street Bridge is one of the four projects developed under the Humanities Virtual World Consortium (HVWC) and is funded by The Andrew W. Mellon Foundation. Four virtual worlds are being constructed, each released as Unity3D builds (2015), while a shared virtual world platform ensures the sustainability and interoperability of these as well as future projects.   Historical Background and Purpose of the Digital Simulation The Battle of Mount Street Bridge has attracted much scholarly attention not only due to its significance during the Easter Rising but also because of the varied and contradictory accounts of British casualties, as well as the timeline and British strategy during the battle. Seventeen Irish volunteers took positions at four different buildings in and around on a one block stretch along Northumberland Road in a leafy suburb of Dublin (fig. 1). They prevented two Battalions of British soldiers known as the Sherwood Foresters (some 1,750 men) for over six hours from progressing into the city centre. Although there exists a significant amount of documentation for the battle from both British and Irish sources, some of it written in the 1920s (The Robin Hoods, 1921; Oates, 1920), with other sources given as oral histories between 1945-1955 (see the Bureau of Military History), scholars have not been able to map, both spatially and temporally, the contours of the engagement so as to understand how such a small number of rebels could inflict such heavy losses on a trained group of soldiers. Thus, a  goal of the project is to employ spatiotemporal methods in order to create a model of the environment in which the battle took place as well as simulate certain key events to enable researchers and specialists to visualise the conflict as well as to understand how the conflict unfolded.   Figure 1. Location of buildings occupied by the Irish Volunteers    Methodology This project employed a range of research methods in order to obtain relevant information for the battle and inform the decision-making process of building the virtual world. Firstly, conventional archival research and meetings with military historians were carried out to document different sources (both contradictory and fragmentary) that provide evidence for the buildings that were occupied, participant accounts, the weapons they used, as well as the events that took place during the battle. Period photographs were used for the digital (re)construction of the area, while a Lidar Scanning of Northumberland Road helped in creating a highly accurate digital model of the battle scene. Ballistic experts and experiments at the shooting range provided a well-informed recreation of bullets’ trajectories and guns’ sounds and reload rates.   Figure 2.  A 3D Model of Northumberland Road in 3ds Max 2015.    Figure 3. Rendering of the battlefield. Building and features that were significant in the conflict were modelled in more detail, while a schematic view of the broader area provides adequate context to the users.  The above information was used as the basis to construct a 3D model of the battle scene, as well as annotations for the Irish Volunteers and the British forces, weapons, and buildings at Northumberland Road and the adjacent streets. The purpose of this visualisation is not to present an exact representation of the battle. Rather, it is a tool to investigate alternative interpretations, as well as provide a case study to engage in ongoing interdisciplinary debates regarding the nature of digital reconstructions (Bentkowska-Kafel et al. 2012; Clark 2010; Kensek et al. 2004). The 3D model was constructed in 3ds Max 2015 (Autodesk 2015) (fig. 2, 3) and was migrated to the game engine Unity3D (2015) in order to enable a navigable in-browser 3D world for users to explore. The first version of the virtual world (November 2015) used all the capabilities of Unity Web Player (fig. 4). However, given the limited support of NPAPI, the plugin framework that Unity Web Player uses to enable detailed and high resolution models to properly function in web browsers, the second version (February 2016) implemented WebGL technology, which at the time of writing this paper, only supports light-weight applications. Therefore, most models of the virtual world had to be optimised, while detailed models of the city that provide contextual information for the Battle had to be omitted (fig. 5). Users of the virtual environment are able to navigate the world in first person-mode, while not having in-world representation in order to avoid modern figures being visible in the battle field.   Figure 4. First version of the Virtual World embedded within the digital scholarly edition    Figure 5. Second version of the Virtual World in WebGL Unity Web Player    Conclusion The digital simulation of the Battle of Mount Street Bridge provides a novel methodology for knowledge production and understanding in historical research demonstrating how computer-based simulations can augment traditional approaches in historical datasets, enhance the interpretive process, and potentially provide answers in complicated research questions. It was the process of producing the digital simulation and not simply the end-product that provided valuable answers to our questions. Once the model was completed, however, new research questions emerged, as historians interested in the period, as well as the public, began interacting with the model.  Future development of the project includes embedding narratives about in-world objects (buildings, avatars etc.), contextual information and decisions, animations of the broad contours of the battle, an AI simulation of key events, and a Mixed-Reality application tailored to the general public and secondary school students.  ",
       "article_title":"The Computer Graphic Simulation of the Battle at Mount Street Bridge. Problems, Perspectives, and Challenges",
       "authors":[
          {
             "given":"Susan",
             "family":"Schreibman",
             "affiliation":[
                {
                   "original_name":"An Foras Feasa, Maynooth University, Ireland",
                   "normalized_name":"National University of Ireland, Maynooth",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/048nfjm95",
                      "GRID":"grid.95004.38"
                   }
                }
             ]
          },
          {
             "given":"John",
             "family":"Buckley",
             "affiliation":[
                {
                   "original_name":"Department of Film and Media, Dun Laoghaire Institute of Art, Design and Technology",
                   "normalized_name":null,
                   "country":"Ireland",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Brian",
             "family":"Hughes",
             "affiliation":[
                {
                   "original_name":"Department of History, University of Exeter",
                   "normalized_name":"University of Exeter",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/03yghzc09",
                      "GRID":"grid.8391.3"
                   }
                }
             ]
          },
          {
             "given":"Constantinos",
             "family":"Papadopoulos",
             "affiliation":[
                {
                   "original_name":"An Foras Feasa, Maynooth University, Ireland",
                   "normalized_name":"National University of Ireland, Maynooth",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/048nfjm95",
                      "GRID":"grid.95004.38"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "visualisation",
          "historical studies",
          "agent modeling and simulation",
          "knowledge representation",
          "English",
          "interface and user experience design",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction In 1928, the philologue Joseph Bédier explored contemporary stemmas and found them to contain a suspiciously large amount of bifurcations. He gave two potential reasons, one being that the editors constructing a stemma with a bifurcation below the root node, could freely choose the variants for their base text. The second reason termed  force dichotomique postulates that constructing a stemma, because editors are always comparing manuscript pairs, they tend to overseparate. This is illustrated in Figure 1. The philologue would postulate a common ancestor for the closer pair and attach the third sibling together with this ancestor to the parent node producing two bifurcations, where there was a multifurcation originally.     Figure 1. Force dichotomique, example. Left: true stemma. Right: probable philological reconstruction   (Maas, 1937) acknowledges a large amount of bifurcations in stemmas for recensions of Greek texts, but points out that this is rather unsurprising taking into account the number of possible stemmas and the proportion of bifurcations therein. (Felsenstein, 1978) computes the amounts of entirely bifurcating trees for  n labelled leafs, which is certainly correlated with the number of predominantly bifurcating trees (and of root-bifurcating trees). As one can see, the proportion of bifurcating trees steadily declines, weakening Maas’ argument.  (Haugen, 2015) analysed collections of stemmas and found that Bédiers reason number two seems to hold independent of provenience. (Trovato, 2014) found that a realistic estimate of the percentage of lost manuscripts for medieval and earlier traditions ranges realistically above 73%. Related discussions and recent developments are reflected in projects such as  Stemmaweb and the  Parvum lexicon Stemmatologicum.  In this paper, the argument is introduced and investigated that, with a large amount of lost manuscripts, the amount of bifurcations in the true stemmas could naturally be high because of the transformations stemmas undergo when manuscript loss prunes away whole branches and leafs at a rate higher than 73%.   Distributions of outdegrees of manuscripts Two basic distributions are most obviously historically interpretable, a normal distribution and an exponential distribution. In considering the outdegrees to be normally distributed, we assume, that there is one certain number of copies which is most probable, peaking the others; the more the outdegree diverges negatively or positively, the fewer manuscripts with this outdegree will be found. This could translate into a hypothetical historical projection, where many manuscripts of a tradition were copied and wore off at similar rates. Assuming an exponential distribution, things become more hierarchical. For instance, a powerful organization declares some of the manuscripts authoritative which would lead to certain manuscripts being copied many times more than others. (Haugen, 2015) lists a table with the numbers of furcations (though not leafs) in two collections of stemmas for editions of Old Norse texts. Transforming these furcations into vectors, we conduct the Shapiro-Wilk normality test, (Shapiro and Wilk, 1965), and the exponentiality test by Kolmogorov-Smirnov (which are both robust even for smaller sample sizes) using the software  R.  Packages  exptest and  fitdistr have been used.  This results in an estimate on which distribution these stemmas entail for manuscript copying. However, stemmas with only identical values or only 2 furcations are skipped due to not being testable with the above chosen tests. Table 1 reveals a tendency towards exponentiality. Applying other tests for additional distributions, the weibull and log-normal distributions fitted the data best in terms of likelihood, suggesting a similar scenario.  The average mean of the furcation distributions was 1.95 and the average standard deviation 0.78.     Table 1: Tests of distributions of furcations in the collections investigated by (Haugen, 2015). Tests at significance level 0.05     Experiment In order to assess the above mentioned hypothesis, we need large numbers of underlying stemmas from which loss can be simulated and an appropriate model of manuscript loss. Then, we can simulate a large number of stemmas and manuscript loss, whereafter we count the number of bifurcations. Using simulations especially in large scale scenarios is sometimes the only possibility of assessing historical data, especially considering limits imposed onto empirical studies through data sparsity and other hindances.  Manuscript loss Historical loss does not affect all manuscipts evenly. (Canfora, 2002) found private exemplars to be less affected whereas since libraries tend to be burnt in wars, public exemplars suffer loss more easily. Many other factors (climate, authoritativity, etc.) excert influence on manuscript loss most of which have never been made subject to generalizable quantifications and it is questionable if this can ever be done. Herein, we elaborate a simple model of loss using only two basic assumptions. We simulate loss of 73-100% where each node gets a probability related to its age (the older the more probably lost) and its outdegree (the more copied, the more probably kept in good conditions, the less probably lost). Since aging is considered to be stronger than preservational effort, we square the age dependent parameter. The probability of loss for each node is thus determined by       where l is the height of the current node i incremented by 1 and slow(i) is the outdegree slowdown function:       where o(i) is the outdegree of node i. Note, that there is no distinction for nodes with an outdegree of one and leafs. This model of loss produced rather desirable loss probability distributions as is exemplified in the Appendix. However, we also use pure randomization of loss with equal probabilities among all nodes and randomization for probabilities as described but without squaring.   Simulation For the simulation, we use R and  Java.  We used  JgraphT and RCaller, (Satman, 2014).  First, for simulating normally distributed copying, we generate distributions randmonly drawn from a normal distribution using the R function  rnorm. Since  rnorm produces real numbers and negative numbers, we round all values and leave negative values and zeros aside. Since this may lead to a distortion of the so-sampled distribution deviating from a normal distribution, we test for the desired shape and only keep distributions which have a p-value above 0.05. This distribution is now our distribution of outdegrees in the to be simulated stemma, each value represents one outdegree. Starting with root, we randomly choose an outdegree (or make the node (not root) a leaf with a probability of 0.2) and add as many children to the actual node as this outdegree. For each of the so-generated children, we draw another outdegree and add as many children, recording them in the next generation. We iterate the process until all outdegrees are applied exactly once.This results in a differing number of leafs and a differing size of the tradition for each simulation. Since medieval traditions were probably not equal in size, the effect of this sampling is not controlled for further.  One could compare the proportion of surviving to lost nodes in the TRS and stemmas from the literature.  With each tradition, loss is simulated in the three above described ways. We keep all nodes, which are on the path from root to any survivor but delete all other lost nodes. What remains is the true reconstructible stemma (TRS). Since philologues do not have sufficient information (and probably time) for the reconstruction of entirely lost branches, which would be too spurious an endeavour anyway, the stemma we simulated is the maximally faithful reconstruction given our simulated ground truth.    Table 2: Percentages of bifurcations and root-bifurcations given the parametricized simulations      Table3: Furcations (1-6) before and after loss, Loss A      Results and discussion Results can be seen in Tables 2 and 3. The simulated traditions ranged in size from 4 to more than 60 and after loss from 1 to around 20 manuscripts. Percentages of bifurations (even excluding leafs, as in Haugen (2015)) are below the observed values for the collections examined in the previous literature. Under more keeping nodes, which are on the path to root for any survivor as reconstructibles results in a much larger incidence of unifurcations, not bifurcations in the stemmas after loss has applied. Therefore, some model of contraction shall be applied in subsequent research. With a mean of 2 producing large numbers of bifurcations, after loss, their number is reduced sharply in contrast with unifurcations and leafs. Thus,  loss at high rates generally affects furcations of higher orders much more, leading to a naturally higher incidence of bi- and unifurcations and most of all obscuring an underlying distribution of original copies. Aditionally, probably there are too few reconstructions of nodes with indegree and outdegree 1. In other words, philologues could tend to view a copy rather as sloppy than as a copies copy, which would contribute to making bifurcations more common in actual stemmas.    Conclusion Although the simulation is an approximation, the values suggest, that indeed the proportion of bifurcations is suspiciously large in the collections in the literature. On the other hand, Maas was probably right in expecting a larger amount of lower order furcations. However, not their proportion among all possible stemmas, but the effect of massive loss of manuscripts leads to a large probable proportion of those in a pruned TRS.  ",
       "article_title":" Silva Portentosissima - Computer-Assisted Reflections on Bifurcativity in Stemmas  ",
       "authors":[
          {
             "given":"Armin",
             "family":"Hoenen",
             "affiliation":[
                {
                   "original_name":"Goethe Universität Frankfurt, Germany",
                   "normalized_name":"Goethe University Frankfurt",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/04cvxnb49",
                      "GRID":"grid.7839.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-26",
       "keywords":[
          "English",
          "scholarly editing",
          "philology"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" This paper suggests a separation between the content, the carrier, and the presentational forms of the digitised photograph (as opposed to a born-digital image) and presents the draft for a metadata schema that is able to record the circulation of the object. The arguments presented in this paper are adapted from the research conducted for a recently-submitted doctoral thesis. Thesis titled ‘From Evidence to Essence: Curating Thematic Collections with Photographs from the British Raj’ submitted in partial fulfilment of the requirements for the degree of Ph.D to Trinity College Dublin on 30th September 2015. Scholarly study of the photographic image, specifically with the ‘material turn’ in anthropology and cultural studies, Material cultural analysis, from an anthropological position, questions the assumed superiority of language over other forms of expression (Miller, 1987). has attempted to inspect the photograph as a physical object (Batchen, 1997, Edwards, 2002, Edwards and Hart, 2004). This process of inspection expresses the complexity of the social existence of objects and allows the investigation of the photograph as a material object whose understanding is augmented by its form. Photographs have specific modes of circulation, production, and consumption, and their inspection has potential beyond the critiques of representation alone. John Tagg (1988) discusses the manner in which the currency and the value of material objects arise in certain distinct and historically specific social practices. The inherent bias of the indexical qualities of the image over its material properties may overlook the social and cultural contexts within which the photograph is born and used. While current digitisation techniques have found accurate methods of copying the content image, the description of its materiality remains a challenge. If the physicality of the photograph is central to its understanding, this paper inspects the possibilities of providing the digital referent with the material information. It presents an examination of both the materiality of the photographic image (and its transformation into a digital object), and the means through which the presentational forms of the original may be inscribed in the digital referent. The digitised photograph is a material object: to observe this materiality, a separation between the content and the carrier of the object is required. The photographic image is printed on paper, and this paper is the carrier of the content image. Similarly, the content of the book is carried in the physical form of the book — the paper, glue, and ink that hold it together. In the physical object (as opposed to its digital counterpart) the content and the carrier are closely inter-linked to the point where their separation is difficult. However, the carrier may change at different moments of time, which may provide the object with different contexts: consider an image that was first printed on photographic paper, then printed in a newspaper, and then, perhaps, in a book. The different material existences of the image provide contexts that locate it within different points in history. The digital object is, similarly, carried by electronic circuits. That the digital object has materiality, then, is undeniable. The problem, perhaps, in identifying this materiality lies in the degrees of separation between the circuits and the perception of the object. To view an electronic image, a screen — an enabler — is required. The experience of the object then is governed by an intermediary system. The dislocation between the carrier of the object and the experience of the object is perhaps the source of a material fallacy. Joanna Sassoon (2004) presents an engaging examination of the materiality of digital photographs. However, the central rationale in her paper takes premise on the assumption that digital objects have no material existence. If it is acceptable to think of this separation between the content and the carrier, it is possible to argue that the digital object is merely one iteration of a different carrier. The digitised image, then, is the original image, in a new material form. The photograph has multiple lives; it exists within socio-cultural contexts, and to understand it, the content image must be seen in conjunction with its material form. Since the inception of photography, photographic images have been used in a variety of contexts, and have been presented in a multitude of ways; the carrier often determines the use the content image is put to. Whether preserved in photo-albums (arranged thematically or sequentially), sold as postcards for the curious, or published as documentary evidence the presentational form of the photograph weighs heavy on the readings of the image. Presentational forms, in particular, guide the way in which photographs were used after their inception, and also the way they were understood. It is, here, important to distinguish between the carrier and the presentational form: the carrier is always material, while the presentational form is ideological. The materiality guides the technical production of the image, and bears the imprint of time on it. The presentational form reveals the social, political, cultural, and religious contexts within which the photograph is used. A similar concern may be seen in the area of textual editing with concerns about the primacy of the document over the text. For further reading see, for instance, Hans Walter Gabler’s ‘The Primacy of the Document in Editing’ (2007). This paper proposes to include the material specifics of the physical photograph at the level of the metadata of the digitised image. Metadata schemata for visual resources, such as CDWA and VRA Core, articulate the description of objects by distinguishing between Work and Image. VRA Core 4, in fact, is built around three record types — Work, Image, and Collection. This separation attempts to describe the complex relationship between the original and the surrogate. In a similar vein, this paper suggests a separation between the content, the carrier, and the presentational form of the photograph in its descriptive schema. For photographic images, it is more important to record its material transformations than separate the original from the copy. Walter Benjamin in his seminal essay ‘The Work of Art in the Age of Mechanical Reproduction’ contends that for photographs, asking for the mechanically produced, original print is non-sensical. While the content image remains the same, different material specifics of the photograph change as the object adopts new carriers and new presentational forms. The proposed schema is able to record multiple carriers and presentational forms for the same photographic object. This would, potentially, help to examine the circulation of the photographic object — a key concern of scholarly research in the field. This schema also presents the possibility of being extended to a linked data format that multiple curators can add to in order to articulate information about the same object. The paper presents a blueprint for the proposed schema — the structural and the functional aspects of its design. ",
       "article_title":"Materiality and Metadata of Digitised Photographs: A Theoretical Inquiry",
       "authors":[
          {
             "given":"Vinayak",
             "family":"Das Gupta",
             "affiliation":[
                {
                   "original_name":"Maynooth University, Ireland",
                   "normalized_name":"National University of Ireland, Maynooth",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/048nfjm95",
                      "GRID":"grid.95004.38"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-07",
       "keywords":[
          "cultural studies",
          "art history",
          "digitisation - theory and practice",
          "English",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction To what extent can topic models explain variation in perceptions of literary quality? We try to find correlations between topics and judgments of literary quality using a topic model of 401 recent bestselling Dutch novels. Instead of examining topics on a macro-scale in a geographical or historical interpretation (e.g., Jockers 2013; Riddell 2014), we take a new perspective: whether novels have a dominant topic in their topic distributions (mono-topicality), and whether certain topics may express an explicit or implicit genre in the corpus. We hypothesize that there is a relationship between these aspects of the topic distributions and perceptions of literary quality. We then interpret the model by taking a closer look at the topics in a selection of the novels.    Riddle survey and corpus  This research is part of a Dutch computational humanities project called The Riddle of Literary Quality. In the project we aim  to identify textual features that may play a role in readers’ evaluations of a novel as being good or bad and as high or low literature. We analyze a corpus of 401 contemporary Dutch-language (including translated) novels in search of textual features they have in common. Within our corpus there is a small variety of novelistic genres, which can be roughly divided into suspense, romantic and general novels. The readers’ judgments were gathered in a large online survey. We asked a general public to rate the novels they had read on a 7-point scale from  definitely not through  highly literary. Approximately 14,000 respondents participated, providing us with much data on the perceived quality of our 401 novels.  Extensive details on the survey will be published in two articles, one of which is in submission. The mean rating over all 401 novels is 4.2, with 2.1 being the lowest rating for  Fifty Shades of Grey by E.L. James, and 6.6 the highest for Julian Barnes’  The Sense of an Ending.      Figure 1: Overview of topics, sorted by proportion of the corpus      Topic model  A topic model aims to automatically discover topics in a collection of documents. We use Latent Dirichlet Allocation (Blei et al., 2003), which assumes the documents have been generated from a fixed number of probability distributions (the topics) over words. The topics reflect word co-occurrence patterns. We preprocess the novels by lemmatizing the words, removing punctuation, function words and names, and splitting the remaining text in chunks of 1000 tokens. We use MALLET to create a topic model with 50 topics. Fig. 1 shows an overview of the topics with their proportion across the corpus. We have attempted to identify topics for novels with high literary ratings, and topics specific for suspense and romantic novels. According to Jockers and Mimno (2013), the topics can be used to identify literary themes. They use the terms “theme” and “topic” as “proxies for [...] a type of literary content that is semantically unified and recurs with some degree of frequency or regularity throughout and across a corpus” (p. 751).  We found that three topics are specific to a single author (for instance t3), and about a third seem genre specific. By inspecting the most important words for each topic we found that most topics (genre related or not) indeed cohere with certain themes (cf. Fig.1). This suggests that the choice for 50 topics is neither too small nor too high.    Quantitative analysis  We aim to gain insight into the distribution of topics in relation to the literary ratings of the novels (predicting literary ratings is not the main aim here). In order to interpret the topic distributions, we introduce the concept of mono-topicality.  After submitting this abstract, we discovered the Literary Lab pamphlet by Algee-Hewitt et al. (2015), who independently devised a concept called mono-topicality. A mono-topical novel contains little diversity in topic distribution, which means that one or two topics are dominant throughout the novel. A novel which shows more variation in topics has a more even distribution of topics, i.e., such a novel has a larger topic diversity. Fig. 2 shows an example of both cases.  The x-axis shows the distribution of topics, sorted from least to most prevalent. In John Grisham’s  The Appeal, topic 5 (“lawsuits”) has a proportion of 47.8 % of all 50 topics. This novel is more mono-topical than the Franzen’s  Corrections, which has a more balanced distribution of topic proportions.  We hypothesize that the less mono-topical a novel is, the higher the perceived literariness by readers will be. And indeed, Fig. 3 shows that there is a statistically significant correlation between the diversity of topics of a book and its perceived literariness. Books with a single, highly prominent topic, such as Grisham’s, tend to be seen as less literary.    Figure 2: Distribution of the top 15 topics in novels with high (left) and low (right) mono-topicality    Table 1: Themes in fifteen highly literary novels, all of which are originally Dutch Table 2: Six topics from the model that address themes present in the fifteen highly literary novels, but which are not the most prominent as topics in those novels     Figure 3: Correlation between share of themost prominent topic per book and mean literariness ratings      Figure 4: Correlation between topic 29 proportion and mean literariness ratings        Interpretation  There are several possible explanations for the correlation. Genre novels could have a tendency to single out certain topics, as they deal with more ‘typical’ or genre-specific subject matter than do general novels. If this were the case, we would simply be finding that genre novels are considered to be less literary than general novels, and this would tell us little about literary quality in a more general sense. General novels in the other hand, deal with all sorts of subjects and themes, across and beyond ‘genre’ subjects, and therefore a topic model may not be able to single out thematic groups of words common to these novels, and thus may  not find one single prominent topic. A third explanation could be that highly literary novels  do deal with specific themes and subjects which are also part of genre novels, but that these are described in wordings that are more implicit or subtle, and therefore do not come up as single, clear topics. If this were the case, that would bring us closer to an explanation of what topics have to do with literary quality. These explanations are not mutually exclusive and we will explore the topic model here to examine the value of the second and third explanation.  The topic that shows the highest correlation (r=0.49) with literary appreciation is topic 29; cf. Fig. 4. This topic is most prominent in fifteen originally Dutch general novels. The twenty words in topic 29 with the highest weights are  begin, music, play, occasion, first, the first, sing, only, year, one, stay, sometimes, even, new, own, always, high, exact(ly), bike, appear. They show little coherence, making it hard to interpret their context, although ‘music and performance’ appears to be part of it. To find out more about the novels in which this topic is prominent, we consult a Dutch website maintained by librarians called  Literatuurplein, which provides information on the themes and content of Dutch novels.   Table 1: Themes in fifteen highly literary novels, all of which are originally Dutch Table 2: Six topics from the model that address themes present in the fifteen highly literary novels, but which are not the most prominent as topics in those novels  Author_Title Survey rating % topic 29 Relationships   Complications  Artistic profession  True story       Friend / family parent- child  love rivalry illness & death writer / editor other  autobio- graphic  history   Hart_Verlovingstijd 5.9 24.8 X   X        Rosenboom_ZoeteMond 6.2 22.5   X X        Lanoye_Sprakeloos 6.4 21.8  X   X X  X    Dewulf_KleineDagen 6.0 19.6 X     X  X    Rosenboom_Mechanica 6.2 17.0      X      Heijden_Tonio 6.3 16.8  X   X X  X    Verhulst_LaatsteLiefdeVan 5.8 16.5  X  X        Lanoye_HeldereHemel 5.8 15.7 X   X     X   Springer_Quadriga 6.0 15.7   X   X   X   Mortier_GestameldLiedboek 6.5 15.5  X   X X  X    Kooten_Verrekijker 5.0 15.2  X    X  X    Moor_SchilderEnMeisje 5.9 14.7   X    X  X   Zwagerman_Duel 5.5 14.6       X     Giphart_IJsland 5.3 13.0  X     X     Dorrestein_Stiefmoeder 5.5 8.7 X X          Most of these novels show similarities in themes, such as family relationships. In ten of the novels the protagonist has an artistic profession: a couple of writers, a painter and a stand-up comedian. None of them has a musical or acting career, despite the 'music and performance' words; and vice versa, none of the twenty most prominent words concern writing.  All in all, at first glance topic 29 seems not to address the themes and content of the novels, whereas most other topics in the model do concern specific themes (cf. Fig. 1 and Table 2).   Table 2: Six topics from the model that address themes present in the fifteen highly literary novels, but which are not the most prominent as topics in those novels  Topic Name Top 10 words with highest weight   2 Family relations I father, mother, child, year, son, girl, brother, woman, older, daughter   6 Health I doctor, body, pain, illness, pill, blood, death, medicine, child, patient   11 Family relations II child, mother, mom, baby, dad, little, cry, hand, time, grandma   12 Writing & memories picture, letter, write, read, paper, book, year, day, enveloppe, memories   33 Health II doctor, hospital, patient, women, bed, lie, nurse, room, hall, hour   41 Novels book, writing, story, year, word, writer, human, novel, time   For instance, topic 2 and 11 address family relations, topic 12 and 41 are about writing novels, and topic 6 and 33 concern health issues. These topics are present, but as smaller topics. This shows that the second explanation, of the general novels not sharing themes, is not valid. It could be an indication though that the highly literary novels indeed use a more subtle way of describing themes similar to other novels in our corpus, our third explanation. As a final note, in topic 29 there are proportionally more adverbs than in the other topics mentioned, which contain more nouns. Perhaps this shows that style is a more shared element in literary novels than the choice of words. In other words, this brief analysis shows that there is merit to our third explanation. This will therefore become a new hypothesis for further research.     Conclusion  We have explored a topic model of contemporary novels in relation to genre and literariness, and shown that topic diversity correlates with literary ratings. Most topics express a clear theme or genre. However, topic 29, the most literary topic, does not. It rather appears to be associated with a particular Dutch literary writing style.  ",
       "article_title":" Topic Modeling Literary Quality  ",
       "authors":[
          {
             "given":"Kim",
             "family":"Jautze",
             "affiliation":[
                {
                   "original_name":"Huygens ING, Royal Netherlands Academy of Arts and Sciences, Netherlands",
                   "normalized_name":"Royal Netherlands Academy of Arts and Sciences",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/043c0p156",
                      "GRID":"grid.418101.d"
                   }
                }
             ]
          },
          {
             "given":"Andreas",
             "family":"van Cranenburgh",
             "affiliation":[
                {
                   "original_name":"Huygens ING, Royal Netherlands Academy of Arts and Sciences, Netherlands; Institute for Logic, Language and Computation, University of Amsterdam",
                   "normalized_name":"University of Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04dkp9463",
                      "GRID":"grid.7177.6"
                   }
                }
             ]
          },
          {
             "given":"Corina",
             "family":"Koolen",
             "affiliation":[
                {
                   "original_name":"Institute for Logic, Language and Computation, University of Amsterdam",
                   "normalized_name":"University of Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04dkp9463",
                      "GRID":"grid.7177.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "literary studies",
          "stylistics and stylometry",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The  Letters of 1916 is Ireland’s first public humanities project. It collects, digitises, transcribes, encodes, and makes available through its electronic platform epistolary documents written between 1st November 1915 and 31st October 1916. The year 1916 was one of transition for Ireland: between its involvement in the Great War and the rise of militant nationalism, the country was divided by sentiment, separated by ideals. 2016 sees the centennial commemoration of the Easter Rising   The Easter Rising was an armed insurrection launched by a minority of Irish nationalists against the British Empire in 1916. The Rising was contained to Dublin and suppressed following a week of fighting. It is seen as the genesis of the later Irish War of Independence. For further reading see, for instance, Max Caulfield’s  The Easter Rebellion (1998).   across Ireland. A more complex interpretation of the events that transpired in Easter week 1916 has entered national consciousness and with it the interest in the smaller and more personal accounts of those caught up in the ensuing violence. The rhetoric of the letter presents personal perspectives and individual memory traces; the collected letters provide an insight into those fragmentary stories that, inspected together, constitute a collective consciousness. Letters are shared experiences that connect people across geographical spaces.   Janet Gurkin Altman, contemplatig about the form of the letter, contends that while letters connect two geographical points they also serve as a bridge between the sender and the receiver. The epistolary author can either choose to emphasisne either the bridge or the distance (1982).  The unique personal perspective of the epistolary form challenges the perceptions of established history, questioning the role of memory and the acts of commemoration that this era suggests.   Since Jeff Howe’s coining of the term ‘crowdsourcing’ in 2006 (Howe 2006), a number of pioneering projects have provided legitimacy and validity to the process.   Since Howe’s definition of the term there have been several pioneering project based on crowdsourcing. For instance, see  Transcribing Bentham,  Papers of the War Department 1874-1800 and  What’s on the Menu?   The  Letters of 1916 considers crowdsourcing in the widest possible sense of the term; the processes of collection, transcription, and curation are done through public engagement. The focus of this paper rests on the volunteer community associated with the project   As of October 2015 there are 1159 registered users on the  Letters of 1916 site. These users transcribe, on average, 192,409 characters a month.    and provides an inspection of the levels of its engagement, a study of its interests and motivations, and how future projects can adapt this investigation into community interaction. Sharon Leon, discussing her observations while working on the  Papers of the War Department 1784-1800,  identifies and categorises these motivations into six fields: (a) an interest in history, (b) a sense of civic duty, (c) a specific point of scholarly research, (d) engagement based on genealogical and family research questions, (e) educational assignments, and (f) curiosity about how the transcription tool and process works (Leon 2014). This paper extends these assumptions, inspecting specifically the affect of the epistolary form on the transcriber. The topical nature of this project raises questions beyond the ones that Leon raises.   The investigation in this paper traces the manner in which the community engages with the content, a hundred years since they were written, not merely as historical documents but as individual memory traces that express personal sentiments. The centenary creates a renewed vigour in the study of these documents and the paper questions if the engagement with these letters produce a more nuanced understanding of this conflicted time. As Leon points out, one of the driving forces of community transcription lies in an active interest in scholarly research. This paper attempts to understand this very engagement in the  Letters of 1916 project; some of the letters in the resource, particularly those received from personal collections, have been unavailable to the public until this point. Does the possibility of discovering these little stories and personal narratives that are weaved within the politics of the time, create an interest that emphasises the novelty of discovery? As the transcriber actively engages and researches these documents, do her motivations lie in the possibility of unearthing new knowledge?   A data-driven examination of transcriber-activity, as evidenced in the project, suggests that individual members of the community create self-fashioned roles. The transcriber who reads and re-authors these letters, the encoder who attributes TEI   The Text Encoding Initiative (TEI) is a consortium which develops and maintains a standard for the representation of texts in their electronic forms. For further information see http://www.tei-c.org/index.xml  markup to the transcribed text, and the researcher who provides contextual information for the letter are all employed in the production of these electronic documents but their engagement is at different levels. This paper attempts to understand this division of roles based on the modes of engagement that are apparent. Re-authoring of these letters raises another question: as Barthes suggests, the relationship between the reader and the writer is complex and deeply problematic (1977). Does the re-authoring of these letters create a deeper investment in the narrative? This investment is reiterated when we consider the private emails exchanged between members of the  Letters of 1916 volunteer network and the project staff where concerns are raised over the urgency, validity, and authority of the initial transcription; the accuracy and the model of the transcription becomes a point of contention between the members of the community.  This paper considers the proposed questions in three stages and at three levels to gain a clearer understanding of the role of the transcriber within the  Letters of 1916 project. In the first instance, a statistical inspection of the metadata for the transcribed letters provides an examination of the volume and the rate of transcription over the course of the project. The engagement of the individual transcriber with specific themes   Letters can belong to one or multiple themes including  The Easter Rising,  World War I,  Family Life,  Love Letters,  Official Documents,  Politics, the  Irish Question and more.   within the collection is revealed in the process; this mode of investigation aids in viewing the community, not as a homogenous cluster, but as individuals with specific interests and different points of engagement. The visualizations generated from these analyses illustrate the interests of the community at a macro level. In the second instance, the  Letters of 1916 volunteer network is approached with a focused survey. This ongoing survey presents the second phase of coordinated feedback that this project records. The questionnaire is designed to cover a range of topics that both reinforce the results of the statistical analysis and ask questions that lie outside the scope of data-driven inquiry. The focused design of the questionnaire is informed by the preliminary phase and attempts to tease out the motivations that lie at the heart of crowd-sourced projects such as the  Letters of 1916. In the third and final instance, the most-prolific transcribers   Each month the  Letters of 1916 Project generates a list of “top” transcribers. This list is determined by the number of characters a specific individual has transcribed in that time frame.   in the project are approached for interviews regarding their role in the process of transcription. These interviews are detailed and provide a closer study of the desires of the community. The methods utilised in this paper move from a macro to a micro level, from the data to the individual, in order to derive a concrete and axiomatic base to study the modes of engagement that the  Letters of 1916 project and, perhaps, all crowdsourced projects have.   The success of a crowdsourced project lies in creating effective engagement between the community and the resource. This paper provides an investigation of the motivations and the desires of individuals that drive these forms of public history projects forward. The topical nature of this research, combined with the affect of epistolary documents, creates a unique opportunity for this study, a model for future projects to further develop their volunteer community, and the critical foundations on which future study of community transcriptions may be based. ",
       "article_title":"Notes from the Transcription Desk: Modes of engagement between the community and the resource of the Letters of 1916",
       "authors":[
          {
             "given":"Vinayak",
             "family":"Das Gupta",
             "affiliation":[
                {
                   "original_name":"Maynooth University, Ireland",
                   "normalized_name":"National University of Ireland, Maynooth",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/048nfjm95",
                      "GRID":"grid.95004.38"
                   }
                }
             ]
          },
          {
             "given":"Neale",
             "family":"Rooney",
             "affiliation":[
                {
                   "original_name":"Maynooth University, Ireland",
                   "normalized_name":"National University of Ireland, Maynooth",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/048nfjm95",
                      "GRID":"grid.95004.38"
                   }
                }
             ]
          },
          {
             "given":"Susan",
             "family":"Schreibman",
             "affiliation":[
                {
                   "original_name":"Maynooth University, Ireland",
                   "normalized_name":"National University of Ireland, Maynooth",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/048nfjm95",
                      "GRID":"grid.95004.38"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-07",
       "keywords":[
          "project design, organization, management",
          "cultural studies",
          "encoding - theory and practice",
          "user studies / user needs",
          "historical studies",
          "English",
          "crowdsourcing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction In her plenary address at the 2014 Digital Humanities conference, Bethany Nowviskie urged the field to consider how “broken world thinking,” an approach equal parts ethical, ontological, and methodological, might enrich digital humanities practice (2015, n.p.). Nowviskie borrows the phrase from information theorist Steven Jackson, who argues for a reparative rather than productivist approach to the study of media and technology, and more specifically, for an increased emphasis on the “moments of breakdown” that allow us to “see and engage our technologies in new and sometimes surprising ways” (2013, 230). In this paper, we take up this shared call and extend it, elaborating an approach to broken-world thinking that is simultaneously informed by examples of historical fabrication in the digital humanities (e.g. Elliott et al. 2012, Sayers 2015) and theories of breakdown and repair from the field of design (e.g. Jackson, 2013; Gabrys, 2011). We take the time-series charts of William Playfair, the eighteenth-century data visualization pioneer, and recreate them using D3.js, a data visualization library commonly employed in digital humanities work (Bostock, 2011). In doing so, we gain valuable purchase on the historical concepts that contributed to the creation of Playfair’s charts, many of which-- such as data-- still hold sway today. But by remaining equally attentive to the “moments of breakdown” between the original artifact and our contemporary recreations, we are also able to open new perspectives on the “affordances” of our own visualization tools (Murray, 2011). Our digital “fossils,” as we term them, following the work of Jennifer Gabrys, suggest a generative new point of intersection between the fields of digital humanities and design (2011).    Project overview William Playfair is widely considered the “inventor” of modern data visualization (Tufte, 32). The graphical forms that he first developed, including the bar chart and the pie chart, remain among the building blocks of visualization today (Wilkinson, 2005), and the charts he created are still employed as examples of the crystallizing power of data visualization (Klein, forthcoming). And yet, the techniques he employed, such as copperplate engraving, have long been supplanted by newer technologies. This project seeks to understand how Playfair’s techniques affected the images he created, and how our techniques, in turn, affect the images and interactions we create today.    Figure 1: William Playfair, “Exports and Imports to and from all North-America,” from The Commercial and Political Atlas, 3rd ed. London, 1801  To recreate Playfair’s chart, we selected D3, the javascript visualization library employed in contexts ranging from data journalism to scientific research to the digital humanities (e.g. Meeks n.d., Schmidt n.d). In comparison to off-the-shelf software such as Microsoft Excel or Tableau, D3 provides additional control over the structure and style of the data, an advantage when attempting to achieve fidelity to an original image. In addition, D3 is open source; this allowed us to consider additional aspects of the library’s design. Finally, D3 was developed in an academic context; its own design choices therefore support a conceptual as well as technical analysis.  We took two approaches to recreating Playfair’s chart: the first by adhering to the original as closely as possible, including the use of the original data; and the second by adapting Playfair’s design for use with contemporary trade data, taking advantage of D3’s emphasis on data transformation. (We employed the US Census Bureau’s data on foreign trade). In the sections that follow, we describe these approaches in more detail, with particular attention to “moments of breakdown” and the new perspectives that they granted.    First approach: remediating playfair’s original chart as a digital fossil Jennifer Gabrys, a design theorist who, like Jackson, views instances of breakdown and failure in a generative light, suggests that we view cast-off objects as “fossil forms” rather than waste (2011, 7). These digital fossils provide “evidence of more complex and contingent material events,” as well as “traces of the economic, cultural, and political contexts in which they circulate.” By recreating Playfair’s chart in D3, we also remediate its “fossil form,” granting us access to the various contexts in which the chart circulates, both historical and contemporary.    Figure 2: True-to-form recreation of William Playfair’s original chart. The gray area emphasizes the uncertainty of the data, while the green area matches the original. Implementation and image by C. Foster  Our interest in creating our digital fossil was to induce the moments of breakdown that might alert us to the contextual differences between past and present; the nature and status of statistical data was one such difference. When consulting the third edition of Playfair’s  Statistical Atlas as a reference, we found no actual data accompanying his charts. D3 assumes that the developer will begin with data, so without it, we could not begin. To compensate, we turned to a data table from a previous edition, but it contained data for only a portion of the date range, from 1770 to 1782. We began by recreating that section of time, but to create the entire chart, we estimated the additional data points. The resultant chart resembled the original, but was premised upon two different data sources, with different degrees of accuracy.   This instance of breakdown and repair illustrates how D3 assumes that a dataset will be presented in a certain format, and that the data will be well-defined, clean, and accurate. The context of D3 is revealed as representative of a culture fixated on data-driven solutions. Rather than present our numbers, actual and interpreted, as the same, we used a technique developed by Kevin Schaul (2013) to create dashed lines for the interpreted numbers. The code he developed, what some might view as a “hack,” might be understood as a “repair” of a breakdown within D3, one that enables the visual presentation of defined and undefined data together. By contrast, Playfair’s original chart shows us that precise data were not a necessary component of its initial success. Playfair drew his charts’ data lines freehand. In fact, there is little evidence that Playfair plotted any actual data points before engraving the lines (Klein, forthcoming).   Second approach: creating an interactive chart in the style of playfair Our second recreation, an interactive version of Playfair’s chart supplemented by modern trade data, revealed additional contexts and biases encoded in D3’s design. D3 was designed to facilitate the creation of interactive visualizations (Bostock, 2011). Its built-in functions worked smoothly once we traded out the original dataset for a more consistently formatted, if substantially larger, contemporary one. It was when we attempted to recreate Playfair’s customized labels that we encountered a significant moment of breakdown. All Playfair did to create his labels was to pick an appropriate spot and engrave them. While less extensible than any computational method, Playfair’s technique allowed for more flexibility in the visualization’s layout. Since we were dynamically generating the charts, we weren’t able to use the human eye. Instead, we had to determine a set of rules for where to place text, and then encode them in D3. To ensure legibility, we had to verify three things: 1) that the label was not placed on a part of the chart where the import and export lines were too close; 2) that the label did not intersect with a line; and 3) that the text was placed along a part of the graph that had a consistent slope. As it turned out, determining the points of intersection was a non-trivial task. Even though the ability to illustrate the intersections between lines-- or more generally, the relations among different slices of a particular dataset-- would seem to be a basic requirement of any visualization platform, D3 was constrained by the affordances of its underlying technologies. Playfair thought hard about how to facilitate a “comparative perspective” through the design of his charts, but employing contemporary tools that are constrained for various reasons can affect the range of knowledge that is produced (1801, x).     Figure 3: Interactive version of Playfair’s time-series charts. The user selects the country to display through a drop-down menu. Implementation and image by E. Pramer    Conclusions  This project illustrates some of the insights that emerge from broken-world thinking as applied to digital humanities tools. Through the process of recreating Playfair’s charts, we introduced moments of breakdown and prompted our repairs. We became alerted to the changed relation between data and image, and to how the hidden affordances of both software and platform affect the forms of knowledge that D3 can produce. Copperplate engraving allowed greater flexibility and less reliance on the dataset. By contrast, D3 imposes limits on design and is heavily reliant on a clean dataset. This project shows how an uninterrogated reliance on popular tools can limit the creative expression of humanistic data. We have since extended this study by recreating the visualizations Elizabeth Peabody (1804-1894). Her visualization techniques are far more difficult to recreate using standard tools, underscoring how historical fabrication allows us not only to better understand the past, but also to illuminate the present.   ",
       "article_title":"Repairing William Playfair: Digital Fabrication, Design Theory, and the Long History of Data Visualization",
       "authors":[
          {
             "given":"Caroline Rebecca",
             "family":"Foster",
             "affiliation":[
                {
                   "original_name":"Georgia Institute of Technology, United States of America",
                   "normalized_name":"Georgia Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01zkghx44",
                      "GRID":"grid.213917.f"
                   }
                }
             ]
          },
          {
             "given":"Erica",
             "family":"Pramer",
             "affiliation":[
                {
                   "original_name":"Georgia Institute of Technology, United States of America",
                   "normalized_name":"Georgia Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01zkghx44",
                      "GRID":"grid.213917.f"
                   }
                }
             ]
          },
          {
             "given":"Lauren",
             "family":"Klein",
             "affiliation":[
                {
                   "original_name":"Georgia Institute of Technology, United States of America",
                   "normalized_name":"Georgia Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01zkghx44",
                      "GRID":"grid.213917.f"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-05",
       "keywords":[
          "visualisation",
          "media studies",
          "historical studies",
          "knowledge representation",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction  In fictional prose narrative such as novels and short stories, various forms of speech, thought, and writing representation are ubiquitous and have been studied in great detail in linguistics and literary studies. However, beyond quotation marks, what are linguistic markers of direct speech? And just how ubiquitous is direct speech really? Is there systematic variation in the amount of direct speech over time or across genres? Especially for the field of French literary history, where typography is not a reliable guide, we really don't know.  This is regrettable, because being able to quickly and automatically detect direct speech in large collections of literary narrative texts is highly desireable for many areas in literary studies. In the history of literary genres, this allows to observe distributions and evolutions of a fundamental, formal aspect of the novel on a large scale. In narratology, differentiating narrator from character speech is a precondition for more detailed analyses of narrator speech, e.g. with regard to text type (descriptive, narrative, argumentative text). And in authorship attribution, it hereby becomes possible to discard character speech from a set of novels and perform authorship attribution on the narrator speech only, something which may improve attribution.  Against this background, the work presented here addresses both the question of how to identify direct speech in French prose fiction and that of how prevalent direct speech is in different subgenres of the nineteenth-century French novel.    Aims and hypotheses Our first aim has been to use machine learning to automatically identify direct character speech in a small collection of French-language fictional prose. This is less trivial than it seems to be since in the French typographical tradition, direct speech is usually not marked with opening and closing quotation marks (figure 1). Rather, a long hyphen usually indicates the beginning of direct speech, whereas the end is left unmarked. In figure 1, the first highlighted direct speech continues after the insertion revealing who has just spoken (“lui dit-il, tout bas,”;  he quietly said to him). In the second example, the direct speech ends after the speaker has been indicated (“dit une voix à la portière”;  said a voice at the door). Our hypothesis is that there are enough linguistic markers of direct speech to make it possible to identify it automatically and reliably (for an overview of such markers, see Durrer, 1994).     Figure 1: Detail from Paul Féval, La Louve, 1857, p. 126 (Source: http://gallica.bnf.fr/ark:/12148/bpt6k6366934b)   Our second aim has been to use the best-performing algorithm to identify direct speech in a larger collection of French nineteenth-century novels and to study its distribution. Here, we hope to detect significant differences in the proportion of direct speech found in different novelistic subgenres. Research about other literary traditions supports this hypothesis (e.g. Allison et al., 2011).   State of the Art Speech, thought and writing representation are common topics in narratology and stylistics (Genette, 2008, Leech/Short, 2007). Semino and Short’s 2004 quantitative study finds that direct representation is clearly the most frequent type in their English fiction sub-corpus. Brunner 2015 confirms this trend for her corpus of German short narratives. Here, the percentage of sentences containing direct speech is about 35% and varies widely over different texts (2%-72%).  Frequently, speech representation recognition is an auxiliary step to other tasks, e.g. knowledge extraction or speaker recognition (Krestel at al., 2008, Elson and McKeown, 2010, Iosif and Mishra, 2014, Sarmento/Nunes, 2009). Weiser and Watrin 2012 used a rule-based approach to extract unmarked quotations in French newspaper texts with success rates of 0.745-0.789. Brunner 2015 focuses on speech, thought and writing representation in German short literary narratives. Using machine learning with random forests, she reports an F1 score of 0.87 for direct speech in a sentence-based cross-validation.   Data    Figure 2: Distribution of novels per subgenre and decade   Our text collection contains 127 French novels published between 1840 and 1889. Three generic subsets can be distinguished, each of which is represented by approximately 40 texts: general novelistic fiction (so-called ‘littérature blanche’) is contrasted with specific subgenres, crime fiction (‘policier’) and fantastic novels (see figure 2). The narrative perspective is largely heterodiegetic.   Methods  Manual Annotation To obtain a gold standard, 40 chapters from 20 different novels were randomly chosen from the collection and annotated manually. 5734 sentences were marked as either containing direct speech or not containing any direct speech; the former also include mixed sentences.   Preprocessing To prepare feature generation, preprocessing was performed, the pipeline consisting of the Stanford CoreNLP-Tokenizer and Sentence-Splitter, as well as the TreeTagger for POS-Tagging and Lemmatization.   Feature generation We modeled 81 features which we believed to be useful cues for the classification task (see the annex for a ranked list). Features are generated on a sentence-based level and can be divided into different categories:  Character-based: e.g. long hyphen marks, exclamation marks, question marks. Lexical: e.g. deictic expressions, interjections. Semantic: categories of verbs, from WordNet and the French equivalent WOLF: e.g. verbs of motion or perception. Morphological: e.g. part-of-speech, verb-tense, lemma. Syntactic features: e.g. number of commas, sentence length.     Classification For the binary classification task (sentences containing vs. not containing direct speech), we used an annotation and classification framework developed by Markus Krug (Würzburg) wrapping LibSVM Support-Vector-Machine (Chang and Lin, 2011), Maximum Entropy (Nigam et al., 1999) and Naïve Bayes (John and Langley, 1995) and implemented in MALLET (McCallum 2002). Random Forest (Breiman, 2001) and JRip (Cohen, 1995) were applied using Weka. All experiments were validated using 10-fold cross-validation unless otherwise stated.   Error analysis The machine learning algorithms’ incorrect assignments on the gold standard (false positives and false negatives) were manually analyzed in order to detect the errors' underlying causes.   Automatic tagging of unseen texts Using the best-performing model, all sentences in the text collection were tagged for containing direct speech or not. The distribution of ratios of direct speech / non-direct speech was calculated for the three subgenres and five decades covered by the collection. Performance on these unseen texts was checked manually on a random sample. (We sampled 2300 sentences, i.e. 100 random sentences each from a sample of 23 novels stratified by ratio of direct speech.)     Results and Discussion  Recognition of direct speech Table 1 depicts the performance for different conditions.    Table 1: Performance (10-fold cross-validation on the gold standard)   Our baseline is using the speech sign (i.e. the long hyphen) as the only feature, which yields an F1 score of 0.734. Random-Forest performs best, with an F1 score of 0.939, which we consider to be an impressive result. Even when excluding the speech sign from the features, we still reach an F1 score of 0.924, much better than the hyphen alone.  After inspecting the models, it becomes clear that only very few features carry strong cues for direct speech, namely (and unsurprisingly) the initial long hyphen. Most other features, taken separately, carry weak signals in either direction, but become relevant in combination.  Error analysis reveals that incorrect assignments (false positives and negatives) are frequently due to imperfect sentence segmentation. Several features which have been previously used to define and recognize direct speech (question / exclamation marks, interjections, verbal tenses) also cause incorrect assignments, especially in the context of homodiegetic narration, where the narrator is somewhat involved in the plot so that his narrator speech is similar to direct speech. Finally, letters are sometimes mistaken for direct speech, which makes  sense given that in most of them, one person addresses one or several other people.   Distribution of direct speech in the corpus We applied the best-performing algorithm (Random Forest) to the entire text collection. Evaluation shows a certain drop in performance, with a weighted average success rate of 0.844, indicating less-than-perfect generalization. We noted a welcome absence of any strong bias for either direct or non-direct speech. Our results suggest that the average proportion of direct to non-direct speech across the collection is 61% sentences with direct speech (and 39% without direct speech).    Figure 3: Ratio of direct to non-direct speech in 127 novels   While variance is considerable (see figure 3), the proportion of direct speech in French nineteenth-century novels is overall much higher than expected (and higher, for example, than the 35% reported by Brunner 2015 for German novellas).  Figure 4 shows that both fantastic novels and crime fiction have a significantly higher median for proportion of direct-speech than ‘littérature blanche’, but do not differ significantly from each other (for significance tests, we used the non-parametric Kruskal-Wallis test at a significance level of 1%).     Figure 4: Distribution (left) and significance (right) of direct to non-direct speech ratios across three subgenres   Figure 5 shows that only the ratios for the 1850s and the 1880s have a significantly differing level. However, because the decades do not have perfectly balanced subgenre proportions, this is probably due to a subgenre imbalance rather than an effect of the time period.    Figure 5: Distribution (left) and significance (right) of direct to non-direct speech ratios across five decades       Conclusions and Future Work  Using a wide range of linguistic markers allows the reliable identification of direct speech, even in the absence of clear typographic markers. Performance is excellent to good (F1-score of 0.94 on the gold standard, weighted average success rate of 0.844 on unseen texts). Using our method reveals that nineteenth-century French novels contain a large proportion of sentences with direct speech (61% on average). Also, there are previously unseen differences in direct speech proportion for subgenre, but not for time period. For future work, we plan to use several strategies to improve performance. One is to add more sequential information to our set of features. Examples include the position, inside a sentence, of certain lexical or typographical features as well as linguistic cues preceding and following direct speech. Also, we plan to expand our corpus to make it more balanced in terms of genres and decades. This will allow us to discover genre-related patterns of interest to literary historians in a more reliable manner and assess their significance with more confidence.   Supplementary material Supplementary material can be found at: https://github.com/cligs/projects/tree/master/2016/dh.   Annex A: Features used List of features used, sorted by descending rank by a one-rule classifier. average merit average rank attribute 74.028 +- 0.168 1 +- 0 79 SPEECHSIGN 71.743 +- 0.16 2 +- 0 57 VER:impf 65.847 +- 0.234 3 +- 0 54 VER:pres 63.893 +- 0.155 4 +- 0 55 VER:simp 63.248 +- 0.136 5 +- 0 6 PUNCMARKDOT 59.48 +- 0.12 6 +- 0 29 MATCHINGPPER_SON 58.835 +- 0.094 7.7 +- 0.64 30 MATCHINGPPER_SES 58.695 +- 0.208 8.1 +- 0.94 24 MATCHINGPPER_IL 58.713 +- 0.104 8.4 +- 0.92 35 VERB_MOTION 58.364 +- 0.083 10.6 +- 0.49 28 MATCHINGPPER_SA 58.344 +- 0.417 10.8 +- 1.78 7 SENTENCELENGTH 58.172 +- 0.078 11.7 +- 0.46 61 VER:subi 57.492 +- 0.091 14 +- 1.41 25 MATCHINGPPER_ELLE 57.422 +- 0.103 14.5 +- 1.36 44 VERB_PERCEPTION 57.387 +- 0.248 14.9 +- 1.51 50 INNERSUBCLAUSE 57.356 +- 0.4 15.8 +- 2.09 48 UNKNOWNLEMMA 57.213 +- 0.07 16.5 +- 1.02 31 MATCHINGPPER_LEUR 57.143 +- 0.162 17.3 +- 1.1 60 VER:ppre 56.672 +- 0.042 20.2 +- 0.98 36 VERB_BODY 56.672 +- 0.115 21 +- 1.84 52 VER:cond 56.62 +- 0.136 21.7 +- 2.1 40 VERB_EMOTION 56.567 +- 0.072 22.3 +- 1.19 26 MATCHINGPPER_ILS 56.497 +- 0.033 23.9 +- 1.3 41 VERB_COGNITION 56.428 +- 0.044 25 +- 1 46 VERB_CONSUMPTION 56.201 +- 0.005 34.5 +- 4.06 20 MATCHINGPPER_VOTRE 56.339 +- 0.176 35.4 +-18.69 32 COMMAS 56.201 +- 0.005 35.8 +- 4.19 21 MATCHINGPPER_VOS 56.201 +- 0.005 35.8 +- 6.4 22 MATCHINGPPER_TOI 56.201 +- 0.005 36.3 +- 4.2 17 MATCHINGPPER_TES 56.201 +- 0.005 37.6 +- 7.35 5 PUNCMARKCOLON 56.195 +- 0.018 37.7 +-13.33 18 MATCHINGPPER_NOTRE 56.201 +- 0.005 38.2 +- 3.16 23 MATCHINGPPER_MOI 56.424 +- 0.296 38.4 +-25.85 47 VERB_COMMUNICATION 56.201 +- 0.005 38.6 +- 6.45 4 PUNCMARKEXCL 56.201 +- 0.005 38.7 +- 3.44 16 MATCHINGPPER_TON 56.201 +- 0.005 39.4 +- 4.82 15 MATCHINGPPER_TA 56.201 +- 0.005 39.6 +- 6.45 3 PUNCMARKQUSTION 56.201 +- 0.005 40.2 +- 8.81 8 MATCHINGPPER_JE 56.201 +- 0.005 41.8 +-10.17 9 MATCHINGPPER_TU 56.201 +- 0.005 43.5 +- 9.19 10 MATCHINGPPER_NOUS 56.201 +- 0.005 43.5 +- 2.84 13 MATCHINGPPER_MON 56.201 +- 0.005 44.6 +- 4.43 12 MATCHINGPPER_MA 56.201 +- 0.005 44.7 +- 6.47 11 MATCHINGPPER_VOUS 56.261 +- 0.436 45.6 +-27.28 1 AmmountOfPPER 56.201 +- 0.005 45.8 +- 9.65 75 INTERJECTION_FI 56.201 +- 0.005 48 +-14.72 76 INTERJECTION_HEP 56.201 +- 0.005 50.2 +- 9.34 73 INTERJECTION_EH 56.201 +- 0.005 50.2 +- 6.27 74 INTERJECTION_EUH 56.201 +- 0.005 51.3 +- 3.66 81 INTERJECTION_MADAME 56.203 +- 0.08 51.3 +-23.56 37 VERB_COMPETITION 56.201 +- 0.005 52.1 +-15.75 58 VER:infi 56.201 +- 0.005 52.3 +-16.54 56 VER:futu 56.201 +- 0.005 53 +- 8.91 78 INTERJECTION_OUSTE 56.201 +- 0.005 54.7 +-17.43 34 VERB_CONTACT 56.162 +- 0.116 55.6 +-18.7 33 VERB_WEATHER 56.201 +- 0.005 56.9 +- 6.55 64 INTERJECTION_OH 56.201 +- 0.005 57.3 +- 4.5 63 INTERJECTION_AH 56.135 +- 0.087 58 +-24.31 19 MATCHINGPPER_NOS 56.193 +- 0.015 58.7 +-13.46 77 INTERJECTION_OUF 56.201 +- 0.005 59.3 +- 9.42 67 INTERJECTION_HÉLAS 56.143 +- 0.07 59.6 +-16.69 14 MATCHINGPPER_MES 56.201 +- 0.005 59.9 +- 4.5 42 VERB_STATIVE 56.201 +- 0.005 60.2 +- 2.64 62 VER:subp 56.201 +- 0.005 60.6 +- 8.39 71 INTERJECTION_CHUT 56.201 +- 0.005 62 +- 6.36 70 INTERJECTION_HEM 56.193 +- 0.015 62.5 +-10.87 66 INTERJECTION_HEIN 56.197 +- 0.011 62.6 +- 8 65 INTERJECTION_HÉ 56.201 +- 0.005 62.7 +- 5.87 51 DEIKTIKA 56.201 +- 0.005 63 +- 4.07 80 INTERJECTION_MONSIEUR 56.201 +- 0.005 63 +-11.79 53 VER:impe 56.005 +- 0.298 63.8 +-24.78 38 VERB_POSSESSION 56.201 +- 0.005 64 +- 5.67 39 VERB_SOCIAL 56.201 +- 0.005 64.3 +- 4.86 45 VERB_CHANGE 56.197 +- 0.013 64.7 +- 8.74 68 INTERJECTION_BAH 56.201 +- 0.005 64.7 +- 5.27 59 VER:pper 56.197 +- 0.015 65.2 +- 7.08 69 INTERJECTION_HOLÀ 56.139 +- 0.062 66.7 +-20.16 27 MATCHINGPPER_ELLES 56.183 +- 0.008 71.8 +- 5.23 72 INTERJECTION_BRAVO 56.005 +- 0.121 74.2 +- 9.41 43 VERB_CREATION 56.079 +- 0.038 77.4 +- 1.56 2 AmmountOfDET 55.99 +- 0.142 78.1 +- 3.73 49 POSNPP   Annex B: Text collection   author-name title year subgenre narration   Balzac Pierrette 1840 blanche heterodiegetic   Balzac TenebreuseAffaire 1841 policier heterodiegetic   Balzac AlbertSavarus 1842 blanche heterodiegetic   Sue MysteresParis02 1842 fantastique heterodiegetic   Sue MorneDiable 1842 fantastique heterodiegetic   Sue MysteresParis01 1842 fantastique heterodiegetic   FevalPP LoupBlanc 1843 blanche heterodiegetic   Dumas Eppstein 1843 fantastique heterodiegetic   FevalPP MysteresLondres1 1843 policier heterodiegetic   FevalPP FanfaronsRoi 1843 blanche heterodiegetic   FevalPP MysteresLondres3 1843 policier heterodiegetic   Sue MysteresParis04 1843 fantastique heterodiegetic   Sue MysteresParis05 1843 fantastique heterodiegetic   Sue JuifErrant 1844 fantastique heterodiegetic   Sand PecheAntoine 1845 blanche heterodiegetic   Sue PaulaMonti 1845 fantastique heterodiegetic   FevalPP Quittance2Galerie 1846 blanche heterodiegetic   Sand LucreziaFloriani 1846 blanche homodiegetic   Balzac CousineBette 1846 blanche heterodiegetic   Gautier PartieCarrée 1848 fantastique heterodiegetic   Sue MysteresPeuple02 1849 fantastique heterodiegetic   Dumas Fantômes 1849 fantastique homodiegetic   Dumas Olifus 1849 fantastique homodiegetic   Dumas ColliersVelours 1850 fantastique heterodiegetic   Sue MysteresPeuple03 1850 fantastique heterodiegetic   Sue MysteresPeuple04 1850 fantastique heterodiegetic   Sue MysteresPeuple07 1851 fantastique heterodiegetic   Sue MysteresPeuple06 1851 fantastique heterodiegetic   Aurevilly Ensorcelée 1852 fantastique homodiegetic   Ponson Baronne 1852 fantastique heterodiegetic   FevalPP ReineEpees 1852 blanche heterodiegetic   Ponson FemmeImmortelle 1852 fantastique heterodiegetic   Sue MysteresPeuple09 1853 fantastique heterodiegetic   Sue MysteresPeuple08 1853 fantastique heterodiegetic   Sue MysteresPeuple11 1854 fantastique heterodiegetic   Sue MysteresPeuple10 1854 fantastique heterodiegetic   Sue MysteresPeuple12 1855 fantastique heterodiegetic   FevalPP MadameGilBlas 1856 blanche homodiegetic   Gautier Avatar 1856 fantastique heterodiegetic   FevalPP Louve2 1856 blanche heterodiegetic   Sue MysteresPeuple13 1856 fantastique heterodiegetic   Gautier RomanMomie 1857 fantastique heterodiegetic   Sue MysteresPeuple16 1857 fantastique heterodiegetic   Dumas MeneurLoups 1857 fantastique heterodiegetic   Sue MysteresPeuple15 1857 fantastique heterodiegetic   Ponson ClubValets2 1858 policier heterodiegetic   Ponson ExploitsRocambole3 1859 policier heterodiegetic   Ponson ExploitsRocambole2 1859 policier heterodiegetic   Ponson ExploitsRocambole1 1859 policier heterodiegetic   Sand ElleLui 1859 blanche heterodiegetic   Ponson Chevaliers 1860 policier heterodiegetic   Féval Ténèbre 1860 fantastique heterodiegetic   FevalPP ChevalierTenebre 1861 fantastique homodiegetic   Aimard RodeursFrontieres 1861 blanche heterodiegetic   Hugo Miserables1Fantine 1862 blanche heterodiegetic   Ponson TestamentGrainDeSel 1862 policier heterodiegetic   About OreilleCassée 1862 fantastique heterodiegetic   Villiers Isis 1862 fantastique heterodiegetic   FevalPP HabitsNoirs1 1863 policier heterodiegetic   Aurevilly PrêtreMarié 1864 fantastique homodiegetic   Féval Vampire 1865 fantastique homodiegetic   Gaboriau Lerouge 1865 policier heterodiegetic   FevalPP HabitsNoirs2Coeur 1865 policier heterodiegetic   Ponson Breda 1866 fantastique heterodiegetic   Ponson ResurrectionRocambole2 1866 policier heterodiegetic   Verne CapitaineHatteras 1866 blanche heterodiegetic   Ponson DernierMot3 1867 policier heterodiegetic   Ponson DernierMot4 1867 policier heterodiegetic   Gaboriau EsclavesParis2 1867 policier heterodiegetic   Ponson DernierMot2 1867 policier heterodiegetic   Ponson MiseresLondres3 1868 policier heterodiegetic   Aimard Ourson 1868 blanche heterodiegetic   Ponson MiseresLondres2 1868 policier heterodiegetic   Ponson MiseresLondres4 1868 policier heterodiegetic   FevalPP HabitsNoirs3Rue 1868 policier heterodiegetic   Ponson FéeAuteuil 1868 fantastique heterodiegetic   Flaubert Education 1869 blanche heterodiegetic   FevalPP HabitsNoirs4Arme 1869 policier heterodiegetic   FevalPP HabitsNoirs5Maman 1869 policier heterodiegetic   Gouraud EnfantsFerme 1869 blanche heterodiegetic   Gaboriau MonsieurLecoq2 1869 policier heterodiegetic   Zola FortuneRougon 1870 blanche heterodiegetic   Ponson CordePendu1 1870 policier heterodiegetic   Ponson CordePendu2 1870 policier heterodiegetic   Gaboriau VieInfernale2 1870 policier heterodiegetic   Gaboriau Degringolade1 1872 policier heterodiegetic   Gaboriau Degringolade3 1872 policier heterodiegetic   Gaboriau Degringolade2 1872 policier heterodiegetic   Gaboriau CordeCou2 1873 policier heterodiegetic   Zola VentreParis 1873 blanche heterodiegetic   Gaboriau CordeCou1 1873 policier heterodiegetic   Gaboriau Argent1 1874 policier heterodiegetic   Gaboriau Argent2 1874 policier heterodiegetic   FevalPP VilleVampire 1875 fantastique homodiegetic   Zola AbbeMouret 1875 blanche heterodiegetic   Verne HectorServadac 1877 fantastique heterodiegetic   Malot Cara 1878 blanche heterodiegetic   AimardAuriac AigleNoirDacotahs 1878 blanche heterodiegetic   Stolz SecretLaurent 1878 blanche heterodiegetic   FevalPP HommeSansBras 1881 policier heterodiegetic   Loti RomanSpahi 1881 blanche heterodiegetic   Boisgobey Omnibus 1881 policier heterodiegetic   Gaboriau AmoursEmpoisonneuse 1881 policier heterodiegetic   Stolz Mesaventures 1881 blanche heterodiegetic   FevalPP HistoireRevenants 1881 fantastique heterodiegetic   Gouraud ChezGrandMere 1882 blanche heterodiegetic   Aurevilly HistoireSans 1882 fantastique heterodiegetic   Maupassant UneVie 1883 blanche heterodiegetic   Rachilde MVénus 1884 fantastique heterodiegetic   Boisgobey Voilette 1885 policier heterodiegetic   Zola Germinal 1885 blanche heterodiegetic   Ohnet GrandeMarnière 1885 blanche heterodiegetic   Zola Oeuvre 1886 blanche heterodiegetic   Villiers EveFuture 1886 fantastique heterodiegetic   Boisgobey RubisOngle 1886 policier heterodiegetic   Malot Zyte 1886 blanche heterodiegetic   Loti PecheurIslande 1886 blanche heterodiegetic   Mary RogerLaHonte 1886 blanche heterodiegetic   Malot Conscience 1888 blanche heterodiegetic   Boisgobey OeilChat1 1888 policier heterodiegetic   Boisgobey Chat2 1888 policier heterodiegetic   Gouraud QuandGrande 1888 blanche heterodiegetic   Boisgobey MainFroide 1889 blanche heterodiegetic   Boisgobey Opera2 1889 policier heterodiegetic   Boisgobey MainFroide 1889 policier heterodiegetic   Boisgobey Opera1 1889 policier heterodiegetic   Boisgobey DoubleBlanc 1889 policier heterodiegetic    ",
       "article_title":" Straight Talk! Automatic Recognition of Direct Speech in Nineteenth-Century French Novels  ",
       "authors":[
          {
             "given":"Christof",
             "family":"Schöch",
             "affiliation":[
                {
                   "original_name":"University of Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          },
          {
             "given":"Daniel",
             "family":"Schlör",
             "affiliation":[
                {
                   "original_name":"University of Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          },
          {
             "given":"Stefanie",
             "family":"Popp",
             "affiliation":[
                {
                   "original_name":"University of Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          },
          {
             "given":"Annelen",
             "family":"Brunner",
             "affiliation":[
                {
                   "original_name":"Institut für deutsche Sprache, Mannheim",
                   "normalized_name":"Institute for the German Language",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00hvwkt50",
                      "GRID":"grid.443960.c"
                   }
                }
             ]
          },
          {
             "given":"Ulrike",
             "family":"Henny",
             "affiliation":[
                {
                   "original_name":"University of Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          },
          {
             "given":"José",
             "family":"Calvo Tello",
             "affiliation":[
                {
                   "original_name":"University of Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "french studies",
          "genre-specific studies: prose, poetry, drama",
          "literary studies",
          "natural language processing",
          "data mining / text mining",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Digital Scholarship Training Programme 2012-2015 Research libraries and cultural heritage institutions must be able to adapt to a changing research landscape and invest in the development of staff skills and core competencies to match if they are to continue to effectively support and engage with modern scholars (Adams, 2013). The Digital Research team at British Library, which includes the BL Labs project and the Digital Curator team, engages with those operating at the intersection of academic research, cultural heritage and technology to enable innovative use of our digital collections, and creates opportunities for library staff to develop skills necessary to support emerging areas of modern scholarship, particularly the Digital Humanities (DH). This paper presents the final report on the pilot Digital Scholarship Training Programme delivered to staff at British Library between 2012-2015. It provides an evaluation of the skill-building initiative, incorporating preliminary findings from research into the current trends and international developments in the field of DH that will inform the next phase of staff training. In 2012, the Digital Curator team embarked on a plan to design and deliver a bespoke training programme for staff (McGregor et. al, 2013). Four objectives were set to guide and ultimately measure the success of the programme:  Staff across all collection areas are familiar and conversant with the foundational concepts, methods and tools of digital scholarship. Staff are empowered to innovate. Collaborative digital initiatives flourish across subject areas within the Library as well as externally. Our internal capacity for training and skill-sharing in digital scholarship are a shared responsibility across the Library.  In consultation with experts from within the Library and institutions on the leading edge of digital scholarship we designed and delivered in-house a catalogue of 19 one day courses suited to building the digital skills of information professionals in the research library and cultural heritage sector. Though much of the programme was rooted in and inspired by the field of digital humanities, the wider umbrella of Digital Scholarship was retained to future-proof the programme and to envelope developments in related fields like social and computer sciences. The following titles represent a cross-section of courses created:  Behind the Screen: The Basics of the Web Crowdsourcing in Cultural Heritage (Ridge, 2015) Georeferencing and Digital Mapping Social Media: An Introduction to Blogging and Twitter Working with Digital Objects: From Images to A/V Information Integration: Mash-ups, APIs and the Semantic Web (Stephens, 2014a) Cleaning up Data (Stephens, 2014b)  The four-member Digital Curator team oversaw the running of 88 hands-on courses (or roughly 30 a year) between November 2012 and September 2015. Courses were delivered by a mix of internal and external trainers. Over 400 individual staff members came through the programme, on average attending two or more courses each. Throughout the pilot we collected feedback formally via post-course evaluation forms, and informally through avenues such as a weekly Digital Research Clinic, and personal conversations that arose in the course of our daily work. Colleagues were asked to provide comments to help us improve the course, including what they enjoyed most out of the day, what they anticipated using in their work, and what was not clearly articulated.  Hands-on practical exercises were cited most often as the most enjoyable element, though not to the exclusion of the lecture and discussion time which participants felt provided necessary context. While many attendees cited specific technologies such as Open Refine as something they anticipated using in their work, they also tended to comment that having the technology underpinning innovative digital research projects demystified was helpful inspiration for future projects. Topics which could have been more clearly articulated centred on a lack of clarity on practical steps for turning aspiration into application.  Looking specifically at the objectives set, there is ample evidence to support the continuation of the training programme, such as the incorporation of the programme in staff induction for established Library projects such as Qatar Digital Library. As staff across all collection areas have become more familiar and conversant with the foundational concepts, methods and tools of digital scholarship, we have witnessed its profile increase across the Library. For instance three new PhD placements were offered this year specifically within the digital research domain (Sheperd, 2016) for the first time. Staff have felt empowered to innovate, and collaborative digital initiatives have flourished. A particularly cogent example is that of curator Dr. Sandra Tuppen, who attended one of our courses on cleaning up data and went on to secure a £79,000 grant towards a research project which enriched and cleaned British Library catalogue data so that it could be successfully aligned with other printed music datasets in support of a big data approach to the history of music (Tuppen, 2014).  Our internal capacity for training and skill-sharing in digital scholarship has become a shared responsibility across the Library, with internal course instructors now outnumbering external instructors. With the increasing number of Library staff being involved in projects and other programmes that include digital research activities and methodologies, we have been able to integrate more in-house expertise to the courses offered. Additionally the Digital Curator team has prioritised our own upskilling through a monthly informal “Hack & Yack” where we work through online tutorials with a view towards incorporating them into training, as well as more formal courses such as Train the Trainer aimed at enhancing our teaching strategies, combining theory and practical methodologies in the planning and delivery of the courses.    Looking to the future  We will continue the Digital Scholarship Training Programme and for 2016/2017 will maintain 7 of the 19 courses in their current form: 1. 101 This is Digital Scholarship 2. 103 Digitisation at the British Library 3. 105 Crowdsourcing in Libraries, Museums and Cultural Heritage Institutions  4. 108 Geo-referencing and Digital Mapping 5. 109 Information Integration: Mash-ups, API’s and Linked Data 6. 116 Metadata for Electronic Resources 7. 118 Cleaning up Data Focusing on delivering this smaller core of courses will free up resource to improve upon how we:  Reach staff who are keen and could most make use of the information but have not yet engaged Providing guidance and support to staff who are looking to implement what they have learned Addressing more explicitly the challenges and opportunities for working with complex collection materials, such as with non-Western materials   We aim to provide a more diverse training offering to ensure that there are sufficient opportunities for staff in a variety of roles at the Library to engage with digital research. Often colleagues have said they would like to attend a course, but either their workload is such that they feel they cannot spare a full day for it or they work a rota, as is the case with our colleagues who staff reading rooms. Finding creative ways to articulate more clearly and succinctly the practical value of time spent on a course, for example through shorter more frequent taster sessions explaining how a digital tool or method might help solve a specific problem, may help to reach those who have yet to engage. We are also working in partnership with reading room staff on rota to develop new approaches for conveying the training (perhaps through short informational videos). On the opposite end of the spectrum is the need to support increasing numbers of library staff who have engaged with the programme and are now looking to implement what they have learned. In an ideal world we could offer 'just in time' training to colleagues at the point of their immediate need. However, as Miriam Posner (2012) and others have discussed, each question a colleague asks may bring with it a hidden overhead of time taken to respond well. In some cases we may seek to hire existing trainers to support specific project needs but more practically, we will look to better promote and leverage our weekly Digital Research Clinic, a drop-in service for staff to get guidance on any aspect of digital research. A collection of practical 'Getting Started' guides will be further developed and shared via an internal wiki. Additionally, Digital Curators sit on key infrastructure development projects so as to directly inform the development of these in support of digital research.  Finally, digital scholarship is a complex and global affair, as evidenced by the rapid expansion of DH centres around the world. Our courses to date have tended to deal with relatively simple forms of digitised material such as digitised printed English language books. However for colleagues working with non-Western texts for instance, knowledge of cutting edge developments in transcription and Optical Character Recognition would be highly beneficial in helping ensure these materials can be leveraged by digital scholars. Our collections are as global and diverse as the DH communities interests worldwide, and future staff training provision must more accurately address the complexities and opportunities of working with our vast non-Western materials.  ",
       "article_title":"The Digital Scholarship Training Programme at British Library: Concluding Report & Future Developments",
       "authors":[
          {
             "given":"Nora",
             "family":"McGregor",
             "affiliation":[
                {
                   "original_name":"British Library, United Kingdom",
                   "normalized_name":"British Library",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/05dhe8b71",
                      "GRID":"grid.36212.34"
                   }
                }
             ]
          },
          {
             "given":"Mia",
             "family":"Ridge",
             "affiliation":[
                {
                   "original_name":"British Library, United Kingdom",
                   "normalized_name":"British Library",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/05dhe8b71",
                      "GRID":"grid.36212.34"
                   }
                }
             ]
          },
          {
             "given":"Stella",
             "family":"Wisdom",
             "affiliation":[
                {
                   "original_name":"British Library, United Kingdom",
                   "normalized_name":"British Library",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/05dhe8b71",
                      "GRID":"grid.36212.34"
                   }
                }
             ]
          },
          {
             "given":"Aquiles",
             "family":"Alencar-Brayner",
             "affiliation":[
                {
                   "original_name":"British Library, United Kingdom",
                   "normalized_name":"British Library",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/05dhe8b71",
                      "GRID":"grid.36212.34"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "project design, organization, management",
          "teaching and pedagogy",
          "digital humanities - pedagogy and curriculum",
          "digital humanities - institutional support",
          "GLAM: galleries, libraries, archives, museums",
          "archives, repositories, sustainability and preservation",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Background The background for this paper is work in progress in E-ARK: an EC FP7 pilot B project   E-ARK is funded by the European Commission’s FP7 PSP CIP Pilot B Programme under Grant Agreement no. 620998.  having as its main objective the creation of an open source, digital archiving system with attendant standards and tools to be deployed in seven pilot instances. Hence practical application is at the heart of the project, which is led by archivists, researchers, SMEs, digital preservation / archiving membership organizations and government home offices, who together seek to fill the current digital archiving lacuna. E-ARK is a wide-ranging project: we are taking and integrating existing best practices into a digital archiving system, so that it is suitable not only for national archives and government agencies, but also for regional, local, business and research archives of many shapes and sizes. A legal study taking account of varying national legal directives delineates how the archiving system can be deployed against a pan-European backdrop.  At the heart of the system is the OAIS standard (OAIS), with data arriving at an archive via Submission Information Packages (SIPs), which are then stored in the archive as Archival Information Packages (AIPs) and subsequently retrieved upon access as Dissemination Information Packages (DIPs). E-ARK is also addressing an eclectic range of sources: both structured and unstructured data, atomic and complex, including records from Electronic Records Management Systems (ERMSs); databases; geodata; and computer files. The project stakeholders are similarly also drawn from a wide pool of varied users, and include public administrations, public agencies, public services, citizens, researchers and business. Re-use of information is a key project objective, and we are employing the latest Big Data tools / techniques / architecture such as Hadoop and Lily to present users with innovative access methods, such as the data mining showcase using geodata. We are also building on techniques used in creating an Oracle data warehouse of US 1880 census data (Healey and Delve, 2007).  Although E-ARK is being spearheaded by national archives, it is a key objective to be relevant and useful to a broad church, and to that end the paper should be of interest to many in the Digital Humanities community, not just archives, so we will be including use cases tailored to this end.    Scope of the Paper E-ARK has conducted a GAP analysis that identifies user requirements for access services, which are described in project report D5.1 (Thirifays et al., 2014). The study investigates the current landscape of archival solutions regarding currently-available access components and identifies gaps and requirements from the perspective of national archives and third party users, as well as content providers. This report has identified a major gap in the identification process, where users browse and search collections to identify material of potential interest. It states that a lack of comprehensive metadata that is available and indexed in finding aids, compromises their performance and efficiency, which directly impacts the user experience and the user’s access to the archival holdings in their entirety. To fill this gap, work on the E-ARK Faceted Query Interface and Application Programming Interface (API) aims to establish a scalable search infrastructure for archived content. It is important to note that here we are not working with the whole content ecosystem of an archive, but instead concentrating only on indexing and searching of the born-digital E-ARK Information Packages (IPs). The goal is not to replace existing systems but to augment these components (like available catalogues) with a “content repository” that can be searched based on a full text index. This content repository concentrates on search and access based on the content (ie. data/files) contained within an AIP rather than selected metadata elements. The reference implementation employs scalable (cluster) technology, as scalability issues must be taken into account when operating a detailed content-based search facility. A major task in the context of the reference implementation is the development of a faceted query interface for searching archived content that can be utilized directly by end-users or integrated with other software components like archival catalogues.  Work on  Query and Indexing concerns the configuration and generation of a repository index that holds detailed information on the archives’ digital holdings. For developing a reference implementation, it is important to provide a solution that is flexible and configurable with respect to a range of requirements. The exact configuration of the faceted search interface will be driven by requirements of the access components (like DIP creation)   The specific access component requirements are being currently defined in E-ARK and are already partially described in (Thirifays et al., 2015), the report D5.2 “E-ARK DIP draft specification”  as well as individual institutional requirements and content specific aspects. As a consequence, the reference implementation developed within E-ARK must provide a configurable query interface that should be accessible via a service API. This API can be used through a web interface and/or an access component for searching the repository based on a full text index.  The reference implementation integrates this query API with a repository implementation, which in turn provides access to an application layer via its access API. The application layer implemented in E-ARK develops end user components for search, access, and display of archived records. Figure 1 provides a conceptual overview of the architecture and workflow supported by the reference implementation.     Figure 1:   Icons made by  Freepik   IPs are received and processed by an ingest system like the ESSArch Preservation Platform   http://www.essarch.org/  . As part of the ingest workflow, the information packages are written to an archival storage medium. In addition   The full-text search and access component developed in E-ARK does not replace an existing archival system (like catalogues) but can be utilized to augment these systems.  , these packages are also ingested into a content repository that provides basic data management capabilities and search. The created repository records are indexed and can be queried by an end-user application through a service interface. At the repository level, random access is provided on an item/file based level.   The goal of the E-ARK Faceted Query Interface and API is the establishment of a reference service that enables application components to search through the entire archived data and to link the applications with the data management layer   Here, data management refers to functionality provided by the content repository introduced by the E-ARK infrastructure, which is intended to augment the existing archival ecosystem.   (provided through the content repository). The reference implementation will concentrate on data management functionality that supports search, access, and data mining (like providing a CRUD API and support for versioning). The implementation of a fully-fledged archival data management system, however, is out of focus for the reference implementation.  The search functionality is provided by an indexing infrastructure which generates a full-text index for data being ingested into the data management component (ie. the content repository). The goal is to enable end users to efficiently search archival records based on different aspects (or facets) extracted from the archived data and metadata. The search index includes enclosed archival descriptions (metadata) but most crucially the archived data itself (e.g. based on extracted text portions) and generated technical metadata (like file format information). The employed indexing techniques are not intended to provide a finding aid based on archival metadata, as for example provided by archival cataloguing systems. The intention of the E-ARK Faceted Query Interface and API is to provide a complementary service that takes advantage of information retrieval techniques like full text indexing, faceted search, and ranking to improve the search through archived data. The indexing workflow is however configurable and able to extract specific information from the archival metadata. This flexibility can for example be utilized to develop specific search facets and/or to handle information related to data confidentiality. The Faceted Query Interface and API are being developed as part of the E-ARK reference implementation which builds upon a scalable technology stack. The intention is to provide an archiving and search solution that works for different payloads. The reference implementation can therefore be scaled from a single host out to a cluster deployment that is capable of maintaining large volumes of data, e.g. in the magnitude of hundreds of terabytes of archived data organized in hundreds of millions of repository records. The indexing infrastructure is however intended to be deployed next to established archiving systems in order to extend the functionality of the available finding aids. The intention is not to replace the existing systems but rather to extend these infrastructures. The final paper proposes to add further details of deploying the above scenario with use cases making use of geographic data integrated with the peripleo tool from the Pelagios project   https://github.com/pelagios/peripleo . We will describe the implementation of a complete archival workflow that includes conversion procedures necessary to support text-based search as well as geographic information retrieval and spatial browsing. We will also show how Big Data techniques such as denormalisation and dimensional modelling used in creating the AIPs can facilitate the discovery methods we outline.    ",
       "article_title":"Using Big Data Techniques For Searching Digital Archives: use cases in Digital Humanities",
       "authors":[
          {
             "given":"Janet",
             "family":"Delve",
             "affiliation":[
                {
                   "original_name":"University of Portsmouth, United Kingdom",
                   "normalized_name":"University of Portsmouth",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/03ykbk197",
                      "GRID":"grid.4701.2"
                   }
                }
             ]
          },
          {
             "given":"Sven",
             "family":"Schlarb",
             "affiliation":[
                {
                   "original_name":"Austrian Institute of Technology, Austria",
                   "normalized_name":"Austrian Institute of Technology",
                   "country":"Austria",
                   "identifiers":{
                      "ror":"https://ror.org/04knbh022",
                      "GRID":"grid.4332.6"
                   }
                }
             ]
          },
          {
             "given":"Rainer",
             "family":"Schmidt",
             "affiliation":[
                {
                   "original_name":"Austrian Institute of Technology, Austria",
                   "normalized_name":"Austrian Institute of Technology",
                   "country":"Austria",
                   "identifiers":{
                      "ror":"https://ror.org/04knbh022",
                      "GRID":"grid.4332.6"
                   }
                }
             ]
          },
          {
             "given":"Richard",
             "family":"Healey",
             "affiliation":[
                {
                   "original_name":"University of Portsmouth, United Kingdom",
                   "normalized_name":"University of Portsmouth",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/03ykbk197",
                      "GRID":"grid.4701.2"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-05",
       "keywords":[
          "information retrieval",
          "GLAM: galleries, libraries, archives, museums",
          "cultural infrastructure",
          "data mining / text mining",
          "archives, repositories, sustainability and preservation",
          "English",
          "geospatial analysis, interfaces and technology"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Citing and comparing texts Explicit and tacit assumptions of traditional text criticism have been questioned for decades,  As early as 1934, Pasquali already pointed out that the necessary assumption of the “Lachmannian method” that each copy derives from a single archetype is demonstrably wrong in many instances (Pasquali, 1934). but the creation of digital scholarly editions has provoked discussion ranging from practical questions of method, to theoretical debate about what constitutes the nature of an edition in an electronic environment.  See for example the collection of essays edited by Sutherland and Deegan (Sutherland and Deegan, 2009), or the recent broad survey assembled by Pierazzo (Pierazzo, 2015). In this paper, we address questions of what it means to compare two scholarly editions, and demonstrate applications of our approach to compare manuscripts of the  Iliad.  One characteristic of a scholarly edition in any medium is that it makes a text canonically citable. Canonical citation is an essential prerequisite: in order to compare more meaningful units than streams of characters, we must be able to identify and align passages in different versions of a text. We use the technology-independent CTS URN notation  On CTS URNs, see http://cite-architecture.github.io/ctsurn/. to identify books and lines of the  Iliad. CTS URNs could be applied to any citation scheme, but logical schemes (such as those typically used to cite biblical passage by chapter and verse, or legal document by numbered section and subsection) are superior to arbitrary schemes based on physical artifacts (such as citing Plato by Stephanus page, or referring to the physical page of a specific edition of the works of Jane Austen) since they ensure that our comparison is organized in chunks of texts meaningful for scholarly analysis.    A model for text comparison Traditional Homeric scholarship offers a useful model for comparing aligned citable texts. Homerists use the terms “vertical difference” and “horizontal difference” to describe two kinds of variety: “vertical difference” refers to entire lines that are present in one text but absent in the other, or that occur in a different sequence in the two texts. “Horizontal difference” refers to lexical differences within a single line. We can generalize the two dimensions of this approach, and understand our comparison of texts as determining the structural and lexical variation between texts. Structural differences are simply differences in citation structure. For texts cited by CTS URNs, then, we can reduce the determination of structural differences to a comparison of ordered lists of each document’s CTS URNs. Lexical differences are differences in the readings within a single citation unit (line of the  Iliad, subsection of a legal text, etc.) The crucial question is: what produces a “reading”? Simply comparing streams of characters, or assuming that a stream of characters can be parsed into tokens based on some criterion such as splitting word tokens by white space or punctuation is dangerously underconceptualized. Instead, we recognize that any analysis that tokenizes a citable unit of text for a specified purpose produces an ordered list of tokens that we can compare in order to determine lexical differences between two texts. The lexical type of the token will depend on the goal of the comparison. As we subsequently illustrate, for example, we could analyze literal textual tokens, orthographically normalized tokens, or even morphologically or metrically analyzed tokens.    Implementing the model When we determine structural (“vertical”) variation by comparing ordered lists of URNs and lexical (“horizontal”) variation by comparing ordered lists of tokens for each citable unit of text, we are performing exactly the same operation: comparison of ordered lists. This is one of the most studied and best understood problems in computer science, and a typical exercise in first-year programming courses. We have implemented the standard algorithm for Longest Common Subsequence (or LCS)  https://en.wikipedia.org/wiki/Longest_common_subsequence_problem in a library freely available in source code or binary .jar for JVM languages. In addition to solving the LCS, the library determines what items appear in one list but not the other, and what items appear in both lists but in a different order.    Applying the model to the  Iliad  We illustrate the possibilities of this approach by repeatedly comparing incompletely published manuscripts of the Iliad, focusing especially on  Iliad 8 in two manuscripts, in the Biblioteca Marciana in Venice and in the Escorial monastery near Madrid. All of our comparisons find the same structural differences. (The run of lines from  Iliad 8.466-8.468 is present in some manuscripts, for example, but absent from others.) The lexical comparisons, on the other hand, vary depending on the features we analyze.  We begin with a simple tokenization of the literal diplomatic text split on white space. Inventorying the tokens in each manuscript is essentially the collation phase of a traditional edition, but when we fully account for differences in punctuation, accent, abbreviation and spelling, the vast quantities of differences between manuscripts informs us about aspects of Byzantine orthographic practice that are normally suppressed in critical editions. We next tokenize the same text to a normalized orthography eliminating punctuation, and adapting both accents and spelling to modern conventions. This comparison most closely approaches what we find in a typical critical edition (except that its listings of tokens present, absent or relocated in different manuscripts is comprehensive, rather than selective). Viewed from this perspective with orthographic differences removed, we find much greater agreement in the text of the Venice and Escorial manuscripts, although we still find passages like 8.137 where the reins of Nestor’s chariot are either “shining” (σιγαλόεντα) or “red-purple” (φοινικόεντα). This comparison also reports differences in passages like  Iliad 9.3, where a few manuscripts have βεβλήατο against the majority with βεβολήατο. The “differences” are actually equivalent forms of the same verb (an epic pluperfect of βάλλω). Depending on our interests, we might prefer to view these two literal variants as identical. We next tokenize the text not to representations of the specific form found in the text, but instead to the lexical entity (“dictionary form”) from which the word derives. In this tokenization, the same lexical entity is given for each of the two variant forms, and the lines are, by this reading, equivalent.  Since the formulaic variation illustrated by different readings for the same passage is metrically conditioned, we might also want to tokenize the text to metrical units. Like the preceding tokenizing to an abstract lexical entity, this is often considered beyond the scope of a critical edition, but we do not need to make any procedural distinction in our digital comparison. Reading the same line 9.3 metrically, for example, we next tokenize the dactylic hexameter into six metrical feet. In the majority manuscripts with βεβολήατο, we “read” the text with a dactyl in the third foot, —⏖ —— —⏖ —⏖ —⏖ —× while in the minority manuscripts with βεβλήατο we read a spondee —⏖ —— —— —⏖ —⏖ —× Each of these comparisons captures a distinct feature of the text. In every case, the analyses are keyed to the CTS URN of the text they analyze, so we can readily combine and compare the results of distinct analyses. In the Iliadic examples, we could equally easily identify passages that are metrically identical with either different vocabulary items or different forms of the same vocabulary item; or, as in 9.3, metrically distinct passages with identical vocabulary in different forms.   Rethinking digital editions We recognize, as others have before, that many assumptions in the traditional practice of critical editing are self-contradictory and unnecessary in a digital environment. Digital editors are not constrained to eliminate the evidence of manuscripts judged not valuable (eliminatio codicum descriptorum); they do not have to select only significant variants (selectio) based on the evaluation of a limited set of crucial passages (examinatio locorum criticorum); they do not have to present material supporting their editorial choices in a critical apparatus that is flawed both by its circular logic of selectively publishing evidence and by its notational deficiency (a deficiency that has been clearly recognized only when the apparatus is computationally processed).  An important but largely unrecognized implication of Federico Boschetti’s work parsing a critical apparatus of Aeschylus is that more than 10% of the entries in the critical apparatuis were not clearly enough expressed to be correctly mapped on to the section of the main text they annotate. This was not due to lack of diligence by the editors: rather, it reflects the notational ambiguity of the traditional apparatus. (Boschetti, 2007) One of the most significant consequences of working with scholarly editions in a digital environment is the potential to automate systematic and comprehensive comparisons of various classes of features across a set of full diplomatic editions.  The comparisons of Iliadic manuscripts presented here further show that the analysis underlying a traditional critical edition is functionally no different than any other kind of analytical comparison: critical editing is one approach to analyzing a comparable set of texts. Our model of textual comparison compels us to specify unambiguously the process that generates our sequence of lexical tokens. This permits us to apply completely generic tools for comparing ordered lists, and to construct increasingly complex cascades of aligned analyses.  ",
       "article_title":" Comparing Digital Scholarly Editions  ",
       "authors":[
          {
             "given":"David Neel",
             "family":"Smith",
             "affiliation":[
                {
                   "original_name":"College of the Holy Cross, United States of America",
                   "normalized_name":"College of the Holy Cross",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05dwp6855",
                      "GRID":"grid.254514.3"
                   }
                }
             ]
          },
          {
             "given":"Sephanie",
             "family":"Lindeborg",
             "affiliation":[
                {
                   "original_name":"College of the Holy Cross, United States of America",
                   "normalized_name":"College of the Holy Cross",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05dwp6855",
                      "GRID":"grid.254514.3"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "medieval studies",
          "classical studies",
          "cultural infrastructure",
          "scholarly editing",
          "data mining / text mining",
          "content analysis",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Reading in translation is an impoverishment, not so much because of the fluctuating quality of a translation or the loss of a perceived 'original work' but because of the elision of sociolinguistic context and the difficulty in conveying that lost context to readers. That world literature is taught almost always in translation at universities in America and elsewhere (a situation driven by the low capacity for foreign language instruction at the university level and the broad linguistic reach of world literature courses) compounds this problem. Of the estimated 20.2m American undergraduates in 2015, an estimated 1.6m are enrolled in foreign language courses, and, historically, only 17% of those reach the higher levels of proficiency necessary to read literature (Goldberg et al., 2013).  The problem of American monolingualism has even affected the publishing industry; as noted translation theorist Laurence Venuti argued (1998), there are more translations of English texts into other languages than ever before while fewer foreign texts are translated into English. This trend means that American students are even less likely to feel comfortable with literature in translation than students from non-English speaking countries. However, conversations about literary summarization, a core practice of critical reading, take place globally and are preserved in the background of pages on every national language Wikipedia. As each language community summarizes their translations of works of world literature in the form of Wikipedia articles, they generate for every crowdsourced entry a discussion page and a history of that page; together, they reveal that literary work's history of reception for a reading community in a given language. By striving to synthesize an authoritative, peer-reviewed summary of text native to or translated into that community's language, each group highlights their concerns, thought processes, and the challenges posed by a given work. The goal is to make these differences and conversations more visible to readers through techniques from natural language processing for automatic translation, topic modeling, and the visualization of topic models; in so doing, we aim to develop a method by which the digital humanities can address a core problem in comparative and world literature. Machine translated Wikipedia discussions can help reveal to monolingual audiences the degree to which cultural pragmatics influence the reception of key (and popular) works of world literature.  Although flawed, automatic translation has been found in some languages to be comparable with human translation, at least in regards to cohesion and formality (Li et al., 2014). Presenting parallel national-language conversations – such as the national language conversations about topics like translating the title for Camus' L'Étranger / The Stranger / Der Fremde/ The Foreigner and a visualization of the change-over-time of Goethe's Faust I summarizations in English, German, and Spanish – would help demonstrate the practical reception of a work in a language community.  While work has been done on multi-lingual topic models (Ni et al., 2009), our research assumes that there will be both alignment and misalignment of topics across the various languages of a work; as such, our project resists the urge to normalize those topics into one category on the basis of an imperfect vector model of semantic similarity. Furthermore, experiments on iterative summarizations, such as elements of The Tale of Genji, demonstrate how even basic tasks in literary scholarship contain cultural dimensions and can thus reveal strong cultural patterns and biases held by different populations (Kashima, 2000). Along with the works mentioned above, this research explores the Wikipedia conversations around J.D. Salinger's Catcher in the Rye / Der Fänger im Roggen / El guardián entre el centeno o El cazador oculto / L'Attrape-cœurs / Il giovane Holden and Homer/Omero's  The Odyssey / Die Odyssee / Odisea / L'Odyssée / Odissea across English, German, Spanish, French, and Italian Wikipedia pages. In an increasingly digitized cultural landscape, the pedagogical use of Wikipedia and similar platforms has gained a great deal of traction in certain fields. While it is often derided as a dubious source for information, Wikipedia has proven to be a successful arena for instructing students on the distillation of information, writing in the public sphere, and collaborative writing (Purdy, 2009; Vetter, 2014; Sweeney, 2012). As a multilingual space, Wikipedia has offered many scholars the opportunity to examine the ways in which the cross-cultural sharing of information takes place (Nothman et al, 2013; Filatova, 2012). In some instances, Wikipedia specifically has been used as a place for the comparison of knowledge and representation across cultural and geographic divides (Callahan and Herring, 2011). These scholars provide a framework for examining the cross-linguistic aspects of Wikipedia in order to highlight cultural differences and deconstruct colonial power structures that privilege the English language (Ensslin, 2011). In the available scholarship, the use of Wikipedia in teaching literature has been largely ignored; only a few studies exist and those mainly address literary studies' reticence to incorporate Wikis into pedagogy (Bayliss, 2013). However, quite a few studies suggest that Wikipedia can occupy a distinct operational space in the university that supplements established pedagogical spaces and practices (Gorard and Selwyn, 2015; Knight and Pyke, 2012). Our computational framework follows these possibilities to supplement two established methods of teaching translation in a world literature classroom. The first method consists of placing a work of literature in translation – a passage from a novel, for example – in relation to the same work in its original language. The hope is that students can analyze the differences between the original and the work in translation and, thereby, understand the way the literary text undergoes a 'new life' in translation. This method not only assumes a sophisticated knowledge of foreign languages among the majority of students but also brackets the question of how these works are read in the original language by native-language readers. The second method juxtaposes several translations of the same work into English. This has the advantage of not assuming knowledge of a foreign language. For example, students might read various translations of the 19th-century French poet Baudelaire in English, starting from English translations from the late 19th century and continuing through translations that have been published in the last twenty years. The advantage of this method is clear: students can easily see, in their own language, the way translation can change the meaning of a poem as they read it from one translation to another. While this method successfully bypasses the problem of foreign language competency, it amplifies the second problem: the student is even more divorced from the source-language context since all emphasis is placed on the way English speakers respond to a work of literature. New methods, therefore, are needed to expose foreign texts and foreign contexts. Using computational methods to enter the themes and arguments about specific texts in other languages precludes some of the need for high-level competency in a foreign language in order to understand the debates about world literature in a non-American – and especially non-English – context. Automatic translation and topic modeling facilitate encounters with Wikipedia 'talk' pages in other languages. The rough output of these automated and annotated procedures preserves some of the estrangement of working across languages as the translation is rough, clearly communicating its nature as translation — and therefore serves its purpose without fully replacing the source text.  With such experiments, our project broadly addresses two questions: how different language populations create summaries that are culturally distinct, and how these differences can be folded back into meaningful encounters for readers of works in translation. The overall method for this project is to identify the subset of well-documented and significant works, mine the relevant Wikipedia entries and conversations, and develop preliminary code to identify the linguistic features of the entry (e.g. topics, use of modals, noun density, phraseological structures, complexity measures, etc.) and the nodal points of the crowd's conversation that yielded the page. As an example, consider the discrepancies across the Italian and English discussion pages of The Odyssey / Odissea as represented by ten 5- to 10-term topic models.  Table 1. Named Topics Models from English and Italian Discussion Pages of Omero/Homer's  The Odyssey /  Odissea    Terms, Italian Discussion Page   Topic Name    odissea originali palla contributi registrarmi Contributions and Registration   wikipedia forum migliorarla posto figlio arrivassero tenter tentare importanza Need for More Expert Contributors   ulisse niente aggiunto dante manchi riferimento elenco procedo cielo parte pietose condizioni References to Dante   poseidone divina generale voglia  Unclear   commedia incontra quesiti accecato ostacola competenti voce Other Web Sources   ripristino aggiungerei siti porre partenza traducendo manna materia versione Older Versions are Better than Newer Versions   dir notizie rete evidente polifemo Unclear   inglese qualcuno pensa piccola inferno appositi pagina discutere film serve deciso Using English Wikipedia to Backfill Italian   esattamente pecca opere fin tema passo potere utenti Reflections on Contributors   pare so troia attendo basandomi mesi trova provvisorio Reflections on What to Write        Terms, English Discussion Page   Topic Name    odyssey homer work titles guideline searching works section promotional directed Editing Guidelines and Work Title   february page common dab crazynas people epic redirect iliad dictionary Genre and Other Works   talk odysseus december journey extant account term proposal giu september The Journey   word article wp called style subject similar adventures locations toronto Locations in the Story and Writing Style   utc play topic primary part poem link note odisseus july Poetry   title article refer word davidiad edited cynwolfe don titles voyage Voyages   edit western preceding map request www apology talk zcc suggest Mapping the Journey   medea akhilleus euripides noun don current literature click crazynas english Characters   utc april added source staged meant argument written fall cite Sourcing and Staging   talk comment unsigned university musical oldest review tedickey tomb point Ceremony   What comes across in this comparison is the somewhat different concerns of the two reading communities. The Italian discussion reflects concerns with the expertise of the editing community, a social reflection common to Wikipedias, and with connections between Homer and Dante, a figure more central to Italian literary identity. The English page reflects a concern with the Odyssean journey and its possible real-world correspondences. More commonality was found in another example comparing the discussion pages of J.D Salinger's The Catcher in the Rye. These alignments and misalignments of reader's concerns can destabilize the primacy of concerns held by a given reading community and speaks to one of the core benefits of reading world literature. ",
       "article_title":"Contextualizing Receptions of World Literature by Mining Multilingual Wikipedias",
       "authors":[
          {
             "given":"Ben",
             "family":"Miller",
             "affiliation":[
                {
                   "original_name":"Georgia State University, United States of America",
                   "normalized_name":"Georgia State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03qt6ba18",
                      "GRID":"grid.256304.6"
                   }
                }
             ]
          },
          {
             "given":"Cindy",
             "family":"Berger",
             "affiliation":[
                {
                   "original_name":"Georgia State University, United States of America",
                   "normalized_name":"Georgia State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03qt6ba18",
                      "GRID":"grid.256304.6"
                   }
                }
             ]
          },
          {
             "given":"Sayan",
             "family":"Bhattcharyya",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana Champaign, United States of America",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Tommaso",
             "family":"Caselli",
             "affiliation":[
                {
                   "original_name":"VU University, Amsterdam, the Netherlands",
                   "normalized_name":null,
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"David",
             "family":"Kelman",
             "affiliation":[
                {
                   "original_name":"California State University, Fullerton, United States of America",
                   "normalized_name":"California State University, Chico",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/027bzz146",
                      "GRID":"grid.253555.1"
                   }
                }
             ]
          },
          {
             "given":"Jennifer",
             "family":"Olive",
             "affiliation":[
                {
                   "original_name":"Georgia State University, United States of America",
                   "normalized_name":"Georgia State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03qt6ba18",
                      "GRID":"grid.256304.6"
                   }
                }
             ]
          },
          {
             "given":"Jay",
             "family":"Rajiva",
             "affiliation":[
                {
                   "original_name":"Georgia State University, United States of America",
                   "normalized_name":"Georgia State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03qt6ba18",
                      "GRID":"grid.256304.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-02",
       "keywords":[
          "digital humanities - multilinguality",
          "digital humanities - pedagogy and curriculum",
          "teaching and pedagogy",
          "literary studies",
          "natural language processing",
          "cultural studies",
          "data mining / text mining",
          "English",
          "multilingual / multicultural approaches"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Through the spatial arrangements of temples, houses, roads, and more, the built environment provides a window to human interaction (Lawrence and Low, 1990). Spatial configurations influence how people negotiate their surroundings. People “read environmental cues, make judgments…and then act accordingly” (Rapoport, 1990: 139), and these decisions in turn, affect the frequency and intensity of interaction (Fletcher 1981). While many factors influence interaction within landscapes, in this paper I focus on visibility.  The visibility, intervisibility, and invisibility of features communicate information that guides pedestrian movement, and consequently, structures social interaction and community organization (Llobera, 2003, 2006; Gillings, 2015). Building on these ideas, this paper uses Geographic Information Systems (GIS) and 3D visualization to explore the role of visibility in ancient landscapes asking:  How might visibility influence where people went, what they did, who interacted with whom, and how did these interactions shape their daily experiences?     Figure 1: Map of Copan’s location at southeastern periphery of Maya region (Map: H. Richards-Rissetto)    Case Study: Copan, Honduras The case study is the ancient Maya polity of Copan (Figure 1) ruled for over four-hundred years by a line of dynastic kings who by the late eighth century were facing mounting sociopolitical and environmental problems (Fash, 2001). Copan’s final dynastic ruler,  Yax Pasaj, like the rulers of many other Maya polities, was coping with strenuous environmental, demographic, and sociopolitical circumstances that would ultimately lead to the kingdom’s demise. Yet during this time of stress, it seems that  Yax Pasaj carried out a major urban renewal project commissioning several new temples in the city center that elevated Copan’s skyline. Given the changes to Copan’s urban fabric,  Yax Pasaj’s reign is an ideal case study to investigate the role visibility may have played in the production or reproduction of social interaction among the ancient Maya.   At Copan, as at other Maya centers, imagery on ceramics, murals, and freestanding monuments depicted deities floating over lords who subsequently looked down over lower-ranking persons. Maya architecture replicated this vertical succession by elevating royal compounds above other architecture, and in essence linking Maya rulers to the heavens (Messenger, 1987). In terms of visibility, epigraphic decipherments indicate that “seeing” afforded high status, and sight had an authorizing gaze and witnessing function—similar to Foucault’s (1995) Panoptic gaze—where those who were all-seeing were all-knowing (Houston et al. 2006: 173). In order to be all-seeing or to give such an impression, however, Maya rulers needed to be seen, and so often located themselves in physically high and easily visible places or built tall temples that dominated the landscape. While we know that Maya kings typically constructed highly visible temples, we actually know very little about the role visibility may have played in structuring social connections and daily interactions among social groups. To do this we need to broaden our view from civic-ceremonial precincts to encapsulate the broader landscape (Doyle et al., 2012; Richards-Rissetto 2010; Landau, 2015).    Background: GIS & 3D Visualization Early visibility studies in the Maya region focused on astronomical alignments among structures, freestanding monuments, and the sky (Aveni and Hartung, 1986). Later, ethnographic studies showing that contemporary Maya often use sight lines to mark out spaces (Hanks, 1990) inspired researchers to investigate whether non-astronomical lines-of-sight also existed at ancient sites; and in fact, archaeologists identified sight lines between a major temple and outlying stelae at the site of La Milpa, Belize (Hammond and Tourtellot, 1999). Recent research has moved away from lines-of-sight between two objects to study the relationships that an object may have to the many objects or features found within a landscape, referred to as a visualscape (Llobera, 2003). Simple line-of-sight measurements cannot provide data on the relationships among multiple objects because they are done along a fixed line; however, visualscapes can be measured using viewsheds that calculate an object‘s entire 360° field-of-view using GIS.  A GIS links mapped features to attributes stored in a database and overlays different data layers such as land usage, elevation, and buildings to help reveal complex patterns, relationships, and trends that are not readily apparent using other tools such as traditional databases not linked to maps.   Pros: In regard to visibility analysis, GIS allows archaeologists to move beyond line-of-sight analysis to viewshed analysis. A viewshed uses raster data (pixels) to identify all cells visible from one or more viewpoints in a landscape; all non-visible cells are assigned a 0 and all visible cells are assigned a 1. This basic binary schema allows for complex mathematical calculations, for example, Boolean operations or map algebra, to calculate topographic prominence of individual features (or classes of features) and percentage of intervisibility among features.    Cons: “Viewsheds depicted in a GIS map bear little resemblance to what people experience on the ground” (Conolly and Lake, 2006: 233). This limitation occurs because viewshed data are 2.5D. In other words, viewsheds store heights and elevation, but they are not actually 3D models (Figure 2). For digital humanists, these flat maps lack a sense of mass, scale, and aesthetics integral to human perception and experience, and the numerical outputs fail to differentiate visibility of a building’s façade versus its sides or back—essential for close reading interpretation. Technically, data resolution (i.e., ratio of pixel size to earth’s surface) can dramatically affect viewshed results—low spatial resolution often masking variation and too high a spatial resolution underestimating visibility (King et al., 2015).     Figure 2: Cumulative viewshed illustrating number of valley stelae visible at locations at Copan  3D technologies offer an alternative to GIS. 3D data acquisition (e.g., airborne LiDAR, terrestrial laser scanning, and photogrammetry), 3D modeling (e.g., SketchUp, 3D StudioMax, Agisoft), and interactive 3D visualization (e.g., Unity, Oculus Rift) are transforming archaeological practice. But, what impact are such 3D technologies having on visibility analysis across ancient landscapes? Airborne LiDAR, for example, rapidly collects 3D data for archaeological sites across vast areas (Thompson and Prufer, 2015). Most LiDAR data are of unexcavated mounds requiring subsequent 3D modeling of architecture and proper alignment within terrains in order to perform visibility analysis—traditionally time-consuming tasks (Richards-Rissetto, 2013).  While most visibility analyses of archaeological landscapes use traditional 2.5D GIS, recently archaeologists have been exploring the potential of 3D approaches for visibility analysis in archaeology. Paliou (2014) developed a computational visibility approach to analyze the visual range of paintings first using 3D modeling programs (3DStudioMax and AutoCAD) and then converting the results into raster maps to be analyzed in a GIS. Dell’ Unto and colleagues (2015) bring georeferenced 3D architectural models (using laser scanning and photogrammetry) into a GIS to calculate visibility of building interiors at Pompeii. While Saldana and Johanson (2015) also use 3DGIS, they employ procedural modeling to rapidly generate alternative 3D building reconstructions based on a set of architectural rules and attributes stored in a GIS to explore visibility in Ancient Rome (Saldana, 2015).   Methods Building on this scholarship, I employ an iterative 3DGIS approach to explore the role of visibility at the ancient Maya site of Copan—today a UNESCO World Heritage Site in Honduras. The approach is twofold: computational and experiential. In the computational approach, I employ traditional 2.5D viewshed analysis in GIS to establish a baseline for comparative analysis with viewshed results in 3DGIS.  First, I use ArcGIS 10.3 (standard GIS software) to assign known building heights and interpolate building heights of unexcavated mounds and run viewsheds to calculate topographic prominence and percent visibility in relation to settlement of major temples and classes of architecture (Richards-Rissetto, 2013). Recent acquisition of airborne LiDAR data has generated a 1m resolution terrain allowing for greater accuracy than earlier visibility analyses (Richards-Rissetto, 2010; von Schwerin et al., 2016). Second, I employ CityEngine—a procedural modeling program that convert GIS data to 3D models—to generate 3D models for Copan’s 3,000+ buildings with the LiDAR terrain using the GIS data and a set of architectural rules as well as laser scanned and photogrammetric models of some standing monuments at Copan (Figure 3) (Muller et al., 2006; Richards-Rissetto and Plessing, 2015; von Schwerin et al., 2013). These procedurally-generated 3D models are then returned to ArcScene (a 3D viewer for ArcGIS) and the viewshed analysis is rerun for comparative analysis of 2.5DGIS vs. 3DGIS of visibility at ancient Copan.   Figure 3: Illustrating procedurally-generated models and various data types imported into CityEngine  In the experiential approach, I export the 3D models and terrain from CityEngine into Unity 5—a gaming engine—to interactively explore the 3D models. In this model, vegetation is added to the landscape and avatars proceed along set paths generated from a combined cost surface and visibility analysis (Figure 4) (Richards-Rissetto, 2013; Richards-Rissetto and Landau, 2014). Oculus Rift—a head-mounted virtual reality display—is employed to create an immersive experience for ancient Copan as a means to more intuitively interact with archaeological data (Bartolo et al., 2000; Frisher and Dakouri-Hild, 2008).    Figure 4: 3D Models (from SketchUp using GIS data) visualization in Unity 5 (Richards-Rissetto and Day)    Discussion Strongly embedded in the Digital Humanities, this 3DGIS iterative approach tacks back and forth between 2.5D and 3D data to compare results and potentially derive new methods and interpretations for visibility analysis of ancient landscapes—analyses that would not be possible without taking advantage of the digital to cross-cut the computational and experiential.    Acknowledgements The Layman Award, University of Nebraska-Lincoln provided a seed grant to carry out initial procedural modeling tests. This research would not be possible without permission and assistance from the Honduran Institute of Anthropology and History (IHAH). The MayaArch3D Project has generously providing the airborne LiDAR data and the laser scanned and photogrammetric models for this research. I want to thank UNL students Zachary Day, Stephanie Sterling, and Rachel Plessing for their important work on the visualizations.   ",
       "article_title":"An Iterative 3DGIS Analysis of the Role of Visibility in Ancient Landscapes",
       "authors":[
          {
             "given":"Heather",
             "family":"Richards-Rissetto",
             "affiliation":[
                {
                   "original_name":"University of Nebraska-Lincoln, United States of America",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-29",
       "keywords":[
          "visualisation",
          "games and meaningful play",
          "digital humanities - pedagogy and curriculum",
          "maps and mapping",
          "databases & dbms",
          "archaeology",
          "virtual and augmented reality",
          "standards and interoperability",
          "archives, repositories, sustainability and preservation",
          "English",
          "geospatial analysis, interfaces and technology"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction The basis of historical study or historical understanding consists of collecting historical materials (mainly literature materials such as old documents, old diaries, …), precise reading of the materials, and source criticism. In order to perform them, an identification of a personal name is one of important methods or works, and the researchers of history cannot avoid it. The personal name identification is not simple issue, because there is a diversity in the name representations in the materials. The main representing patterns of the diversity are the follows: a) Written real or original name b) Written first name only c) Written nickname, epithet, or alias  d) Written omitted name e) Written role name  f) Written using different characters g) Described Kao (which is a stylized signature or a mark.)  Examples of the representations of “伊集院忠棟 (Ijuin Tadamune)”, who is a senior statesman of “島津家 (Shimazu family)” and the relatives of the family and is in the 15th century in Japan, there are “伊集院” (which is his family name), “忠棟” (which is his first name), “幸侃” (which is a nick name, is often appeared in the old documents), “伊右衛門大夫” (which is a nick name), “伊右”, “右衛門”, “伊大夫” (which are his nick names called by familiar persons), “忠金” (which is his original name). Here is a difficult problem that the various represented names should be identified. In the above example, if you understand the various alternate names can be identified with “伊集院忠棟”, the problem is not hard. However, in practice, there are no persons (including researchers of the history) who know and understand all historical persons. For the solution of the problem, we consider that the results of personal name identifications which can be performed by researchers of history should be managed. Furthermore, in the search against historical materials, if the results can be available, the performance of the search can be surely improved compared to simply full-text search.  In the paper we introduce a management method of personal names and the alternate names of the persons and a search method using the managed names. In (Ho, 2015) and (Bol et al., 2015), personal names can be extracted and tagged automated against target documents based on China Biographical Database (CBCB; http://isites.harvard.edu/icb/icb.do?keyword=k16229) as a biographical dictionary. Unfortunately, there are no exhaustive the encyclopedias or dictionaries for the names of Japanese historical persons. Moreover, methods introduced in (Ho, 2015) and (Bol et al., 2015) can be performed better if you can treat a document which is a secondary source like a \"地方志 (difangzhi)\" in which almost personal names indicates its real name. Most of documents which we treated in the work is primary source and hardly have real names of the person.    2. Extraction of personal names and alternate names At first, in order to collecting personal names, we used “上井覚兼日記 (Uwaikakken nikki)” which is a diary of Japanese medieval period (from 1574 until 1586) written by “上井覚兼 (Uwai kakken)” who is a senior statesman of “島津家” of Japan. For the historical study in Kyushu (which is a local area of Japan) or “島津家” in medieval period, the diary is one of important historical materials and Japanese national treasure. The text of the diary has been stored in “The Full-text Database of the Old Japanese Diaries” which has been published by Historiographical Institute, The University of Tokyo. In the text the number of characters is about 1.4 million (for 1777 days; note that there are days which he was not written in the diary). The format of the text is very simply, because the text is just plain text and does not have tags such as XML, TEI. The sample is as follows: …一、此朝、入来院（重豊）殿太刀を、東郷（重尚）殿次に拳可有之由被申候、御老中よりハ、東郷・祁答院・入来之事ハ同家にて候間、東郷之次に者根占殿（禰寝重長）太刀を可被召成之由、… This is a part of the text in the diary of “天正2年8月１日” (which indicates A.D. 1574-08-17)”. There are three persons (“入来院 (Irikiin)”, “東郷(Togo)”, “根占(Nejime)”) in the part. If an alternate name against written name could be solved, the alternate name was added using parenthesis.  The representation can be understood by human (who can read and understand the sentence), but machine can not be solved if the machine doesn’t know or understand the pattern. Due to machine usable, we extracted a written name, an alternate name of the name and the date and we managed the result as a set. As shown in the example, a real name or well-known name is hardly written in the Japanese historical materials, and alternate name added by researcher what indicates real name in most cases. The added name can be used for personal name identification, because this is controlled by the researchers who added the alternate names, and the notation is consistent if the same person. We performed the identification method and could obtain a name pair of 520 sets. In the process of the method, a method of a personal name extraction was needed. We could extract personal names using Machine Leaning method (which consists of an appearance patterns of the names and SVM (Support Vector Machine)). Figure 1 shows examples of the appearance pattern, which an expression of a sequence, an extraction pattern and an extracted result. We used SVM to judge whether the extracted results indicate personal name or not. The SVM results were fed back to the appearance patterns, and we performed the extraction based on appearance patterns and judgment with SVN again. The feedback was preformed several time in the work.    Figure 1 Name extraction patterns  We prototyped text search system which supports alternate names using the above constructed personal pairs rather than simple text matching. In the search, for example you queries “忠棟”, then you can obtain the results including “忠棟” as a string, “伊集院忠棟” which is controlled name (well-known name or real name), and alternate names such as “幸侃”, “伊右衛門大夫”, “伊右”, “右衛門”, “伊大夫”, “忠金” (which are mentioned above).    3. Conclusion Personal name extraction method which we introduced above is useful only target document is “上井覚兼日記 (Uwaikakken nikki)”. In order to extract more generalization, preparing a pattern suitable to each historical material is necessary. We constructed a database which can be stored the personal name pairs. Currently we also have been collecting personal names from other texts and databases, and storing it in the database. The data in the database indicates the results as an identification of personal name. The data can assist to identify personal names in a material which reading comprehension has been not yet done. We expect that the method can be useful to progress of the study of Japanese history.  ",
       "article_title":"A Management of Personal Name with Alternate Name and its Searching for Japanese Historical Study",
       "authors":[
          {
             "given":"Taizo",
             "family":"Yamada",
             "affiliation":[
                {
                   "original_name":"The University of Tokyo, Japan",
                   "normalized_name":"University of Tokyo",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/057zh3y96",
                      "GRID":"grid.26999.3d"
                   }
                }
             ]
          },
          {
             "given":"Satoshi",
             "family":"Inoue",
             "affiliation":[
                {
                   "original_name":"The University of Tokyo, Japan",
                   "normalized_name":"University of Tokyo",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/057zh3y96",
                      "GRID":"grid.26999.3d"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016",
       "keywords":[
          "medieval studies",
          "semantic analysis",
          "data mining / text mining",
          "historical studies",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" 1. Introduction Computational stylistics now has thirty years or so of publications and has been celebrated as one of the success stories of Digital Humanities (McCarty, 2014: 289). It brings together statistical methods and literary analysis, inferring meaning from the frequency of literary features. In this paper I explore this connection between frequency and meaning, and consider some of the objections which have been made to the statistical approach to style.  Much research in linguistics focuses on individual sentence-level structures. Stylistics introduces a new dimension of extension and cumulation, placing a net of continuing co-occurrence over a language sequence. The added dimension of time, or extent, opens up the analysis of meaning not only in the instance itself but in the series that it forms with other instances of the same feature. Computational stylistics takes a routinely quantitative approach to this cumulative aspect. Critics have laid down significant challenges to this frequentist approach. They have questioned whether language features are really countable, whether frequency matters in meaning, whether the inevitable choice of features to count undermines the objectivity of the results, and whether quantitative results can ever usefully relate the text to any wider context. 2. Are language features countable? The first key enabling assumption of computational stylistics is that the language features being counted are homogenous. In his 1970s articles attacking stylistics, Stanley Fish argues that meaning is constructed by the reader at the moment of reading and concludes that stylistics is therefore an invalid practice. There is no meaning in the word on the page, so it is pointless to count instances of the word, of combinations of words, or of any other language feature (Fish, 1973; Fish and Graham, 1979). John Frow, likewise, argues that in literary study features are not stable or commensurate but relational, so counting them is pointless (cited in Bennett, 2009: 287). 3. Is frequency meaningful? Many would also question the relation between frequency and salience. It seems unwise to assume that an unusual accumulation of a feature is necessarily noticed by writer or reader. Fish in a recent online post critiquing digital humanities argues that only patterns intended by the author are worth discussing (Fish, 2012). Different frameworks influence the noticeability of language elements, and a single instance may be highly salient, and a cluster of instances may pass without any conscious reaction.  4. Function words Considering function words as a basis for counting helps counter these objections. Computational stylistics has a natural alliance with function words. Function words lend themselves to computation since they are easy for a machine to recognise and appear regularly and in large numbers, offering opportunities for analysis by statistical methods whose power is well established in other domains. On the other hand, computation has a special benefit for function words analysis because counting on a scale not possible for the unaided reader makes it possible to reveal hitherto latent patterns in the behaviour of these words. Function words do not have a semantics in the usual sense:  if has a structural function rather than a meaning. The stylistic import of the word only becomes clear in repetition. By contrast, lexical words are rich in meaning in the individual instance and do not necessarily achieve any cumulative effect through a series. Function words bear traces of larger structures and hence, though not salient in themselves, their frequencies bear meaning as indexes to wider discourse orientations. They help show how a language feature can be sufficiently homogenous to justify counting, and how frequencies can have a literary dimension.  There are two other important objections to consider: the possible bias arising from the fact that a judgement has to be made about which features to count, and the difficulty of relating patterns found within a corpus to extra-textual factors.  5. Features have to be chosen, so results are arbitrary Tony Bennett points out that researchers have to choose the units to count in – there are no \"given units\" -- and argues that this choice has a necessary influence on results, which undermines any claims to objectivity (2009: 290, 291).  This is a fundamental critique of quantitative study, i.e. of any quantitative study. The logical extension would be that the choice of units always determines the results, so there can be no surprises and nothing new can be learned. It is easy to show that there are cases where this is not so. If we ask, do women write differently from men? - we have a way of validating the units: if the pattern of use of a given unit shows a significant difference in a balanced and commensurate sets of samples of the writing of women and the writing of men, then it does not matter how the unit was chosen. Here we have an external basis, the difference between two objectively based classes, on which to discard some units and accept others. Then there are cases of classification, e.g. by author and by date. We can seek markers of the classes, check them with known members of the classes, and then apply them to disputed cases. We have an objective way of validating the units, so we don't care much about where they came from. 6. Formalism Computational stylistics begins with textual features, focuses on finding patterns in their use, provides striking visualisations of the patterns, and then struggles to relate the patterns to extra-literary events. The textual data is well defined, easy to explore, and with the help of statistics it can be shown that there are robust structures within it. The world of possible causation beyond is hard to limit and hard to quantify. If there is (say) a consistent and marked increase in the Shannon Entropy of the language of Victorian novels from early in the period to late, how could that be described in terms of the reading experience? And how could that be related to the forces acting on the novel? Computational stylistics is lop-sided: very well developed on the textual side, but weak - tentative and fragmentary - in relating statistical findings to the extra-textual world. Another way of saying this is to call computational stylistics formalist. In this sort of approach the evidential force of the explanation for a pattern will always be less than that for the pattern itself. However, it is only fair to point out that in this it is in the same situation as other literary methods. A literary effect may be demonstrable, but its genesis in composition, and the larger forces to which it relates, are always matters of judgement and selective contextualisation. The text is available, even if dauntingly complex, but the conditions which made it possible have to be painstakingly and always speculatively recreated. It is easier to show that Hamlet changes in the course of his play than that this observed change relates to Early Modern beliefs about the typical course of melancholia.  7. Conclusion Computational stylistics has proved itself in the realm of classification. In this area the methods can be thoroughly tested and success or otherwise can be demonstrated. There are some well-established and significant findings, leading to a reassessment of some commonplaces such as the downplaying of authorship as a factor in style (Egan, 2014). This presents a problem for those who think that counting literary features is inherently unsafe, that frequencies in language cannot have any real force, and that all feature choice is fatally arbitrary. Beyond classification, though, these objections still have some force, and a new one intrudes, the argument that computational stylistics is disablingly formalist. Computational stylistics now needs to produce findings in more properly stylistic areas of the same weight as its justly celebrated classification ones, findings which match the style within a corpus to the world beyond it. Only then will we be confident that frequency in literary language is linked to meaning, and that computational stylistics has the methods to do justice to this link.  ",
       "article_title":"Frequency and Meaning",
       "authors":[
          {
             "given":"Hugh",
             "family":"Craig",
             "affiliation":[
                {
                   "original_name":"University of Newcastle, Australia",
                   "normalized_name":"University of Newcastle Australia",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/00eae9z71",
                      "GRID":"grid.266842.c"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "English",
          "literary studies",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The creation and analysis of digital collections is a key aspect of the digital humanities. Deciding what to archive, how to organize the documents, and how to present them to the public are never easy questions. But in the case of theatre and performance archives, these questions are especially complex since theatre performances are necessarily transient and they depend on the embodied co-presence of actors and spectators. In order to document the embodied and ephemeral nature of theatre performances, digital archives often try to include a wide array of documents (video recordings, photographs, motion capture data, 3d models of theatrical spaces and technical scripts). The makers of digital theatre archives are at pains to contextualize their materials and account for aspects that cannot be easily documented and transmitted. Can interaction design help communicate these aspects? Is it possible to imagine and construct interfaces that can communicate cultural context, transcending the limitations of a computer screen, mouse and keyboard? To investigate these questions within the context of an Indonesian performance archive, I developed a Tangible User Interface (TUI) using open source hardware (Arduino microcontrollers and an array of sensors) and culturally-coded objects that are important for the performances in the archive. This interface was developed as an interactive artwork for educational museums and as a thought experiment on how tangible, culturally-specific interfaces can constitute instances of academic research outputs in the digital humanities. The archive in question is the Contemporary Wayang Archive (CWA,  http://cwa-web.org), a collection of digital recordings and metadata pertaining to Javanese wayang kulit (shadow puppetry), which I have been developing with my collaborators at the National University of Singapore since 2012. Wayang kulit is the oldest theatre form of Indonesia and one of the most important theatre traditions of Southeast Asia. It consists of a form of puppetry where a dalang (narrator-puppeteer) singlehandedly moves all the puppets, speaks all the character parts, jokes with the audience and directs the musicians. He is at the same time a puppeteer, a storyteller, an orchestra conductor and a stand-up comedian. In order to communicate with the audience and the orchestra, he uses different objects in order to control the progression of the story. For this project, I wired different sensors into these objects in order to develop a TUI that recreates the spatial setup of a conventional wayang performance and three key material components of this setup:   A kerlir or screen where videos from the CWA are projected.  The kayon. This puppet is shaped like a leaf and it has different narrative functions. In between scenes, the dalang rotates it around its axis and places it on a banana log at a specific angle (either 45⁰, 90⁰ or 135⁰) to indicate the progression of the story. This visual cue is important for audience members that don’t watch the entire show. A conventional performance lasts eight hours and the audience members often come and go, drifting in and out of attention. Depending on the specific angle of the kayon, knowing audience members can estimate the specific moment in the development of the performance (which is divided into three main acts). For my interface, I used an Inertial Measurement Unit (IMU) and a wireless sensor in order to detect rotation (angular velocity) in the puppet. By rotating an actual puppet, users of this interactive artwork can navigate to a different digital video in the collection. The cempala. By hitting this wooden mallet against the puppet chest, the dalang cues the musicians to start and stop the musical accompaniment. For the interactive artwork, I wired the cempala to a Piezoelectric sensor to measure vibration. When the users of the artwork knock the cempala against a box that mimics the puppet chest, they can start and stop the videos and additional contextual information appears on the projection screen. The usage of these objects is not exactly the same in this interactive artwork display as it is in an actual wayang performance. But it can invite users to think about the importance of materiality and embodiment for this particular theatre tradition. In such a way, this interactive artwork can be considered a piece of digital scholarship, a research output of the DH research into wayang kulit that complements the forthcoming online version of the archive and other publications that might arise from this research project (see, for example www.wayangkontemporer.com). Although this artwork has a very specific origin and function, I hope it will resonate with scholars working in other areas of the digital humanities who are engaged in building and theorizing new artefacts for the communication of academic research. The objective of this short paper is to frame the creation of this artwork within the larger context of the digital humanities, exemplifying modes of scholarship that can emerge at the intersection of cultural knowledge, open hardware and digital technologies.     Figure 1. Conventional wayang kulit setup    Figure 2. IMU sensor attached to the kayon (left) and piezoelectric sensor attached to the cempala (right)    Figure 3. A user interacting with the TUI  ",
       "article_title":"Arduino Circuits and Javanese Puppets: ‘Re-materializing’ Digital Archives through Tangible Interfaces",
       "authors":[
          {
             "given":"Miguel",
             "family":"Escobar Varela",
             "affiliation":[
                {
                   "original_name":"National University of Singapore, Singapore",
                   "normalized_name":"National University of Singapore",
                   "country":"Singapore",
                   "identifiers":{
                      "ror":"https://ror.org/01tgyzw49",
                      "GRID":"grid.4280.e"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "creative and performing arts, including writing",
          "archives, repositories, sustainability and preservation",
          "English",
          "asian studies",
          "audio, video, multimedia",
          "interface and user experience design"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Ian Hodder has recently pointed to a “return to things” in the humanities and social sciences, a mode of analysis that explores the relationships between people and the objects we use to construct and make sense of the world (Hodder, 2014, p.19). In digital humanities we can see this in Matthew Kirschenbaum’s focus on the forensics of computer hard disks (Kirschenbaum, 2007), the development of platform studies to investigate the relationship between computing culture and the consoles and other hardware that enables it (Montfort and Bogost, 2009), and the appearance of maker cultures that seek to explore the humanities through practical experimentation (Dieter and Lovink, 2014). It suggests a desire to pay attention to and interact with the material world, rather than retreating to a purely digital one. Some commentators go beyond this. They propose that entanglement with material objects represents a ground of being for humans and their societies, that it presents a postphenomenological “dialectic of dependence and dependency between humans and things” worthy of deep contemplation (Hodder, 2014, p.19). People rely on the things they have created to such a degree, so the argument goes, that our identity has become inseparable from them. In Donald Ihde’s original conception, it amounts to “recognition that “consciousness” is an abstraction, that experience in its deeper and broader sense entails its embeddedness in both the physical or material world and its cultural-social dimensions” (Ihde, 2009). Knowledge, art, religion, and science are entangled, in turn, with books, oil paint, churches, and laboratories: “thing theory” grounds epistemology in the myriad interactions between the physical and non-physical world (Preda, 1999). If we apply these insights to digital infrastructure we begin to see how humanists have become entangled with complex systems, a situation that might prompt us to pause for thought. Analog books, archives, and libraries presuppose a degree of entanglement with the material world, undoubtedly, but those are relatively well understood: we have had centuries to understand and critique them. Digital infrastructure, however, is rarely “symbolically or politically construed” (Knorr-Cetina, 1992, p.119). It is merely requested in an email to a manager or Information Technology (IT) helpdesk, or held to be something humanists need to do their work. Little attempt is made to define the critical ground or, much less, to understand the object of enquiry prior to investment. Rather, in denial of the epistemological significance of things, humanities infrastructure is treated as something we should merely go to the store or work with our IT department to buy. The result has been an ongoing failure to provide the kind of infrastructure needed by humanities researchers, a misalignment of the debate, and often a rejection of the very notion of digital infrastructure itself.  As Matt Ratto points out, so-called ‘critical making’ provides new ways of dealing with difficult issues like this. Rather than attempting to solve complex problems in their entirety, critical making encourages the development of prototypes and basic models in the context of wider critical discourses, thus blending “practice-based engagement with pragmatic and theoretical issues” and fostering the possibility that creative solutions will be found to long-standing problems. He suggests the approach can be particularly useful in the context of so-called “wicked problems” (Ratto, 2011, p.253), defined by architectural scholar Horst Rittel in the 1960s and 1970s (Rittel and Webber, 1973). This class of problem is characterized by the existence of “many clients and decision makers with conflicting values, and where the ramifications in the whole system are thoroughly confusing” (Rittel was interested in problems associated with large-scale planning projects) (Churchman, 1967, p.B141). Significantly, he claimed there is a moral element to such problems, in that it is immoral to solve only one component of a wicked problem when such an approach will leave the larger issue unresolved. Prototyping and critical making can thus be positioned, not as inadequate tinkering, but as a mode of activity well-suited to the resolution of very complex problems. In this way we come to the intersection of critical making, cyber-infrastructure, and the humanities. This project migrated my personal website jamessmithies.org from Wordpress.org (a free, fully hosted service) to a home server running on a Raspberry Pi 2 Model B minimal computer, a device built in the United Kingdom at Sony’s manufacturing plant in Pencoed in South Wales and supported by a registered charity: the Raspberry Pi Foundation. The computer measures 85.60mm x 56mm x 21mm (or roughly 3.37″ x 2.21″ x 0.83″), has 1GB of Random Access Memory (RAM) and is powered by a 900MHz quad-core ARM Cortex-A7 Central Processing Unit (CPU). The VRE application is built using Django, a Python-based web framework designed for newspaper websites but now deployed in a wide variety of scenarios. The social media service Pinterest is one of the largest services to use it, with over 46 million unique visitors between 2011 and 2015 (Statista, 2015). The framework is thus highly adaptable, and could be used to develop almost any functionality a humanities researcher might need. The website is served by the Gunicorn application server and light-weight Nginx web server (used by NASA), with content saved in Postgres, one of the more advanced database systems available. All of these products are available free through the open source community. They require a reasonable level of technical proficiency to install and configure but there are many tutorials available online and their user communities share knowledge openly. It speaks to an interesting aspect of this project. Although there is a massive gap between jamessmithies.org and well-funded cyber-infrastructure projects, the nature of the open source software movement means there is only a small gap (if any) in terms of scalability and potential functionality. One of the most powerful things about the project - in both technical and tactical terms - is the level of control conferred by the architecture of the ‘stack’. Not only is the Pi itself accessible and configurable, but its operating system can be changed, and Gunicorn and Nginx can be configured at both an administrative level and through their core code base. Django can be programmed to support an extremely wide range of functionality. To extend the metaphor of control towards the incomprehensibly large infrastructures used by multi-national digital corporates (to escape the criticism that the Pi is a fundamentally limited device, or a mere toy), static files like CSS style sheets and images are hosted on the Amazon Web Services (AWS) cloud, integrating the Pi with a truly enormous global data infrastructure. These could have been hosted on the Pi, but it is considered best practice to deliver them separately for Django projects. It means, essentially, that much of the ‘heavy-lifting’ has been outsourced to a high performance computer, allowing almost limitless options for expansion of the site.  Perhaps counter-intuitively given the dominance of ‘bigger is better’ cyberinfrastructure discourse, the migration from Wordpress.org servers to a lowly Raspberry Pi has produced a personal VRE capable of significant further development. The intention is not necessarily to create a finished and reproducible product, but to take control of – and experiment with - all aspects of the computing architecture in order to gain a better understanding of my scholarly infrastructure needs, from the hardware the site runs on, to maintenance of the Internet domain name, the content management system that helps me organize content, and the firewalls that secure it from malicious actors. The conclusion after this phase of the project is that issues like ethical hardware, net neutrality, data sovereignty and security, and the ability to extend and configure the code that supports my research activities, are central to my work – and identity - as a humanities scholar. ",
       "article_title":"Full Stack DH: Building a Virtual Research Environment on a Raspberry PI",
       "authors":[
          {
             "given":"James Dakin",
             "family":"Smithies",
             "affiliation":[
                {
                   "original_name":"King's College London, United Kingdom",
                   "normalized_name":"King's College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/0220mzb33",
                      "GRID":"grid.13097.3c"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-01",
       "keywords":[
          "information architecture",
          "digital humanities - diversity",
          "software design and development",
          "digital humanities - institutional support",
          "digitisation, resource creation, and discovery",
          "cultural infrastructure",
          "English",
          "publishing and delivery systems"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" In this paper we discuss our strategy for the creation and exploration of graphs based on co-occurrences of named entities using the recently developed open source version of histograph and present a live demo. histograph builds on previous work conducted in cooperation with several European partners in the FP7 funded project CUbRIK but has been further developed by the Centre Virtuel de la Connaissance sur l’Europe (CVCE) in 2015. It combines the graph-based exploration of large cultural heritage collections with crowd-based indexation. histograph opens up a new perspective on CVCE’s collections which contain some 20.000 digitized text documents, photos, audio recordings and videos that document the history of European integration since 1945.    Figure 1: A screenshot from a CVCE ePublication on the Werner report. The navigation on the left shows the hierarchically organized themes, on the right a list of expert-curated documents  histograph adds an explorative approach to the hierarchically organized, expert-curated collections of CVCE.eu: Users decide what interests them and find their own path through the collections. A user who is interested in Pierre Werner will for example see a page similar to the screenshot below.   Figure 2: A search for „Pierre Werner“ reveals a short biographical overview with a list of frequently co-occurring entities (left column) and a gallery view of related documents which can be filtered e.g. by resource type and time  The left column provides an overview of Pierre Werner’s biography and a list of persons with whom he co-occurs in the document base. The middle column lists all documents in which he is mentioned. The graph view provides a bird’s-eye-perspective on the people who co-occur with Werner. The graph is interactive and reveals the documents which constitute a co-occurrence relationship.   Figure 3: A graph representation shows with whom Pierre Werner co-occurs in documents. A click on a link lists the documents which mention two entities, here Werner and Walter Hallstein  histograph uses a Neo4j (Neo4j Console, 2016) graph database to store relations between entities. This approach facilitates queries that would be computationally expensive in relational databases but are easily available in graph databases, such as the calculation of paths between entities that are not directly connected. Figure 4 shows the result of a query for all documents, which connect three persons.   Figure 4: A query for paths between Pierre Werner, Elena Danescu and Jean-Claude Juncker reveals all relevant documents as well as an interactive graph of co-occurrences (who co-occurs with whom)  In contrast to the hierarchical, expert-curated collections, histograph enables an interest driven exploration by the users and provides them with an effective way to retrieve and explore the relations which are of interest to them. Compared with the museum-like order of the classical CVCE collections, histograph models a more or less targeted visit to an archive, which holds the promise of serendipitous discoveries. Methods for the annotation of named entities such as persons, institutions and places have reached a very high degree of maturity and are used in different applications. For histograph we tested a variety of web services for the detection and disambiguation of entities (NER) among them TextRazor (TextRazor - The Natural Language Processing API, n.d.) and YAGO (Max-Planck-Institut für Informatik: YAGO, 2016). While these services perform well depending on the context of use (language, domain, etc.) even in a best case scenario they only allow to identify the occurrence of an entity within a logical unit such as a text but do not allow to clarify the nature of the connection between persons, places and organisations that occur together in the same unit. In our previous experiences with the detection and identification of faces in historical image sets (Wieneke et al., 2014) the format and context of the images as official photographs of specific events enabled us to understand the nature of the relationship as more easily defined: the simple working hypothesis that persons that co-occur together in a photo have some kind of connection proved to be very efficient. In the case of text however, these semantics are significantly more complex.  We therefore decided to follow two directions: first based on the nature of the text document, where letters for example constitute a relationship between the sender and the receiver and second through a mathematical modelling of the relationship based on the distance between entities in the text and their distribution within the corpus. More specifically, we work with Jaccard distances and co-occurrence frequencies weighed by tf-idf specificity. The latter step became necessary as not all of our text documents fall in the category of clearly structured formats such as letters and even if they do, a strictly format based co-occurrence approach could hide interesting relationships that would foster exploration, e.g. in our case a written exchange between two politicians where they discuss a third person. Entities who appear within a certain distance from each other can be linked based on the assumption that their co-occurrence in the text is not arbitrary and that there is a high probability that they have  something to do with each other. The boundaries for such relations can be defined freely, for example based on sentences, paragraphs, or documents. Alternatively, a window approach considers all entities related if they appear within  n characters from each other. In addition, such co-occurrences can be further defined by the recognition of the semantic relationships within a sentence. We have, however, not yet systematically tested their performance and therefore limit ourselves here to a more basic approach.  Despite all efforts to further specify the significance of such relations, co-occurrences are elusive:  It is hard to further specify what more or less connected mean It is hard to further specify which types of relations are at play It is hard to assess which relations were missed because entities were named differently  A graph representation of such data can only allow rather general statements: We can reasonably assume that entities, which co-occur often, are more connected than those who do not. We can also assume that entities, which never co-occur together, are less or not at all connected. Finally, we can assume that entities, which tend to cluster together, are more likely to have something to do with each other without further specifying their relationship. In contrast, most social network analysis (SNA) research questions require very well defined relations given that they treat social networks as models of highly complex social interactions. Here, a graph represents a meaningful reduction of such complexity and allow insights into specific dimensions of social relations. Graphs are used to represent and/or illustrate highly complex matters, which are otherwise hard to express. Such data is typically generated and curated for the purpose of specific research questions and its value is limited to their respective context in which it was created. CVCE and cultural heritage institutions in general however address heterogeneous user groups and wish to make available their data to different audiences including educators, researchers and interested laymen. A graph representation of relations between entities therefore will serve different purposes than in a SNA context. Here, graphs need not be meaningful models of social relations but need to be multi-purpose means for the discovery of materials and acknowledge wildly different interests. This means that it is impossible to predict, which relations a user will consider relevant. Against this background we embrace co-occurrences despite their inherent shortcomings. In order to improve the quality of the various relations we display we need to balance a) higher quality or more meaningful relations and b) the potential to make highly unexpected, yet meaningful discoveries in the data. In order to achieve this, we decided against a rigid ontology of relationship types, which would significantly limit the chances for unexpected discoveries, beyond format based assumptions. Instead we use text synopses and full document text for the generation of relations and filters on time periods, media types and entities in order to further specify our graphs. In addition, we use a two-fold crowd-based approach: generic crowds help clean the data and report obvious mistakes; expert crowds provide high-quality annotations which required a highly domain-specific knowledge.  Generic crowds perform tasks which do not require specialist knowledge, for example an answer to the question “Is ‘N.A.T.O.’ a person?” Expert crowds are identified based on past performance and are presented with tasks related to entities and documents they have worked on before.  The resulting graphs can be considered hybrids: They are inasmuch based on co-occurrences as they are on user specifications. Such graphs, we argue, have the potential to meet both the necessities for the automated generation of graphs and still provide meaningful structural information which can be the starting point for a deeper investigation of the materials.  We are committed to further increase the quality of co-occurrence relationships in the future.  ",
       "article_title":"Introducing HistoGraph 2: Exploration of Cultural Heritage Documents Based on Co-Occurrence Graphs",
       "authors":[
          {
             "given":"Lars",
             "family":"Wieneke",
             "affiliation":[
                {
                   "original_name":"CVCE, Luxembourg",
                   "normalized_name":null,
                   "country":"Luxembourg",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Marten",
             "family":"Düring",
             "affiliation":[
                {
                   "original_name":"CVCE, Luxembourg",
                   "normalized_name":null,
                   "country":"Luxembourg",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Daniele",
             "family":"Guido",
             "affiliation":[
                {
                   "original_name":"CVCE, Luxembourg",
                   "normalized_name":null,
                   "country":"Luxembourg",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-01",
       "keywords":[
          "information architecture",
          "visualisation",
          "linking and annotation",
          "software design and development",
          "GLAM: galleries, libraries, archives, museums",
          "historical studies",
          "networks, relationships, graphs",
          "knowledge representation",
          "archives, repositories, sustainability and preservation",
          "English",
          "audio, video, multimedia",
          "crowdsourcing",
          "interface and user experience design"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The so-called ‘web 2.0 revolution’ heralded in the middle of the last decade has so far neither replaced the author as the source of genuine creation nor turned conventional production processes in research and industry upside down. Rather than blurring the boundary between experts and amateurs, it questions the role of the institution as a source of authority by empowering individuals, small groups and communities (Wieneke 2010). Even today, institutions struggle to turn the productivity and impulses of their respective user communities into clear added value that benefits the institution and the community alike. In this paper we present and discuss our experiences in the development, implementation and management of user-created content built using the MyPublications tool on CVCE.eu. MyPublications enables users to create and publish their own enhanced publications using documents and resources on the European integration process available on CVCE.eu.  The CVCE’s goal is to contribute to a deeper understanding of European integration by developing a dedicated digital research infrastructure. One component of the infrastructure is a series of digital collections of publications (ePublications) on themes and topics associated with the European integration process, as curated by a researcher or team of researchers. The ePublications are themselves aggregations of diverse research objects (resources), including researcher-written contextual articles, historical documents, press articles, photographs, interactive diagrams and timelines as well as other multimedia material, each with a set of descriptive, publishable metadata and a unique and persistent identifier.  State-of-the-art ePublication frameworks are built upon a set of underlying principles encapsulated by the presentation of research knowledge alongside mechanisms for describing, sharing, discovering, reusing and repurposing the scientific content (Bechhofer, Roure et al., 2010). The CVCE’s publication model is no different. It encapsulates the following principles: (1) the ability to provide aggregations of content derived from many different published objects; (2) the provision of a unique and persistent identifier, ensuring sustainability of access; (3) the possibility of tracing the steps a researcher took to produce the ePublication; (4) the potential to reuse objects in a different context; (5) the ability to change the way objects are used by repurposing them; (6) the ability to reuse objects in compliance with IPR constraints.  The current CVCE ePublication framework is expert-led, and research outputs are based on the centre’s research questions and strategic topics. This approach leads to digital expert-curated ePublications based on themes and topics that are robust and reliable but — and this is both an advantage and a limitation — constrained by organisational priorities. On the other hand, the content itself can be combined and contextualised in different ways, both to highlight other perspectives and readings but also to address topics out of scope of the hosting institution. The challenge we face is therefore in the development of innovative tools and methods that offer new ways of reusing and repurposing historical objects by leveraging the potential of our user base to contribute knowledge themselves. This will in turn foster outputs that create a genuine surplus value for other users of the site by covering issues that we partially address or do not include at all.  The MyPublication tool empowers users to create tailor-made ePublications comprising resources in line with their personal needs, and encourages them to publish them on our website, thereby providing a plethora of different perspectives on the European integration process that ultimately enhance the value of our site for other users. The tool builds on the CVCE’s multilingual ePublication model and infrastructure for research, teaching and learning activities in European integration studies.   The tool MyPublications is the first instalment of the ‘Digital Toolbox’ at CVCE.eu. This suite of tools is designed to enable users to reuse or customise our resources for their own purposes. Using the MyPublications authoring tool, users can create ePublications that are personally curated collections of the many historical resources available on CVCE.eu alongside their own text, thoughts, ideas and critical analysis. The tool has been developed with the following workflow in mind: select, organise, structure, annotate, author (and edit), read, share and publish. The tool and its development are based on the CVCE’s experience in building and maintaining a previous application called ‘Albums’, which was widely used by the teaching and learning community for assignments (e.g. ‘build an album on the history of monetary union’) or to document ongoing research (e.g. ‘all resources related to the Rome Treaties’).  Once a user is logged into the Digital Toolbox, they can browse collections of objects (referred to as ‘resources’ in the tool) or use the search facility to find and select relevant resources and add them to ‘MyResources’. Users are then able to create a new publication using the MyPublication tool. They are asked to choose a cover picture, provide a short description and title and set the language. They then build a structure by creating subsections, add content to the different sections and write corresponding descriptions while being able to add, sort, organise and annotate the selected resources. Users can read the publication using a slideshow viewer, a simple interface that provides a sequential presentation of the narrative of an ePublication akin to a book. They can also share their publication with colleagues, peers and friends via a link. The publish/unpublish functionality enables users to make their publication accessible to the public in the MyPublications section of CVCE.eu (see figure 1). This should add new user-generated content to the research infrastructure.   Figure 1: The MyPublications tool. Empty authoring interface (top left), MyResources section (middle left), slideshow view (bottom left), Share feature (top right), overview of publicly available MyPublications (bottom right).    The platform The current CVCE data repository and ePublication framework uses the  Alfresco  document management system to manage, store and retrieve a wide variety of historical resources on the European integration process, such as interviews, treaties, legislation, photographs and cartoons, from an extensive array of archives and media outlets. The frontend and backend applications have been implemented in Liferay using HTML 5, CSS and JavaScript.  MyPublications was developed using the iterative design methodology Scrum, an AGILE development approach. This approach enabled us to involve our users in the development process on an ongoing basis, avoiding the need to predefine requirements for complex functionalities. The interface was designed with a focus on usability (Shneiderman and Plaisant, 2005) and the aim of providing an easy-to-use, pragmatic environment. All the tools in the Digital Toolbox, especially MyPublications, were designed with a ‘less-is-more’ approach (Jones and Haklay, 2009) based on pragmatism and prioritising simplicity over complexity. As a benchmark we defined that it should be possible to explain how to use the tool with a two-minute help video. In-house users (historians, economists and political scientists) were engaged in testing, evaluating and providing feedback following each development cycle.    Conclusion The simplicity of MyPublications enables a straightforward process of authoring and content creation for users with varying levels of digital literacy. Such facilities within research infrastructures also have the potential to increase accessibility to and reuse of existing objects to create new content. The tool provides a pragmatic solution for resources that have complex restrictions with respect to their licences for use. The content on our site is hosted by the CVCE, and full licences for permission to use the material on CVCE.eu have been acquired, so scholars, teachers and learners can avoid all the common problems associated with the use of weakly licensed material or unsustainable, transitory URLs. The option of publishing the publications in the CVCE research infrastructure increases the usability of the resources by enabling individual users, in various guises, to actively contribute to an international research infrastructure.  Following the release of the final tool in autumn 2015, we will reflect during our presentation at DH2016 on our experience of maintaining the MyPublications tool and user-created publications while particularly emphasising our lessons learned: how far have we succeeded in mobilising our user base to create and publish MyPublications? Is the incentive of being able to use licensed material, therefore avoiding copyright restrictions, strong enough, or is this not relevant for our users? What kind of content have our users created? Has the focus been on the final outcome (e.g. those publishing their own research) or has the process of creation been more relevant for them (e.g. those completing class assignments)? What tensions do we encounter between the institutional perspective and our user base regarding different perspectives and topics? And finally, what are the practical trials and tribulations of integrating user-created content in an institutional context?  ",
       "article_title":"Adding Value to a Research Infrastructure Through User-contributed ePublications",
       "authors":[
          {
             "given":"Catherine Emma",
             "family":"Jones",
             "affiliation":[
                {
                   "original_name":"CVCE, Luxembourg",
                   "normalized_name":null,
                   "country":"Luxembourg",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Lars",
             "family":"Wieneke",
             "affiliation":[
                {
                   "original_name":"CVCE, Luxembourg",
                   "normalized_name":null,
                   "country":"Luxembourg",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-19",
       "keywords":[
          "English",
          "publishing and delivery systems"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Character speech is an elementary part of novels. When calculating with German-speaking novels, the question arises, if there is a stylistic difference between character and narrator speech. Given the presupposition that character speech imitates authentic communication, I will answer these questions with a structured series of experiments on deictic expressions, based on statistic and linguistic knowledge. Theoretical background Character speech is defined here as direct or cited/quoted speech, the words and sentences one finds between quotation marks. By drawing on these distinct punctuation markers, it can be separated automatically from the rest of the text. With the beginning of modernism, however, these formal structures have started to dissolve. Between the clear marking of character speech and no marking at all, gradual stages are possible. In these cases, a considerable effort is required to automatically separate character speech from the rest of the novel. An examination of the differences between character and narrator speech therefore can be useful to assess the necessity of such a costly separation. Deictics are necessary in communication situations to refer to a point in time, space or to certain objects or persons like the speaker or the addressee. In the sentence Tomorrow I will be there. every word is deictical. Deictics are context-dependent, which means that in different situations they have different meanings. They belong to different lexical categories – but most of them are function words. The presented approach implies the following premises: 1. Different (non-fictional) texts types show a different frequency of deictic terms. These basic text types have been categorised according to three criteria of communication (dialogical,  face-to-face and  oral by Diewald, 1991).  2. Character speech imitates a dialogue – which is one of these basic text types. Transferring the first premise on fiction, literary genre-categorisation has to stand back: character speech in novels and plays would belong to one basic text typ.   Previous Work Previous research on character speech in the humanities shows, that it seems to be a subject mainly in the philosophy of language (see Pafel and Dirscherl, 2015). In literary studies, only one monograph on direct speech exists (Müller, 1981). Recently Brunner (2015) showed that the automated tagging of different kinds of speech in German-speaking novels with computerlinguistic methods is possible, but still has weak results – except for direct speech. Her corpus consisted of 13 different novels, which is a rather small corpus. In computational stylistics it appears that most approaches tend to use smaller corpora (e.g Burrows, 1987; van Dalen-Oskam, 2014).   Method For the corpus, German-speaking plays and novels from the 18 th to the 20 th century are selected randomly. They are separated into several subcorpora consisting of 25 plays and 75 novels, including the Brunner corpus with 13 novels (Fig. 1).     Figure 1: corpus scheme  The subcorpus with the plays serves as a benchmark/reference value: Since plays consist mostly of character speech, they can be compared with the  plays and  novels_character speech only-subcorpus. Gries 2008 serves for the statistical basis. Eder’s structured experiments in “Does size matter?” (2013) serve as a template for the presented analyses. That means parameters are changed in a consistent and transparent way through the experiment:   The experiment – counting the deictic terms – is run several times (Fig. 1): At first, the whole corpus is tested. For the second run the novels are separated from the plays and for the third run character speech is separated from the rest of the text. Finally, the Brunner corpus is run a forth time with the other categories of speech, e.g. free indirect speech, separated as well. Since there is no common definition of deictics, nothing like a deictic lexicon exists. In my analyses, I will use a rather straightforward approach: An expression will be regarded as deictic, if it belongs to one of the main deictic categories like time, space, or person and if the deictic reference is its main function. This definition excludes verbs, but includes all function words like temporal and spatial adverbs and a small group of personal pronouns.    Results sample corpus As a starting point for upcoming research I will present temporary results that are generated by drawing on a small sample corpus of novels in which character speech is separated from the rest of the text and the frequency of deictic terms is analysed (Fig. 2). Then the results are compared with Diewald’s results for the basic text types in non-fiction (Fig. 5).    Figure 2: Percentage of deictic terms (here, now) in character speech and in the rest of the novel (narrator speech), relative frequency.  The average values for character and narrator speech (0.36% in either case) show no difference at all (Fig. 2). However, a closer look on the frequency distribution draws another picture: Figure 2 shows that most of the novels (novels 6 to 14) range around 0,4% deictic terms in character speech. In figure 3 the results seen in figure 2 are visualised: in contrast half of the deictic terms in the narrator speech of the novels range a bit higher than 0,2%. This shift is marked by the blue arrows (Fig. 3 + 4). The stability of these results will be evaluated on the complete corpus (see “Methods”).   Figure 3: Frequency of deictic terms in character speech of the 18 novels of the table in fig. 2, in ascending order    Figure 4: Frequency of deictic terms in narrator speech of the 18 novels of the table in fig. 2, in ascending order    Conclusions According to Diewald’s results the deictic frequency of basic text types is lowest in  letters and  written monologues.  Dialogue and  oral monologue present a higher frequency of deictic terms, but still topped by  telephone conversation (Fig. 5). Regarding their principle structure, character speech can be assigned to the basic text types,  dialogue and  oral monologue, whereas narrator speech resembles  written monologue and  letters.     Figure 5: Different text types and their frequency of personal, local, temporal and objectual deictic terms, the results refer to Diewald 1991: 383.  Accordingly, the results of the sample corpus indicate a lower deictic frequency in narrator speech compared to character speech, which matches DIEWALD’s results on deictic frequency in the basic text types.  Still, for the sample corpus, it is quite problematic to compare these numbers with DIEWALD’s results, because in the non-fictional discourses all deictic terms are analysed instead of only two prototypic ones ( here, now). Since it seems that there is a difference between both types of speech in novels, it nevertheless has to stand the test of the broader experiments with the bigger 100 texts corpus. In addition, for further research another series of this experiment should be run with the same corpus scheme, but set up as a stylometric analysis to see if similar patterns can be found. If the results show a significant difference between character and narrator speech, it is necessary to have character and narrator speech separated in all novels as part of the preparation for digital text analyses.   ",
       "article_title":"Does Character Speech Matter? A Quantitative Approach",
       "authors":[
          {
             "given":"Peggy",
             "family":"Bockwinkel",
             "affiliation":[
                {
                   "original_name":"University of Stuttgart, Germany",
                   "normalized_name":"University of Stuttgart",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/04vnq7t77",
                      "GRID":"grid.5719.a"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "speech processing",
          "literary studies",
          "corpora and corpus activities",
          "philology",
          "stylistics and stylometry",
          "german studies",
          "data mining / text mining",
          "linguistics",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Massive digitization of archival material, coupled with automatic document processing techniques and data visualisation tools offers great opportunities for reconstructing and exploring the past. Unprecedented wealth of historical data (e.g. names of persons, places, transaction records) can indeed be gathered through the transcription and annotation of digitized documents and thereby foster large-scale studies of past societies. Yet, the transformation of hand-written documents into well-represented, structured and connected data is not straightforward and requires several processing steps. In this regard, a key issue is entity record linkage, a process aiming at linking different mentions in texts which refer to the same entity. Also known as entity disambiguation, record linkage is essential in that it allows to identify genuine individuals, to aggregate multi-source information about single entities, and to reconstruct networks across documents and document series. In this paper we present an approach to automatically identify coreferential entity mentions of type  Person in a data set derived from Venetian apprenticeship contracts from the early modern period (16th-18th c.). Taking advantage of a manually annotated sub-part of the document series, we compute distances between pairs of mentions, combining various similarity measures based on (sparse) context information and person attributes.    Task Definition Major challenges when dealing with people-related data are homographic person names referring to different persons as well as the existence of name variants referring to the same person. These are well-known issues in the field of Natural Language Processing for which various approaches have been devised, first via mention clustering (Mann and Yarowsky 2003; Artiles et al. 2008), more recently via linking to a knowledge base (Ji and Grishman 2011; Shen et al 2015). In the context of historical data, dealing with person name ambiguity is all the more difficult since data is inherently sparse and uncertain (resulting in poor mention context) and since knowledge bases such as DBpedia (Lehmann et al 2013) contain very little about past average laypersons (resulting in poor entity context). It is however an essential step prior to any historical data analysis (Bloothooft et al 2015), which we address as part of the  Garzoni project. This project aims at studying apprenticeship in early modern Venice by extracting information from archival material. Part of this material have been manually annotated, including mention links towards unique entities. Starting from a subset of the current data, we present a method for person record linkage, with the objective to complement its disambiguation coverage and to bootstrap a system to better automate entity disambiguation during annotation, in an active learning fashion.    The Accordi dei Garzoni The  Accordi dei Garzoni is a document series from the State Archives of Venice which originates from the activity of the  Giustizia Vecchia magistracy. This judicial authority was in charge of registering apprenticeship contracts in order to protect young people while they were trained and/or providing domestic services (Bellavitis 2006). As a result of this regulation, information for much of apprenticeship arrangements got centralized, today reflected in a dense archival series.  The  Accordi consists of about 55,000 contracts registered from 1575 until 1772. Each contract features an apprentice, a master and often a guarantor, sometimes two. A sample of 11,000 contracts have been manually annotated and the resulting data is stored in an RDF triple store. For each person mentioned in a contract, annotators created a  person mention and, importantly, linked it to a  person entity. They did so either by selecting an already existing entity in the database or by creating a new one. Given the difficulty of this task, only a limited number of entities have been disambiguated; the annotated dataset can therefore be considered as correct but not exhaustive.  The present work considers annotated documents from the period 1586-1600, for which statistics about contracts and entity/mention ratio are shown in Table 1. We use a subset of this dataset (bolded line in the table) as a  golden set for our experiments.   Table 1.  Entity-Mention stastistical profile for the whole vs. selected period   count whole period 1586-1600   # annotated contracts 11,525 2,687   # mentions 31,952  7,589   # entities 26,641  6,599    # entities with # mention  > 1  1793 382    avg mention per entity  1.09 1.08    avg mention per entity with # mention  > 1  2.44  2.38     Approach Given a set of mentions, our objective is to estimate the likelihood that two mentions refer to the same entity. We represent each mention by a vector of features and compare them pairwise using various similarity measures. The list of selected features at mention and contract levels are presented in Table 2 and 3 respectively.  Table 2.  Mention-level features   Feature Variable Type   first name string   surname string   patronymic string   gender categorical   age integer   profession categorical   geographical origins string    Table 3.  Contract-level features   Feature Variable Type   workshop toponym string   workshop parish string   workshop sestriere  There are 6 sestrieri in Venice, i.e. groups of contiguous parishes.   string   workshop insigna string   contract year integer   contract duration string   master profession categorical   With respect to our dataset and features, several points should be emphasized. First, data sparsity: it is common for a mention to have just a few features. Second, features are not evenly sparse (cf. Figure 1) and do not contribute equally to a possible linkage. Core features such as  name,  surname,  patronymic,  gender and  profession must strongly correspond in order to consider a link as reliable. On the other hand, rare features such as  workshop insigna can be very informative when shared by two mentions and should also be valued by the linkage algorithm. Finally, features are dependent, particularly on the role of the person (e.g. age indicated only for apprentices).    Figure 1. Distribution of features by role  We construct three matrices of size  N × N, where  N is the number of mentions in the dataset. The first matrix Φ, the  feature matrix, stores similarity scores of mentions pairwise. Scores are computed using measures over features as follows:    year of contract: the feature-score is measured via distance and diminishing returns. Each year of distance between 1 and 15 and between 15 and 30 decreases an initial score of 1 by 0.01 and 0.025 respectively, with a definitive cut-off after 30. For example, two contracts from 1590 and 1594 have a score of 0.96.   age: similarly as per year, each year of distance of the difference between two ages decreases an initial score.   gender and  profession: the feature-scores are based on exact matches.   name,  surname,  patronymic and  workshop toponym: the feature-score is based on the Deverau-Levenshtein string metric (Cohen et al, 2003). For example,  Polo and  Pollo have a similarity measure of 0.95.   geographical origins and  insigna: the feature-score is based on a token-based variant of the Jaro-Winkler metric. For example,  Friulano and  del Friuli have a similarity measure of 0.82.   The score of each pair is stored in Φ: it is the L2 norm of the resulting feature-score vector. The second matrix Γ, the  combination matrix, stores values that indicate whether a pair of mentions shares similar feature combinations or not. To build such matrix, we leverage the  golden set and identify combinations of features which produced a linkage on a role-by-role basis (e.g. master-master or guarantor-master). Features are considered activated when their feature-score is equal or above 0.84 It has been shown in comparable settings that edit distance with cut-off at distance 3 (which for us is distance > 0.85) provides good results (Kleanthi et al. 2015). and we filter out combinations occurring once. The score of a mention pair in Γ is 1.0 if the combination of activated features is valid for the given role pair; 0.5 if the role pair does not match but the combination is valid; 0.0 otherwise. This matrix accounts for feature dependencies and the different ways to name a person with respect to his/her role.  The third matrix Δ, the  filtering matrix, scores mention pairs according to the number of activated core features (1.0 if 3 +  features – out of 5 – are activated, 0.0 otherwise Features are activated when their similarity is above 0.84.).  Given the three matrices, we normalize them and consider the following function to compute the similarity score of a mention pair p:    where δ p is a boolean parameter taken from Δ which activates the filter over core features for pair  p;  π p is the feature score taken from Φ; γ p is the combination score from Γ; and λ is a parameter giving priority over vector features or combinations of features. δ ∈ {0, 1} and 0 ≤ λ, π, γ ≤ 1. This function allows us to adjust the different parameters: core vs sparse features (δ), feature scores (π) and feature combinations (γ).    Evaluation We evaluate our approach in terms of coverage and precision. With respect to coverage, we verify our method over 100 thresholds from 0.99 to 0.0. For each threshold, we compare linkage curves as the percentage of links obtained over the total possible against the coverage of the  golden set. Precision is based on manual annotation of 50 randomly selected linkages.  Both procedures are repeated with λ ∈ {0.1, 0.5, 0.9} and δ activated or not, for a total of 6 configurations. The objective is to understand the individual contributions of the three components to our function.   Results and Discussion Results for the first and second evaluation procedure are presented in Figure 2 and Table 4 (resp.). Highest precision (0.61 and 0.3 in Table 4) is obtained with a balance between feature combinations and feature scores (λ = 0.5). δ proves very useful for filtering the input space (from 28,7M possible pairs to 44,2K), and lowers the number of false positives, especially for links between apprentices (cf. line ‘w-o AA’ in Table 4). The combination of the two (filtered input space and equal weights) provides the best results, especially for masters and guarantors. Linkage curves can be explained similarly: low λ entails a step-like curve (three steps at 0.0, 0.5 and 1.0), while high λ creates a Gaussian over the disambiguation space.  Table 4.  Precision with threshold >=  0.9 (* means not-significant statistics)    δ active δ not active     λ = 0.1 λ = 0.5 λ = 0.9 λ = 0.1 λ = 0.5 λ = 0.9   all  0.21 0.3 0.21 0.0 0.26 0.15   without A-A  0.22 0.61 0.22 0.0 0.48 0.67   This confirms that a balanced approach might be the best solution in a setting where data is sparse (high λ), the  golden set is present but of limited coverage (low λ), and some prior assumptions on the required features can be made (δ). As shown in Figure 3, the graphs with λ = 0.5 and δ = 1.0 collapse more gradually, providing the widest effective linkage space to explore. Eventually, results also suggest to proceed in an active learning fashion, where the system learns iteratively with new data as part of the  golden set.    Figure 2. Linkage curves for the 6 parameters settings, over thresholds    Figure 3. Graph properties for the 6 parameter settings, over thresholds  Finally, in order to further motivate our work, Figure 4 shows the largest components of the deduced social network with and without automatic disambiguation. The linkage method has the nice property of enlarging small components before gradually connecting them.   Figure 4. Largest components of social networks from golden set (left-most) and from disambiguated datasets (center and right-most)    Conclusion and Future Work This paper presented a system to perform record linkage over mentions of persons from sparse historical data. It deals with different constraints such as data sparsity and limited prior knowledge. We plan to apply the system to different datasets and to integrate it into a transcription and annotation interface, in order to use it for live, aided record linkage.  ",
       "article_title":"A Method for Record Linkage with Sparse Historical Data",
       "authors":[
          {
             "given":"Giovanni",
             "family":"Colavizza",
             "affiliation":[
                {
                   "original_name":"EPFL, Switzerland",
                   "normalized_name":null,
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Maud",
             "family":"Ehrmann",
             "affiliation":[
                {
                   "original_name":"EPFL, Switzerland",
                   "normalized_name":null,
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Yannick",
             "family":"Rochat",
             "affiliation":[
                {
                   "original_name":"EPFL, Switzerland",
                   "normalized_name":null,
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "natural language processing",
          "linking and annotation",
          "digitisation, resource creation, and discovery",
          "data mining / text mining",
          "content analysis",
          "archives, repositories, sustainability and preservation",
          "English",
          "networks, relationships, graphs"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction One of the key problems in historical political research is that many relevant research questions can only be answered by means of a prolonged and painstaking analysis of large archival series. Questions like: “How did the relationship between the government, the parliament and the political elites change over time?”, “What role did political traditions and rituals play?”, and “In what ways did the information economy influence the political process?”, still require scholars to systematically work through vast bodies of archival material. Only a few scholars, who appeared not to be intimidated by such a daunting task, have come up with long-term analyses of political patterns. This paper proposes a new, digital approach to avoid these time-consuming activities and to open up major archival series for automated text analysis, by applying various tools for text recognition and automated structuring, as well as by using reference data and re-using metadata.  The case study selected to  demonstrate the potential of this approach is the opening up of the Resolutions of the Dutch States-General from 1576 to 1795. This archival series of the central assembly of the seven Provinces forming the Dutch Republic is an excellent example of the type of source suitable to answer the above mentioned research questions. Editing the Resolutions has been a task of the Huygens Institute for the History of the Netherlands and its predecessors since 1915 (Japikse and Rijperman, 1915-1970, Van Deursen et al., 1971-1994).  http://resources.huygens.knaw.nl/besluitenstatengeneraal1576-1630/index_html_en (accessed 3 March 2016)  This task is hindered, though, by the vast size of the resolutions, which approximately consists of 200,000 pages. The classic edition process, not even aimed at providing a full transcription of the resolutions but only abstracts, reached the year 1625 in 1994. After that, a project was carried out to edit the resolutions from 1626 to 1630 only in digital form, with the help of xml-coding (Nijenhuis, 2006; Nijenhuis et al., 2007). This project ended in 2007 and was not pursued further, because it was clear that this method also was too time and money consuming.   In 2015 we started a totally different approach as an alternative to editing this vast collection of documents. Our goal is to make this important collection accessible, searchable, and analyzable for historical research by applying various advanced digital humanities tools. We will do this by using the metadata of the already printed edited Resolutions, and by enriching and linking the data to other relevant research data. The advantage of this approach is that the Resolutions will be made accessible for researchers in a far quicker way in comparison to the classic way of editing. Also, this approach is to provide insights which will be useful for comparable projects dealing with important archival series in the future, and may provide an alternative to full scale editing of large historical sources in the digital era.   Reusing metadata On the basis of experience with digital editing, we know that performing a small scale experiment is the best way of establishing best practices and avoiding huge costs. Therefore, we have chosen to apply a multilevel approach with several pilot projects, using various tools which may be applicable for exploring the content of some 200,000 pages of resolutions for historical research. These projects depart from the principle of using what is already there. This means, in practice, that we will construct a framework consisting of the metadata added by the previous editors, like indexes; mark-ups of names, places and institutions; and classification of subjects, as well as contemporary indexes and marginal subject-notes in the resolutions. This framework will serve as a reference to make the resolutions corpus accessible.   Automated Handwritten Text Recognition Secondly, we experiment with tools for Handwritten Text Recognition (HTR). The software developed by researchers from the Universitat Politècnica de València, which is integrated in the  Transkribus platform, offers the most valuable results (Sanchez et al., 2013).  https://transkribus.eu/Transkribus/ (accessed 3 March 2016)  For the HTR-experiment we manually transcribed 40 pages of handwritten resolutions. Of these pages, 30 were used for training and 10 for testing. The resulting Word Error Rate for this experiment was 40%. We realize that using an only partly correct transcription does not deliver the results one can expect from a traditional edition. The automated transcriptions generated by the HTR-tool should be seen as an alternative that enables researchers to search the text. Of course with a Word Error Rate of 40% this will not deliver perfect results. The HTR may be improved by expanding the training set and by the use of reference data, which is discussed below. In case our approach will be financed for the whole series of resolutions, we intend to use crowdsourcing to improve the HTR results on the handwritten resolutions via the  Transkribus platform. As has been demonstrated by the developers from Valencia, manual correction of incorrect transcriptions of the HTR-software leads to a recalculation diminishing the mistakes the software made in the rest of the text. This will speed up the work towards an accurate transcription for the whole series. Nevertheless, with the limited number of people able to read 17 th-century Dutch handwriting taken into account, we cannot expect crowdsourcing to provide us with a perfect transcription of the whole series of resolutions within a few years.     Automated annotation Finally, we investigated the use of tools for enriching the resolutions with annotations that will improve exploring the digitized material. For this purpose we used contemporary printed resolutions, of which a series exists from 1703 to 1796. We selected a set of 100 pages from the year 1725, containing 366 resolutions. The text of the resolutions was transcribed and marked up manually using TEI. The printed material consists of blocks of text that can be categorized as follows: “session”, consisting of the date of the meeting, the name of the chairman, and the attendees representing each of the seven provinces; “resumption”, the summary of the previous meeting; “resolution”, the body of the resolutions themselves; and “insertion”, mostly incoming letters. We used a standard machine learning approach. The text blocks were marked up with their type manually. Part of the material was used for training the automated categorizer, the other part for testing. The categorizer was trained using as features fixed expressions the successive clerks of the States-General used in their account of the meetings. The comparison of the results with the manual categorization turned out to be promising. We were able to categorize the different types of text blocks with a 98,6% precision.  The next step was to extract information from the text blocks. Building on software of the  Stanford Natural Language Processing Group we developed a rule-based tool for recognizing dates with a 99,1% precision. The dates in the “session” text blocks could be identified easily, for they have a fixed structure; this allows us to annotate each resolution with its date. Software for identifying more complex dates in the text blocks (for example “the resolution taken yesterday”) is yet to be written; it will be used to annotate resolutions with references to other resolutions.  Furthermore, we developed a tool using a Naïve Bayes Classifier for categorizing the attendance list. The manuscript and printed resolutions list the attendants according to the province they represented; the provinces were mentioned in a fixed order. However, at some meetings a province was not represented. With the tool we are able to identify the provinces the attendants as well as the chairman represented in these cases also. Finally, we took some steps in interpreting the content of the resolutions. Again using fixed phrases and a Naïve Bayes Classifier, it turns out to be possible to recognize in most cases whether or not a decision was taken in a resolution (96,9% precision) and whether the States-General asked a person or a body for advice (99,3%). The results for analyzing to whom the States-General asked advice; and whether and to whom they asked for a report, are yet inconclusive because of the limited amount of training material. Apart from improving the results for this last mentioned analysis, our future work will be dedicated to several issues. Firstly, applying Named Entity Recognition to the resolutions in combination with the use of reference data. The Huygens ING owns and hosts several relevant data collections, for instance the  Biography Portal of the Netherlands  http://www.biografischportaal.nl/en/ (accessed 3 March 2016)  and the  Compendium of Office-Holders and Civil Servants 1428-1861  http://resources.huygens.knaw.nl/repertoriumambtsdragersambtenaren1428-1861/index_html_en (accessed 3 March 2016) with which we will be able to identify persons and institutions mentioned in the resolutions. Secondly, we aim at improving the OCR for the printed resolutions, for the benefit of the automated annotation. Thirdly, we will investigate whether the tools for automated structuring can be applied also to the automated transcription of the handwritten resolutions.   ",
       "article_title":"From Handwritten Text to Structured Data: Alternatives to Editing Large Archival Series",
       "authors":[
          {
             "given":"Ronald",
             "family":"Sluijter",
             "affiliation":[
                {
                   "original_name":"Huygens Institute for the History of the Netherlands",
                   "normalized_name":"Huygens Institute for the History of the Netherlands",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04x6kq749",
                      "GRID":"grid.450092.a"
                   }
                }
             ]
          },
          {
             "given":"Marielle",
             "family":"Scherer",
             "affiliation":[
                {
                   "original_name":"Huygens Institute for the History of the Netherlands",
                   "normalized_name":"Huygens Institute for the History of the Netherlands",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04x6kq749",
                      "GRID":"grid.450092.a"
                   }
                }
             ]
          },
          {
             "given":"Sebastiaan",
             "family":"Derks",
             "affiliation":[
                {
                   "original_name":"Huygens Institute for the History of the Netherlands",
                   "normalized_name":"Huygens Institute for the History of the Netherlands",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04x6kq749",
                      "GRID":"grid.450092.a"
                   }
                }
             ]
          },
          {
             "given":"Ida",
             "family":"Nijenhuis",
             "affiliation":[
                {
                   "original_name":"Huygens Institute for the History of the Netherlands",
                   "normalized_name":"Huygens Institute for the History of the Netherlands",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04x6kq749",
                      "GRID":"grid.450092.a"
                   }
                }
             ]
          },
          {
             "given":"Walter",
             "family":"Ravenek",
             "affiliation":[
                {
                   "original_name":"Huygens Institute for the History of the Netherlands",
                   "normalized_name":"Huygens Institute for the History of the Netherlands",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04x6kq749",
                      "GRID":"grid.450092.a"
                   }
                }
             ]
          },
          {
             "given":"Rik",
             "family":"Hoekstra",
             "affiliation":[
                {
                   "original_name":"Huygens Institute for the History of the Netherlands",
                   "normalized_name":"Huygens Institute for the History of the Netherlands",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04x6kq749",
                      "GRID":"grid.450092.a"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "information retrieval",
          "digital humanities - diversity",
          "data mining / text mining",
          "historical studies",
          "scholarly editing",
          "content analysis",
          "English",
          "text analysis",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Given the rise in recent years of GeoHumanities projects, this paper considers how historical data are translated and represented geographically within online mapping projects, giving insights into how these representations influence acquired meaning. GeoHumanities predominantly use geographical tools to create, present and explore different types of historical evidence and resources, for example original paper maps transformed into digital representations or geocoding of place names within texts. Factors impacting the resulting representation of historical evidence in geographical form include: choice of the underlying geographic data models, form and style of the original (historical) objects, as well as the selection, transformation and encoding practices. In this study we observe which data models are used and their context, to understand how these choices may influence acquired meaning and cognitive understanding.    Method The GeoHumanities special interest group of the Alliance for Digital Humanities Organisations has a catalog of 312 geohumanities based projects (data from July 2014). From this list a sample of 30 projects were selected (sample selected based on language, availability and production since 2010) . In these projects, the cartographic representation of historical evidence was evaluated according to a set of 70 different criteria grouped into 4 categories: (1) data structures used to model data; (2) forms of representation (typology, symbolism, use of transparency and uncertainty); (3) data interaction possibilities offered to users and (4) spatial data analysis possibilities offered to users.  The following research questions were investigated:   How are historical data represented in web mapping applications?  How rich are geohumanities interfaces? How do they influence knowledge acquisition?   In this paper we consider the first research question.    Results: Exploring raster representations In the sample we observed a mix of both vector and raster data models. Of the 30 projects, 40% (n=12) used raster data models to integrate georeferenced versions of historical paper maps, with 4 types of use scenarios (see figure 1). Firstly, either paper map sheets belonging to a map series were stitched together or individual maps were used as a basemap layer enabling viewing of historical context or comparison of raster map to digitally encoded vector data.  The digital versions of the paper map were presented as an optional data layer in the interface that, in the main, could be switched on or off by the user.   For 26 out of 30, the historical map was overlaid on top of a contemporary raster base map which displayed either a street map or an aerial view. In only 2 very early period maps, the Agas Map of Early Modern London and The Gough Map of Great Britain, was it not possible to view a contemporary reference basemap.  The second and third types of historical paper map representations in raster format were observed in crowdsourcing methodologies. In these projects users are asked to interact with the map to create new digital data. One category of projects asked members of the public to undertake the transformation from a digital scan of the paper map to a true digital raster using the processes known as georeferencing. The other made use of georeferenced historical maps and asked users to vectorise historical data. Users were asked to encode labels or draw and confirm shapes of buildings. In this type of task the users are effectively tracing the historical map to create a vector representation. The final use observed focused on the search of historical data through the map interface to find metadata records and locate historical maps. The coordinate extents of the original paper maps were used to draw a minimum bounding rectangle within the web-map interface to highlight the represented areas. Users could then search for places and discover which old maps are available digitally at different archives.    Figure 1: show screenshot of the different models of historical raster representation   Influences of raster representations  After such extensive treatment, to digitally transform the paper map into a web map, it becomes relevant to consider notions of materiality between the source and the transformed map. The “materiality” of the raster map is influenced by the transposition from paper to digital, with the associated loss of integrity. Take the action of cropping the map, observed in two thirds of all the rasters. The deletion of contextual map elements implies a certain loss of meaning from the source. Lost information commonly include: title, coordinate system, data of production, map producer, original legend, handwritten notes, archive stamps and other information. Thus the cropping process ensures that only purely functional cartographic data are retained, whilst the social document context is neglected.  A new type of digital materiality is derived from the transformation of the paper map to the digital into a “ slippy map”, where the historical  raster map is cut up into a sets of map tiles that provide small images which are seamlessly joined together (in the same way Google Maps API and others provide users access to contemporary basemaps). By cropping the historical map to show only the cartographic map many map sheets from the same original paper map series can be stitched together to provide a detailed view across a wider geographical extent without having overlapping boundaries or edge effects of the different map sheets that would result otherwise. Of course such a large map with street level detail would not have been possible to draw using traditional paper maps and is facilitated only by advances in technology and web-mapping interfaces. Once the georeferenced maps are tiled, the slippy map enables users to zoom and pan effortlessly. There are certain advantages: (1) users can explore an entire city, region or continent as one seamless entity; (2) viewing of the map is instantaneous as tiling methods do not require lengthy loading times (which can be the case if providing high resolution georeferenced raster maps that are not tiled); (3) users are provided with new types of interactions for engaging with the map and importantly (4) users are provided with digital transformation to enable new ways of consuming “original” historic material.      Results: Exploring point representations Vector data, especially point objects, were by far the most common representation of historical data in Geohumanities projects. Nearly all of the samples, almost ninety percent of the projects (n=26) used points for their digital conceptualisations of historical phenomena, ideas or events, see figure 2. Moreover, it was particularly common to simultaneously use both raster and point data structures within the interface. Whereby, the historical map in raster form was adopted as the basemap, ie a reference map with the point data provided users with representations of more specific data. Such a mix was observed in 22 projects. A small number of projects also made use of the line (26%) and area (33%) geometry objects that are available as part of the vector data format. No projects used text as a way of representing information (except to label objects label objects).    Figure 2: Example of map interfaces uses point data   Influences of Point for representing historical data The point data object is a ubiquitous form of representation. It is a pervasive feature of geohumanities web-mapping projects and was observed in nearly all the sample projects (almost 90%). The point is used to represent a digital conceptualisation of historical phenomena, ideas, places, events etc. Despite the ubiquitous use of this data model, it is not without issues.  Given that a point marks an exact location, it was often observed that points (miss)represented phenomena that ranged in scale from a building, city and region or wider, sometimes representing all in the same data layer. Such confusing representations were exacerbated by (1) choice of representation - the use of the map pin which implies certainty in location; (2) failure to represent uncertainty in data therefore implying absolute facts – that are actually ambiguous (3) low usage of icons as map symbols reducing the cognitive understanding of the map. The point encapsulates a sense of location accuracy, precision and factual detail that often represented fuzzy historical constructs at best. Thus, in much the same way as text is transposed into digital form (Kirschenbaum, 2001; Hayles, 2005; Drucker, 2011), the digital map transformations and resulting cartographic representations influence the meaning of the mapped data and how it is read, interpreted and understood. By failing to consider carefully the cartographic choices in how the digitised historic data are embodied, it is likely that the cognitive load placed on the user and the efforts required to extract meaning from the maps are significantly increased. It also will strongly influence how these digital translations are interpreted and understood by the user.      Thoughts and Conclusions This evaluation is a starting point for discussions between humanities scholars and geo specialists, designers, information scientists and human computer interactions experts. It is obvious that digital transformations of historical data into digital geodata world provides many benefits, not least that associated with pattern recognition and presentation of data but it is not without its limitations. Initial results indicate that existing models and cartographic representations of georeferenced historical data need to be extended. It is plausible to suggest that the point data structure is inadequate for representing complex historical phenomena or broad geographical concepts but due to the lack of alternatives it is widely accepted as standard practice. We will reflect on the point as an appropriate mechanism for representing complex historical data? Explore the extent digital representations should stay true to original historic map evidence? And investigate how  digital map translations influence understanding and aid/hinder interpretation?    ",
       "article_title":"Exploring and Evaluating Cartographic (miss)Representation in a Sample of Web-based Geohumanities Projects",
       "authors":[
          {
             "given":"Catherine Emma",
             "family":"Jones",
             "affiliation":[
                {
                   "original_name":"CVCE, Luxembourg",
                   "normalized_name":null,
                   "country":"Luxembourg",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-14",
       "keywords":[
          "maps and mapping",
          "English",
          "geospatial analysis, interfaces and technology"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Even before Harper Lee’s “new” book,  Go Set A Watchman, was published earlier this year (2015), rumors as to its authorship abounded. Alabama police looked into alleged abuse of Lee’s rights; suspicion suddenly (re)surfaced about the strange fact that one of the greatest bestsellers in American history was its author’s only completed work; Lee’s childhood friendship with Truman Capote (portrayed as Dill in  To Kill A Mockingbird) and their later association on the occasion of  In Cold Blood fueled more speculations on the two Southern writers’ possible, or even just plausible, collaboration; finally, the role of Tay Hohoff, Lee’s editor on her bestseller, was discussed. Desperate media turned to the usual front for stylometry, Matt Jockers, who graciously ceded this opportunity onto us. A story about our early results appeared in  The Wall Street Journal (Gamerman, 2015), and it echoed even in our native Poland, where the country’s major newspaper,  Gazeta Wyborcza, also devoted a whole page to this international success of Polish stylometry (Makarenko, 2015).  The truth proved to be at once much less sensational than most of the rumors – and much more interesting. Stylometric evidence is very strong in this case: Harper Lee is the author of both  To Kill A Mockingbird and  Go Set A Watchman. The first method applied here was part of stylo, a stylometric package (Eder et al., 2013) for R (R Core Team, 2014): series of most-frequent word frequencies in a collection of texts were compared using Burrows’s Delta measure of distance (Burrows, 2002); Delta distances were compared for each pair of the texts in this corpus by cluster analysis, and the results of clustering were used to create a bootstrap consensus tree. The resulting Fig. 1 shows the two Harper Lee books as two nearest neighbors just as it does the other authors included for comparison here. More importantly, perhaps, Truman Capote is far away. Most importantly, her editor’s only available book,  Cats and Other People, betrays no similarity to her charge. Since this sort of diagram is oriented at deciphering the strongest signal in word usage, authorship, the various rumors should be finally set at rest – the more so as the two Harper Lee novels have always been each other’s nearest neighbors in a whole series of rigorous machine-learning classification tests performed using stylo’s “classify” function.     Figure 1: Harper Lee and selected authors of the American South, compared at 100–2000 most frequent words   Lesser affinities between texts are preserved in Fig. 2, which presents a network analysis of the same data treated with an enhanced version of the aforementioned consensus statistical method (Eder, 2015b) and produced with the Force Atlas 2 layout (Jacomy et al., 2014) in Gephi (Bastian et al., 2009). The degree of similarity is shown by the thickness of the curves that connect the particular texts: the thicker the line, the stronger the similarity. Additionally, the algorithm also spatially distributes the nodes (representing each text) to provide an additional visualization effect.    Figure 2: Network analysis of the same collection of novels   It is no surprise that this diagram echoes the previous one as far as the strongest similarities are concerned. Lee is still Lee; now, Faulkner stands almost alone. But then the lesser forces, represented by the slightly narrower connections, also count. The first thing that strikes the eye in the Lee neighborhood is the  Watchman’s affinity to  In Cold Blood and a more heterogeneous pattern for the  Mockingbird: the book researched by Capote with Lee is still linked to her 1960 bestseller, but now only by the minutest of lines. This rephrases the Lee/Capote question in a more interesting way. Is there a drop of Capote in Lee? Perhaps not in the entirety of her work – perhaps just in a passage or two. This should be answered with a modification of the method: since it is difficult to see overlapping stylometric signals in an entire novel, one can see much more when the novel is split into equal and smaller fragments; then, the usual stylometric analysis is applied to the particular slices according to the “rolling.classify” procedure (Eder, 2015a).  The most reasonable texts to be thus compared to  To Kill A Mockingbird are Capote’s  In Cold Blood (since Lee helped with the research for that book), Lee’s own  Go Set A Watchman (to see how much of the  Watchman might be found in the  Mockingbird) and Tay Hohoff’s  Cats and Other People (to find out how much of Lee’s rewriting of her original proposal might have been influenced by her experienced editor). This is presented in Fig. 3, and the result is quite interesting.     Figure 3: To Kill a Mockingbird contrasted sequentially against Capote’s In Cold Blood (red), Hohoff’s Cats and Other People (blue) and Lee’s Go Set A Watchman (green). The lower band represents the strongest authorial signal; the upper band (in less intense colors) is the second-strongest signal   The signal in a little more than a half of the segments in  To Kill A Mockingbird is that of the novel she originally brought to be published by Lippincott. It is highly significant that its longest stretch coincides with the trial that was only mentioned in the  Watchman and became the focus of the book in the  Mockingbird. This seems to suggest that while this refocusing of the book was made following the advice of the editor, the rewriting was indeed done by Harper Lee.  The rest of the  Mockingbird is a veritable mosaic of her own and her editor’s hand. Tay Hohoff’s impact seems to be especially visible towards the end of the story, and it coincides with the novel’s climax in Chapter 28: Scout, dressed in her elaborate and cumbersome ham costume, is attacked by Bob Ewell, who, following the struggle with Jem and then with Arthur “Boo” Radley, is left with his own knife stuck under his ribs.  We will never know, of course, whether Tay Hohoff really wrote that scene (and the others that seem to bear her mark) for Lee. But it is sensible to argue that while  To Kill A Mockingbird is obviously a novel by Harper Lee, traces of someone who helped her along the way for two whole years – and who, at one point, talked the author into running down to the street to collect the manuscript that had been flung through the window in frustration (Shields, 2006: 121) – must be there somewhere. The results produced by the different functions of stylo are not in conflict when they show the overall strength of the  Watchman signal in the  Mockingbird and the possible echoes of Hohoff (or even, at the very onset of the novel, of Capote) in selected segments. Rather, they seem to provide new insights into the traces of various people involved in the making of a novel – and into how some of these traces may be identified and discerned by stylometry. It is equally sensible to find such traces in a work of a very particular kind: a novel that has been reprocessed almost beyond recognition in a long process of authorial and editorial collaboration; where the final version keeps the setting and the characters of the first, but changes its focus, its historical moment in time and, perhaps more importantly, its ideological message.  ",
       "article_title":" Go Set A Watchman while we Kill the Mockingbird in Cold Blood, with Cats and Other People  ",
       "authors":[
          {
             "given":"Maciej",
             "family":"Eder",
             "affiliation":[
                {
                   "original_name":"Pedagogical University, Krakow, Poland; Institute of Polish Language, Polish Academy of Sciences",
                   "normalized_name":"Pedagogical University",
                   "country":"Mozambique",
                   "identifiers":{
                      "ror":"https://ror.org/0331kj160",
                      "GRID":"grid.442441.3"
                   }
                }
             ]
          },
          {
             "given":"Jan",
             "family":"Rybicki",
             "affiliation":[
                {
                   "original_name":"Jagiellonian University, Krakow, Poland",
                   "normalized_name":"Jagiellonian University",
                   "country":"Poland",
                   "identifiers":{
                      "ror":"https://ror.org/03bqmcz70",
                      "GRID":"grid.5522.0"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-05",
       "keywords":[
          "literary studies",
          "english studies",
          "stylistics and stylometry",
          "authorship attribution / authority",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" This paper presents a transparent and quantitative analysis of the overall development of Finnish book production between 1640-1828. The work is based on automated information extraction from library catalogues, and introduces the concept of open analytical ecosystems as a novel research tool for digital humanities. This extends our earlier pilot project on the use of the English Short Title (ESTC) catalogue ( https://github.com/rOpenGov/estc). In this new work we focus on Scandinavia, further demonstrating the potential of digitized library catalogues as a valuable resource for digital humanities and reproducible research. We continue our experimental analysis of paper consumption in early modern book production, and provide a practical demonstration on the importance of open-science principles for digital humanities.   Compared to our earlier British analysis (Lahti et al., 2015) we now integrate data across multiple library catalogues from Finland and Sweden. This analysis transcends national boundaries and brings forward key questions in metadata integration such as entry harmonization and duplicate identification. We propose a set of best practices for such tasks in automated large-scale analyses, and exemplify their use in the Scandinavian context. Such pilot project is crucial to later integrate data across the Heritage of the Printed Book database that eventually covers all of early modern Europe. Our emerging data analytical ecosystem supports these goals concretely.  Instead of ready-made standard software, such as Open Refine, Palladio, or similar user-friendly software, we have developed a set of custom tools in the R statistical programming environment to combine automation with full flexibility and access to state-of-the-art data analysis and visualization algorithms. An important contribution in comparison with related earlier work, for example Kalev’s GDELT ( http://blog.gdeltproject.org/mapping-212-years-of-history-through-books/), is that we have drastically refined the metadata, for instance by harmonizing synonymous entries and by enriching the data with external information such as name-gender mappings and geographical information. The bibliographic metadata in national library catalogues follow international standards and, as we demonstrate, the fully open source computational data analysis tools introduced within this project are immediately relevant and widely applicable in further studies based on library catalogue metadata.  We focus on the extraction and statistical analysis of library catalogue metadata to study the emergence and development of public discourse in Finland (1640–1828). The main data source for our analysis is Fennica, Finnish National Bibliography ( https://github.com/rOpenGov/fennica). This is complemented with further metadata and content analysis of Finnish newspapers and journals and material from Sweden, from the Kungliga collection, Stockholm ( https://github.com/rOpenGov/kungliga), and include comparisons with further library catalogue material from other countries as well. The analysis allows us to provide concrete, quantitative figures on publication activity, places, and topics and compare these to political, technological, and social ruptures. The quantitative analysis of print culture will allow us to study how the development of Finnish book, newspaper and journal production compares to European trends.  It is not enough, however, to see European public discourse by combining nationally organized knowledge. The hypothesis is that the European map of knowledge production will have local flavors in different corners of Europe. The aim should thus be to integrate data across library catalogues to analyze different streams of influence and varying regional perspectives and uncover potential asymmetries that may have guided intellectual life. The comparison between neighboring countries also allows for the detection of local publishing networks in the Baltic Sea region.   Fig. 1. Paper consumption of documents recorded in Fennica until 1828 by place of publication (Turku, other places in Finland and elsewhere including Sweden)  Our aim in this paper is particularly to study the development of publishing houses in Finland and their spread from Turku to other parts of Finland. We will also identify overlooked moments of transformation in public discourse in Finland by blending historical and computational approaches. The research undertaken will reflect on how social change and public discourse are intertwined, and how cultural, institutional, legal and technological changes are reflected both in publication metadata and the textual content of the publications. In terms of the historical timeframe, our study begins with the founding of the first Finnish press at the Academy of Turku in 1640, tracks the overall publishing history of the country until 1828 when Helsinki starts to play a major role in Finland. Public discourse in Finland has been largely approached from the perspective of the breakthrough of the Finnish language, the role of elite discourse at the university, early Swedish-language newspapers, and book history. We combine these perspectives, and further analyze how language-barriers, elite culture and popular debate, as well as different publication channels interacted. Large-scale quantitative analysis of library catalogues opens novel opportunities to characterize the general impact of the turn from Swedish to Russian rule in early nineteenth-century regarding public discourse in Finland. Previous historical research on the development of civility in nineteenth-century Finland has lacked appropriate quantitative tools to take an objective ‘bird-eye’ view of these complicated and crucial transformations. Questions of how, for instance, the establishment of the university in Turku/Åbo (1640), the introduction of freedom of print (1766), the formation of a Finnish Grand Duchy in the Russian Empire (1809–1812), the changes in the enforcement of censorship, the decision to transform Helsinki into a capital city (1819), the lack of estate representation in the Grand Duchy, and the slow emergence of a Finnish written language resonated in publication practices are explored from a quantitative perspective.  Our open data analytical ecosystems provide powerful and flexible data analytical tools that can best serve the needs of genuinely data-intensive research, in contrast to traditional point-and-click interfaces that are suitable for simple query tasks but not designed for fully transparent, reproducible and automated large-scale algorithmic data mining. The open source ecosystems will also enable new collaboration models around digital data collections that are now becoming increasingly available for research and other purposes. This emphasis on transparent and collaborative methodology, already wide-spread in other fields of computational science, sets the context for our work within digital humanities. Others can benefit from the new tools and the libraries from the refined data sets. The data analytical algorithms, including data extraction, statistical analysis, summarization and reporting, will be are released in full detail within a unified open source ecosystem in Github ( http://github.com/rOpenGov/fennica), where all steps from raw data to the final results can be traced back and improved further. In this sense, our emphasis on open data analytical process and collaboration model is different from Anderson's and Blanke's discussion on digital humanities ecosystems (Anderson and Blanke, 2012), which focuses on the role of the community of researchers.  This paper continues an ongoing trend of quantitative analysis of publishing history (Moretti, 2013; Towsey et al., 2015). While reuse of library catalogue data has been discussed in recent digital humanities scholarship (for example, Prescott, 2013 and Bode and Osborne, 2014), large-scale library catalogues represent a so far underestimated research resource, containing systematic information on publication activity over years, language barriers, genres, geographical regions and other variables in which the evolution of the public sphere is reflected. Moreover, our work complements the distant reading and related concepts discussed by Bode and Moretti by introducing concrete algorithms and proposing a collaborative development model. Lincoln Mullen has used a related approach to analyze historical texts ( http://lincolnmullen.com/#software). Our ultimate aim is to develop algorithms to extract, harmonize and integrate relevant metadata across the different European library catalogues, regardless of language. This research data will be further enriched by complementing it with data from auxiliary sources, such as linked data on person records, biographies of partner organizations, ontologies and other assets. The enriched information is then used to formulate statistical summaries and systematic quantitative comparisons, as well as interactive and dynamic visual representations of the publication activity, topics and geographical variation and of their evolution over time. To demonstrate the efficiency of this approach, we quantify the importance of Turku in the Finnish publication landscape during 1640-1828. ",
       "article_title":"Printing in a Periphery: a Quantitative Study of Finnish Knowledge Production, 1640-1828",
       "authors":[
          {
             "given":"Mikko",
             "family":"Tolonen",
             "affiliation":[
                {
                   "original_name":"University of Helsinki, Finland",
                   "normalized_name":"University of Helsinki",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/040af2s02",
                      "GRID":"grid.7737.4"
                   }
                }
             ]
          },
          {
             "given":"Niko",
             "family":"Ilomäki",
             "affiliation":[
                {
                   "original_name":"University of Helsinki, Finland",
                   "normalized_name":"University of Helsinki",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/040af2s02",
                      "GRID":"grid.7737.4"
                   }
                }
             ]
          },
          {
             "given":"Hege",
             "family":"Roivainen",
             "affiliation":[
                {
                   "original_name":"University of Helsinki, Finland",
                   "normalized_name":"University of Helsinki",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/040af2s02",
                      "GRID":"grid.7737.4"
                   }
                }
             ]
          },
          {
             "given":"Leo",
             "family":"Lahti",
             "affiliation":[
                {
                   "original_name":"University of Helsinki, Finland",
                   "normalized_name":"University of Helsinki",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/040af2s02",
                      "GRID":"grid.7737.4"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "information architecture",
          "information retrieval",
          "visualisation",
          "maps and mapping",
          "cultural infrastructure",
          "bibliographic methods / textual studies",
          "literary studies",
          "ontologies",
          "networks, relationships, graphs",
          "metadata",
          "geospatial analysis, interfaces and technology",
          "digitisation, resource creation, and discovery",
          "knowledge representation",
          "philosophy",
          "English",
          "interdisciplinary collaboration",
          "media studies",
          "databases & dbms",
          "programming",
          "historical studies",
          "scholarly editing",
          "multilingual / multicultural approaches"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Although representing large corpora through the network of persons’ interactions has become quite popular in the Digital Humanities community (Elson et al., 2010), several parameters can have an impact on the resulting network, especially when it is automatically extracted. In this work, we present a step-by-step procedure to extract persons’ networks from documents and select possible configurations in order to increase readability and ease the interpretation of the obtained information. We also discuss some open issues of the task. We rely on the same assumption as for word co-occurrence networks: two persons who tend to be mentioned together in a corpus share some commonality or relation from the author’s perspective.   The Methodology We implemented a novel tool for the automated extraction of a persons’ network from a corpus in the online ALCIDE platform http://celct.fbk.eu:8080/Alcide_Demo/ (Moretti et al., 2014). The module is based on the following steps: i) the corpus is first analysed with the Stanford named entity recognizer (Finkel et al., 2005), in order to recognize persons’ mentions (e.g. John Kennedy, F.D. >Roosevelt, etc.). In the network representation, we assume that persons correspond to nodes and edges express co-occurrence within a given token window; ii) We build a person-person matrix where we assign an edge weight of 1 every time two persons are mentioned together within a certain context window. Every time a co-occurrence is repeated, the weight is increased by 1; iii) The final output is a weighted undirected network where edge weights are the co-occurrence frequency. In the default configuration, name mentions are collapsed onto the same network node only if they have an exact match. In order to allow a more flexible creation of the network, a “Person Management” functionality (Fig. 1) has been implemented, through which users can collapse nodes referring to the same person (e.g. J.F. Kennedy and John Kennedy). This manual check is done through an interface, without the need to access directly the underlying matrix. From a technical point of view, the information is stored in a relational database management system, in order to grant multi-user access, good performances and high flexibility regarding the queries. The persons’ co-occurrence matrix is visualized as a network by means of the d3 javascript framework http://d3js.org. During the conversion of the matrix in the json used by d3, the nodes are enriched with additional information, such as the list of documents containing the corresponding entity and the number of connections.    Fig. 1 View of the Person Management tool   Some settings such as the co-occurrence window type (sentence or token) and width (number of sentences/tokens) are arbitrary, although they have a relevant impact on the extracted network and on its readability. Therefore, the system gives the possibility to change such settings and regenerate the co-occurrence matrix at runtime. In the following sections we will discuss some of these parameters and explain their impact in the light of a use case related to Nixon and Kennedy’s speeches of the 1960 presidential campaign. The corpus consists of 282 documents by Nixon (830,000 tokens) and 598 documents by Kennedy (815,000 tokens) http://www.presidency.ucsb.edu/1960_election.php. All networks displayed in the following sections are screenshots of the system output and are dynamically displayed.   Default configuration In our default configuration, the tool extracts persons’ networks using 1 sentence as a co-occurrence window and collapsing on the same node only name mentions with an exact match. As shown in Fig.2, this basic configuration is enough to highlight the differences between Kennedy’s and Nixon’s networks: the first is much larger and much more connected, with several cliques that tend to emerge from the overall picture. Nixon’s network, instead, is smaller (i.e. less persons are mentioned in his corpus) and less dense.     Fig. 2 Persons co-occurrence network extracted from Kennedy’s (left) and Nixon’s (right) speeches  By zooming in the pictures, it is possible to focus on single nodes of interest. For example, if we compare Nixon’s mentions appearing in Kennedy’s speeches, and the other way round (Fig. 3, left and right resp.), we observe that in both cases the opponent is frequently mentioned with ‘enemies’ of the time such as Fidel Castro and Khrushchev. However, this association with negative figures is much more frequent in Kennedy’s speeches (e.g. Nixon is mentioned also with Trotsky and Lenin), probably because Nixon had already a prominent role in US foreign policy being Vice-President.     Fig. 3 Co-occurrence network of “Nixon” mentions extracted from Kennedy’s corpus (left) and of “Kennedy” mentions from Nixon’s (right) speeches    Changing configuration parameters The tool allows users to move from the default configuration to a more customizable one, where it is possible to change the type (sentence or token) and the size of the context window taken into account for the co-occurrences as well as set a threshold to the edges’ weight (number of co-occurrences). By tuning these parameters, it is possible to transform the networks presented in Fig. 2 to obtain a more readable representation, filtering minor nodes and emphasizing information previously hidden by the large amount of information.  Reducing the co-occurrence window increases the probability to extract persons that are more strictly related. At the same time, by increasing the minimum edge weight threshold, we reduce the information visualized, filtering out all the persons co-occurring only once in favour of persons co-occurring consistently through the entire corpus.  Fig. 4 shows the networks, obtained from the data in Fig. 2, generated by setting the maximum token range to 10 (thus, on average, less than the sentence length adopted in Fig. 2) and the minimum edge threshold to 2. The result is a visualization with less but more readable data. In Nixon’s network, we can easily spot some well-defined clusters such as the one grouping the leaders of the communist world (i.e Khrushchev, Stalin, Mao Tse-tung), the cluster of the main representatives of international politics in 1960 (e.g. de Gaulle, Nehru, Adenauer) or a cluster reflecting Nixon’s attitude to refer to previous U.S. Presidents (e.g Andrew Jackson, Thomas Jefferson). Also the network from Kennedy’s corpus is more understandable, including a cluster with prominent Communist politicians (e.g. Khrushchev, Castro, Kadar), but also clusters defining local democratic representatives, for instance those from California (e.g. Pat Brown, John Moss) or those from Pennsylvania (e.g. David Leo Lawrence, Joseph S. Clark).    Fig. 4 Persons co-occurrence network extracted from Kennedy’s (left) and Nixon’s (right) speeches using a 10 tokens windows and a minimum threshold of 2.      Discussion As shown in the above examples, building a persons’ network in an automated fashion implies making some a-priori choices that strongly affect the outcome of the analysis. Such choices are influenced by the type of analysis required by the user. If distant reading is the main goal, the parameters proposed as default by our tool, as shown in Fig. 2, seem to be appropriate. This analysis gives an overview of the overall network dimension and density, and makes it possible to compare two networks at a glance. Instead, if close reading and a qualitative analysis of the connections are more relevant, reducing the window width and displaying only the most connected nodes is necessary. Since in a typical research scenario distant and close reading are both present, and users need to zoom in and out frequently, a tool that changes the network on demand in real time should be implemented. In this respect, Gephi (Bastian et al., 2009), probably the most widely used tool for network analysis in the Digital Humanities community, shows some limitations. Although it provides useful in-built metrics for analysing a network structure, it does not offer the possibility to test different parameters on the fly. Also its integration in an online, browser-based environment is quite complex, as well as the connection to a text analysis pipeline. Other issues related to the automated creation of persons’ networks are worth mentioning. Since natural language processing tools are involved in the pre-processing step, users should be aware of the possible mistakes introduced by this pipeline. In particular, Named Entity Recognizers may label as persons other types of entities, wrongly introducing nodes in the network. Other possible mistakes are more difficult to spot and concern homonyms (i.e. nodes that should be split). Such cases can be solved only resorting to a cross-document entity coreference system, where mentions can be resolved by linking them to the entities they refer to. A finer-grained outcome could be achieved integrating also an intra-document coreference system, able to link mentions referring to the same person in a text. Cross- and intra-document coreference would be necessary to ensure that all persons mentioned in a corpus are included in the extracted network. Nevertheless, the impact of automated processing on the quality of the network needs to be further investigated.  ",
       "article_title":" Building Large Persons’ Networks to Explore Digital Corpora  ",
       "authors":[
          {
             "given":"Giovanni",
             "family":"Moretti",
             "affiliation":[
                {
                   "original_name":"Fondazione Bruno Kessler, Italy",
                   "normalized_name":"Fondazione Bruno Kessler",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/01j33xk10",
                      "GRID":"grid.11469.3b"
                   }
                }
             ]
          },
          {
             "given":"Sara",
             "family":"Tonelli",
             "affiliation":[
                {
                   "original_name":"Fondazione Bruno Kessler, Italy",
                   "normalized_name":"Fondazione Bruno Kessler",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/01j33xk10",
                      "GRID":"grid.11469.3b"
                   }
                }
             ]
          },
          {
             "given":"Stefano",
             "family":"Menini",
             "affiliation":[
                {
                   "original_name":"Fondazione Bruno Kessler, Italy; University of Trento, Italy",
                   "normalized_name":"University of Trento",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/05trd4x28",
                      "GRID":"grid.11696.39"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-02",
       "keywords":[
          "visualisation",
          "software design and development",
          "data mining / text mining",
          "content analysis",
          "English",
          "networks, relationships, graphs",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"    Figure 1: The workflow using MILC for digital archiving    Figure 2: Directory structure for acquired data    Figure 3: Application Schema for Local Knowledge (ver.2015-10-11)    Figure 4: Digital Archiving of photographic dry plates    Figure 5: Brightness distribution of iPad Air       Figure 6: The resulting digital archive system    Figure 7: Handmade photo studio for Danjiri elements    Preface The declining birth rate and resulting increase in the proportion of the elderly are serious issues for contemporary Japan. The National Institute of Population and Social Security Research reported that the 2010 Japanese population of about 128,057,000 is expected to decline to about 86,737,000 in 2060 with depopulation accelerating most acutely in local regions (The National Institute of Population and Social Security Research, 2014). Currently, in 7,878 villages in Japan, over half of the population is over the age of 65. From this figure, it is predicted that 2,500 villages will vanish in the next 10 to 30 years (Ministry of Land, Infrastructure, Transport and Tourism, 2014; Masuda, 2014). Given this situation, the preservation of local knowledge is essential and standardized digital archiving methods are required. In contrast to national treasures, digital archiving for local cultural properties is limited in various respects. Digitizing devices should be versatile because the kind of materials buried is unknown at the onset of the survey. Budgets are therefore limited because most people in possession of such items do not have sufficient financial background. Therefore, the methods for archiving local items and knowledge should be as low-cost as possible. In addition to these problems, a standardized digital archiving workflow and information management rules are required to perform distributed autonomous digital archiving activities. Formulating digital archiving rules that are both open and standardized is also essential in terms of the Long Term Preservation issue (Lorie, R.A., 2001).  Responding to these issues, this paper will propose a low-cost digitizing workflow using an Mirror-less Interchangeable Lens Camera (MILC) and the information standard for local memories and knowledge with the concept of a Work-Oriented Approach (WOA) (Fujimoto 2011). The proposed methods have already been used in two different types of experimental projects and the results will also be summarized in this paper.   The digital archiving workflow The digital camera is excellent in terms of saving space, versatility, speed of digitizing and later correction, and is therefore the ideal device for basic digital archiving. In particular, the Mirror-less Interchangeable Lens Camera (MILC) has a rich choice of lenses, and is lighter and therefore preferable to than the general Digital Single Lens Reflex camera (DSLR). Figure 1 shows the workflow of constructing a digital archive using the MILC. In this figure, the workflows are denoted with UML activity diagrams, with automated activities in red, and yellow designating those activities conducted manually. Each automated activity is described by BASH scripts to ensure the clarity of specific procedures, and some call python scripts utilizing open source libraries. In practice, Graphical User Interfaces (GUI) should be provided for ordinal users.  This activity diagram is separated into two lanes, with the left lane showing the digital archiving workflow and the right lane representing the post-production and publishing workflow. In the digital archiving workflow, taking photographs, writing investigative reports, and transferring acquired images to the work station are manually conducted, whereas other activities are done automatically. To perform these automatic activities, a working directory should be defined as shown in Figure 2. The raw images acquired are automatically copied to the “raw” directory and JPEG format images are initially copied to “main”, and then the raw images are developed and saved in the “developed” directory. Thumbnails are saved in “thumbs”. Finally, unused images are moved to “sub” directories. By defining the directory structure, all projects are generalized, and creating an automatic workflow becomes much easier.     The application schema Promoting such shared and mutual use is essential and will help preserve local memories and knowledge for future generations. To do so, it is important to conform to conventional international standards (Fujimoto, 2009). The ISO 191XX series is a versatile information standard for database construction. This international standard is based on the idea of object-oriented GIS, and it defines a meta-model of geographic features and spatio-time objects. The ISO 191XX series is generally adopted for public geographic information suchas census or infrastructure data. However, it has many other possible applications. Additionally, this standard provides encoding rules by XML, and metadata, geometric information, raster format datasets and tabular form attributes can be denoted as XML elements. Therefore, this standard is also effective in terms of Long Term Preservation issues. The Figure 3 is the application schema proposed in this paper, which enables the storage of unknown tangible as well as intangible cultural properties in compliance with the standard. This application schema is based on the proposed working flow, and weighs heavily in extendibility. The fundamental classes including the  Consolidation class,  Material class,  Surface class and  DenotedSubject class are  Abstract classes, which are actually implemented in specific ways. The grey-colored classes are examples applicable to photographic dry plates and “ Danjiri” elements. The attributes for these classes are minimum essential attributes, and attributes relevant to each specific item can be defined using the Attribute class. This class specializes three classes: ThematicAttribute, SpatialAttribute and TemporalAttribute classes. Any kind of attribute can be defined using this classification.     Case studies The proposed standardized workflow and the database schema have been used in two case studies. One is the digital archiving project of dry photographic plates taken between the  Meiji period and mid-Showa period, while the other is a “Danjiri”, a traditional large wooden cart used for traditional Japanese festivals, which was completely destroyed by a flood.  In the first case a tablet device, iPad Air, was used as a substitution for a light box, and SONY A6000 was used as a digitizing device (Figure 4 and Figure 5) (Fujimoto, 2015). By using a set of these ordinary devices with three workers, more than three hundred old dry plates were duplicated in about 13 hours in total. In this project, all of the operations were composed almost entirely of open-source software, and performed in batches. Although this project was successfully completed, some improvements were later found for making linkages between the acquired images and investigative reports and application schema.  In the latter study, the digital archiving workflow and the application schema, originally designed for photographic dry plates, were modified for versatility, to make them applicable to the Danjiri elements. In this project, more than three hundred fragments of Danjiri elements were archived in one week by four workers with one day required for setting up the photographic studio, four days for taking photos, and two days for developing the digital archive system (Figure 6). In contrast to the former project, the digital archiving subjects were three-dimensional and lighting instruments were required. To achieve better results within a limited budget, a lighting studio was built using wooden blocks and domestic fluorescent lamps, which were procured on the site (Figure 7).    Conclusions In digital archiving projects focusing on important cultural properties, intended objects are well known and professional and/or specialized equipment are used to achieve with the best possible results. However, to construct digital archives of locally-kept cultural properties, versatile and inexpensive methods are required. Additionally, a standardized workflow and database schema covering various kinds of materials should be considered. In this paper, using the concept of WOA, a reasonable MILC is used as a digitizing device, and a standardized workflow enabling automation and database schema compliant with ISO 191XX are proposed. Because the proposed workflow, information schema and libraries utilize the existing international standard and open-source technologies, outcomes including metadata and source codes can be opened. This method is therefore effective in terms of the Long Term Preservation issues. These standardized methods have been tried in two different types of experimental case studies. Although both projects were successfully completed, some continual refinement is necessary to perform a fully automated workflow, especially post-production. Finally, the methods proposed in this paper can be also applicable in disaster restoration. It is important to preserve memorial items of ordinary people such as family photo albums in order to incite the energy vital for the restoration process. Unfortunately, in fact, after the Great East Japan Earthquake in 2011, a large number of mementoes and keepsakes of the local people were discarded. In such cases low-cost, swift and standardized digital archiving methods are essential.  ",
       "article_title":" Standardized Digital workflow for Archiving Local Knowledge  ",
       "authors":[
          {
             "given":"Yu",
             "family":"Fujimoto",
             "affiliation":[
                {
                   "original_name":"Nara University, Japan",
                   "normalized_name":"Nara University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/03a42q045",
                      "GRID":"grid.440917.f"
                   }
                }
             ]
          },
          {
             "given":"Yasuhiko",
             "family":"Horiuchi",
             "affiliation":[
                {
                   "original_name":"NPO “The Field”, Japan",
                   "normalized_name":null,
                   "country":"Japan",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-24",
       "keywords":[
          "software design and development",
          "data modeling and architecture including hypothesis-driven modeling",
          "standards and interoperability",
          "digitisation, resource creation, and discovery",
          "digitisation - theory and practice",
          "archives, repositories, sustainability and preservation",
          "English",
          "databases and dbms"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" This is an extension of our previous work in which a longitudinal (1989-2014) co-citation analysis of literature in Digital Humanities was (DH) conducted to explore the degree of interdisciplinarity in DH. The selection of the literature was based on a combination of keyword search with Scopus and articles published in the key journals in the field (i.e. journals published by members of ADHO). A dual trend of gradual increases in both topical diversity and network cohesion, two hallmarks of interdisciplinarity (Porter et.al., 2007; Rafols and Meyer 2010), were found in the co-citation network. For example, as shown in Figure 1, the average path length remained steady relative to the gradual increase of the network diameter, which suggests continuing growth of the literature and its gradual consolidation (the sharp rise at the tail being the artifact of citation window). On the other hand, the growing diversity was demonstrated by the steady increase of distinct author-assigned keywords over time.   Figure 1. Growing of co-citation network and average path length  In this article, the results of further analyses were reported that aimed to, firstly, further explore of the issue of disciplinary cohesion using co-authorship and bibliographic coupling networks; and secondly, to identify cohesive subgroups or specialties in the DH literature.  Disciplinary cohesion of DH Based on the assumption that the knowledge integration process in research communities depends heavily on the topology of the underlying social network, Moody used the structure of collaborative (i.e. co-authorship) network to represent the integration of knowledge in Sociology over time (Moody, 2004). The concept of “structure cohesion” or “connectivity” in network analysis was used to measure the degree of social cohesion in Sociology, which is defined as “the extent to which a network will remain connected when nodes are removed from the network (Moody and White, 2003).” Moody (2004) discussed three types of network structures: star production, small world, and structurally cohesive and surmised on the corresponding collaborative practices each represents. Moody (2004) believe that a cohesive collaboration network signals the presence of “permeable theoretical boundaries and generic methods” that allows scholars specialized in particular theoretical, empirical or theoretical skills to collaborate freely. He added that, if enough scholars engage in this kind of cross-fertilization, mixing across multiple areas, there will be few clear divisions presented in the collaborative network (Moody, 2004). Similarly, Carolan (2008) used network structure of articles published by a leading journal in Education to examine how well the heterogeneous set of ideas and practices were integrated within the discipline.  As shown in Figure 2, contrary to the co-citation network, the DH co-authorship network is very sparse and highly fragmented. The percentage of the nodes in the main component hovered only below 20% even after discounting the isolates, which is extremely low compared to other disciplines or research areas (See Table 1). Notice the contrast is especially striking with sciences, medicine, and IT. The low percentage of nodes in main component (Figure 2), coupled with extremely high clustering coefficient and modularity (Figure 3), indicated most collaboration took place at the local level, lacking global “shortcuts” found in the small-world model to hold the network together (See Figure 4).    Figure 2. Percentage of nodes in the main component in the co-authorship network     Figure 3. Trends of clustering coefficient and modularity in co-authorship     Figure 4. Part of the co-authorship network     Figure 5. Three largest components in the co-authorship network  A closer examination of the co-authorship network shows that, beyond the two largest components, there were relatively few international collaborations (See Figure 5). The largest component was composed of scholars mainly from the U.S. (28.53), Canada (27.12%), U.K. (26.84%), and Germany (10.45%); the second largest component was composed of scholars from the U.S. (39.71%), the Netherlands (30.88%), and Japan (5.88%); and the third component was composed of all Italian scholars.  The identification of specialties in DH  Modularity maximization graph partition was applied to both the co-citation and bibliographic coupling networks to identify subgroups or specialties in DH. Both co-citation and bibliographic coupling have been widely used to establish similarity or linkages between documents in bibliometrics (See, for example, Yan and Ding, 2012; Boyack and Klavans, 2010). The co-citation network was construed by using Google Scholar’s citation tracing function. The citations received by every article in our target set were first identified and downloaded then pair-wise matching was performed to identify shared citations. The bibliographic coupling was generated by pair-wise comparison of cited references retrieved from Scopus. A threshold of shared 3 citations in the reference lists was set to dichotomize the network. Figure 5 and 6 show the results of the modularity-based partition resulted from co-citation and bibliographic coupling networks, respectively. Due to the lack of global cohesion, only the giant component in either network was analyzed. We are currently in the process of identifying the research topics represented by the clusters in either network, which will be done by examining the author-assigned keywords and authors appearing in each cluster. The labeling of the clusters by each’s prominent authors has been shown to be an effective way of visualizing a field (e.g. White and McCain, 1998). Interviews with experts who have broad knowledge in the field of DH will be done in order to help us interpret the meaning of the clusters. Efforts will also be made to explore the reasons behind the lack of cohesion in the DH co-authorship network.    Figure 6. Modularity analysis of co-citation network     Figure 7. Modularity analysis of bibliographic coupling network  ",
       "article_title":"A Study of Knowledge Integration in Digital Humanities Based on Bibliographic Analysis",
       "authors":[
          {
             "given":"Muh-Chyun",
             "family":"Tang",
             "affiliation":[
                {
                   "original_name":"Dept. of Librarya and Information Science, National Taiwan University, Taiwan, Republic of China",
                   "normalized_name":"National Taiwan University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/05bqach95",
                      "GRID":"grid.19188.39"
                   }
                }
             ]
          },
          {
             "given":"Yun Jen",
             "family":"Cheng",
             "affiliation":[
                {
                   "original_name":"Dept. of Librarya and Information Science, National Taiwan University, Taiwan, Republic of China",
                   "normalized_name":"National Taiwan University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/05bqach95",
                      "GRID":"grid.19188.39"
                   }
                }
             ]
          },
          {
             "given":"Kuang Hua",
             "family":"Chen",
             "affiliation":[
                {
                   "original_name":"Dept. of Librarya and Information Science, National Taiwan University, Taiwan, Republic of China",
                   "normalized_name":"National Taiwan University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/05bqach95",
                      "GRID":"grid.19188.39"
                   }
                }
             ]
          },
          {
             "given":"Jieh",
             "family":"Hsiang",
             "affiliation":[
                {
                   "original_name":"Dept. of Computer Science and Information Engineering National Taiwan University",
                   "normalized_name":null,
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-07",
       "keywords":[
          "visualisation",
          "networks, relationships, graphs",
          "English",
          "bibliographic methods / textual studies",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" This paper presents work on documenting text reuse of fragmentary authors and of extant works. By  fragmentary we mean authors whose texts are lost and known through quotations and references by other authors. Within ancient Greek literature 60% of authors is preserved only in fragments, showing the challenge of working with innumerable pieces of reuse scattered in our textual heritage (Berti et al., 2009). This work is necessarily prior to any specific research questions. We cannot inquire into, e.g., the historical works of Istrus the Callimachean until we can comprehensively and precisely catalogue the surviving fragments of Istrus; nor can we ask “how did intellectuals in the 3rd century CE read epic poetry?”, until we can comprehensively identify instances of Homeric text reuse and work with them in their context.  The term  fragment is the result of print editorial practices, where chunks of text preserving traces of lost authors and works are extracted from their contexts and reprinted in separate collections. Even if such editorial workflow has produced invaluable results for reconstructing lost authors, the concept of  textual fragment is problematic: It includes different kinds of text reuse and implies a certain degree of originality, which is difficult to assess and represent because the original text from which the reuse derives is hidden by the  cover text, i.e., by the intention of the quoting author and the characteristics of the preserving context (Most, 1997; Schepens, 2000; Berti, 2013).  Our data model defines taxonomies of text reuse for representing references to authors and works not as separate chunks of text but as contextualized annotations, expressing their nature of reuse of textual evidence. These annotations include not only the portion of text classifiable as a reuse, but also biographical and bibliographical data preserved in the source text. Text reuse of fragmentary authors presents the challenge of documenting text aligned with no extant exemplar. Text reuse of extant works presents additional challenges of aligning as precisely as possible (but no more precisely than is possible) two or more extant passages of text that may differ in small ways or large. Our data model documents uniquely instances of text reuse and it is developed on the Canonical Text Services (CTS), which is a protocol for identifying and retrieving passages of text based on concise, machine-actionable canonical citation. It is founded on the assumption that a “text” can be modelled as “an ordered hierarchy of citation objects” (Smith and Weaver, 2009). CTS URNs can identify passages more grossly or more finely; they can identify a range of passages at various levels of specificity; by the addition of an indexed substring, a CTS URN can identify a particular string within a passage of text (Blackwell and Smith, 2012). CTS is one component of a larger digital library architecture, developed for the  Homer Multitext project and called CITE (Collections, Indices, Texts, and Extensions): http://www.homermultitext.org/hmt-doc/cite/.  In order to produce citable analyses of text reuse in their context, we have been working with the  Deipnosophists of Athenaeus of Naucratis, which is the account of a banquet where learned men quote authors and works of Greek literature concerning a wide range of topics related to dining and food. The  Deipnosophists is significant because it is a very rich collection of many different kinds of text reuse of fragmentary authors and of extant works (Braund and Wilkins, 2000; Lenfant, 2007; Jacob, 2013).  Our data model specifies four subjects of analyses:   Authors: enumerate and identify authors reused by Athenaeus;   Works: enumerate and identify works reused by Athenaeus;   Mentions: catalog every mention of authors and works in the text of Athenaeus, including his vocabulary for identifying them. For example, Athenaeus may mention that a work by Archestratus of Syracuse was known by four different names (i.e.,  Gastronomy,  Life of Pleasure,  Science of Dining, or  Art of Cooking); this would generate five entries in this list: one mention of Archestratus, and four mentions of the same work.   Reuses: uniquely identify instances of text-reuse in the text of Athenaeus.   A fifth analysis will also include the twenty-two learned men who take part in the banquet described by Athenaeus and who are actually the  characters who quote and reuse a huge amount of authors and works.  We need seven records to produce citable analyses of the above mentioned subjects:   Analysis Record URN. Every documented instance of text reuse (authors, works, mentions, reuses) has a CITE URN uniquely identifying this instance in a CITE collection.   Sequence Number. The collection of instances of text reuse is an  ordered collection; each item has a sequence number, reflecting the item’s sequence in the text of Athenaeus. This value is programmatically generated by a CTS-aware script before publishing the collection.   Analyzed Text. A CTS URN defining, as precisely or imprecisely as necessary, the span of text in the  Deipnosophists that is the subject of this analysis of text reuse. The scope of the  Analyzed Text is determined by the nature of the text reuse. In the case of authors and works, this CTS URN identifies a passage in the  Deipnosophists that serves to justify the inclusion in the respective list. When an author or a work is reused often, the passage should be a clear, unambiguous reference (e.g., “Homer says …”).   Reused Text. While the  Analyzed Text identifies a coherent and contiguous span of text, as it appears in the edition being analyzed, the  Reused Text is a string identifying only the text being reused. The  Analyzed Text provides context and a basis for alignment, while the  Reused Text gives us the flexibility to call out non-contiguous text, to normalize text, or even to promote morphological forms determined by indirected statement to those appropriate for direct speech, without doing violence to our source-edition. The  Reused Text record allows us to represent different intepretations of the same text reuse, especially in the case of non-verbatim quotations.   Alignment URN. This collection documents reuse of extant authors and works, for which we have extant editions with canonical citation. The  Alignment URN is a CTS URN pointing to the quoted extant author (identified with a CtsGroupUrn) or to one specific edition of the reused work (identified with a CtsWorkUrn) that (a) justifies our claim of text reuse, and (b) is the basis for attaching a citation of a still extant work to this analysis.   Analytical Edition URN. The collected instances of text reuse of extant work in the  Deipnosophists represent a new edition of these works, whose text-content is based on our analysis of our project’s edition of Athenaeus. The  Analytical Edition URN is a CTS URN to an  Athenaeus Edition of these works; the citation-value is based on that of the  Alignment URN; the text-content of this edition is the  Reused Text in Athenaeus. The  Analytical Edition gives us an orthogonal view of the text reuse of extant authors in Athenaeus.   CITE Collection of Lost Works. For text reuse of lost authors and works, there is no citation scheme, nor any inherent order to the text. For these, we produce a collection of text-reuse. This Collection can be cited by CITE URNs.   Initial work on documenting text reuse has been focused on references to Homer’s  Iliad in the  Deipnosophists (data available at http://digitalathenaeus.github.io/). The aim is to extend our data model including the categorization of different kinds of text reuse and further concrete examples of references to fragmentary authors and extant works in the  Deipnosophists of Athenaeus of Naucratis.  ",
       "article_title":"Modelling Taxonomies of Text Reuse in the Deipnosophists of Athenaeus of Naucratis: Declarative Digital Scholarship",
       "authors":[
          {
             "given":"Monica",
             "family":"Berti",
             "affiliation":[
                {
                   "original_name":"University of Leipzig, Germany",
                   "normalized_name":"Leipzig University",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/03s7gtk40",
                      "GRID":"grid.9647.c"
                   }
                }
             ]
          },
          {
             "given":"Mary",
             "family":"Daniels",
             "affiliation":[
                {
                   "original_name":"Furman University, USA",
                   "normalized_name":"Furman University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/04ytb9n23",
                      "GRID":"grid.256130.3"
                   }
                }
             ]
          },
          {
             "given":"Samantha",
             "family":"Strickland",
             "affiliation":[
                {
                   "original_name":"Furman University, USA",
                   "normalized_name":"Furman University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/04ytb9n23",
                      "GRID":"grid.256130.3"
                   }
                }
             ]
          },
          {
             "given":"Kimbell",
             "family":"Vincent-Dobbins",
             "affiliation":[
                {
                   "original_name":"Furman University, USA",
                   "normalized_name":"Furman University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/04ytb9n23",
                      "GRID":"grid.256130.3"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016",
       "keywords":[
          "information retrieval",
          "concording and indexing",
          "classical studies",
          "data modeling and architecture including hypothesis-driven modeling",
          "scholarly editing",
          "authorship attribution / authority",
          "English",
          "bibliographic methods / textual studies",
          "text analysis",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Dealing with historical geographic places (Southall et al., 2011) is important in museums, libraries, archives, and media companies, but challenging: 1) Historical places change in time. 2) It is difficult to understand the spatial and temporal context of the places. 3) Historical place names can often be seen only on historical maps. 4) Historical geographic data is scattered across multiple sources that can be incomplete and/or mutually conflicting. 5) To preserve semantic interoperability across Cultural Heritage (CH) datasets, there is a need to find out how the same place is represented in different repositories. 6) If a place is nowhere to be found—a situation quite common—there should be a mechanism to suggest and share new place concepts among the CH community. To tackle these challenges, we have developed a Linked Open Data brokering service model HIPLA for using and maintaining historical place gazetteers and maps based on distributed SPARQL endpoints. Using Linked Data technologies, HIPLA provides a common search interface to historical geographic data like place names with coordinates and historical maps. Contextual information, e.g. historical events or photographs related to a geographic location, is provided to help the user to gain a deeper understanding of the historical place. HIPLA also serves as a sustainable and evolving repository of historical places by implementing Dynamic Ontology Services for Evolving Ontologies (Hyvönen et al., 2015). Cultural Heritage organizations can connect their legacy cataloguing systems to HIPLA using a widget or an API in the same vain as in the ONKI ontology service (Tuominen et al., 2009). The general HIPLA model is being implemented to create and manage a national level gazetteer and map service Hipla.fi. Hipla.fi is based on four Finnish datasets in SPARQL endpoints totalling some 840,000 geocoded places, on 450 historical maps from two atlas series aligned on modern maps, and on the Getty Thesaurus of Geographic Names (TGN) SPARQL endpoint in the US. This paper first presents Hipla.fi’s user groups (section 2) and the end-user interface (section 3), complementing the crowdsourcing view to the system (Hyvönen et al., 2015). Then the system architecture is outlined (section 4), and finally lessons learned are discussed (section 5). Hipla.fi is available at  http://hipla.fi.    HIPLA user groups The audiences of HIPLA are 1) collaborative geo-ontology developers, 2) cataloguers of historical content, 3) information searchers, and 4) application developers. For group 1 HIPLA facilitates a sustainable model for aggregating historical place names in shared data repositories as time goes by. For groups 2 and 3 HIPLA provides a combination of historical and contemporary maps, linked contextual data, and semantic federated search to find and understand historical places. User group 4 can utilize distributed SPARQL endpoints, URI resolving services, and an autocompletion text search widget.   Finding and understanding historical places in context  Federated search Our first focus in developing Hipla.fi has been on modeling, storing, and searching Finnish place names in multiple SPARQL endpoints, and on displaying them on historical and contemporary maps. The datasets used are stored in separate RDF graphs, which makes it possible to offer dynamic selection of data sources for the user interface or external data consumers. Table 1 presents the datasets currently connected to Hipla.fi, most of them available on the Linked Data Finland platform  http://www.ldf.fi (Hyvönen et al., 2014).  Figure 1 depicts the Hipla.fi user interface. For finding, disambiguating, and examining historical places, there is an autocompletion search input field (a). Place names can be searched from multiple SPARQL endpoints at the same time based on the user's choice (checkboxes above (b)) with the following functionalities:  Hovering the cursor over the search results shows where the places are: the corresponding marker bounces on the map. A click on a search result label opens the info window of the place, showing its context (c). A click of the menu button on a result row (a) shows the place data in a Linked Data browser for investigating the data in detail.     Figure 1. Hipla.fi user interface    Table 1. Datasets connected to Hipla.fi  Dataset Original source Place type Size Description   Finnish Municipalities 1939–44 National Archives of Finland municipality 612 Finnish National Archives research project “Finland, prisoners of war and extraditions 1939–1955” produced a map application, from where the war time municipalities were obtained.   Karelian map names 1922–44 Jyrki Tiittanen / National Land Survey of Finland village, house, etc. 34 938 Historical places in the Karelia region of Finland and Russia.   Finnish Spatio-Temporal Ontology SeCo municipality 1 261 A spatio-temporal ontology of Finnish municipalities.   Finnish Geographic Names Registry National Land Survey of Finland 61 place types 800 000 The place name dataset comprises natural and cultural names whose spelling has been checked by the Institute for the Languages of Finland.   The Getty Thesaurus of Geographic Names J. Paul Getty Trust 1800 place types 2 156 896 TGN is a structured vocabulary containing names and other information about places. Names for a place may include names in the vernacular language, English, other languages, historical names, names and in natural order and inverted order. Among these names, one is flagged as the preferred name.   Senate atlas National Archives of Finland map  414 Series of maps of Southern Finland drawn by the Russian Army topographic troops in the end of the 19th and the beginning of the 20th centuries in scale 1:21 000.   Karelian maps National Land Survey of Finland map 47 The National Board of Survey and Topografikunta produced four-colour topographic maps in scale 1:100 000 during 1928–1951.     Map-based multiple dataset browsing If the user does not know the name of the place, but has some idea where the place is located, she can pan and zoom the map view to the area. After this it’s possible to use the “View all places on current map view” button. This way places from different datasets connected to Hipla.fi are rendered on the map, and the user can check if the place exists already in some of the datasets, and compare places in different gazetteers.   Fetch historical maps The \"Historical maps\" tab (Figure 1 (b)) provides a list of old maps that intersect the current map view. The map images are fetched from the Hipla.fi's Map Warper service  http://mapwarper.onki.fi and their metadata is queried with SPARQL from the map RDF graph of the HIPLA service. Each map has a checkbox for rendering the map on the main map view, a thumbnail image, information about map series, scale and type, and a link to view the map in Map Warper. All map series are visible by default, but with the series button it is possible to filter the maps by their series. Once one or more historical maps have been selected with the checkboxes, the opacity of the historical maps can be controlled with the slider that is located on the top right corner of the map. If the user pans or zooms the main map view, clicking the \"Refresh map list\" button updates the map list.    View contextual data When the user selects a place, contextual data (Figure 1 (c)) is provided for connecting the place to other relevant data sources. This functionality is first piloted with the spatial datasets of the WarSampo portal (Hyvönen et al., 2016), providing, e.g. 160 000 historical photos of the Second World War related to the places, and a timeline of historical events. In addition to this, the spatial perspective  http://www.sotasampo.fi/en/places/ of the WarSampo portal uses customized Hipla.fi user interface elements to visualize wartime places and their connections to other WarSampo datasets.    Extend with new gazetteers The HIPLA model is adaptable to various geographic data models and both contemporary and historical gazetteers. The only requirement is that the gazetteer is published in a SPARQL endpoint. Because there is no standard for how to express the temporal extent of spatial data, the spatial dimension of gazetteer data can be utilized in the user interface (e.g. when disambiguating place names) by individual configurations.    System architecture Figure 2 depicts the components of the HIPLA model. The Hipla.fi prototype is implemented using the Linked Data Finland platform (Hyvönen et al., 2014), based on Fuseki  https://jena.apache.org/documentation/serving_data/ with a Varnish  https://www.varnish-cache.org front end for serving the linked data. The end-user interface of Hipla.fi is a lightweight HTML5 single page map application, which provides access to multiple data sources with SPARQL queries and autocomplete search functionality using typeahead.js  https://twitter.github.io/typeahead.js/. Embedded Google Maps view is used to visualize historical places. Hipla.fi's Map Warper is an instance of the open source Map Warper tool of the New York Public Library for georectifying old maps on top of modern ones.     Figure 2. HIPLA system architecture     Related work and discussion HIPLA is an ontology library service (d'Aguin and Noy, 2012) for historical places. Complementing traditional gazetteers, HIPLA not only publishes the data for humans but also for machines using the SPARQL endpoint API. In addition, historical maps and contextual linked data about the places are provided. Thesauri of historical places, published as Linked Data, include the Getty TGN  http://www.getty.edu/research/tools/vocabularies/tgn/ of some 1.5 million records, 'Pelagios: Enable Linked Ancient Geodata In Open Systems'  http://pelagios-project.blogspot.fi/p/about-pelagios.html, and Pleiades  http://pleiades.stoa.org. Pelagios and Pleiades are based on crowdsourcing volunteers' work in ontology development. The novelty of HIPLA from a user interface viewpoint lays in the idea of combining multiple geographic data sources, offering a unified view for examining and comparing them. In addition, HIPLA makes it possible to crowdsource the creation of the gazetteer to cataloguers of Cultural Heritage content, as a side effect of their daily work, as discussed in Hyvönen et al., 2015.  The Historical Gazetteer of England's Place-names  http://www.placenames.org.uk is a service of over 4 million names that can be searched and viewed on modern maps as well as on historical ones. HIPLA has a similar local flavor focusing on places in Finland, but is based on Linked Open Data. OldMapsOnline  http://www.oldmapsonline.org is a search engine for finding historical maps covering a given area. In contrast to the systems above, HIPLA includes a map service for aligning and viewing georectified historical maps, as in the New York Public Library's Chronology of Place gazetteer  http://nypl.gazetteer.us. HIPLA also publishes the metadata of the historical maps as Linked Open Data and the dynamic and transparent selection of data sources makes it possible to understand the origins of the data.  Our research was supported by the Finnish Cultural Foundation and Wikidata Finland.  ",
       "article_title":" Contextualizing Historical Places in a Gazetteer by Using Historical Maps and Linked Data  ",
       "authors":[
          {
             "given":"Esko",
             "family":"Ikkala",
             "affiliation":[
                {
                   "original_name":"Aalto University, Finland",
                   "normalized_name":"Aalto University",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/020hwjq30",
                      "GRID":"grid.5373.2"
                   }
                }
             ]
          },
          {
             "given":"Jouni",
             "family":"Tuominen",
             "affiliation":[
                {
                   "original_name":"Aalto University, Finland",
                   "normalized_name":"Aalto University",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/020hwjq30",
                      "GRID":"grid.5373.2"
                   }
                }
             ]
          },
          {
             "given":"Eero",
             "family":"Hyvönen",
             "affiliation":[
                {
                   "original_name":"Aalto University, Finland",
                   "normalized_name":"Aalto University",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/020hwjq30",
                      "GRID":"grid.5373.2"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-11",
       "keywords":[
          "information retrieval",
          "visualisation",
          "software design and development",
          "maps and mapping",
          "archives, repositories, sustainability and preservation",
          "semantic web",
          "ontologies",
          "metadata",
          "spatio-temporal modeling, analysis and visualisation",
          "digital humanities - facilities",
          "digitisation, resource creation, and discovery",
          "English",
          "interface and user experience design",
          "digital humanities - institutional support",
          "data modeling and architecture including hypothesis-driven modeling",
          "standards and interoperability",
          "GLAM: galleries, libraries, archives, museums",
          "crowdsourcing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  IntroductionThis research was supported in part by the Ministry of Science and Technology of Taiwan, under the grant 104-2221-E-004-005-MY3. Tang (618-907 AD) and Song (960-1279) dynasties are two very important periods in the development of Chinese literary. The majority forms of the poetry in Tang and Song were Shi (詩) and Ci (詞), respectively. Tang Shi and Song Ci established crucial foundations of the Chinese literature, and their influences in both literary works and daily lives of the Chinese communities last until today. Recognizing the importance of Tang Shi, a Chinese emperor of the Qing dynasty (1644-1912), Kangxi, ordered to compile a collection of Tang poems,  Quan-Tang-Shi ( QTS, 全唐詩).  QTS contains nearly 50 thousand works of about 2200 poets. A similar effort for compiling a collection of Song Ci from the private sector began in the Ming dynasty (1368-1644), and achieved a collection called  Quan-Song-Ci ( QSC, 全宋詞) in the early Republican period of China (ca. 1937).  QSC contains around 20 thousand works of about 1330 poets. The exact statistics about  QTS and  QSC may vary slightly depending on the sources.   In the past more than a thousand years, literary and linguistic researchers have had done a myriad of research about the poetry of the Tang and Song dynasties. Hence, it is beyond our capacity and not our objective to review the literature in this abstract. Traditional researchers studied and compared poetic works that were produced by different authors and in different time periods to produce insightful and invaluable analyses and commentaries (Tao, 1999). Most of the time, the researchers focused on the poems of selected poets. Even when computing supports become available, studying poems of specific poets (Jiang, 2003) is still an important and popular type of research in poetry. Software tools facilitate the analysis of poetry from a panoramic perspective, and may lead to applications that would be very challenging in the past. For instance, Zhou and his colleagues (2009) analyzed the contents of collected couplets and Tang poems for creating couplets. Yan and his colleagues (2013) considered topic modeling in automatic composition of Chinese poetry. Lee (2012) concentrated on the linguistic analysis and teaching of  QTS.  In this presentation, we will discuss some interesting findings in some quantitative analyses of  QTS and  QSC.     Colors and Imageries Colors are an important ingredient in everyday lives, and actually carried important meanings in religion and social statuses in pre-modern Chinese societies. Wong (2011), a specialist in colors, discussed hidden meanings of colors in China in his book on “The colors of China”.  The beauty and imageries conveyed in the poetry originate from the collocations of the written words in the poems. Lo (2008) and Huang (2004) attempted to classify terms in the poetry by their semantic categories. The results can then serve as a foundation for analyzing the imageries hidden in the poems. Liu and his colleagues (2015) emphasized that colors play a crucial role in painting the imaginaries of poems: “colors in poems are like audios in movies”, and they analyzed the words related to colors in  QTS.   Using methods for text analysis, one may analyze occurrences and collocations of colors in  QTS and  QCS. The main contribution of our work will be illustrating meaningful applications of text analysis for linguistics and literary. The collocations of color words that appeared frequently in  QTS are certainly interesting (Liu, 2015). Yet, the analysis can be extended in at least two directions. First, did a poet have specific preferences on some collocations? Second, how were the collocations used by different authors?   Bai Juyi   Bai Juyi: 白居易; Liu Changqin: 劉長卿  has the largest number of works in  QTS. He used “白髮”    白髮: bai2 fa3; 青衫: qing1 shan1; 青雲: qing1 yun1; 丹砂: dan1 sha1; 青山: qing1 shan1; 白首: bai2 shou3; 紅塵: hong2 chen2; 滄洲: cang1 zhou1 ; 青春:qing1 chun1  with “青衫” 3, “青雲” 3, “丹砂” 3, and “青山” 3, and “白首” 3 with “青山” and “紅塵” 3 relatively often. Liu Changqin 2 , another important poet in the mid Tang period, used “白髮” with “滄洲” 3, and “白首” with “青山”, “滄洲”, and “青春” 3 relatively often. These different words and collocations convey the imagery of “aging”, and the variations in the word choices shed light on the subtle differences between the poets about how they expressed emotions about aging.  “白雲”   白雲: bai yun, white cloud  is a very frequent word in  QTS. Collocating with different words would create different imageries in the poems, e.g., “黃葉”   Collocations of “白雲” and “黃葉” (huang2 ye4, yellow leaves) appeared in poems of 劉長卿(4 times), 盧綸(1), 常袞(1), and 賈島(1). , “滄海”   Collocations of “白雲” and “滄海” (cang1 hai3, broad ocean) appeared in poems of 劉長卿(4), 姚合(1), 崔峒(1), and 賈島(1). , “清露”   Collocations of “白雲” and “清露” (qing1 lou4, light dew) appeared in poems of權德輿(1) and 賈島(1). , and “流水”   Collocations of “白雲” and “流水” (liu2 shui3, running water) appeared in poems of劉禹錫(1), 姚合(1), 皇甫冉(1), 皇甫曾(1), 賈島(1), and錢起(1). . Each of these collocations may brew a different scene in readers’ minds, and some of these collocations are more popular than others. It should be interesting for researchers to extract the source poems   Two examples by 劉長卿: “白雲留永日，黃葉減餘年” and “近北始知黃葉落，向南空見白雲多”.   from the  QTS to thoroughly study them.  Liu et al. (2015) reported that white (“白”) is the most frequent color in  QTS. We may check and find that red (“紅”) is the most frequent color in  QCS. It is interesting to investigate the changes (and their causes) of the popular colors from  QTS to  QCS. Again, using text analysis methods, we can find a good approximation of the trend, though obtaining the precise frequencies of the colors requires the techniques of word sense disambiguation (WSD). For instance, “金”   金: jin1, gold  could represent a material or a color, so WSD is necessary to achieve precise statistics.  In  QSC, the most frequent six colors are in the order of “紅”, “青”, “黃”, “綠”, “白”, and “碧”   白: bai1, white; 青: qing1, blue; 紅: hong2, red; 黃: huang1, yellow; 碧: bi4, green; 綠: lu4, green , while, in  QTS, the most frequent six colors are “白”, “青”, “紅”, “黃”, “碧”, and “綠”. “紅塵”, “殘紅”, “紅妝”, “紅葉”, “紅袖”, “紅日”, and “紅樓”   紅塵: hong2 chen2; 殘紅: can2 hong2; 紅妝: hong2 zhuang1; 紅葉: hong2 ye4; 紅袖: hong2 xui4; 紅日: hong2 ri4; 紅樓: hong2 lou2  are some of the most frequent red words in  QSC. The changes in the dominant colors from  QTS to  QSC may be a result of the selection process and may be a result of the cultural shift, and is an academically interesting issue to purse further.    Word Inventions and Influences Liu and Wang (2012) proposed a method to measure and compare the influences of the poems of a poet. Their methods considered whether or not the poems were selected to be included in famous collections.  While our goal is not to challenge Liu and Wang’s viewpoint, we would propose to consider also whether poets created new words that were used by later Chinese generations. Isn’t it practically meaningful and academically significant to create new words that future generations continue to use? At the time of Tang and Song, poets were at an excellent stage of the Chinese history to achieve such a cultural impact. We conduct an analysis of frequent bigrams in  QTS and  QSC, and compare the differences. Words that appeared only in  QSC are candidates of new words which were invented in Song dynasty. Words that appeared only in  QTS are candidates of words that failed to survive in Chinese language. Although this process does not really guarantee a water-proof theoretical foundation for word invention, the findings should still serve as a persuasive factor in linguistic, literary, and historical research.  Here are some of such findings. “紅塵”    紅塵: hong2 chen2; 惺忪: xing1 song1; 空門: kong1 men2; 武皇: wu3 huang2　  appeared in both  QTS and  QSC. “惺忪” 13 is a word that appeared in  QSC   For instance, in〈浣沙溪〉of周邦彦 we read “ 薄薄纱厨望似空。簟纹如水浸芙蓉。起来娇眼未 惺忪”.   but not in  QTS, and is still being used in modern Chinese. “空門” 13 is a word that appeared in  QTS but not in  QSC, and is being used in Chinese. “武皇” 13 is a word that appeared in  QTS but not in  QSC, and is not normally used in Chinese. “酴醾”   酴醾: tu2 mi2; also written as “酴醿” in  QTS   represents another type of instance. This word appeared much often in  QSC than in  QTS.    Discussions A major challenge in analyzing the words in Chinese poetry is word segmentation. Traditional experience indicates that most words in poetry consist of one or two characters (2005). Relying on this heuristic, we can algorithmically analyze the corpora containing about 4.9 million characters at least approximately. To really understand and appreciate the poetry, one should not read them verbatim. Metaphor recognition can be essential for revealing the real intentions of the poets. Observations resulting from quantitative analysis of  QTS and  QSC open windows to promising research opportunities about  QTS,  QSC, and their transition. In addition to colors, terms about astronomical objects, floral entities, meteorological phenomena, and geographical sights, are important participants in poetry. Innovative collocations of them paint impressive imageries in readers’ minds. Good computational tools can help researchers explore poets’ worlds more efficiently.    Responses to the Reviews We are thankful for the precious critiques and comments of the DH2016 reviewers. Our responses, which could not exceed 300 words, for the comments cannot fit into this final version because of length constraints, so we place our complete responses in a separate file online at < http://www.cs.nccu.edu.tw/~chaolin/papers/dh2016liu.responses.pdf>.   ",
       "article_title":"Quantitative Analyses of Chinese Poetry of Tang and Song Dynasties: Using Changing Colors and Innovative Terms as Examples",
       "authors":[
          {
             "given":"Chao-Lin",
             "family":"Liu",
             "affiliation":[
                {
                   "original_name":"National Chengchi University, Taiwan, Republic of China",
                   "normalized_name":"National Chengchi University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/03rqk8h36",
                      "GRID":"grid.412042.1"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "cultural studies",
          "corpora and corpus activities",
          "content analysis",
          "English",
          "asian studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Initiated in 2007, the project Corpus Coranicum of the Berlin-Brandenburg Academy of Sciences and Humanities aims at building a comprehensive digital information system by providing access to relevant materials for the history of the Qur’an such as digitized versions of the oldest qur'anic manuscripts and their transliterations, comparisons of variant readings for each verse, texts from the environment of the Qur’an as well as providing commentaries for each sura, taking all the aforementioned elements into account. Both, manuscript evidence and variant readings, can be seen as the foundation for a future critical edition of the Qur'an. Following the German philological approaches to the history of the Qur'an before World War II such as the “Wissenschaft des Judentums” – a reform movement founded by Abraham Geiger (1810-1874) – and Gotthelf Bergsträßer's (1886-1933) “Korankomission” of the Bavarian Academy of Sciences, the Qur'an project in Potsdam is working on implementing a sustainable solution for exploring the history of the Qur’an, this time digitally.   This information system does not confine itself to the digital reproduction of the holy text but utilizes international standards like XML, Unicode and TEI to ensure the long-term readability and archivability of the conducted research, text analyses and editorial efforts. The print edition of the Qur'an produced in Cairo in 1924 is used as a reference for the documentation of the material for the textual history, since that print had a tremendous influence on following prints during the 20 th century. following the analytical approach of Theodor Nöldeke (1836-1930), the project produces rich philological commentaries for each sura, exposing their chronological order and putting emphasis of the development of their literary forms across the 22 years of the prophet's proclamation.    Viewing the Qur’an as a text proclaimed in Arabia in Late Antiquity, the Corpus Coranicum project provides access to a collection of testimonies labeled as “Texte aus der Umwelt des Korans” (“Texts from the Environment of the Qur'an”). There, texts in Hebrew, Syriac, Greekt, Arabic, Ancient South Arabian, Ethiopian and others are being gathered, transcribed and translated, in order to highlight intersections and point out differences between and other documents from Late Antique culture, religions and traditions: Thus, the messages of the Qur'an can be viewed and understood in their respective contexts and in a new light.  Furthermore, the project gathers archeological evidence and conducts radiocarbon datings of qur’anic parchments in an ongoing German-French cooperation (2011-2014 Coranica, from 2015-2018 Paleocoran) to contribute substantially to the understanding of the Qur'an's history and the emergence of Islam. A joint goal of Corpus Coranicum and Paleocoran is to bring together all manuscripts that were originally kept in old Cairo and are now scattered around the world in a digital format for presenting them in their original form and order. On top of mere digitizations of the manuscripts, the Corpus Coranicum provides modernized transliterations of the original Arabic scripture. These transliterations are being shown in a self-developed font “Coranica” since other Arabic fonts like MS Typesetting or Amiri fail to display all the necessary characters occurring in the relevant texts of the project.   Next to commentaries, contextualization and analyzing manuscripts, the Corpus Coranicum project is building a corpus of variant readings on a word-level. Since the earliest time, the various readers of the Qur’an have recited the text in their own way. For each sura, each verse and word-coordinate in the Qur’an, the project compares the variant readings according to the written source with each other in order to show the varying tradition and interpretation of the holy text.  With the variant data accumulated so far the variances cannot only be analyzed on a word-to-word basis, but they can also be utilized to compute a general similarity measure between readers of the Qur’an by mapping the variant readings into a vector space which can be represented as a multi-dimensional matrix.  For each word occurring in Qur’an, the Qur’an matrix is being assigned a dedicated row. Each variant reading of that word at this particular sura-verse-word-coordinate creates a column in that row, assigning the variant reading as its value. The same procedure is then applied to create matrices for each reader of the Qur’an. Whereas the Qur’an matrix can have multiple non-empty values in a row, a reader matrix can only have one: the corresponding coordinate of the variant reading the reader uses at that particular sura, vers and word position. Since the cutting angle between two vectors or matrices represents the similarity to each other, the Euclidian distance (see below) is being utilized to compute that similarity measure.   All the aforementioned branches of the Corpus Coranicum project, the history of the text, the manuscripts, analyses of variant readings, the literary and chronological commentary as well as the texts surrounding and having influenced the Qur’an are bundled together to develop a new perspective on the text. The website of the Corpus Coranicum goes beyond a traditional digital edition of the Qur’an and can better be described as a digital framework or digital information system for the Qur’an, as a variety of tools and different texts are present.   The project tries to pick up and go digitally beyond where German Qur’anic science tradition has left off. On top of the content created and functionalities implemented thus far, the projects aims at extending its range of features by providing internationalized versions of the website (English, French, Turkish) as well as integrating the Rafi Talmon (1948-2004, Arabist, University of Haifa) concordance to offer chrono-morphological and statistical analyses of the holy text.   The talk will give an overview about the current state of the project, will portray philological approaches and their technical applications as well as present results of the similarity computations mentioned above.   ",
       "article_title":"Exploring The History Of The Qur’an Digitally",
       "authors":[
          {
             "given":"Tobias",
             "family":"Jocham",
             "affiliation":[
                {
                   "original_name":"Berlin-Brandenburg Academy of Sciences and Humanities, Germany",
                   "normalized_name":"Berlin-Brandenburg Academy of Sciences and Humanities",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/05jgq9443",
                      "GRID":"grid.420264.6"
                   }
                }
             ]
          },
          {
             "given":"Michael",
             "family":"Marx",
             "affiliation":[
                {
                   "original_name":"Berlin-Brandenburg Academy of Sciences and Humanities, Germany",
                   "normalized_name":"Berlin-Brandenburg Academy of Sciences and Humanities",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/05jgq9443",
                      "GRID":"grid.420264.6"
                   }
                }
             ]
          },
          {
             "given":"Oliver",
             "family":"Pohl",
             "affiliation":[
                {
                   "original_name":"Berlin-Brandenburg Academy of Sciences and Humanities, Germany",
                   "normalized_name":"Berlin-Brandenburg Academy of Sciences and Humanities",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/05jgq9443",
                      "GRID":"grid.420264.6"
                   }
                }
             ]
          },
          {
             "given":"Markus",
             "family":"Schnöpf",
             "affiliation":[
                {
                   "original_name":"Berlin-Brandenburg Academy of Sciences and Humanities, Germany",
                   "normalized_name":"Berlin-Brandenburg Academy of Sciences and Humanities",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/05jgq9443",
                      "GRID":"grid.420264.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "concording and indexing",
          "hypertext",
          "classical studies",
          "databases & dbms",
          "archaeology",
          "corpora and corpus activities",
          "philology",
          "programming",
          "data mining / text mining",
          "near eastern studies",
          "content analysis",
          "English",
          "multilingual / multicultural approaches",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Jonathan Edwards (1703-1758) is generally considered the most eminent and versatile thinker in early American history. His impact on the shaping of the theological thought and the preaching tradition of the colonial period was profound and long-lasting. Today he remains one of the best studied figures of the American past and different elements of his impressive output are continually reprinted by both academic and commercial publishing houses. Over his life Edwards authored more than a thousand sermons, hundreds of letters and a number of theological treatises. The Jonathan Edwards Research Center at Yale University edited and published most of these texts in their complete form as  The Works of Jonathan Edwards, led by Harry Stout as general editor and Kenneth Minkema as executive editor. The series of almost thirty volumes is described by Phillip Gura, a former editor of  Early American Literature, as the “most important editorial project in American cultural history in the past 50 years” (2004, 149). Edwards’ life is so well documented that there are hardly any stones unturned in the life of the Northampton divine. Especially, his relevance for the events of the Great Awakening, a powerful social-religious movement of colonial America, underwent close scrutiny and the most notorious sermon of America, “Sinners in the Hands of an Angry God” which he authored, has been the studied linguistically, rhetorically and stylistically.   Like most people of his age, Edwards was a diligent diary-keeper and an avid letter-writer. His private texts offer a comprehensive insight into his daily struggles and ambitions – in consequence, the very writing and publishing process of his texts is relatively well documented. Yet, surprisingly – in spite of such extensive research conducted upon Edwards – the relationship between him and Thomas Foxcroft (1697-1769), his editor and literary agent has not been extensively studied.  Foxcroft was a minister at First Church in Boston, Massachusetts and Jonathan Edwards's ally in the pro-revival debate. Their collaboration began most probably in 1849; Edwards had great trust in his erudition and skill to carry out the authorial intent expressed point-by-point in his commentaries to the suggestions of corrections. Foxcroft sometimes included Edwards's correction verbatim, exactly as indicated by the author, at other times, he paraphrased them, while preserving the author's thought. Edwards entrusted Foxcroft with the editing, the correction and the publication process – as he writes in a letter sent from Stockbridge – a small mission he was sent to after the dismissal from his own parish of Northampton: “I should be glad that you would endeavor that this book may be printed in a pretty good paper and character, and may be printed correctly, and that particular care may be taken that the printer don't skip over a whole line as they sometimes do. And if the bookseller can be agreed with to let me have a number for the copy, it would be pleasing”. (30 June, 1752). Edwards also consulted Foxcroft about the correctness of his interpretation of other authors: \"(…) it is very difficult, and almost impossible, for another to enter into all the views of a writer, or to know everything he has in view in all that he says; and therefore a little variation of sentiment, may much thwart and disappoint his design, insensibly to another. But this I should take as a very friendly part and much desire, that if you observe, that in any instances I have mistaken Mr. Williams' meaning, and misrepresented him, or in any respect injured him (…)\" (30 June, 1752). The extent to which the style of the editor (whose idiosyncratic style can be described on the basis of numerous publications he himself authored) permeated the author's writings in this case has not been determined. The influence of Foxcroft's thought and style in Edwards's writings seems to be potentially very strong and demands close investigation and the stylometric approach seems a most fitting tool to be employed for such a study.  The analysis was performed with two quantitative methods: frequencies of most frequent words were compared between the texts using the Delta procedure (Burrows 2002); then, an analogous procedure (this time using Support Vector Machines) was used to look for traces of the editor’s signal in consecutive segments of several treatises by Edwards (“rolling.classify,” Eder 2015a). The analyses were performed with  stylo (Eder et al. 2013), a package for R, the statistical programming environment (R Core Team 2014), postprocessed with Gephi network analysis software (Bastian et al. 2009).  A general view of stylometric similarities and differences between the writings of Edwards and Foxcroft is presented in the network diagram in Fig. 1. It shows, above all, a good separation of the signal of the two preachers, especially when Edwards’s spiritual texts; sermons, treatises and Biblical comments are concerned, these, in turn, exhibit a degree of separation by subgenre – as opposed to Foxcroft’s generally more uniform stylometry.   Figure 1. Network analysis of texts by Edwards (red) and Foxcroft (green).  In the more detailed search for the editor’s signal with the “rolling.classify” method, longer texts by Edwards, i.e. his treatises, were compared against his own signal averaged over the rest of his  oeuvre and against that of Foxcroft, bearing in mind the suggested caesura of 1749. Sure enough, consecutive segments of Edwards’s works written before that date exhibited no traces of the editor (as exemplified by Fig. 2), and then surfaced in a series of works (as visible in Fig. 3). Interestingly, the editor’s signal disappears again in 1758, the year of Edwards’s (not Foxcroft’s) death.    Figure 2. Consecutive segments of Edwards's  Mind (1723); throughout the work, Edward's signal (red) dominates over the (absent) signal of Foxcroft.     Figure 3. Consecutive segments of Edwards's Humble Inquiry (1749); in many other fragments, dominated by Edwards (red), Foxcroft's impact is still visible. The lower band shows the strongest signal; the upper, the second strongest.  These results have two significant consequences. The first is that we have now produced a quantitative confirmation of the extent of collaboration between two major colonial authors. But the fact that the quantitative agrees so well with the qualitative (or historical) evidence also shows that editorial traces can indeed be found with stylometry, perhaps to a greater degree than we might have anticipated. Acknowledgements This research is part of project K/PBO/000331, supported by Poland’s National Science Center. ",
       "article_title":"Jonathan Edwards and Thomas Foxcroft: In Pursuit of Stylometric Traces of the Editor",
       "authors":[
          {
             "given":"Michał",
             "family":"Choiński",
             "affiliation":[
                {
                   "original_name":"Jagiellonian University, Krakow, Poland",
                   "normalized_name":"Jagiellonian University",
                   "country":"Poland",
                   "identifiers":{
                      "ror":"https://ror.org/03bqmcz70",
                      "GRID":"grid.5522.0"
                   }
                }
             ]
          },
          {
             "given":"Jan",
             "family":"Rybicki",
             "affiliation":[
                {
                   "original_name":"Jagiellonian University, Krakow, Poland",
                   "normalized_name":"Jagiellonian University",
                   "country":"Poland",
                   "identifiers":{
                      "ror":"https://ror.org/03bqmcz70",
                      "GRID":"grid.5522.0"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-02",
       "keywords":[
          "literary studies",
          "english studies",
          "stylistics and stylometry",
          "authorship attribution / authority",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Up-to-date research in the humanities today depends as much on digital methods and digital data. However, the use of computer-based methods and online sources in the humanities still faces several challenges, including the difficulty of ensuring the longevity of research data, the lack of common basic services, inadequate standardisation of data formats, insufficient training in digital methods and best practices, and weak international Digital Humanities networks. Digital documents are accumulated, organised and annotated using electronic databases. However, the necessary infrastructure is most often established in a project- specific way and is not designed for the long-term preservation of data. After the completion of a research project, these digital resources quickly become unavailable if they, and the software and hardware they rely on, are not properly maintained. Keeping digital data accessible after the end of a project is costly in terms of money and labour and is usually not included in the project funding. While the digitisation of analogue sources produces large numbers of digital documents, these documents usually have a simple structure. By contrast, the data produced during the research process is much more complex, consisting of interlinked information (databases, annotations etc.). Because of the complexity of this research data, it is very difficult to make it permanently available. However, there are several reasons for doing so:  Transparency: As research data is the foundation on which published results are based, it be- comes necessary to have access to this data in order to evaluate the results.   Reuse: New research projects can reuse existing research data to propose different answers to the same questions, or to ask entirely new questions, especially if the datasets from different projects can be linked.   Citability: Digital sources may only be referenced in scientific texts if they can be accessed permanently without modification. The long-term accessibility of arbitrary digital objects (together with permanent links and unique object identifiers) is usually not possible.    Organisational form The Swiss Academy of Humanites and Social Sciences (SAHSS) therefore decided to establish in collaboration with the Digital Humanities Lab (DHLab) of the University of Basel a new “national research infrastructure” (Data- and Service Center for the Humanities, DaSCH) which takes this kind of digital research data into custody and preserves the direct online access. The primary goals are:  Long-term curation of research data  Permanent access and reuse  Services for researchers to support data life-cycle management   The secondary goals are:  Promoting the digital networking of databases created in Switzerland or in other countries  Carrying out a pilot project in close proximity to humanities research  Collaboration and networking with other institutions on developing digital literacy  During a pilot phase lasting two years that ended in July 2015, the data of about 25 different research projects ranging from ancient history to musicology have been passed to new institution for preserving long term accessibility. In order to copy with such heterogeneous data, the platform has to be extremely flexible and versatile. Since Switzerland is a highly federalist country, a balance between a central/decentral approach had to be chosen. We decided to form of a network that currently consists of several “satellite” nodes and a central office which acts as coordinator, main provider of technology and software development. The individual locations have a great deal of freedom to take local decisions (e.g. which research projects are considered important to be included in the platform). At each satellite location, it is necessary to have both a broad knowledge and experience available in humanities research as well as in IT and software development skills. The central office provides second-level support.   Technological issues Our daily experience seems to suggest that digital data is quite volatile and unstable. Everybody who works with computers on any scale has suffered the unfortunate experience of data loss. In a recent interview, Vincent Cerf, often regarded as one of the \"fathers of the internet\", says he is worried that all the images and documents we have been saving on computers will eventually be lost: \"Our life, our memories, our most cherished family photographs increasingly exist as bits of information – on our hard drives or in \"the cloud\". But as technology moves on, they risk being lost in the wake of an accelerating digital revolution.\" (Cerf, 2015) Thus, it appears that \"long-term archival\" and \"digital\" are diametrically opposed concepts. However, the digital domain offers some unique characteristics that allow the long-term preservation of digital data. However, guaranteeing long-term access to digital information remains a tedious and difficult process. There are only a few fundamental methods for long-term preservation of digital data:  Emulation The software and to some extent the hardware of obsolete computer system can be emulated (\"simulated\") on modern computers. Thus data can be rendered using vintage software.   «Eternal» media The «eternal» media approach requires the digital data to be recorded onto the most robust and durable media available.   Migration In the context of long term archiving, migration is defined as the process of periodically copying digital data onto new, up-to-date storage media and, if required, converting the file formats to new, well-documented standard formats.  The OAIS reference model for a digital archive is based on the migration model. In addition to a formal process description, it also covers the ingest of data into the archive and the dissemination of archived data to a user. An important aspect of the OAIS reference model is the systematic approach to metadata that is distinguished between the metadata required to identify and find a «document», and the technical metadata required for the management of the migration processes. The OAIS approach can be adapted for complex «objects» such as relational databases or NoSQL-databases (e.g. using the SIARD-suite (Ohnesorge, 2015), a standard adopted by European PLANETS project and as Swiss eGovernment Standard eCH-0165), however in order to browse or use the data, the whole dataset has to be retrieved from the archive and converted back into a working RDBMS using the SIARD-Suite – a «quick overview» is not possible. Complementary to the OAIS archival process model,  keep-alive archiving keeps a system of data, data management and access methods online and permanently up-to-date. Whenever the technology evolves (e.g. a new stable version of the data management software or a new version of a file format is released), the whole system is migrated to conform to the new environment. The keep-alive archives are especially well suited to complex data such as databases which are accessed very frequently. However, there two fundamental problems with keep-alive archives:  If the data management system does not offer a method to record all changes, the history will be lost.  It is virtually impossible to keep each projects IT-infrastructure – especially the software – running forever. Each project uses its own software (Filemaker Version XY, MySQL, PHP, ruby, Excel, etc.) and data models. The adaption to the evolving technology would overwhelm each institution. The DaSCH implements a modified keep-alive concept. It has chosen to use the Resource Description Framework (RDF, standardised by the W3C) as a common ground for representing the data. It provides a very simple but highly flexible representation of digital information. RDF allows the definition of ontologies which formalise the semantic relationship of digital objects. We defined a base ontology which implements some required basic concepts (e.g. timestamp based versioning, annotations, access rights etc.). Starting from this base ontology, for each research project taken into custody a specific ontology is being derived. On delivery of the data, the original data structure is translated into this ontology preserving the important features and relationships of the data. This technological framework thus allows the «simulation» of almost any data models (relational databases, XML hierarchies, TEI-encoded texts, graph networks etc.) in a common infrastructure using open standards such as RDF, RDFS   RDS-Schema for expressing simple ontologies.  and OWL   Web Ontology Language for expressing complex ontologies and relations. .  The pilot phase has made it clear that project-specific access applications (such as online graph- ical user interfaces) have to be preserved. While this approach does not make it possible to directly reuse the original applications, it has been shown that is easy to re-implement their basic functionality as well as their look and feel.  Using the common platform, it is straightforward to create new tools and applications that reuse existing data by combining information from different datasets. Thus, new research methods can be implemented, e.g. using methods of «big data» analysis  Due to the success of the pilot phase where about 25 projects have been integrated, some with individual user interfaces, the Academy has decided to ask for funding. The request is awaiting the approval of the swiss national parliament.  ",
       "article_title":"Implementation of a National Data Center for the Humanities (DaSCH)",
       "authors":[
          {
             "given":"Lukas",
             "family":"Rosenthaler",
             "affiliation":[
                {
                   "original_name":"University of Basel, Switzerland",
                   "normalized_name":"University of Basel",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s6k3f65",
                      "GRID":"grid.6612.3"
                   }
                }
             ]
          },
          {
             "given":"Beat",
             "family":"Immenhauser",
             "affiliation":[
                {
                   "original_name":"Swiss Academy of Humanities and Social Sciences",
                   "normalized_name":"Swiss Academy of Humanities and Social Sciences",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/005381c03",
                      "GRID":"grid.458385.7"
                   }
                }
             ]
          },
          {
             "given":"Peter",
             "family":"Fornaro",
             "affiliation":[
                {
                   "original_name":"University of Basel, Switzerland",
                   "normalized_name":"University of Basel",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s6k3f65",
                      "GRID":"grid.6612.3"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "information architecture",
          "linking and annotation",
          "digital humanities - institutional support",
          "databases & dbms",
          "internet / world wide web",
          "digital humanities - facilities",
          "standards and interoperability",
          "archives, repositories, sustainability and preservation",
          "English",
          "publishing and delivery systems",
          "semantic web"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction  The ancient Maya civilization flourished from around 2000 BC to 1600 AD and left a great amount of cultural heritage materials, in the shape of stone monument inscriptions, folded codex pages, or personal ceramic items. All these materials contain hieroglyphs (in short glyphs) written on them. The Maya writing system is visually complex (Fig. 1) and new glyphs are still being discovered. This brings the necessity of better digital preservation systems. Interpretation of a small amount of glyphs is still open to discussion due to both visual differences and semantic analysis. Some glyphs are damaged, or have many variations due to artistic reasons and the evolving nature of language.    Figure 1. A stone inscription found in Pomona, Tabasco (Mexico), Panel 1 from 771 AD (Photograph by Carlos Pallán Gayol for AJIMAYA/INAH Project© 2006, Instituto Nacional de Antropología de Historia, Mexico)   Signs following ancient Mesoamerican representational conventions end up being classified according to their appearance, which leads to potential confusions as the iconic origin of many signs and their transformations through time are not well-understood. For instance, a sign thought to fall within the category of 'body-part' can later be proven to actually correspond to a vegetable element (a different semantic domain). Similarly, several signs classified as 'abstract', 'square' or 'round' could actually be pars-pro-toto representations of a larger whole.    Figure 2. Maya glyph samples from several categories (according to Thompson's catalog) that illustrate the within-class variety and between-class similarity    Fig. 2 illustrates the challenges to analyse Maya glyphs visually. Adding functionalities that take context (i.e., co-occurrence statistics, characteristics of the data) and part-whole relations (i.e., highlighting diagnostic parts) into account would bring guidance during decipherment tasks. The tools we envision are different from existing almanac-by-almanac visualization systems (Vail and Hernandez, 2013). They are also more engaging for users (i.e. visitors in museums), and offer promising perspectives for scholars.  This motivates the study of data visualization. In this paper, we built a prototype for visualization of glyphs based on visual features. We introduce (1) an approach to analyse Maya glyphs combining a state-of-the-art visual shape descriptor, and (2) a non-linear method to visualize high-dimensional data. For the first component, we use the histogram of orientation shape context (HOOSC) (Roman-Rangel et. al., 2011a; Roman-Rangel et. al., 2011b; Roman-Rangel et. al., 2013) which has similarities to other descriptors of the recognition literature (Belongie et. al., 2002; Dalal and Triggs, 2005; Lowe, 2004), but is adapted to shape analysis (Franken and van Gemert, 2013).   For the second component, we use the t-distributed Stochastic Neighbourhood Embedding (t-SNE) (Van der Maaten and Hinton, 2008), which is a dimensionality reduction method from the machine learning literature that has value for Digital Humanities (DH), as it can highlight the structure of high-dimensional data, i.e., multiple viewpoints among samples. As analysis of DH data is often based on attributes like authorship, produced time, and place, observing these variations as smooth transitions with t-SNE becomes a relevant feature.   We show that the proposed methodology is useful to analyse the extent of spatial support used in the shape descriptor and to reveal new connections in the corpus through inspection of glyphs from stone monuments and glyph variants from catalogue sources. In particular, we hope that the presentation of our use of t-SNE can motivate further work in DH for other related problems.   Methodology    Figure 3. Overall flow for visualization with t-SNE    The analysis process is illustrated in Fig. 3. First, for each glyph, a standard visual bag-of-words representation (BoW) is computed from the HOOSC descriptors. Second, dimensionality reduction is performed on the BoW representation of a glyph collection to generate the visualization. The main steps are described below.   Datasets We analyse our visualization pipeline on two individual Maya glyph datasets.  Monument data    Figure 4. Sample glyph images, corresponding Thompson annotations, and syllabic values (sounds) of selected 10 classes from the syllabic monument glyph dataset   We use a subset (630 samples from 10 classes, Fig. 4) of hand-drawings (Roman-Rangel et. al., 2011), corresponding to syllabic glyphs inscribed in monuments. These samples are collected by archaeologists (as part of Mexico’s AJIMAYA project) from stone inscriptions spread over four regions (Peten, Usumacinta, Motagua, and Yucatan). As an additional source, around 300 glyph samples are taken from existing catalogues (Thompson and Eric, 1962; Macri and Looper, 2003).    Thompson catalogue  Secondly, we use 1487 glyph variants cropped from the Thompson's catalogue. These variants belong to 814 categories and divided as main sign and prefix/suffix groups in the catalogue.    Visual feature representation    Figure 5. HOOSC computation at a sample position of the shape    The HOOSC is a shape descriptor proposed in our research group for Maya glyphs (Roman-Rangel, 2011b). It is computed in two main steps (Fig. 5). First, the orientations of a set of sampled points are computed. Secondly, for a given sampled position, the histogram of local orientations are computed using a small number  Na of angle bins forming a circular grid partition centred at each point. The HOOSC descriptor is obtained by concatenating all histograms, and applying per-ring normalization. Basic parameters are the spatial context  sc defining the extent of the spatial partition; the number of rings  N r; and the number  N s of slices in a ring. With  N a =8,  N r =2,  N s=8, HOOSC has 128 dimensions. We have used HOOSC for usual retrieval and categorization tasks (Hu et. al., 2015).    Dimensionality reduction: t-SNE  Proposed in (Hinton and Roweis, 2002), SNE is a non-linear dimensionality reduction method. It relates the Euclidean distances of samples in high-dimensional space to the conditional probability for each point selecting one of the neighbours. In t-SNE (Van der Maaten and Hinton, 2008), these distributions are modelled as heavy-tailed t-distributions. t-SNE aims to find for each data point, a lower-dimensional projection such that the conditional probabilities in the projected space are as close as possible to those of the original space (measured with KL divergence (Kullback and Leibler, 1951)).  In our application, first, we project the BoW representation to a 30-dimensional space using PCA, then applied t-SNE to these projections to get 2-dimension mapping. t-SNE keeps track of the local structure of the data as it optimizes the clusters globally.    Results and discussion  The full-scale visualization of the glyphs are available at  .   Glyph monument corpus structure    Figure 6. Monument data: t-SNE plots with visual representations obtained at four different spatial context levels    Fig. 6 shows the monument corpus. The region encoded in the visual descriptor varies from almost whole glyph (sc=1/1) to small local parts (sc=1/8). One question is how spatial context influences visualization of the representation. Regarding the visual clusters, with the most global representation (sc=1/1), our method extracts more distinct clusters, e.g. T229 and T126 in Fig. 7 (navy and magenta in Fig. 6 and 9). Please see Fig. 9 for roughly-coloured clusters of the glyphs. As the descriptor gets more local, the categories with common patterns mix up (Fig. 6). Yet, our method is able to capture meaningful common local parts and maps the samples based on these elements, i.e. parallel lines, hatches, and circles.     Figure 7. Monument data: Close-up of two clusters (T229 on the left and T126 on the right), corresponding to navy and magenta clusters in Fig. 6 with the most global HOOSC descriptor (sc=1/1)    For Maya epigraphers in our team, a more neatly differentiated grouping of signs, e.g. obtained by HOOSC with sc=1/1 is preferable. However, work on the effects of parameter choice is required to obtain groupings that make more epigraphic sense. Clearer 'borderlines', less 'outliers,' and less 'intrusive' signs (e.g. T25 and T1) within each cluster would be desirable. Our results in this regard are preliminary, but they open promising research questions.    Figure 8. Monument data: Close-up of two clusters (T59 on the left and T116 on the right), which exhibit smooth transition between samples corresponding to place or temporal variations    Another important epigraphic point is that we observe interesting visual transitions between samples of the categories. Fig. 8 shows examples from category T59 and T116, which illustrate a smooth dilation of samples in one direction. These kind of observations are interesting for archaeologists, since they might correspond to modification of the glyph signs over time or place.    Figure 9. Monument data: Visualization of all class samples with the most global HOOSC descriptor (sc=1/1)      Glyph variants from Thompson catalogue     Figure 10. Catalogue data: A visual cluster of main signs from the Thompson's catalogue, with the most global HOOSC descriptor (sc=1/1). Many of them are impersonated main signs that corresponds to gods or animals. In this part of the visualization, the upper left part has more visually complex variants than the rightmost samples   From the visualization of glyph variants in Thompson's catalogue with the largest spatial context level (sc=1/1), we observe that visually similar categories are grouped together, while exhibiting smooth transitions. These transitions may correspond to some characteristics of the data. Fig. 10 shows a cluster of personified main signs in which degree of visual internal detail decreases in the indicated direction. We also observe separate visual clusters for hatched, horizontal and vertical glyphs.     Conclusion  Our goal in this study is to help DH scholars to visualize data collections not as isolated elements, but in context (visually and semantically). Even though early catalogues are built based on visual similarities, i.e., (Thompson and Eric, 1962) or (Zimmermann, 1956) relied on graphic cards to study similar patterns, the categorization methods were poorly understood and were not easy to reconfigure.  Furthermore, due to the limited knowledge at the time about semantics and sign variants, these catalogues turned out to be inaccurate or outdated. Similarly, Gardiner’s list (Gardiner, 1957) is insufficient to elucidate sign variability in the 'Book of The Dead' (Budge, 1901).  With the proposed tool, however, considering details at different scales as semantic/diagnostic regions in the visualization can help archaeologists to discover semantic relations. In this way, overlapping notions such as 'colours', 'cardinal directions' and specific toponyms from earthly, heavenly or underworld realms can be studied in greater detail.  Finally, illustrating all variations with different visual focus in a fast and quantitative manner brings out the characteristics of signs. This also helps experts match samples from various sources (i.e. monuments, codices, and ceramic surfaces) to corpus data more efficiently; and trigger the decipherment of less frequent and damaged signs. Hence, our work is a step towards producing a more accurate and state-of-the-art sign catalogue.   Acknowledgements This work was funded by Swiss National Science Foundation as part of the MAAYA project.   ",
       "article_title":" Ancient Maya Writings as High-Dimensional Data: a Visualization Approach  ",
       "authors":[
          {
             "given":"Gulcan",
             "family":"Can",
             "affiliation":[
                {
                   "original_name":"Idiap Research Institute and EPFL, Switzerland",
                   "normalized_name":"Idiap Research Institute",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/05932h694",
                      "GRID":"grid.482253.a"
                   }
                }
             ]
          },
          {
             "given":"Jean-Marc",
             "family":"Odobez",
             "affiliation":[
                {
                   "original_name":"Idiap Research Institute and EPFL, Switzerland",
                   "normalized_name":"Idiap Research Institute",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/05932h694",
                      "GRID":"grid.482253.a"
                   }
                }
             ]
          },
          {
             "given":"Carlos",
             "family":"Pallan Gayol",
             "affiliation":[
                {
                   "original_name":"Abteilung für Altamerikanistik und Ethnologie, University of Bonn, Germany",
                   "normalized_name":"University of Bonn",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/041nas322",
                      "GRID":"grid.10388.32"
                   }
                }
             ]
          },
          {
             "given":"Daniel",
             "family":"Gatica-Perez",
             "affiliation":[
                {
                   "original_name":"Idiap Research Institute and EPFL, Switzerland",
                   "normalized_name":"Idiap Research Institute",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/05932h694",
                      "GRID":"grid.482253.a"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-01",
       "keywords":[
          "visualisation",
          "archaeology",
          "English",
          "image processing",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Research This presentation will focus on the development of the Online-Archive “Forced Labor 1939-1945. Memory and History”.  The collection of narrative interviews was compiled in 2005 and 2006 by the Institute of History and Biography at FernUniversität Hagen. In a joint project, the Foundation “Remembrance, Responsibility and Future”, the Freie Universität Berlin, and the German Historical Museum aim to safeguard and provide easy access to these multilingual audio and video interviews and accompanying materials for research and education. The online archive contains 583 comprehensive life story interviews with concentration camp survivors, prisoners of war, and “civilian” forced laborers. In 27 countries, mainly in Central and Eastern Europe, 192 video and 391 audio interviews were conducted in the native languages of the witnesses. Each interview is accompanied by additional material: a short biography, a transcript of the interview, a translation of the transcript into German, a table of contents showing the structure of the interview, additional photos and documents, as well as basic biographical information. All content is accessible worldwide for any users who registered with the site. So far there are no standards on how to document and index Oral History Collections and we will show examples of different approaches which are used at the moment. In this context we will describe our indexing method, with its internal working interfaces and the process involved, as well as the public online application and its functionalities. We will present the different functionalities (content-based indexing, full-text search and an interactive map application) that enable a targeted search that leads directly to individual passages of the interviews.     We will also discuss considerations involved in designing an online platform to avoid the use of the interviews as a mere quotations quarry and instead supports a comprehensive understanding of the whole testimony in its narrative structure and its biographical meaning. An annotation function will be presented. The function is meant to benefit from the specific knowledge of users to add to the understanding of the interviews.  Finally, the archive has been designed multilingually and runs in German, English, and Russian in order to accommodate the needs of a greater international audience. This presentation doesn´t focus on a special research problem. Instead it shows a powerful tool which enables academics to work effectively with testimonies to answer their own research questions.    Education The online-platform aims to support education as well, and there is the option to give an overview of our approaches in this context, too. We created an online-learning-environment for the use in the classroom which will be available from the beginning of 2016. Based on short biographical films (half an hour) and additional provided material (maps, documents, photographs, additional films) students are asked to work on a number of didactically framed tasks. Most tasks are historical in nature and working with them is useful for the teaching of history. But also for other school subjects exercises are available as for example in language education or religious instructions. The pupils are asked to write their answers in an online-editor and combine them with selected materials which are easy to import. The results can be saved and printed individually.   Teachers have additional options and can for example create their own questions based on the films and the materials.  The software offers versions of the learning environment for different countries. These versions vary in language and content and are drawn up by experts in the respective countries. From the beginning of 2016 a Czech and a German version are available. A Russian version is on the way.  The online-learning-environment is responsive and can be used with different mobile devices.     Links http://www.zwangsarbeit-archiv.de https://lernen-mit-interviews.de/  ",
       "article_title":" The Online Archive \"Forced Labor 1939-1945. Memory and History\". A digital Application for Research and Education ",
       "authors":[
          {
             "given":"Doris",
             "family":"Tausendfreund",
             "affiliation":[
                {
                   "original_name":"Freie Universität Berlin, Germany",
                   "normalized_name":"Freie Universität Berlin",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/046ak2485",
                      "GRID":"grid.14095.39"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "digital humanities - multilinguality",
          "digital humanities - pedagogy and curriculum",
          "teaching and pedagogy",
          "concording and indexing",
          "linking and annotation",
          "maps and mapping",
          "digital humanities - facilities",
          "historical studies",
          "scholarly editing",
          "content analysis",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" As a subfield of the wider Digital Humanitiesdigital humanities, Digitaldigital hHistory is concerned with the incorporation of digital methods in historical research practices. Digital hHistory thus aims to do historical research using methods, concepts, or tools from other disciplines, making it a form of  methodological interdisciplinarity (Klein, 2014){Formatting Citation}. However, how this interdisciplinarity affects the practices of Historyhistorians, on the methodological and the epistemological levels, remains underexplored. The PhD research presented in this paper aims to address this question by investigating the interdisciplinary interactions in which historians take part.  Three forms of interaction are of interest for this research, which are not necessarily an exhaustive list of digital history interactions. These forms are not mutually exclusive, but occur interchangeably and simultaneously, or one form could lead to another:  Digital history as collaboration with, among others, the computer science discipline.  Digital history as end-users of tools.  Digital history as building tools independently.  In order to look into such interactions, this PhD research will employ Galison’s concept of  trading zones, described as “an arena in which radically different activities could be locally, but not globally, coordinated” (Galison, 1996, p. 119).  When different groups interact with one another over a period of time in a trading zone, it is likely that the two groups will influence one another through  acculturation: “the process by which the beliefs and practices of one community diffuse across the boundaries of another and subsequently alter the second community’s practices and interpretations” (Barley et al., 1988). At the community level, acculturation involves changes of social structures, institutions, and cultural practices. At the individual level, it involves the behavioural repertoire of a person. By studying the acculturation of practitioners of digital history, as individuals and in groups, we may get a view of the types of trading zones and how these change over time. To model the different types of trading zones, we use three dimensions based on research by Berry (1997, 2005) and Collins et al. (2007):   Contact & Participation, i.e., how the two groups meet.  Cultural Maintenance (from homogeneous to heterogeneous), i.e., how the two groups define themselves and to what extent they aim to maintain their identity. On this scale, more homogeneous means the two groups become more alike to form a single group, while more heterogeneous means they remain two distinct groups.  Coercion (from collaborative to coercive), i.e., what the power relations in the trading zone are. On this scale, more collaborative means the two groups are both acting out of free will, while more coercive means one group is imposing practices upon the other.   The concept of trading zones has been used before to describe the digital humanities field. McCarty (2005) argues that humanities computing should rather be seen as a third space, neither belonging to one group nor the other, rendering it no longer a trading zone. However, in the terminology of Collins et al. (2007), this would constitute a collaborative-homogeneous trading zone, termed an inter-language. Svensson (2011, 2012a,b) suggests digital humanities is a collaborative-heterogeneous, termed fractioned, trading zone; a meeting place of two groups. Klein (2014) also describes digital humanities as a fractioned trading zone, and, like Svensson, emphasises that this may lead to a shared language, or jargon, between the different communities. Hunter (2014), without employing the concept of trading zones, describes digital humanities as a bridge or translation between two cultures, which we can describe as a collaborative-heterogeneous trading zone, termed interactional expertise. Rieder and Röhle (2012) use the concept to argue however that not the language should be central, but the interactions on the level of methodology, where not the terminology but the method itself is negotiated. In contrast to these authors, Mounier (2015) contends that there is a coercive political dimension underlying the field, which in the terminology of Collins et al. would suggest that digital humanities constitutes a coercive-heterogeneous, termed enforced, trading zone. This is not to say that this is how digital humanities will always be, but Mounier argues this should be better understood before we can move further and perhaps diffuse new digital methods into the wider humanities. However, what is striking about these discussions of digital humanities as trading zones is that very little research into the  local practices has been done, with the exception of Hunter (2014) who does not actually employ the concept of trading zones. Instead, digital humanities is discussed as a global phenomenon; this is in contrast with the original use of the concept by Galison as described above. This paper aims to reintroduce the concept of trading zones to describe local phenomena of digital humanities.  To this end, this paper investigates local manifestations of trading zones in digital history using the three dimensions described above. The analysis focuses on the first form of interaction described above, collaboration, and is based on interviews with practitioners of digital history, i.e., historians, computer scientists, and other collaborators, focusing on the diverse aspects of interdisciplinary collaboration. The interviews cover five distinct subjects, which together give an insight into the trading zones from each interviewee’s perspective. The first subject is that of  boundary work, concerning how practitioners characterise their own discipline. Moreover, it is of interest how practitioners characterise the other disciplines in the collaboration. In previous research involving students of journalism, a lack of understanding what computer science is appeared to result in disinterest and performance anxiety (Cook, 2015). Furthermore, this subject covers the extent to which the practitioners aim to have their digital history research meet their discipline’s values. This subject thus not only works towards the Cultural Maintenance dimension, but also already gives hints towards the Coercion dimension regarding how interested practitioners are in the collaboration with the other discipline.  The second subject is the  practice of research, the research activities. This concerns a description of their research, both within and outside the trading zone, and the tools potentially used at different steps in their process.  The third subject concerns their  incentives for practising digital history. In previous research on the collaboration between earth scientists and computer scientists, it was found that the two groups had different incentives for the collaboration (Weedman, 1998). This difference introduced difficulties for the collaboration and impacted the understanding of the other discipline. This subject thus works towards the Coercion dimension.  The fourth subject concerns the  organisation of the collaboration. This concerns how often the groups meet, and where they are located, e.g., is it a collaboration between different departments at different places (or in different countries), or a sharing of office space. This subject thus works towards the Contact & Participation dimension.  Finally, the fifth subject concerns the  epistemological positions. A criticism in the digital humanities debate is the incorporation of different epistemological positions such as positivism or objectivism in humanities scholarship (Drucker, 2011, 2013). It is therefore of interest whether practitioners in the trading zone (unconsciously) shift their epistemological position. A first question concerns their practice of reasoning; do they reason in a research question-driven deductive way, a more data-driven inductive way, or an abductive way to try to discover patterns (Dixon, 2012)? Other questions are related to epistemological positions. Roth and Roychoudhury (1994) developed a short qualitative questionnaire which allows to describe interviewees as more objectivist or more constructivist. Thus, this subject provides insight into the acculturation of practitioners, and works towards the Cultural Maintenance and Coercion dimensions on the epistemological level.  This paper will present preliminary findings of interviews held for this PhD research, focusing on the aspects of  incentives and  organisation. We will present a preliminary taxonomy of collaborations on the Contact & Participation dimension, and describe several digital history interactions on the Cultural Maintenance and Coercion dimensions. With these results, we aim to gain a better understanding of how digital history works as an interdisciplinary interaction, and how this impacts the practices of the involved groups and individuals.  ",
       "article_title":"Trading Zones of Digital History",
       "authors":[
          {
             "given":"Max",
             "family":"Kemman",
             "affiliation":[
                {
                   "original_name":"University of Luxembourg, Luxembourg",
                   "normalized_name":"University of Luxembourg",
                   "country":"Luxembourg",
                   "identifiers":{
                      "ror":"https://ror.org/036x5ad56",
                      "GRID":"grid.16008.3f"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "English",
          "digital humanities - nature and significance",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" In the late 19th century, thousands of industrially produced consumer items flooded into extraterritorially governed, internationally regulated, Chinese, treaty port cities. Foreign commodities were products, and formed the backbone of new, urban, popular consumer culture. Consequently, the advertising industry infiltrated commodity brands and branding techniques into everyday life making commodity images a paramount symbol of civilized urban life. Advertising ephemera thus provides researchers with the conditions for thinking about modernity par excellent since it breaks data free of its origins to demonstrate how concepts embedded in ads ingratiate all consumer cultures (Barlow, 2012).  To force advertising to speak clearly, we launched the Chinese Commercial Advertisements Archive (“CCAA”) and ‘metadated’ (Lev Manovich’s term) more than ten thousand high quality images from microfilm copies of three, major, commercial, Chinese newspapers, in the period of 1880 to 1940 (Manovich, 2002). CCAA applies customized metadata schema based on the structural standard, Dublin Core, to each digital image of advertisement, entering all relevant information e.g., cartoon, brand icon, word texts and syntax, plus street names and business titles. Our metadata include: descriptive content, contextual information, bibliographical, technical and image sources of location, copyright status, and owning institution.  Scholars had already studied categories like hygienic/卫生), modern/现代, human/人类, eugenics /优生, and female/女性in commercial/common ideas. They sampled image-based advertisements in libraries using newspapers, facsimiles and microfilm/fiche. Though more recent research projects have done a poor job of digitizing advertising, still we cannot ignore ad digitalization because historians are still generalizing from a fraction of ads that make up any potential archive. To avoid wasting time and to collaborate with other scholars developing what Franco Morreti calls ‘distance reading,’ we seek to connect concepts appearing in advertisements to concepts found in sociological texts employing statistical text mining of advertising copy (Hayles, 2012).  Space prevents a full literature survey here, but we have met with pioneering researchers Professor Peter Bol of ‘China Biographical Database Project’ at Harvard University and Professors Zheng Wenhui and Liu Zhaolin, co-PIs, ‘Database for the Study of Modern Chinese Thought and Literature (1830–1930) ’at Taiwan National Chengchi University, and now have available over 30,000 annotated ad images which our proposed paper will use to augment evidences and expand analysis. On the basis of these 30,000 annotated, newspaper, advertising images, our work is generating a text-mining model for advertising language, a language presenting historically anchored technical difficulties as follows:  Lack of word boundaries and punctuation. Word boundaries in Chinese are invisible; worse, ad slogans are not punctuated. Raw data is just a sequence of unsegmented Chinese characters which means text mining in Chinese is comparatively tough. Lacks definitions of vocabulary. Ad texts contain lots of instable, idiosyncratic technical terms, like company names written in different ways, transliterated brand names, product names and so on that we discover during the text mining process. Lack of training data. Most Chinese text mining methods depend on high quality training data, and will fail if the target texts are remarkably different from the training data. Considering that the advertisements that interest us are from regional newspapers over a long period, ad writing style is uncertain due to local linguistic differences. We cannot rely on current training data employing modern Chinese to establish models for mining 1920s syntax, vocabulary, punctuation (or lack of it) word use, semantic references, ideograph variation for 100-year-old print media.  Difficulties distinguishing technical and background words. Ad texts are a mixture of technical and background phrases, so it is not a trivial task to distinguish technical terms, our true interest , from noise, words rarely used a century after the ads were published.  We have overcome many of these roadblocks using statistical methods for Chinese text mining and knowledge discovery. Text mining allows us to: 1) discover potential associations among features and terms extracted from advertisements; 2) build links among these and ideological trends in the treaty port urban areas of China during our period by developing Deng Ke’s statistical text mining method to establish indices of technical terms (“TT”) and metadated association patterns among technical terms (“APTT”) (Deng, Geng and Liu, 2014). Word Dictionary Model (“WDM”) and Advanced Word Dictionary Model (“AWMD”) are tools for word discovery, text segmentation and entity recognition of Chinese texts when training data are not available. WDM can be extended into an AWDM to achieve automatic recognition of TT (distinguishing technical terms from background words/phrases). In this case, technical terms mean the specific phrases we choose from datasets or metadata of images, and establish as concepts in the network. To this purpose we are developing the following indices: 1) Bibliographical (volume, issue, page numbers, location, date) to enable statistical analysis of ad publication frequency in one or several newspapers over the course of one or many years. 2) Contextual Informational (brand, product category, company, agency, retailer’s address, registered nationality) allowing users to establish a statistical picture of a commodity, in specific newspapers, geographical locations and decades. 3) Content index (sorting by drawing of male, female, elders, youth, middle age people, infant, human, animal, plant, Chinese, foreigner) meaning ad images are hybrid artifacts, mixing text and cartoons; 4) Theoretical categories (the modern, human, woman) to identify categories used aggressively in ads. Once TT in each and every advertisement have been successfully located and the indices of TT identified, we can reveal the APTT of ads, defined as subsets of technical terms that tend to co-occur in an advertisement frequently. With TDM, association pattern discovery can be converted into a statistical inference problem and solved by statistical means. Second, we seek concept networks that connect key concepts embedded in ads to sociological theories. The Concept Network (CN) is a graph that can efficiently present domain knowledge and reasoning based on it. Each CN node is a concept corresponding to an entity or a technical term. Thus if concept A appears in the definition of concept B, we add a direct link from A to B and domain topology will eventually reflect the structure of the knowledge system: closely related concepts are direct neighbors or locate in the same neighborhood, while concepts belonging to different disciplines or areas will be far away from each other in the graph. Building CN requires indices of concepts and their descriptions. Traditional dictionaries might be a source and our period shows an efflorescence of dictionary publication. Another source is online knowledge databases, like Wikipedia. However domain knowledge of sociological theories are not represented in any language or in any period anywhere on the World Wide Web. To compensate, we are erecting an ontological knowledge database of sociological theories as these appeared in journals, books, articles and the archived documents, ‘Social Thought in Modern China, 1830–1940’ (STMC). With an ontological database, we can open our sharing platform to define and describe key concepts and relationships among them. Interdisciplinary by design CNMACMS users are welcome to participate by entering data into the databases to improve our model. Notes As for the studied have been done by schoalrs on categories like hygienic/卫生), modern/现代, human/人类, eugenics /优生, and female/女性in commercial/common ideas, please see Jin Guantao(金观涛), Liu Qingfeng(刘青峰),  Studies in History of Idea: The Building of Basic Political Concepts in Modern China ( 观念史研究:中国现代重要政治术语的形成), Falv Press, 2009; This book has investigated the origins and transformations of tern basic concepts of “gonghe” (republicanism), “minzhu” (democracy), “quanli” (rights), “geren” (individual), “geming” (revolution), “kexue” (science) in modern Chinese history by using the data of “Database for the Study of Modern Chinese Thought” (1830–1930). Huang Kewu (黄克武), “從申報醫藥廣告看民初上海的意料文化與社會生活1912–1926” explored the idea of “disease” in advertisements published on  Shen Bao during early 19 th century. Tani Barlow’s published papers and book,  In the Event of Women (Durham: Duke University Press, 2017) establish a historical parallel connecting advertisement ephemera, social theory and the woman category.   ",
       "article_title":"Concept Modeling the Advertising Chinese Modern Society",
       "authors":[
          {
             "given":"Tani",
             "family":"Barlow",
             "affiliation":[
                {
                   "original_name":"Rice University",
                   "normalized_name":"Rice University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/008zs3103",
                      "GRID":"grid.21940.3e"
                   }
                }
             ]
          },
          {
             "given":"Jing",
             "family":"Chen",
             "affiliation":[
                {
                   "original_name":"Nanjing University",
                   "normalized_name":"Nanjing University",
                   "country":"China",
                   "identifiers":{
                      "ror":"https://ror.org/01rxvg760",
                      "GRID":"grid.41156.37"
                   }
                }
             ]
          },
          {
             "given":"Ke",
             "family":"Deng",
             "affiliation":[
                {
                   "original_name":"Tsinghua University",
                   "normalized_name":"Tsinghua University",
                   "country":"China",
                   "identifiers":{
                      "ror":"https://ror.org/03cve4549",
                      "GRID":"grid.12527.33"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-14",
       "keywords":[
          "data mining / text mining",
          "historical studies",
          "English",
          "asian studies",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  What is Sustainability? Sustaining the results of digital humanities research projects remains an ongoing challenge within the wider DH ecosystem. This is particularly the case for research infrastructure developments, where the scale, complexity and indeed the overarching aim of the project to serve a potentially still emerging research community makes their continued accessibility even more important, and even more difficult.   Like any enduring challenge, sustainability of complex digital resources has been subjected to a certain amount of scholarly investigation (though less overall than one might expect). What a survey of this literature shows, however, is that the perspectives on how to face the challenge of sustainability are highly dependent on how a project or infrastructure views itself: for example, when viewed primarily as a  form of organisation or institution, the sustainability model proposed will likely focus on the necessary ‘business model’ for maintaining the services created (Maron et al., 2009). The Archives Portal Europe (APE http://www.apex-project.eu/index.php/en/), for example, established itself as a Foundation after the end of the project, in order to maintain the vital functions of the infrastructure and to further connect with other projects and similar initiatives. Alternatively, when project outputs are viewed as a  tool or technical platform, a sustainability proposal will primarily take into consideration issues and practices such as migration and curation of elements such as the repositories where the data are stored and continued maintenance of work environments and specific tools. The TextGrid project https://textgrid.de/, for example, has been hugely successful in rolling its activities forward over a long period, continuing to make its services available to users. At its best, this approach results in a broad focus on software durability, documentation of processes and the modularity of services (Buddenbohm et al., 2015). But to understand sustainability thoroughly we must also engage a second huge and unresolved issue in digital humanities, that is the reuse of project outputs and data. In particular, the “Log Analysis of Digital Resources in the Arts and Humanities, or the LAIRAH project (Warwick et al., 2008) has contributed significantly to our understanding of what factors enable digital projects and tools to be found and adopted by users. From the results of this project we can see another model for the sustainable project to emerge, in which the  communication and branding of the project is a key element of its success.    The CENDARI Project and its Approach to Sustainability This presentation brings forward the hypothesis that a successful approach to sustainability for Research Infrastructures needs to be comprehensive; an approach that doesn’t just consider data or technology, community, communications or processes, but in fact all of them simultaneously.  In addition, it should focus not only on a project as a collection of tangible and intangible assets, but also on the potential user base for these assets, and what these users consider valuable about them. Discussion of this user-centred approach to sustainability will be based on the experiences of the Collaborative Digital Archival Research Infrastructure (CENDARI http://www.cendari.eu/) project’s year-long sustainability planning exercise, conducted from January 2014-January 2015.  This exercise, which built upon previous work in the project and a strong link to the Digital Research Infrastructure for the Arts and Humanities (DARIAH ERIC https://www.dariah.eu), resulted in a set of principles and processes for mapping and sustaining user value from the project for the medium and long terms.  Although both the generic process (which will be released as a sustainability toolkit at the end of the project) and the specific actions implemented by the project match on some level the specifics of the CENDARI development, they also reflect the reality, identified by Joris Van Zundert, of the “fluidity” of research infrastructure, caught up in both the digital information lifecycle and the creation of knowledge by end users, as well as the software components (Van Zundert, 2012).   The Sustainability Planning Cycle The CENDARI sustainability planning process was comprised of a series of 4 stages, from pre-planning to closure and post-project actions, each of which contributed to the overall, holistic sustainability strategy.  This cycle was intended to counteract a natural impetus within projects to view sustainability as a concern only for the final phase of the project, rather than one to be integrated into the project’s development and even its conception.     The CENDARI Assets and Multilevel Sustainability Action Plan As a key component of the second and third phases of the CENDARI project sustainability planning process, the project carried out a thorough audit (including a stakeholder validation meeting) to refine its understanding of what assets the project had generated and how they could be maintained, shared and indeed passed on to its key users for further development.  This process identified 7 categories of assets as most likely to find future usage, each of which posed unique challenges in how they could be captured, made visible and sustained.  It has been one of the greatest challenges of the CENDARI sustainability planning process to ensure that for each of these areas we could find a solution, as we would for our personal work data, to make them findable and reusable in a contextualised manner, and preserve them in ‘multiple formats and multiple locations.’  For each asset type, the audience for potential future use is different, and therefore the solution proposed is as well. The CENDARI  portal is the most visible of its assets, representing the final synthesis of the project’s activities and its main point of access.  For many projects, this would be where sustainability planning would not only begin, but end.  CENDARI approached this sustainability challenge via a three pronged strategy, guaranteeing 3 years of access through the German arm of DARIAH but also ensuring new communities and new approaches would be recruited to continue development.    But the portal is not only useful in its complete final form, but also as a collection of unique  services, tools and components optimised to support DH research.  This possible reuse of the project outcomes was foreseen from the beginning, and a very modular, service oriented architecture was adopted for the project.  The tools therefore require a sustainable pathway outside of the portal.  That said, however, connecting tools with potential user bases is a constant challenge.  The software community practice of using GitHub to share software was adopted, but further awareness raising was also required to ensure the maximal future use for the tools.  CENDARI holds a lot of  data from different sources, some unique to the project, others well signposted elsewhere, and with different requirements and expectations for sustainability. This has been its legacy as a project seeking to reuse archival data for historical research, where the culture and ability to share data is unevenly developed. The CENDARI data portal gives access to this data, and the project’s data agreement and license have been developed with DARIAH as a co-signatory, so in many ways DARIAH had already agreed from an early point in the project to sustain this data.  But DARIAH is not well-known as a data provider or source, and this solution alone may not maximise visibility and reuse.  Therefore a redeposit protocol for unique data with an external trusted source has also been facilitated.  The  Archival Research Guides exist as a particular subset of the data unique to CENDARI, but their status as both primary and secondary research sources justifies their consideration as an asset class in themselves, in particular because of the manner in which they challenge existing norms of publication, communication and evaluation in the discipline of history.  As extended and enhanced publications, incorporating analysis, links to data sources, multimedia objects, and links to project ontologies, these guides need to be delivered within the project portal.  But to sustain these unique works of scholarship only in that format would again potentially limit their visibility.  They will therefore be offered in one or more export formats, as well as becoming the focus of both a review publication and a research paper to be submitted to a mainstream (not digital) historical journal.  In this way, their contributions to scholarship can be recognised as independent from the format in which they have been delivered.  Given how particular many of the project experiences in building for the DH community had been, a specific audit of CENDARI’s  tacit knowledge was also undertaken, and several white papers and process oriented toolkits have emerged from the project on the foot of this (including, for example, a ‘White Book of Archives’ documenting the project’s experience of federating highly heterogenous data from traditional collection holding institutions).   As a related issue, some of the project’s  management assets may also have a future utility for others.  Perhaps the least easily defined and sustained aspects of the CENDARI project will be the  communities - mixed and homogenous groups of historians and other humanistic scholars, collections experts and technologists - it has brought together and formed.  Interconnectivity between cognate projects will be a key resource for this, as some communities will have interests across these projects, and networks can and should be shared.  But some of the community aspects are very unique to CENDARI, and will have a specific role in guiding the future use of the portal and its components: for this reason, the project will use another DARIAH mechanism, the working group, to provide a structure for continued development of project concerns and assets.  As can be seen from this description, CENDARI has made both its own sustainability and the potential future role of the DARIAH ERIC in the sustainability of medium- to large-scale digital projects in Europe into key areas of applied research and development.  The resulting tool-kit for sustainability will hopefully assist future projects in extending both their sustainability planning and strategies in the future.     ",
       "article_title":"Toward A Use-Value Paradigm For The Sustainability Of Digital Research",
       "authors":[
          {
             "given":"Francesca",
             "family":"Morselli",
             "affiliation":[
                {
                   "original_name":"Data Archiving and Networked Services (DANS), Den Haag, The Netherlands",
                   "normalized_name":"Data Archiving and Networked Services",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/008pnp284",
                      "GRID":"grid.500519.8"
                   }
                }
             ]
          },
          {
             "given":"Jennifer",
             "family":"Edmond",
             "affiliation":[
                {
                   "original_name":"Trinity College Dublin, Ireland",
                   "normalized_name":"Trinity College Dublin",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/02tyrky19",
                      "GRID":"grid.8217.c"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-28",
       "keywords":[
          "GLAM: galleries, libraries, archives, museums",
          "user studies / user needs",
          "archives, repositories, sustainability and preservation",
          "English",
          "publishing and delivery systems",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction ‘A hype already’, was the Friday 28 January 2011 headline on the first page of the Dutch newspaper  NRCnext. ‘It’s called the literary rat race K2, the simultaneous publication of Herman Koch’s  Zomerhuis met zwembad and Kluun’s  Haantjes’. The front-page story continues with literary critic Arjen Fortuin presenting an analysis of the two novels that were published the week before, both written by well-known Dutch authors and with the amazing first print runs of 80,000 (Kluun) and 100,000 (Koch) copies. These two novels are from totally different authors, who began their careers on opposite sides of the literary spectrum. However, Fortuin states, they seem to be converging. Koch started out as a ‘literary’ author not selling very well, but with his last book before  Zomerhuis met zwembad ( Summerhouse with Swimming Pool),  Het diner (2009, translated into English as  The Dinner, 2012) he turned to a wider audience, thus - according to literary critics - severely damaging his literary reputation. Kluun (the one-word alias of Raymond van de Klundert) started out as writing popular fiction with no literary pretentions at all with his much read but openly despised  Komt een vrouw bij de dokter (2003, translated as  Love Life (2007), under the name Ray Kluun). Quite unexpectedly, on 23 January 2011, Kluun’s new novel  Haantjes (which could be translated as ‘Alpha-males’) got a positive review from prominent literary critic Arjan Peters in  de Volkskrant. Fortuin found this an additional reason to compare the two novels. His conclusion is: ‘It’s an uneven literary match – Kluun plays in a lower league – but the commercial battle of K2 could be a close tie – although here Koch also seems to have the best chances: the Alpha-males are in fact very light-weight’.    The Riddle of Literary Quality Both novels are on the list of 401 novels analyzed in the project  The Riddle of Literary Quality ( http://literaryquality.huygens.knaw.nl/). The aim of the project (running until 2017) is a stylistic analysis of novels in Dutch and to compare this analysis with readers’ opinions. The corpus was based on a list of most sold and most lent titles in The Netherlands from 2010 to 2012, excluding titles first published in Dutch before 2007. It includes Dutch originals (such as the novels by Kluun and Koch) and translations. The list contains a lot of genre fiction such as thrillers, ‘literary thrillers’, and chick-lit, and many titles the publishers categorized as ‘literary novels’, among which the K2 titles (Koch’s  Zomerhuis met zwembad and Kluun’s  Haantjes). In 2013, in an online survey titled  Het Nationale Lezersonderzoek (‘The National Reader Survey’) we asked a wide audience of readers to indicate which of the 401 novels they had read, and for a smaller set of these novels how they rated them on two scales: one on the scale of literariness, from 1 (not literary at all) to 7 (highly literary) and one on the scale of general quality, from 1 (very bad) to 7 (very good). In total, 13,782 respondents completed the survey. By combining these data with an analysis of stylistic characteristics of the novels we expect we can discover which textual features may play a role in the current Dutch conventions of literariness. In this paper I will compare the K2 novels both in an exploratory analysis of the survey results on the scale of literariness and in stylometric analysis. Do the opinions of the K2 readers agree with the stylometric picture we get?     Zooming in on style ‘Style is a property of texts constituted by an ensemble of formal features which can be observed quantitatively or qualitatively’ (Herrmann et al. 2015). For the K2 case, I use the Stylo package in R (Eder et al. 2013). This R package is mainly used for authorship attribution. It compares texts based on the frequencies of a range of words or characters. Since words give more insight into the texts themselves than characters, Stylo can also be used for literary analysis beyond verifying authorship. In Fig. 1 I used Stylo to measure the distance between the ten ‘literary’ novels by male authors with the highest scores for literariness and the ten novels that got the lowest scores. The bootstrap consensus tree (a harmonization of cluster analyses) was based on the most frequent 100, 200, 300 etc. words (MFW) until 1000 MFW. There is a complete distinction between the “HIGH” and the “LOW” group.   Fig. 1  However, the ten novels with the highest scores are all written by men and the ten with the lowest scores by women. The “HIGH” novels are labelled by the publishers as ‘literary novel’ and the “LOW” ones are mostly marketed as genre fiction such as chick-lit. The graph therefore probably does not distinguish literary quality but genre.  I now zoom in on the ‘literary novels’. The Riddle corpus contains 96 titles labeled as ‘literary novel’ written by a male author or by only male co-authors, and 66 by a female author or only female co-authors. For the whole set of 401 novels, 191 are written by male and 196 by female authors or co-authors, which shows female authors are not underrepresented in the corpus. In Fig. 2 all ‘literary novels’ are categorized according to the mean scores for level of literariness. The general trend seems to be that the respondents see the female authors ‘playing in a lower league’ than the male authors. Riddle-PhD-students Corina Koolen and Kim Jautze will deal with gender issues in detail in their dissertations.   Fig. 2 Literary novels (originally Dutch and translated into Dutch)  I will leave the gender topic to my PhD-students to publish about, and I will for now limit my presentation to an analysis to novels written by male (co-)authors. The lowest score on the level of literariness was 3.1, for two titles written by the Dutch author (and sports reporter) Mart Smeets. Kluun’s novel  Haantjes is directly above these two novels, with a score of 3.5.  Zomerhuis met zwembad clearly did better. It got a mean score of 5.1. The highest score is 6.6, for Julian Barnes’  Alsof het voorbij is ( The sense of an ending). Where Barnes’ novel ranks first on the list of 96 novels, the new Koch ends up at rank 74 and the new Kluun at 94. The respondents of  The National Reader Survey thus agree with Fortuin that Koch ranks higher on the scale of literariness. From the list of 96 novels I select thirty novels: ten with the highest (including Barnes), ten with the lowest (including Kluun), and ten with intermediate scores for literariness (including Koch). If we use the same settings in Stylo as above, the 30 novels end up as shown in Fig. 3.     Fig. 3  The results are not very clear. The groups “HIGH”, “MIDDLE”, and “LOW” do not have their own specific clusters. For now, we can conclude that literary quality for this corpus does not reside in shared word frequency patterns. We are currently gathering as many measures of linguistic features as possible to apply these to the selected corpus. One of the assumptions to test is whether the scores for literariness correlate with features that relate to linguistic complexity. A suite of tools for Dutch is currently being developed. A simple test using HyperPo does show that some features normally related to the level of difficulty of a text need further inspection (Fig. 4, 5, 6) ( http://tapor1.mcmaster.ca/~sgs/HyperPo/).    Fig. 4    Fig. 5    Fig. 6  In each of these figures, the 30 novels are represented on the x axis arranged from highest score for literariness to lowest. Koch is at data point 20 and Kluun at 28. Fig. 4 shows that the scores for literariness display a trend that is opposite to the average frequency of words – this suggests that for this corpus, lexical density scores perhaps play a role in what makes a novel literary or not. However, Kluun’s score below the trend line is close to that of the top-5, while Koch is on the opposite side of the trend line. Fig. 5 highlights that mean word length does not resonate with scores for literary quality. Fig. 6, however, shows a clear (statistically significant) trend of literary score and average words per sentence. Here, Koch and Kluun do not differ very much. This sneak preview (which will be tested with tools finetuned for Dutch) shows a further analysis of complexity issues is promising. Riddle PhD student Andreas van Cranenburgh is working on syntactic markers of literariness, and all three PhD students have looked into topic variation in the Riddle corpus (see the abstract they have submitted for DH2016).   Conclusion So how do the novels by Kluun and Koch compare? The match is still undecided. We need an analysis of many more linguistic features before we can draw conclusions. This analysis did show in which directions to look. For now, stylistically they are playing in the same league.   ",
       "article_title":"A literary rat race",
       "authors":[
          {
             "given":"Karina",
             "family":"van Dalen-Oskam",
             "affiliation":[
                {
                   "original_name":"Huygens ING, The Netherlands / Universiteit van Amsterdam, The Netherlands",
                   "normalized_name":null,
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-07",
       "keywords":[
          "literary studies",
          "stylistics and stylometry",
          "authorship attribution / authority",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Overview Social Networks and Archival Context (SNAC), initiated in 2010 as a R&D project, is now being transformed into an international cooperative. SNAC’s original research objective was to demonstrate that descriptions of people, embedded in the descriptions of historical records that document their lives, could be extracted and used to reveal the social networks within which their lives were lived  and provide integrated access to geographically dispersed historical records. SNAC’s early success led to plans to establish a sustainable international cooperative to maintain and expand these descriptions of people. The long-term technological objective is a platform to support a continuously expanding, curated corpus of reliable biographical descriptions of people linked to, and providing contextual understanding of, the historical records that function as primary evidence for understanding their lives and work. The SNAC Cooperative will benefit librarians, archivists, and researchers, and will provide traditional historical researchers integrated access to distributed historical records and the social contexts within which the records were created and used. It will provide prosopographic researchers with methods for reconciling and establishing reliable social networks, and will enable archivists and librarians to share descriptive data while also making descriptions more effective.    The Archival Description Source Data  Archival description source data encompasses both descriptions of historical records as well as authority data for corporate bodies, persons, and families documented in historical records. OCLC WorldCat, sixteen archival consortia (representing hundreds of individual repositories), over thirty repositories, and two digital humanities research projects contributed their source data to SNAC. The holdings of over 4000 repositories are represented:  190,000 finding aids, contributed by fifteen consortia and over thirty repositories in the U.S., the ArchivesHub in the U.K., and the Bibliothèque nationale de France (Catalogue Collectif de France (CCFr) and BnF archives et manuscrits) 2.25 million OCLC WorldCat archival descriptions  400,000 authority records contributed by NARA (93,051), the British Library (297,731) the Smithsonian Institution Archives (2,083), the New York State Archives (258); and the Archives nationales, France (2,350) 30,000 correspondent names from the Joseph Henry Papers Project, Smithsonian Institution Archives  2,332 correspondents from The Walt Whitman Archive  1,200 names associated with the Chaco Research Archive     Data Processing During SNAC’s R&D phase (2010-2015), this source data was processed in three distinct steps.   Biographical and historical data was extracted or migrated from existing archival descriptions and assembled into standardized descriptions that identify and document organizations, persons, and families based on an international archival communication standard, Encoded Archival Context – Corporate Bodies, Persons, and Families (EAC-CPF). Each EAC-CPF identity description includes the description of the entity as such (names, life dates, biographical information, etc.), links to descriptions of the historical records from which the data was derived, and links to other identities found in the same source. These links provide the foundation for assembling a vast social-document network or graph.    The EAC-CPF identity descriptions were matched (identity reconciliation) against one another and against descriptions in the Virtual International Authority File, combining records that identify the same entity, to produce a set of unique EAC-CPF records.    We developed a prototype access system, based on Extensible Text Framework (XTF), open source software from the California Digital Library. It has three major functional components: 1) display of the EAC-CPF records; 2) sophisticated searching and exploration of the EAC-CPF records; and 3) exposing the data to enable third-parties to access and use it in other applications.   Extracted or Migrated Data The first step resulted in 6,719,064 Encoded Archival Context – Corporate Bodies, Persons, Families (EAC-CPF: an archival encoding standard hosted by the Society of American Archivists and developed in collaboration with the international archival community).  4,653,365 Persons 1,868,448 Corporate Bodies 197,251 Families    Merged Data After performing identity resolution processing (match and merging), we had:  3,741,262 EAC-CPF records  2,466,425 persons 1,077,588 corporate bodies 197,249 families   Because family names, as traditionally formed, lack sufficient qualifying information and thus commonly result in false positives, no matching was done against family names. In the final production, two family names were rejected as malformed.     7,966,737 links between the 3,741,262 persons, corporate bodies, and families 15,031,209 links to 2,079,504 unique resource descriptions    Prototype History Research Tool The prototype history research tool (http://socialarchive.iath.virginia.edu/snac/search ) allows researchers to find persons, organizations, and families; to read biographic information about them; to explore the social networks within which they existed; to locate historical records that document their lives, related resources, and external links associated with that name. Associated links are provided for ArchivesGrid and Digital Public Library of America, as well as “sameAs” links to Wikipedia, VIAF, WorldCat Identities, and others.     Significance for Researchers Researchers have welcomed SNAC for its research economies: SNAC’s History Research Tool provides integrated access to distributed primary (archival) and secondary (published) resources, eliminating or at least substantially ameliorating the need to track down resources in multiple archival catalogs. Painstakingly locating these resources is a labor-intensive, time-consuming activity in the current research environment, with successful discovery and assembling of the data highly dependent on persistence and serendipity. Indeed it is likely that some of the information found in the SNAC records might never be discovered using current methods. SNAC also makes explicit what has been, at best, implicit in archival description: the social-professional-intellectual networks within which the lives and work of the people documented in historical resources took place. It exposes the vast global social-document network that connects the past to the present. Ed Ayers, President of the University of Richmond and a Civil War historian, wrote that: SNAC promises to change the way history is imagined and written! For all that the digital revolution has revolutionized, the heart of research lies within the primary record embedded in archives large and small. The pioneering work of SNAC will unlock that record, revealing connections and patterns invisible to us now. Alan Liu, Professor of English, University of California, Santa Barbara and Director of Research Oriented Social Environment (RoSE), describes SNAC’s potential: SNAC employs state-of-the-art computational techniques to do three things very well: 1) unlock information originally recorded for specific purposes in library and other archival finding aids to make them usable in new contexts; 2) connect widely-distributed information of this sort from around the world; and 3) marry the “library” or “archive” model of knowledge to a whole other model of social networks that both humanizes our understanding of the way knowledge emerges from communities of knowledge creators and seekers, and speaks powerfully to today’s “social network” generation.   Significance for Prosopographical Research SNAC is building a humanities resource that benefits humanities researchers, but ongoing development and refinement of identity reconciliation techniques are of further benefit to humanists engaged in prosopographical research. Names alone are weak identifiers: multiple people can have the same name and one person may have multiple names. A number of factors influence our ability to reliably identify people. Indeed, the larger the domain from which names are drawn, the higher the likelihood that a name is shared by several people.  Though each step in the processing described above presents intellectual and technical challenges, the most challenging is identity reconciliation. A fundamental human activity in the development of knowledge involves the identification of a unique “real world” entity (e.g., a person or book) and recording facts that, when taken together, uniquely distinguish that entity. Establishing the identity of a person, for example, involves examining available evidence, including the existing knowledge base, and recording facts associated with him or her (such as names, dates and places of birth and death, occupation, etc.). This is an ongoing, cumulative activity that both leverages existing established identities and establishes new identities. Identity reconciliation is the process by which an encountered identity is compared against established identities, and if not matched, is itself contributed to the established base of identities. The networked computing environment presents opportunities for using algorithm-based inference methods to compare newly encountered entities with established identities to determine the probability that a new entity represents the same person or thing as an established identity. This ongoing expansion of the base of reliable identities is an interplay of human research, knowledge recording, and computational methods.   Transforming SNAC into an International Cooperative It became clear early on that the biographical data extracted and assembled from archival resource description constituted a valuable independent resource that could (and should) be maintained and further developed cooperatively. Development of a cooperative began back in 2011 and it recently entered its pilot phase with a group of fourteen inaugural institutional members that support the potential benefits of aggregated description and access demonstrated to date in SNAC, and, further, embrace the idea that the resources amassed should be cooperatively built and maintained in order to fully realize these benefits. The initial members represent research archives, libraries, museums (art and natural history), government archives, and institutional archives. The U.S. National Archives and Records Administration (NARA) serves as the secretariat for the Cooperative, while the Institute for Advanced Technology in the Humanities (IATH), University of Virginia, hosts the technological infrastructure. SNAC is led by IATH, working collaboratively with NARA, the California Digital Library, and the iSchool at UC Berkeley. The National Endowment for the Humanities (2010-2012), the Institute for Museum and Library Services (2011-2013), and the Andrew W. Mellon Foundation (2012-2017) have provided funding for SNAC.   ",
       "article_title":"Social Networks and Archival Context: People and Cultural Heritage",
       "authors":[
          {
             "given":"Daniel",
             "family":"Pitti",
             "affiliation":[
                {
                   "original_name":"Institute for Advanced Technology in the Humanities, University of Virginia, United States of America",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          },
          {
             "given":"Worthy",
             "family":"Martin",
             "affiliation":[
                {
                   "original_name":"Institute for Advanced Technology in the Humanities, University of Virginia, United States of America",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-29",
       "keywords":[
          "digitisation, resource creation, and discovery",
          "agent modeling and simulation",
          "archives, repositories, sustainability and preservation",
          "English",
          "networks, relationships, graphs"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Performance at Tate, an Arts and Humanities Research Council funded project which run between 2014-6, set out to trace the history of performance at Tate from the 1960s to today by investigating practices of collection, display, documentation and exhibition in the museum. At the heart of the project was the desire to conduct a wider re-evaluation not only of the place of performance in the museum, but also of the specific role played by documentation, including digital documentation, as well as the documentation of digital works, within collections, archives and displays. Here, I explore what the introduction of the digital has meant in this particular field by conducting a close examination of Lynn Hershman Leeson’s  Roberta Breitmore (1972-8), in which the artist created a fictional persona and interpreted its role for a period of six years using surveillance technology to capture various moments in her life. I will also discuss her subsequent works  CyberRoberta (1995-8) and  Life to the Second Power (2007) in which the character of Roberta was re-invented across different media. Focussing on the different types of documentations that these works generated, including photos, drawings, a cartoon, a film, a second-life re-enactment, postcards, among others, I then establish a best practice framework towards their curation and preservation that will be applicable more broadly for digital art practice.  To establish the role played by documentation in this context, Performance at Tate aimed to move beyond existing debates on the ontology of the relationship between performance and documentation. These debates may be traced back to the 1970s when, writing on performance-based work, the art historian Douglas Crimp asserted that ‘you had to be there’, implying that performance needed the presence of the spectator to be activated and often required ‘that registration of presence as a means toward establishing meaning’ (1979: 77). This approach underpins the performance studies scholar Peggy Phelan’s well known assertion that ‘performance’s only life is in the present’ and that performance ‘cannot be saved, recorded, documented, or otherwise participate in the circulation of representations  of representations: once it does so it becomes something other than performance’ (1993: 146).   A different position was adopted by media studies scholar Philip Auslander who in his identification of different types of performance (and documentation) counter-pointed that ‘documentation does not simply generate image/statements that describe an autonomous performance’ and states that it can produce ‘an event as a performance’ (2006: 5).   Instead, Performance at Tate aimed to build on approaches initiated by Amelia Jones (1997) and, subsequently, Barbara Clausen, who challenged the positioning of the document as secondary to performance, as well as the positioning of the document as equivalent to performance suggesting that performance should not be seen as beginning with or ending with the ‘authentic experience’, or live moment, but rather that it should be seen as ‘an ongoing process of an interdependent relationship between event, medialization, and reception’ (2005: 7). In other words, performance, in the course of its transcriptions, is subject to significant shifts caused by the constantly altering reception of its documents over time. Performance documents should therefore be considered, utilising Suzanne Briet’s term from 1951, as an ecology of inter-documents, comprising primary documents, created at the time of an event, secondary documents, created from the initial documents, and auxiliary documents, created by a juxtaposition of documents. Rather than delivering remains of an isolated event, the document, for Briet, forms part of a matrix or network of signs. So, she noted, ‘through the juxtaposition, selection, and the comparison of documents, and the production of auxiliary documents’, the content of documentation becomes ‘inter-documentary’.  Performance and documentation have always been somewhat inter-dependent. So, for example, art historian and critic Barbara Rose pointed out the significance of Hans Namuth’s famous photographs of Jackson Pollock’s work as  documents of his practice that radically affected any subsequent perception of his paintings (1979: 12). Likewise, it was performance studies scholar Philip Auslander who noted that Harry Shunk’s photographs of Yves Klein’s  Leap into the Void (1960), a photomontage, in fact constitute the work itself (2006). And it was Paul Schimmel who noted how Chris Buren’s actions, such as in  Shoot (1071), in which the performer asked his assistant to shoot him in his left arm, were ‘distinguished by their ability to be captured by a single photographic image and described in a brief paragraph’ (1998:97) almost as if to imply that the performance was designed so as to work for the photo. Most of these works nowadays exist primarily as documents. One such work is Hershman Leeson’s  Roberta Breitmore (1972-8) which comprises of a series of documents charting Roberta’s internal (i.e. a list of cosmetics for her make-up) and external transformations (i.e. a movement chart), testifying also to her social existence (i.e., she placed an advert, and underwent a psychiatric evaluation) and financial existence (i.e. she owned a checkbook). Nearly twenty years after Hershman Leeson exorcised the character of Roberta at the Palazzo dei Diamanti in Ferrara in 1978, Roberta was re-invented as  CyberRoberta (1995-8), a tele-robotic doll who was dressed identically to Roberta, and whose fictional persona was, as in Hershman Leeson’s words, ‘designed as an updated Roberta’ who navigate the internet, and was described as a ‘cyberbeing’ (1996: 336). Roberta also appeared as a bot in the Second Life remake of an early work by Hershman Leeson,  The Dante Hotel, called  Life to the Second Power (2007-) , which turned parts of the Hershman Leeson archive at Stanford Libraries into a dynamic mixed reality experience where visitors could explore digital reproductions of fragments of the original archive of  The Dante Hotel under Roberta’s guidance in Second Life (Roberta had started her existence when she arrived in San Francisco on board of a Greyhound bus and checked herself in at the Hotel Dante).   The first work,  Roberta Breitmore, consists of documents which are now preserved as the artwork in public and private collections (MOMA, SFMOMA, Tate, Walker Art Center, as well as the Hess collection, to name a few). Each museum also has a documentation of the work, which usually consists of gallery, curatorial, and preservation records. These may disclose significant information about how the artist wishes the work to be installed, for example, or about the work’s preservation strategy.  CyberRoberta, which is in the Hess collection, does not consist of documents, though, unlike in the case of  Roberta Breitmore, some documents were produced by users and are available on social media. These documents are not works, though it is only by seeing them that, if we have not experienced the work ourselves, we can understand how the work operated. No museum, to my knowledge, has been preserving these user-generated documents. Finally,  Life to the Second Power is available on Second Life and a set of photographic documents were collected by Exeter and Stanford Universities at the time the work was shown. Again, a number of visitors generated photographs (both in first and second life) but these were not collected. The work was shown as The Montreal Museum of Fine Arts and at SFMOMA, so it is likely that these museums have kept a documentation of the work, though most museums only do so when the work entered the collection. In short, in the case of  Roberta Breitmore, the artist created the documents that are now known as the work. In the cases of  CyberRoberta and  Life to the Second Power, most documents showing the work in use were produced by users or viewers, and none of them are systematically preserved. In the case of  Life to the Second Power, the work is hosted by a commercial platform and may cease to exist once the platform becomes obsolete or is terminated by its owners.   I conclude by suggesting that Museums should draw from performance studies and digital humanities and create records documenting the experiences of these works, noting also how these have changed over the years. I also look into the challenges paused by their preservation, particularly in terms of born digital works. Finally, I show that by capturing this knowledge, Museums will not only preserve important historical information about the exhibition and reception of these works, but also create, to use Briet’s term, an inter-documentary ecology comprising ‘live’ performance (whether by the artist or the user), documents (created by the artist, the museum or the user) and the digital (showing the web and social media life of a work in different formats). ",
       "article_title":"Performance, the Document, and the Digital: the Case of Lynn Hershman Leeson’s ‘Robertas’",
       "authors":[
          {
             "given":"Gabriella",
             "family":"Giannachi",
             "affiliation":[
                {
                   "original_name":"University of Exeter, GB",
                   "normalized_name":"University of Exeter",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/03yghzc09",
                      "GRID":"grid.8391.3"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-02",
       "keywords":[
          "art history",
          "GLAM: galleries, libraries, archives, museums",
          "archives, repositories, sustainability and preservation",
          "English",
          "audio, video, multimedia",
          "social media"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The work described in this paper came about as a result of reflections made within the “Clavius on the web” Projecthttp://claviusontheweb.it, which studied the correspondence between the Jesuit mathematician and also astronomer and some important scientists of his century, such as Galileo and BraheClavius’ correspondence is contained in the manuscripts APUG 529-530, preserved in the Historical Archives of the Pontifical Gregorian University.. One of the main aims of the project is to make it possible for students and scholars to access the texts on a semantic basis, in order to allow a deeper understanding of the often complex content, they convey. Texts are often the unique source that scholars have at their disposal in order to be able to reconstruct and more completely understand the past author’s thought.  In order for technology to come to the aid of scholars in this effort, the concepts evoked within the text, as well as the terms representing these concepts need to 1) have a structured organization 2) be explicitly and univocally represented and 3) be defined through the relationships that unite them. In order to achieve this, we chose to adopt an ontologybased model, as ontologies are a de facto standard for knowledge representation. Interestingly, the choice to use ontologies raised some issues, also with regard to theoretical aspects: indeed, standard ontological formalisms usually static and crisp proved to be inadequate in modelling the complexity of the knowledge conveyed by the analysed texts. As a result, more refined models as well as appropriate graphical representations needed to be introduced so that computers would be able to process these ontologies and visualize them in a way that students and scholars could understand and work with them.   The ontological model Here we list and briefly describe the main aspects of the knowledge conveyed by the Clavius’ corpus that our ontological model should capture.   Explicit versus  implicit knowledge : our ontology is designed to structure both the entities explicitly evoked in the text (typically denoted by terms) and the entities implicitly entailed as belonging to the background knowledge that the writer implies (which can be possessed by the reader themselves only in part).   Shared versus  individual knowledge : different authors can share, and in fact do share, some aspects of conceptualising the domain, as clearly they have certain theories and beliefs in common. However, our ontology must formally structure the author’s own conceptualisation of the world, as it emerges from specific textual passages of the analysed corpus.   Certain versus  uncertain knowledge : in the case where the authors express confidence in some theories or reject and advance doubts towards others. It is therefore essential for each entity which populates the ontology (a class, an instance, a property) to be associated with a degree of certainty.   S tatic versus  dynamic knowledge : correspondence implies sharing information and knowledge, which can lead to changes in the way the correspondents view the world, sometimes significant. This is particularly the case with scholars. As a result, the ontology needs to be dynamic and temporal, so that it is possible to illustrate the evolution of the author’s conceptualization over a period of time. The specific time is either explicitly indicated by the author in his/her work or reconstructed from other sources.   Other parameters could be considered, such as vagueness, ambiguity and sincerity. The validity of these aspects is not limited to these kinds of texts (i.e. scientific letters), but it applies to any text such as essays, scientific journals, diaries, which expresses an author’s firmly-held or evolving opinion. Consequently, as a case study (see Section 4), we chose Galileo’s Sidereus Nuncius(Galilei, 2001), to prove the applicability of the model outside the epistolary corpus. In the present paper, we will mainly focus on dynamic knowledge and its representation.   Models for representing dynamic knowledge In literature, the problem of representing dynamically evolving information in ontologies has been addressed by adopting several different approaches (Flouris et al., 2008). A very simple solution is to create a version of the ontology for each temporal event that has to be represented (ontology versioning). However, a versioning algorithm is necessary in order to access the different temporal variants of the ontology. Other proposals aim to extend OWL ontologies in order to provide binary relation instances with a time reference. Related approaches are: (Welty et al., 2006) encoding a perdurantist/4D view in OWL, (Krieger, 2008) interpreting original entities as time slices, and (Manola et al., 2004) reifying original relations. For an exhaustive list of works, see (Krieger and Declerck, 2015). However, all of these approaches typically invalidate standard OWL reasoning, and they do not allow the representation of the change in subsumption and instantiation. In (Rizzolo et al., 2009) time semantics is added also to resources by providing temporalvarying classes and individuals, but only for RDF(S) ontologies, by extending the model presented in (Gutierrez et al., 2005). However, domain expertoriented tools for manipulating RDF(S) do not currently exist. Against this background, we chose to conduct our first experiments with a reification-based approach and SKOS Simple Knowledge Organization System - http://www.w3.org/2004/02/skos/ , the latter providing the best compromise between temporal aspects representation, availability of tools, querying and reasoning capabilities.    A case study We propose here a possible representation of the evolution of Galileo’s conceptualization of Jupiter’s moons over a specific week in 1610, reconstructed on the basis of the Sidereus Nuncius. The first observation of the planets dates back to 7th January 1610, when Galileo first saw what he thought were three fixed stars near Jupiter. After several observations on 11 th January, he noticed that their position relative to Jupiter changed in the same way as wandering stars. Two days later, he observed that there existed four satellites orbiting around Jupiter and not three.  Here we present the preliminary version of the ontology which structures the content of portions of the Sidereus Nuncius where Galileo describes his observation of Jupiter’s moons. We first identified the key terms of the text as the terminology (in bold in Fig. 2) upon which we defined the explicit entities of the ontology. In addition we specified the necessary implicit entities to add to it (eg. Galilean moon). The ontology was built using Protégé 5.0.0 (Musen et al., 2000) and the plugins Skos Editorhttps://code.google.com/p/skoseditor/ and Chronos (Preventis et al., 2014), the former to implement an SKOS ontology and the latter to add the diachronic component. The process is described in the following steps:  Structuring of the concepts via the skos: broader relation; the concept  Galilean_Moon has been set as a subconcept of both  Fixed_Star and  Wandering_Star (Fig. 1.a);  Definition of the properties  isNearTo and  revolvesAround ;  Instantiation of these two properties between the four moons (S1,  S2,  S3, and  S4) and  Jupiter;  Conversion of the properties broader,  isNearTo and  revolvesAround into temporal;  Attribution of the correct time interval to each property instantiation.  As a result of this representation, the concept  Galilean_Moon became narrower than  Fixed_Star during the time interval between 7th and 11th January 1610, then it changed to narrower than  Wandering_Star (Fig. 1.b). Analogously, each of the three moons progressed from being simply “nearTo”  Jupiter to “revolvesAround”  Jupiter. Finally, starting from 13th January, the relation broader also links  S4 and  Galilean_Moon (i.e. Galileo spots a fourth object).     Fig. 1: a) The concept hierarchy shown in the “SKOS view” tab of Protégé; b) the temporalized relation “broader” applied to Galilean_Moon.   Browsing the constructed dynamic ontology allows to answer to complex queries such as: “how did Galileo’s vision of Jupiter’s moons evolve in time?” or “which had been Galileo’s main changes of perspective about Jupiter in January of 1610?”.   Visualization of the ontology A visualization can be described as an artefact that helps humans to make decisions, learn and communicate, acting as a visual cognitive support (Card et al., 1999). A visual representation of ontologies can therefore be developed to ease their comprehension by both scholars and non-expert users. In our case, a suitable graphical display allows to make visual comparisons between the different time frames of a dynamic ontology, capturing the evolution of the author’s ideas. Among the available visualization techniques, we adopted the node-link diagram, which is particularly well suited for exploring the topology of a network and for locating paths (Munzner, 2014). We wanted to automatically produce visualizations with a quality resembling that of hand-made diagrams (Dwyer et al., 2006; Kieffer et al., 2013). To do so, we observed the work of experts sketching some ontologies on paper, and derived a series of geometric constraints for an automatic placement algorithm. Since the skos:broader relationship defines a quasi-hierarchy, i.e., a tree with a reasonably small amount of nodes having multiple parents, the constraints we implemented were intended to produce a familiar, tree-like representation. The input for the layout algorithm is the entire SKOS graph, and the output is a single layout for all the time frames. Comparison is then made possible by displaying a series of juxtaposed views, each showing only the items of a specific time frame (Fig. 2). This technique ensures that the same item is given the same position in each view, while differences create easy-to-spot “holes”', thus leveraging the user's spatial memory to carry out the comparison task (Munzner, 2014).   Fig. 2: A prototype visualization of the case study presented in section 4. Scholars or students can see the evolution of Galileo's concepts after each observation of Jupiter's moons. The automatically computed diagram layout ensures comparability while preserving a familiar, tree-like appearance.   ",
       "article_title":" When Traditional Ontologies are not Enough: Modelling and Visualizing Dynamic Ontologies in Semantic-Based Access to Texts  ",
       "authors":[
          {
             "given":"Silvia",
             "family":"Piccini",
             "affiliation":[
                {
                   "original_name":"ILC-CNR, Italy",
                   "normalized_name":null,
                   "country":"Italy",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Matteo",
             "family":"Abrate",
             "affiliation":[
                {
                   "original_name":"IIT-CNR, Italy",
                   "normalized_name":null,
                   "country":"Italy",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Clara",
             "family":"Bacciu",
             "affiliation":[
                {
                   "original_name":"IIT-CNR, Italy",
                   "normalized_name":null,
                   "country":"Italy",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Andrea",
             "family":"Bellandi",
             "affiliation":[
                {
                   "original_name":"ILC-CNR, Italy",
                   "normalized_name":null,
                   "country":"Italy",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Emiliano",
             "family":"Giovannetti",
             "affiliation":[
                {
                   "original_name":"ILC-CNR, Italy",
                   "normalized_name":null,
                   "country":"Italy",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Lorenzo",
             "family":"Mancini",
             "affiliation":[
                {
                   "original_name":"ILC-CNR, Italy",
                   "normalized_name":null,
                   "country":"Italy",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Andrea",
             "family":"Marchetti",
             "affiliation":[
                {
                   "original_name":"IIT-CNR, Italy",
                   "normalized_name":null,
                   "country":"Italy",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "spatio-temporal modeling, analysis and visualisation",
          "visualisation",
          "literary studies",
          "ontologies",
          "historical studies",
          "networks, relationships, graphs",
          "knowledge representation",
          "archives, repositories, sustainability and preservation",
          "content analysis",
          "English",
          "bibliographic methods / textual studies",
          "interface and user experience design"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Project “Haine du Théâtre” This experiment takes place within the “Haine du Théâtre”The directors of this Projet are François Lecercle and Clotilde Thouret. <http://obvil.paris-sorbonne.fr/projets/la-haine-du-theatre> It is one of the many outstanding projects at the Labex OBVIL (Laboratoire d’Excellence: Observatoire de la Vie Littéraire) in Paris headed by Didier Alexandre and Jean-Gabriel Ganascia. Project, which aims at analysing theatre debates in Europe by using scientific approaches and critical editions of polemical texts. The reflections of the team were primarily focused on the discovery of the circumstances and arguments used in theatre all across Europe, not limited to France, but including England, Spain, Italy, and the Germanic area. The timeframe encompassed the last decades of the 16th century up to the beginning of the 19th century. The purpose of the Project is to explore the grey areas of the controversies in order to outline a global overview of the situations which led to these polemics, discovering where and how they began, their chronological discrepancies in the different countries, and the links between them and their contemporary resurgences.   Corpus The total collection of the Haine du Théâtre Project related to France made up of 300 texts in the PDF format. The XML/TEI critical edition of 27 texts has been achieved manually (examples of the main titles of this TEI corpus are D'Aubignac (1666), Conti (1666), Pierre Nicole (1667), Voisin (1671), Vincent (1647), cfr. <http://obvil.paris-sorbonne.fr/corpus/haine-theatre/>) and this small corpus is used for the semantic analysis we present here. This collection is a homogenous combination of texts in which the different authors express their approval or their condemnation of theatre. Using the ontology we created, our interest was to discover to what extent the personal judgment of each author was celebratory or derogatory about theatre and, also, which arguments were mostly involved in their critiques. In this direction, we had two main goals:  Organizing the knowledge about polemics in theatre and their vocabulary; Use this structured lexicon for the corpus annotation and its analysis.    Building of the ontology An ontology is a good way to automatically analyse many texts together along two parameters: documentary and linguistic. On the one hand, by improving a state of knowledge about 17th century French ; on the other hand, by refining the vocabulary linked to theatre controversy. The ontology organizes the knowledge of our domain (polemical texts about theatre) as structured points of view (condemnation, defence, etc.) as well as 44 structured classes (concepts) related to critical controversies. These classes report the axiological points of view (the judgment of the authors), their objects (such as  jongleur,  actrice, etc.), and the thematics of the polemics ( religion,  emotions, etc.).  To detect the salient terms pertaining to each concept, we realized that in most texts of the modern period, authors' judgements revolve around quoted authorities. The context and the deep knowledge of the corpus together with expertise in 17th century French constitute the cornerstones of our approach. It is very important to fully understand the contextual meaning (in the 17th century) of the selected salient terms. The idea we came up was to look at the relative importance of the various semantic fields in each chapter of the collection, then extract a priori the content of a chosen text. We began to use this structured lexicon of outstanding terms related to theatre polemics for the corpus annotation and its analysis. To our knowledge no other comparable ontology about theatre polemics exists; therefore, we will present the method we conceived keeping in mind that those classes are not exhaustive and that new questions will require the creation of new classes. Despite the high specificity of the domain, this ontology model and the automatic annotator are deployable in other contexts, such as literary criticism and theatre critique in different languages. For this purpose we will translate the model into English (the set of the attributes useful for the annotator).   The annotation tool We created an annotator in order to markup all the forms of the outstanding terms, recorded as lemmas in the ontology (“horrible”, “horribles”, etc.), except when:  the exact form matters: the tool annotates only the exact form (“Père” does not match “père”, “pères”); the gender matters: only the feminine forms, singular and plural (\"courtisane(s)\" which is a linguistic sign of the \"Femme\" concept in our ontology does not match the masculine forms \"courtisan(s)\").  The annotation tool enriches automatically the TEI files by setting down some <term> tags to mark all the forms, but also their lemma and the related semantic field.   Snapshot of the TEI annotation (François Hédelin d’Aubignac, Dissertation sur la condemnation des théâtres, 1666, “Chapitre I. Que les Spectacles des Anciens ont fait partie de la Religion Païenne\")  Following this method we find a high density of salient terms (on average 9,5% of words in each the chapters) in our collection of texts. The combination of these linguistic signs and related semantic fields emerges as a semantic descriptor of the corpus content (e.g.  http://obvil-dev.paris-sorbonne.fr/corpus/haine-theatre/vincent_traite-des-theatres_1647/vincent_traite-des-theatres_1647_6).    Snapshot of the HTML highlighting of the annotation (François Hédelin d’Aubignac, Dissertation sur la condemnation des théâtres, 1666, “Chapitre I. Que les Spectacles des Anciens ont fait partie de la Religion Païenne”). The salient words of the religion thematic, very important in this chapter, are highlighted in dark blue (“Dieux”, “Saint”, “Divinités”, “Démons”, “Sacrifices”, “Païens”, “Dieux”, “Prêtres”, etc.)    Semantic analysis Without preconceived notions and previous knowledge of the corpus, it was possible for us to answer numerous research questions. By examining the intensity of condemnation in the corpus, we discovered that the most derogatory chapters belong to Nicole and Conti.   chapter_id condemnation_terms defense_terms chapter_length (words) condemnation intensity (‰)    nicole_traite-de-la-comedie_1667_4 6 0 762 7,87   nicole_traite-de-la-comedie_1675_4 6 0 906 6,62   conti_traite-de-la-comedie_1666_14 21 1 4536 4,63   Examples of the condemnation intensity. By virtue of the possibility of making cross queries across multiple classes, we were able to obtain a list of the arguments developed by the authors – which can be confirmed by reading the chapters concerned. For instance, the most derogatory chapters about theatre were written by Nicole and Conti.   chapter_id “Théâtre” and “Morale Négative” thematics relative scoresThe score is the ratio between the theatre and the condamnation terms over the chapter length (number of words). (‰)   nicole_traite-de-la-comedie_1667_4 66,67   nicole_traite-de-la-comedie_1675_4 56,25   nicole_traite-de-la-comedie_1675_1 41,67   conti_traite-de-la-comedie_1666_14 41,24   nicole_traite-de-la-comedie_1675_13 40,75    The most derogative chapters.  We can also compare the chapters more concerned with, for instance, condemnation and the thematic of “Women”. We discovered that the chapters more concerned with misogyny belonged to Voisin, Conti and Nicole. The discriminant terms for woman are, in these derogatory chapters: “femme, fille, bouffonne, maîtresse, comédienne”. Those are just examples of the queries we can construct with the annotation tool. At this point, the results of the computational analysis reveal some critical conclusions about the textual tradition analysed. Continuing this kind of analysis for every concept present in the ontology, we found that some authors are more concerned with all the elements of theatrical debates, whereas others deal only with some of them. In particular, we found a high concentration of annotated terms in Voisin, Conti, Nicole and Aubignac, whose chapters usually occupy the first 20 results for most thematics. On the contrary, authors like Vincent, Guillot-Gorju, Le Marcant and Gaule score high only when relating to economic issues.    chapter_id “Economy” thematic relative scoresThe score is the ratio between the economy terms and the chapter length (number of words). (‰)   vincent_traite-des-theatres_1647_9 3.77   vincent_traite-des-theatres_1647_7 3.40   anonyme_honneur-theatre_1620_1 3.1   guillot-gorju_apologie_1634_1 2.75   lemarcant_conduite-du-vrai-chretien_1694_1 2.50   nicole_traite-de-la-comedie_1667_18 2.33    The concentration of the economic thematic. These results show that the economy is at the core of the descriptions by another group of authors: Vincent, Guillot-Gorju, Le Marcant.  For example, some very specific arguments are associated in the controversy, like women, passion and the economy. Moreover, results have shown that some authors of the corpus focus on only few topics of the polemics, whereas others, like Vincent, concentrate all the topics of the theatre polemic in his 11th chapter.    Conclusions  The HdT ontology organizes the knowledge about the theatre polemics and their vocabulary. In particular, it presents a hierarchical lexicon of salient terms and it reflects the points of view, objects and thematics of the polemic. This ontology enables the exploration the corpus for research questions: the relative frequency of the salient terms ( lemma) and of their related class ( catégorie d’indexation) is a semantic descriptor of each chapter. Those descriptors can be exploited for the semantic exploration of the corpus, capitalizing on the ontology structure (terms / classes). The researchers can understand the intensity of the concepts they choose to analyse and thus can focus on the most significant chapters of the corpus.   ",
       "article_title":"Analyzing the 17th Century Theatre Critique Texts with a Semantic Annotation Tool Driven by a Dedicated Ontology",
       "authors":[
          {
             "given":"Chiara",
             "family":"Mainardi",
             "affiliation":[
                {
                   "original_name":"Université Paris-Sorbonne, France; Université Pierre et Marie Curie, France",
                   "normalized_name":"University of Paris-Sud",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/028rypz17",
                      "GRID":"grid.5842.b"
                   }
                }
             ]
          },
          {
             "given":"Vincent",
             "family":"Jolivet",
             "affiliation":[
                {
                   "original_name":"Université Paris-Sorbonne, France; Université Pierre et Marie Curie, France",
                   "normalized_name":"University of Paris-Sud",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/028rypz17",
                      "GRID":"grid.5842.b"
                   }
                }
             ]
          },
          {
             "given":"Zied",
             "family":"Sellami",
             "affiliation":[
                {
                   "original_name":"Université Paris-Sorbonne, France; Université Pierre et Marie Curie, France",
                   "normalized_name":"University of Paris-Sud",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/028rypz17",
                      "GRID":"grid.5842.b"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "french studies",
          "semantic analysis",
          "renaissance studies",
          "semantic web",
          "literary studies",
          "ontologies",
          "digitisation, resource creation, and discovery",
          "English",
          "italian studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" In recent decades European libraries have taken a giant step towards the mass digitization of their historical collections and the opening of their contents for the use of the global digital society. However, researchers and teachers experience great difficulties using, enriching or sharing that content. Our project aims to explore the new needs of the users of European digital libraries, databases and repositories in order to evolve the “traditional digital model” towards the  SmartLibrary model, which proposes the compilation, integration and downloading of contents according to the needs of users and in order to enrich the European uses of the history.   Mnemosyne, for the ancient Greeks, was the personification of Memory. In our project the concept of memory comprises two uses:  1)  The recovery of historical memory through European texts we consider rare and forgotten. We will analyse these concepts as conceptual categories in cultural studies (Alonso, 2008; Romero Lopez, 2014). This new paradigm will cover the analysis of a large and complex network of literary manifestations. We aim to record the history of the losers, looking for it in popular and mass culture texts that have been marginalized until now (Labanyi, 2003). This recovery requires making those digitized texts accessible and bringing together their interpretations so that the axes that have governed their oblivion within European cultures can be underlined.  2)  The rewriting of historical memory. Once we have compared texts digitized in the  Scriptorium, researchers and teachers will begin an enrichment of these texts through their collaborative annotation. This reinterpretation will allow historical memory to be restored by setting new categories of knowledge for the understanding of European cultures under common tendencies.  The prefix  SMART- has been used as synonymous to agility, safety, ecology and sharing (Doran, 1981). It is a prefix that has been applied to phones, cars, houses and cities. So far, it has not been applied to libraries. The creation of a European  SmartLibrary implies:    Simple access to integrated European databases on contemporary and alternative European literatures   Motivational search based on specific research content or didactic objects   Appropriate results based on the semantic Web search   Results discharged in a personal  Scriptorium to be enriched with the specific tools   Transference of new digital objects to be shared with the global community   As a “smart digital model”, exportable to other areas of the digital humanities, we have been developing  Mnemosyne: A SmartLibrary for Rare and Forgotten Texts, based on the research that the LEETHI, LOEP and ILSA (see below) research groups in the Faculties of Philology and Information Technology of the Complutense University of Madrid (Spain) are jointly developing. Thanks to the collaboration of specialists in different European literatures and computer experts in the course of several national research projects, we have designed a new model of  Scriptorium which allows for the integration of metadata and the enrichment of digital objects with new tools such as  Clavy –an import/export tool for metadata- and  @Note – a collaborative annotation tool.  About  Mnemosyne. Digital Library of the Other Silver Age (Beta version):  1.-  Mnemosyne. Digital Library of the Other Age of Silver is already accessible on the Internet (http://repositorios.fdi.ucm.es/mnemosine/). As you can see  Mnemosyne contains authors’ data, access to digitized works and research collections. The field of study is rare and forgotten Spanish literary texts (1868-1939). The work is still in progress. Our current project ends by 06/31/2016.   2.-  Mnemosyne records  show the metadata imported  by our tool   Clavy  from   Biblioteca Digital Hispánica   and from   HathiTrust   with the support of the  Complutense Library .  3.- The  Mnemosyne database works as  a laboratory in which we experiment with  Clavy the importation/exportation of metadata, and the tool   @Note   and practise collaborative annotation.  @Note promotes the collaborative creation of free-text and semantic annotation schemas on literary works by communities of researchers, teachers and students and the use of these schemas in a very flexible and adaptive model for the definition of annotation activities.  4.- As  SmartLibrary, Mnemosyne will integrate  @Note  and other digital tools (forthcoming). Of course we are very much interested in DARIAH tools and its research infrastructure and we would like to collaborate with this European consortium. Besides, our  SmartLibrary could be an extraordinary field of study in which to experience the development and integration of new digital tools.  Mnemosyne, as a  SmartLibrary, could become a field of international experimentation for the practice of tools with semantic interoperable networks.  5.- The Spanish  Mnemosyne is the first example of what we would like to build. We would like to regrow our  smart model in the international environment with the support of other European projects, interested, like the authors, in rare and forgotten texts and the uses of the past.    Mnemosyne: SmartLibrary for Rare and Forgotten Texts needs transnational collaboration. This project involves the integration of specialists in different European literatures and researchers in computer science to develop new research and resources for the common use of European citizens.  The research developed in  Mnemosyne is being financed by: 1) Ministerio de Economía y Competitividad. Research Project: “Escritorios Electrónicos para las Literaturas-2”. Reference FFI2012-34666 (2012-2016). Directora: Dolores Romero López, Facultad de Filología, Complutese University of Madrid . 2) I Convocatoria de Ayudas a Proyectos de Investigación de la Fundación BBVA: “Modelo unificado de Gestión de Colecciones Digitales con Estructuras Reconfigurables: Aplicación a la Creación de Bibliotecas Digitales Especializadas para Investigación y Docencia”. Reference: HUM14_251(2015-2016).  Director: José Luis Sierra Rodríguez, Facultad de Informática, Complutense University of Madrid  RESEARCH GROUPS:  LEETHI Research Group,  ILSA Research Group,  LOEP Research Group.  ",
       "article_title":"Mnemosyne: A Smartlibrary for Rare and Forgotten Texts",
       "authors":[
          {
             "given":"Dolores",
             "family":"Romero-López",
             "affiliation":[
                {
                   "original_name":"Universidad Complutense de Madrid, Spain",
                   "normalized_name":"Complutense University of Madrid",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/02p0gd045",
                      "GRID":"grid.4795.f"
                   }
                }
             ]
          },
          {
             "given":"José Luis",
             "family":"Bueren-Gómez-Acebo",
             "affiliation":[
                {
                   "original_name":"Universidad Complutense de Madrid, Spain",
                   "normalized_name":"Complutense University of Madrid",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/02p0gd045",
                      "GRID":"grid.4795.f"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-29",
       "keywords":[
          "literary studies",
          "spanish and spanish american studies",
          "cultural infrastructure",
          "archives, repositories, sustainability and preservation",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Based in five years of experience teaching digital humanities (DH) students at the University of Nebraska-Lincoln (UNL), our paper tells the story of an evolving approach to teaching \"practical\" skills. We argue that the most important skills DH students need to learn are not particular programming languages or specific research methodologies, but team-based problem-solving. Furthermore, an effective way to achieve this learning is for students to work together to design and build a digital project that addresses a real challenge, draws upon their commitment to the humanities, and serves the mission of a local organization.  Background When UNL formed its Graduate Certificate Program in Digital Humanities, the organizing committee knew it wanted students to have a substantial engagement with the intellectual theory undergirding digital humanities as well as with DH praxis. To accomplish this, UNL created two courses to form the heart of the Certificate: the Interdisciplinary Reading Seminar in Digital Humanities and the Internship in Digital Humanities. Full details of the UNL Graduate Certificate Program are available at http://www.unl.edu/dhcert/ The Internship in Digital Humanities, originally available only to graduate students, embedded students within faculty DH projects at UNL. Students worked on these projects for seven hours per week and spent one hour in class learning some \"basic skills\" for digital humanists. After running the course this way for two years, we determined we were not fulfilling the goals of the curriculum or the needs of the students. In the best cases, students were fully integrated into project teams and were challenged with new experiences. Other students, however, performed menial and repetitive work throughout the semester. We also underestimated the challenge of making students collaborators in this limited time, especially when  project staff faced deadlines and needed to focus on production rather than instruction. The weekly class sessions, too, were mere introductions; students could not truly learn new skills because their projects did not provide opportunities to work with the introduced technologies and strategies. The mediocrity of this approach and the inclusion of the course in a new Undergraduate Minor in Digital Humanities forced us to think more deeply about what it means to teach DH project development. We wanted a higher level of student investment in the work and for students to be involved from conception to implementation. And opening the course to undergraduate students made us even more aware of the need to present students with varied projects, not just those emerging from faculty members at a research-intensive university. Our approach acknowledges Jakacki and Faull's perspective that undergraduate digital humanities education often focuses on specific tools or techniques rather than \"habits of mind\" (2014). Our course and pedagogy also take up the \"three major questions in digital humanities pedagogy\" raised by Smith: is there a \"common core of learning objectives\" around which DH programs should be structured? How vocational should DH curricula be? Should DH curricula focus on skills, method, \"or critical perspectives on technology and its application\"? (2014).   Immersive, Community-Based Approach to Digital Humanities Teaching and Learning For the revised version of the course, known as the Digital Humanities Practicum and launched in 2014, we adopted a service learning--or community-based learning--model. Both undergraduate and graduate students enroll in the course, and it is cross-listed among several departments including Anthropology, English, History, and Modern Languages & Literatures. (We are faculty members in the UNL Libraries, and the Libraries also play an important role in the course.) Under the new design, we partner with local organizations who have identified challenges suited to technological, humanities-engaged solutions. Over the course of the semester, students respond to those challenges, first conceptualizing a solution, iteratively building their solution (and sharing iterations with classmates), and then presenting their solution to a public audience at the end of the semester. The skills we teach with this class are consistent with those outlined by Rockwell and Sinclair (2012): working in interdisciplinary teams, managing projects, applying digital practices, and explaining technologies. The practicum engages and implements key values of community-based learning, including a \"recursive style; direct, high-impact method; and emphasis on abstraction embedded in practice\" (Grobman and Rosenberg, 2015). In addition, the course now advances a team-based experience that focuses not only on academia but looks outward to the humanities' roles in society more broadly. A fundamental difference between the Digital Humanities Practicum and the earlier internship course is that the Practicum focuses on team-based problem-solving rather than specific technical skills. For example, Humanities Nebraska, a state-wide humanities advocacy organization, challenged the students to improve communication about their annual Chautauqua event while engaging new audiences. In response, a team of undergraduate students proposed a mobile application that would serve as an information platform and provide opportunities for social media engagement. Entering the course, the students had limited experience with web technologies and no experience with mobile application development. While they researched what might be involved in creating a mobile application, we reached out to others on the UNL campus who could work with students to help them learn specific skills and made sure they would have access to necessary hardware (such as a variety of mobile devices for testing) and software. By semester's end, the application was available in the Google Play store, and shortly after was published to the Apple App Store. During this experience, the students learned much more than new technology proficiencies. They performed research about Chautauqua and the Chautauqua theme (\"Free Land\"), considered how best to communicate this information to the audiences they sought to reach, and interacted effectively with their client and mentors about their ideas--including accepting and responding to criticism of approaches that were not working.  This model requires significant flexibility on the part of the instructors and students. The syllabus is largely unfixed, as it must respond to the students and their needs, based on their background and experiences and also on the solutions they seek to pursue. Therefore, most of the fifteen-week semester cannot be planned more than a week or two in advance. Students, however, use an agile development approach so that they and we learn early where they will confront difficulties in implementing their solutions and what resources--whether people, hardware, software, or strategies--we need to connect them with in order for them to develop their projects.  Successful implementation of this model also requires that faculty and students are frank about knowledge limitations. As the instructors, we confess at the beginning of the course that we don't ourselves know everything the students will need to learn to be successful. But what we offer the students--and model for them--is the ability to figure out the necessary skills and seek appropriate resources. As we routinely tell the students, the requirements to solve these problems are not technical skills, but courage and perseverance. Our students have learned to weather discomfort not only because we implore them to do so, but because the iterative development model insists upon it. By having the students produce and demonstrate results early and often, we get them accustomed to a new kind of relationship with their coursework and to problem solving.  Based on our observations and student evaluations, it is this iterative process that imparts the learning. Furthermore, student investment in the projects is encouraged by the \"realness\" of the challenges. Unlike many classroom assignments, the problems in the Digital Humanities Practicum are authentic challenges brought in by external organizations with real missions, and their work can have application beyond the classroom. Students have assisted the Nebraska Commission on Indian Affairs, for example, in their effort to secure federal legislation designating an historic trail. For the next iteration of the course, offered in Spring 2016, we plan to work with organizations that are not principally humanities organizations. These include a children's museum, a community supported agriculture and food education organization, and a social justice organization. Our goal is to broaden understanding of where humanities work can happen as well as demonstrate possibilities for solving problems by joining diverse areas of expertise. While we do not yet know the outcomes of the 2016 Practicum, the course will conclude before DH2016, and we will share both the student projects as well as an evaluation of the approach.   ",
       "article_title":"Teaching Digital Humanities Through a Community-Engaged, Team-Based Pedagogy",
       "authors":[
          {
             "given":"Andrew",
             "family":"Jewell",
             "affiliation":[
                {
                   "original_name":"University of Nebraska-Lincoln, United States of America",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          },
          {
             "given":"Elizabeth",
             "family":"Lorang",
             "affiliation":[
                {
                   "original_name":"University of Nebraska-Lincoln, United States of America",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-02",
       "keywords":[
          "digital humanities - pedagogy and curriculum",
          "teaching and pedagogy",
          "English",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" 1 Introduction The Digital Repository of Ireland (DRI) is a national Trusted Digital Repository for Ireland's social and cultural data, accredited by the Data Seal of Approval. The repository links together and preserves historical and contemporary data held by Irish institutions, providing a central internet access point and interactive multimedia tools. In June 2015, the Digital Repository of Ireland was publicly launched at the  Digital Preservation for the Arts, Social Sciences and Humanities (DPASSH) conference. At this conference, three organisations were presented with the DRI ‘Decade of Centenaries Digital Preservation Award’, the culmination of a six month project aiming to provide support and training to owners of digital collections relating to the Irish decade of centenaries commemorations. This paper will discuss the implementation of this Irish Research Council-funded project which allowed staff from the DRI to engage with collection owners and provide digital preservation and digitisation services and training. In this paper we outline the aims of the project, the methods by which we engaged with relevant collection owners, and how our findings have helped to determine the status of digital preservation in Irish heritage organisations.  2 Background In June 2015, the first DPASSH conference was hosted in Dublin by the DRI, to highlight the technical, cultural, and social problems, challenges, and opportunities of long-term digital preservation in the arts, social sciences and humanities. The conference theme, ‘Shaping our Legacy: Safeguarding the Social and Cultural Record’, was developed to reflect concerns within the community that digital cultural heritage is at risk of destruction. The conference’s call described the destruction of the Irish Public Records Office (IPRO) in 1922 during the Battle of Dublin. As the Irish Civil War broke out, a priceless archive containing a thousand years of Irish history was destroyed.  The destruction of the IPRO was used as an analogy to highlight the vulnerability and fragility our digital cultural heritage in the long-term. The challenge, however, is that “long-term”, in this context, comes nowhere near the thousand years of history housed in the purpose-built store of the IPRO; as Jeff Rothenberg (1999) states, ‘digital information lasts forever—or five years, whichever comes first.’[1]  It is within this context that we sought expressions of interest from custodians of heritage material relating to the Decade of Centenaries (DC) who wished to digitally preserve their holdings. Funded by the Irish Research Council's New Foundations Award, we aimed to assess the scale of vulnerable digitised collections related to the DC in Ireland, to provide support in digitally preserving those collections, and to create a centralised access point to encourage their wide dissemination. Importantly, we wanted to raise awareness of the issues related to digital preservation and provide resources, best practice advice and guidance to all applicants. We also planned to have an impact on the community beyond the span of the funded project, providing equipment and training to continue to support the digital preservation of Irish cultural heritage. 3 Methodology Our call for expressions of interest was announced in December 2014 and was circulated by the DRI community. The call sought collections that were partially or fully digitised and described, and which contributed to the national narrative on the period 1912-22. The winners were offered resources to ensure the digital preservation of their collections, including staff time from professional archivists and librarians, digitisation services, metadata creation, and the ingestion of content into the DRI for long term preservation. Interested collection owners were invited to submit a detailed application form, providing information on the types of digital assets in their collection, the volume, current storage provisions, and its connection to the DC.  Eight proposals were received, and through the collection assessment and selection phase, three were chosen: the Irish Capuchin Provincial Archives (The Capuchins and the Irish Revolution),   http://dx.doi.org/ 10.7486/DRI.95944s31k   the Dublin City Archives (Dublin City Electoral Lists, 1915),   http://dx.doi.org/ 10.7486/DRI.9593zg12h   and the National Irish Visual Arts Library (Michael Healy Collection).   http://dx.doi.org/ 10.7486/DRI.95944s32v    Our assessment procedure enhanced our understanding of the types of relevant digital assets held nationally, and gave us insight into the challenges faced by collection owners in ensuring that their content is digitally preserved. The second phase of implementation included a scoping exercise and requirements analysis for each collection, the creation of a project plan and allocation of resources. Requirements gathering was conducted through interviews with collection owners, undertaken by the DRI’s Requirements Manager and a Digital Archivist or Librarian in all cases. Work plans were created with tasks including digitisation, metadata creation and standardisation, ingest preparation and collection creation, review and publication. We worked closely with each collection owner to ensure that the processes and workflows we created could be repeated, and that these were reflected in our existing guidelines.  Following the completion of the work plans, a digital preservation workshop was held at the Royal Irish Academy to ensure that collection owners were trained in the procedures required to prepare their content for ingestion into the DRI Repository - all applicants to the original call were invited.  4 Findings From the initial submissions to the call for expressions of interest, it was clear that there is a need in the community, not only for preservation services, support and advice, but also for digitisation support. Digitisation services were requested by nearly all of the applicants, and in some cases digital preservation was not included in the application. It appears that Irish archives lack resources (i.e. staff time and equipment) for digitising their collections. While perhaps not surprising, the technical infrastructure required to provide robust digital preservation was also not available to any of the participants. This confirms the need for shared infrastructures, or indeed shared strategies on preservation, on a national level.  Regarding the three selected collections, it was found that while metadata standards had been applied, they were in some cases customised according to the needs of the collection owner. Furthermore, although ISAD(G) compliant descriptions had been used by two collection owners, these could not be exported from the cataloguing software as EAD-XML, and needed to be manually marked up using an XML editor. These limitations create a barrier to metadata exchange and indicate that some archives are not planning for interoperability with other collections and/or repositories. While this was indicated in our 2012 report, Digital Archiving in Ireland, this project highlighted the practical difficulties involved in overcoming these barriers to interoperability. [2] The DRI guidelines and workflows were tested by the process of preparing content for preservation in the Repository and found to be comprehensive and robust. However, the underlying knowledge in the community regarding standardised metadata creation and the principles of digital preservation was not well developed. The preservation workshop held at the RIA was booked to capacity with a waiting list, and training in basic digital preservation and metadata preparation was identified as a requirement for our stakeholder community. 5 Conclusion The award has allowed the DRI to engage with a number of new stakeholder organisations who had not previously undertaken any digital preservation processes for their collections. Through the award, and the subsequent preservation workshop, the team worked with seven organisations who had not previously deposited content with the Repository. As well as providing training to allow participants to deposit with DRI, advice was also provided on smaller scale, in-house preservation practice which participants could bring back to their organisations.  The DRI believe that digital heritage is at risk of destruction and loss if action is not taken. Digital decay is the gradual decay of digital content. The solution is digital preservation – active ongoing data management. Long-term digital preservation is concerned with providing sustained access to digital objects and content and requires that institutions are cognisant of the processes and procedures required to ensure the form, as well as functionality, of digital objects.  DRI actively engaged with our designated community and have sought participation throughout our phases of development - from our requirements analysis phase, through to user acceptance testing and content population. Therefore, while the DC call provided an opportunity to preserve and publish content it also provided a platform from which to engage with an important national programme of events, communicate with a wider audience and test our user guidelines and documentation. Crucially, it also emphasised the fact that long-term preservation is a socio-technical problem - the solution requires not just digital infrastructures but advocacy, industry and societal engagement, and cooperation with content owners.  In addition to the practical aspect of this work we wanted to highlight the need for the national programme of commemorations to be cognisant that digital collections or projects created now, should be considered as historical artefacts for future historians. That is, our current digital commemorations should be preserved for future use and analysis. Commemorative events (both online and offline) are performative acts of nationalism and are an integral part of how we understand both our current and future selves. Current national projects should consider how they are preserving Ireland’s digital cultural identity for 2116. Acknowledgements We acknowledge the support of the Irish Research Council's New Foundations Programme. ",
       "article_title":"Preserving Ireland’s Digital Cultural Identity towards 2116",
       "authors":[
          {
             "given":"Sharon",
             "family":"Webb",
             "affiliation":[
                {
                   "original_name":"Sussex Humanities Lab, University of Sussex",
                   "normalized_name":"University of Sussex",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00ayhx656",
                      "GRID":"grid.12082.39"
                   }
                }
             ]
          },
          {
             "given":"Rebecca",
             "family":"Grant",
             "affiliation":[
                {
                   "original_name":"Digital Repository of Ireland",
                   "normalized_name":null,
                   "country":"Ireland",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-01",
       "keywords":[
          "English",
          "archives, repositories, sustainability and preservation"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" William Shakespeare is arguably the greatest dramatist of all time. Yet, the man and his works are shrouded in mystery and uncertainty. This paper posits that a man named Shakespeare wrote the First Folio. It is this First Folio that provides the “most certain” body of Shakespeare’s plays. The makeup of the Shakespeare dramatic canon has prompted more attribution studies (traditional and non-traditional) and caused more controversies than any other canon – by far. This paper looks at the several hundred non-traditional studies (and concomitant “flame wars”) and points out some of the more serious problems. There is no doubt that it is a canon in disarray. Most of the scholars involved in the controversies are the “heavyweights” of Shakespearean studies – e.g.:  Rasmussen (1977) vs. Hope (1994) Vickers (2011) vs. Craig and Kinney (2009) Taylor (2015) vs. Stern (2004) Craig vs. Vickers and Jackson (Hirsch and Craig, 2014)  But the most famous controversies involve (1) Donald Foster vs. Ward Elliott and (2) Robert Valenza and Donald Foster vs. the world. In a recent article (Rudman, 2016) I pointed out many caveats to scholars working on authorship attribution on the canon of William Shakespeare. Among these are:  Reproducibility Input Texts Genre Editing Controls Isolation of Variables Choice of Style Markers Statistical Tests Sample Selection and Size Treatment of Errors Collaboration  In this paper, I expand on one of these caveats (Genre), look at and cite examples from many of the non-traditional studies of the Shakespeare canon in order to highlight these problems, and suggest solutions.  Each genre is governed by different linguistic rules and rhetorical purpose – a practitioner should not mix genre. Drama – Comedy, History, Tragedy, Romance – how far down should these be catagorized Verse within the drama – rhymed verse within the verse – how far down should this be broken Music Dialogue vs Dramatic Monologue  Also in this paper, I address the conundrum of using what I deem as seriously flawed studies to show problems with other studies – e.g. if a practitioner mixes genres (history and tragedy) but shows that Shakespeare's style changes over time, I cite this change as evidence that chronological constraints must be employed. No matter how flawed I consider a study, there are parts of that study that are correct – there is no non-traditional study (of the hundreds conducted) that is completely without merit. By invoking the  etiam si non est verum paradigm, I show how almost all of the necessary steps in a valid study exist in the literature. We can look at all of the links in he chain (even the broken ones) and try to piece together what we should do to move the field forward. The evidence in this paper reinforces my conclusion from the  JEMS article that any attribution results are problematic at best.  The Conclusion reached (and I believe supported by very strong evidence) is that all of the non-traditional studies are seriously flawed and that as of today we do not have a valid Shakespeare text to conduct non-traditional attribution studies. The following bibliography is only representative. There are more than 200 non-traditional studies on the Shakespeare canon. My working bibliography for this study is well over 1,000 entries. ",
       "article_title":" Some Problems in the Non-Traditional Authorship Attribution Studies of the Dramatic Canon of William Shakespeare: Are they Insurmountable  ",
       "authors":[
          {
             "given":"Joseph",
             "family":"Rudman",
             "affiliation":[
                {
                   "original_name":"Department of English, Carnegie Mellon University, United States of America",
                   "normalized_name":"Carnegie Mellon University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05x2bcf33",
                      "GRID":"grid.147455.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-29",
       "keywords":[
          "English",
          "stylistics and stylometry"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Compared to the tradition of analytical palaeography it builds on, how is digital palaeography transformative? In this paper  A preliminary version of this paper was presented at the international workshop  Digital Paleography. Projects, Prospects, Potentialities (Università Ca’ Foscari, Venice, April 10-11, 2014), by invitation of the organisers, Università Ca’ Foscari (Venice) and Digital Humanities Lab (DHLAB), EPFL (Lausanne).  I will reflect on the emergent meanings and possible research directions of digital palaeography by analysing the last twelve years of approaches and conceptualisations in the field. Moving between a formal and an historically situated analysis, I will show how digital approaches relate to the scholarly tradition of the study of handwriting and writing systems as a whole. Digital palaeography will emerge well positioned to represent the complexity of handwritten objects from an unfamiliar perspective by departing from the structure of the expression of handwriting (text as shape).    Words in context The adoption and development of digital tools and resources for palaeography and manuscript studies are intertwined with fluctuating cultural attitudes (Busa, 1961; Morando, 1961;  My thanks to Willard McCarty for recommending and providing access to these two references. McCarty, 2014b; Nyhan et al., 2015). The convergence towards the use of digital  On the cultural understanding of digital, calling for a wider semantic spectrum that transcends the opposition with analogue, see for example McCarty (2014a). The  plus sign is a deliberate borrowing of Cecire's reflections on the “problem of the plus” being addictive rather than transformative (McCarty, 2014b: 292): “it should not be possible to have the “plus” without the two terms—“digital” and “humanities”—themselves changing” (Cecire, 2011).  coupled with humanities — digital humanities  For a recent discussion on the first occurrence of the term digital humanities and its uptake in the first decade of 2000 see Faulhaber (2015).  — and therefore palaeography — digital palaeography — denotes the methodology of research being enabled rather than the symbolic form of its objects of analysis or of its outcomes. The scope within which I propose to discuss digital palaeography  My thanks to Peter Stokes for having pointed out to me another sense of digital paleography as attributed by Hirtle (2000), who talks about this new “speciality” as the ability to convert obsolete file formats containing digital information (e.g. HTML, JPEG) into current formats. For an overview on the term see Stokes (forthcoming). is hence mainly methodological.    Terms Semantics Overall emergent meaning   Computational (e.g. Computational Palaeography)  Process-ability of data  Digital Palaeography ⊃ Computational Palaeography    Digital (e.g. digital vs. analogue)  Representational form of data    Digital + (e.g. Digital Humanities; Digital Palaeography)  Research methodology    A formal knowledge of handwriting — e.g. about scripts morphology or terminology used to describe it —  Cf. the 15th century treatises on scriptural typologies e.g. as described in Montecchi (1998: 119). Not only palaeography as a discipline — from the 1930s with Bischoff at least, onwards — has subscribed to analytical methods, but more in general the perception of text as divisible entity in opposition to the notion of the ungraspable composition of images has prevailed in humanistic enquiries with few exceptions (e.g. semiotics of art).  Modern art has exploited this cultural conflict between reading practices that put emphasis on symbolic aspects of written text as opposite to the morphological and sensorial aspects of other artefacts. See for example the works of Information as Material (Dworkin, C. et al., 2012) such as the exhibition Learn to Read Differently (Northern Gallery of Contemporary Art, Sunderland, August 10– September 23, 2013). Computer sciences and image processing techniques offer an  addendum, a perspective that suits nicely methodological traditions and inclinations of the classificatory minds of palaeographers. Yet, my aim in this paper is to identify any transformative aspects (table 1). So, even if digital palaeography follows a long tradition of analytical approaches to handwriting and an even longer human wish to control writing systems, does it actually affect our conceptualisation of handwriting? How is digital palaeography transformative (in the sense of  digital +)? or is there a digital + palaeography?    Projects rationales and self-narratives I will summarise some 2004-onwards projects and activities (Ciula, 2004a, 2004b, 2005a, 2005b, 2005c, 2009;DigiPal, 2011-14;  Exploratory Workshops, 2011)  In addition, various events connected to research in digital palaeography took places towards the end of the first decade of 2000 in Europe; in particular, two dedicated symposia on  Codicology and Palaoegraohy in the Digital Age took place in Munich and resulted in two volumes (Rehbein et al., 2009; Fischer et al., 2011). The ESF workshop also had a follow up in the Dagstuhl Perspectives Workshop on  Computation and Palaeography: Potentials and Limits held in 2012 (see ) which resulted in the publications Hassner et al. (2012, 2013).  which witness a critical engagement with digital technology, informed by diverse modelling processes and a constructive discussion of the limitations of computational tools. These approaches challenge the notion of palaeography as an auxiliary discipline towards a renewed return to an “integral” (Boyle, 1984) perspective which places palaeography within a wider multi and interdisciplinary framework, linking it with philology, linguistics and even cognitive sciences.  See for instance the ORIFLAMMS project blog (Stutzmann) where the connection to neurosciences is explicit (this project started in 2012). The analysis on terminology as well as an overview of practices put the emphasis on a self-reflective approach around the analysis of handwriting beyond strictly computational concerns.    Creating and deflating models In addition, I will reflect on the potentially productive dilemma digital palaeography approaches suffer from; a dilemma that is made more acute in recent practices compared to the already vivid debates in the 1970s between the historical and “Cartesian” approach (Gumbert, 1976) and in the 90s (Costamagna et al., 1995, 1996). On one hand, palaeographers engaged with the digital are busy building things, what Godfrey-Smith (2009: 108) would call a specific type of models or “imaginary concreta”  (creatures in between reality and fiction, between the schematic and the concrete); on the other hand, they are engaged in reflecting about their own practice and in so doing deflate the same models they build.  For a broader adaptation of Godfrey-Smith's “deflationary view” (2009:115) as a “deflationary account of modelling practices” in digital humanities see Ciula and Marras (in press).   In a paper questioning the connections between a scriptorium and its products, Ganz states: “searching for the distinctive details of letter forms shared by scribes may risk the application of an over rigid positivism to the study of manuscripts.” (Ganz, 2015)  Not to dismiss this warning against positivism in digital humanities,  This is a concern beyond digital palaeography with respect, for instance, to trends in distant reading; see for example Eyers (2013).  I will claim that what a digital palaeography approach as contextualised earlier on brings to the fore is precisely this awareness and hence the questioning of the 'mechanics' of a topographical or taxonomical analysis. By asking “what is the unit of handwriting? what we considered it to be?,” a digital palaeographer is aware that even by getting closer to the supposed materiality of the artefact — e.g. through high resolution images and microscopically segmented image features — she does not lose the lenses palaeographers have being using for interpreting such artefacts in the past, but can consciously decide to put them to test. In this lies one of the paradoxes of the digital and of modelling more in general: it brings perceptual materiality to our scrutiny while taking us away from it. The digital models are used to analyse the objects they are models of, but are also self-reflective tools to question those same models.  A similar point with respect to computational modelling of literary novels was made recently by Piper (2015: 68): “We not only gain insights into the specific subset of texts identified by the model, as the model provides the interpretive horizon through which these texts assume new meanings. But we recursively gain insights into the computational model itself through the detailed analysis of the texts it has identified.”     Semantics and materiality Palaeographic research with its focus on the perception of handwriting in morphological terms is a reminder that the handwriting manifests itself as an artefact that is rationalised and divided (hence constructed) only after it is given. By bringing to the fore the picture of writing or the writing as picture (cf. “text as shape” vs. “texts as meaning” in Hassner et al., 2012: 193), palaeographical studies live on the intermedia dimension of handwriting. By this I intend the crossing of the analytical media border between the sensorial and the conceptual qualities of handwriting, form and meaning, visible and invisible, between token and type, langue and parole. An adapted Hjelmslevian semiotic model of language exemplifies this intermedia dimension of writing.  While this model can be applied to handwriting and writing alike, the variety of grades of expressions as demarcated in handwriting are more diverse than, for instance, in printed documents.     Figure 1. Interplay between the substance of the expression (the physical medium, the ink on the parchment) and the form of the content (the semantics of the text, its meaning)   These reflections will allow me to sketch the intermedia borders where I think digital palaeography sits. I will build on an unpublished paper (Ciula, 2006) where I used McCloud's (1994) triangular map of visual iconography to represent the relationships between cultural textual objects and their digital (visual) representations both in graphical and in textual forms.  An alternative circular graph representing the multidimensionality of text encompassing the whole spectrum from physical medium (substance of the expression) to semantics (form of the content) can be found in Sahle's wheel of text (2006, 2013). See also Pierazzo (2013, 2015: 52) who drew two graphs collocating respectively editorial theories and editorial formats on a continuous axis with respect to their relationship with the materiality of text.      Figure 2: McCloud’s Big Triangle or a map of visual iconography ()      Figure 3. By following the analogy to McCloud’s triangle, in Ciula (2006) I showed how the variety of ways of representing a textual object in visual terms can assume both the form of more or less resemblant representations—images of the physical object bearing the handwriting—or their textual counterparts   The visualisation of encoding is an example of structure-oriented visualisation which shows how a graphic rendering of the text does not have to relate unequivocally to features of the textual object as expression (whether form or substance of the expression) but can rather represent one or more supposed structures of the textual content. When made explicit and visible, the structure of the content on the textual-symbolic side can play a fundamental role in the implementation of a thoughtful connection between the image-iconic representation/s of the text and the textual-symbolic content representation/s of a cultural object.  Visualisations of dynamic editions (Buzzetti and Rehbein, 2008) integrating textual expression (form or structure of the expression) and semantic model (form or structure of the content) could also exemplify this connection. A concrete example of a project which is attempting at revealing visually the deep connections between the palaeography of texts (handwriting styles of Scottish charters in this case) with the textual content (the representation of authority) is  Models of Authority (2014-17).      Figure 4. Digital paleographic methods as enhancers of iconic representations of textual artefacts   Following this analysis, digital paleographic methods (inclusive of image processing, image annotation and conceptual models blending morphology and semantics) will be theorised as enhancers of iconic representations of textual artefacts. They can bridge the sensorial/perceptive and structural/conceptual interpretations of handwriting, material and mental knowledge of text, visual and textual, spatial and temporal.  In my 2006 paper, I concluded that for a digital resource to be inspired by and to promote research based on the material/perceptual aspects of a cultural object, a high quality graphical representation of the cultural object is essential but not sufficient. In specular terms to what Buzzetti (2006)  My translation: “The challenge for the representation in digital form of any kind of information and for its adequately exhaustive and functional preservation is therefore given by the possibility of representing the text as a digital complex object and by the ability to reproduce in functional terms the forms of interaction between the structure of the expression and the structure of the content of textual information.” (Buzzetti, 2006: 55)  says about the symbolic components of textual objects, this paper will argue that the scope of digital palaeography lies in anchoring the structure of the expression of image-texts to the structure of their content, in other words in bridging the “semantic model” of a handwritten source to at least some of the material aspects of the artefact.    Conclusions Digital palaeography builds on the tradition of analytical palaeography. How is it then transformative or is there a digital + palaeography? Some digital palaeography projects and initiatives, including my own doctoral research, claim to be transformative by advocating for “integral” palaeography and by distancing themselves from a purely computational approach. Some also adopt a self-reflective perspective on the modelling of handwriting in a digital environment, for instance, by testing ontological commitments, categories, classifications of handwriting; in so doing they deflate the models they build. Further, when contextualised within an analysis of the border between form and meaning of handwritten sources, digital palaeography approaches can can be used to connect the structure of expression of handwriting with structures of meaning. A digital model which embeds both structure of expression and structure of content of the handwriting is then theorised as a unique contribution to reconstruct material textuality of cultural artefacts by bridging visual and symbolic elements of texts, spatial and temporal, perception and interpretation. Ultimately, digital palaeography can be transformative by bridging the semantics of written artefacts with their materiality.    ",
       "article_title":"Digital Palaeography: What is digital about it?",
       "authors":[
          {
             "given":"Arianna",
             "family":"Ciula",
             "affiliation":[
                {
                   "original_name":"University of Roehampton, United Kingdom",
                   "normalized_name":"University of Roehampton",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/043071f54",
                      "GRID":"grid.35349.38"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-25",
       "keywords":[
          "visualisation",
          "cultural studies",
          "history of Humanities Computing/Digital Humanities",
          "media studies",
          "digital humanities - nature and significance",
          "data modeling and architecture including hypothesis-driven modeling",
          "digitisation - theory and practice",
          "knowledge representation",
          "English",
          "bibliographic methods / textual studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction As Susan Schreibman (2014) points out, a digital edition, as opposed to a printed one, is never really complete as several layers of annotation may always be added to represent and enrich the original content. TEI (Burnard, 2014) allows for several types of information - textual, linguistic and semantic - to be layered and made explicit and retrievable by a machine. Such is the case for instance with what is commonly known as semantic tagging.  In this paper, we focus on Named Entities (NE), in particular names of Persons and Geographical Places. Adding NE mentions is supported by TEI with appropriate tags (such as <persName> and <placeName>), whose addition in a digital critical edition has somewhat the same function that indexes of places and persons have in a printed one. As mentions may be ambiguous (same string for different people, same place with different names,....) some referencing and disambiguating identifiers are required. But digital editions allow for much more than simple internal referencing. By pointing to external sources, structured information contained in the form of linked data in the semantic web becomes available to scholarly research.  In this work we present REDEN ONLINE, a system that enables scholars to automatically add external references to annotations of persons and places. The system is a web interface taking TEI as input, where mentions are already marked up, and automatically disambiguates and links such entities to an appropriate linked data set using a graph based algorithm for disambiguation. Moreover, our system provides data aggregation and visualization facilities by using the information found in the reference sources.   Previous work and general context Semantic tagging is a hot topic in the digital humanities. Tools for semantic enrichment are, such as  Pundit (Grassi et al., 2012, 2013), already available and allow for the interactive and intuitive annotation of portions of text. Automatic Named Entity Recognition and Linking techniques may be implemented to detect mentions and to suggest links to external knowledge bases.  Input formats to such systems may vary from plain text to html, but ideally a tool should process available standard formats, such as TEI-XML for text and RDF/OWL for information. Using linked data sources for disambiguation and enrichment is thus strongly recommended. By doing this, external sources of structured and regularly updated information can be made available to the scholar without having to be directly incorporated into the inline annotation, that can be left as simple as possible. This in turn allows for several customizable views, as linked data sources may be queried with the SPARQL query language to retrieve only the amount of external information that is necessary for a given task.  The treatment of spatial and temporal information is a typical task for which this approach is particularly effective; the availability of geographical databases and the complexity of the information are best accessed by pointing from within the digital edition to an external link. But also other types of semantic information seem to be particularly apt for connection to rich linked databases. So for instance bibliometric sources can be used to enrich texts with additional information on authors.  Typical targets for references are DBpedia and Geonames, that, for their genericity and connection to other sources, are at the heart of the linked data cloud. But they may be supplemented by more domain specific sources of information. For instance, Pleiades provides geo-historical information for ancient places.   Our project REDEN ONLINE is set against the background of work carried out at LABEX OBVIL in Paris, where quality digital editions for French literary texts and criticism are produced and used in research and higher education. Recently a series of projects were carried out to semi-automatically annotate and reference places, organizations and authors. Gold standards were also produced, in close contact with researchers in French literature, so as to establish guidelines of annotation that best suit their ongoing research.  The general purpose is to provide tools for both:  augmented close reading, to enable researchers to access more information on a specific text portion distant reading and data aggregation, so as to be able to detect trends in large portions of texts (Moretti, 2007)  OBVIL literary scholars are interested in plotting the distribution of the mentions of given authors over time in French literary criticism, in order to study the appreciation of Molière over the centuries, or in producing charts representing the distributions of professions in authors mentioned in given periods, to trace the influence of scientists and their ideas on art and literature in the age of positivism (Riguet, 2015). Other visualizations captured the emerging influence of foreign countries in the French literary panorama over time by combining the date of the publication of the essays with the detected toponyms. NLP technologies are used to facilitate various aspects of the semantic enrichment of TEI editions, in an annotation echosystem where texts are first processed and then manually checked. The detection of mentions of places, authors (and also organizations) was tackled by using a Named Entity Recognizer and Classifier (UNERD, Mosallam et al., 2014). Once the entities are correctly detected and classified, external references need to be added to disambiguate mentions and to connect them to additional information. To this purpose we developed REDEN   REDEN is open source; you can find the code at https://github.com/cvbrandoe/REDEN. , a Named Entity Linker that uses a graph-based algorithm and linked data sets to identify the correct referent for each mention (Brando et al., 2015, Frontini et al., 2015a, Frontini et al., 2015b for the technical details).   REDEN's input consists of a TEI text with detected mentions and several parameters specifying among others the class of entities to be detected, the reference base to use and a set of pre-compiled indexes. REDEN is applied for each class of entities separately, and works at best when several mentions are disambiguated at the same time. It retrieves all candidate referents for each mention of a context (say a paragraph) and then all the available information from the semantic web. It builds a sub-graph of all candidates and chooses the correct referents for each mention with the help of the formal relations between them. From Figure 1 you can get an intuition of how REDEN works.   Figure 1 The graph based algorithm disambiguates between different possible referents for the mentions of \"Victor Hugo\" (unambiguous in this example), \"Lamartine\" and \"Vigny\" (both having several candidate referents) based on information found in DBpedia. Correct referents (in grey) are chosen based on how well connected they are within the context Here the crucial node is clearly that of yago:RomanticPoets.  So far our efforts have concentrated on the production of a text annotation and referencing pipeline for the production of such enriched TEIs with annotated and referenced mentions. Their exploitation for data aggregation and visualization was carried out offline and with ad hoc processing tools. With REDEN ONLINE we now want to make linking technology available online while at the same time providing users with some generic visualization of the results.  In what follows, we present the REDEN ONLINE interface with some screenshots from an example where two texts of the Labex OBVIL   Find more information on OBVIL and ist digital library at http://obvil.paris-sorbonne.fr/.  digital library have been automatically linked to external sources, namely:  -  L’Hérésiarque et cie, a collection of short stories by Guillaume Apollinaire, published in 1910 - place mentions linked to DBpedia entries.   -  Réflexions sur la littérature a series of essays on French literary criticism by Albert Thibaudet, published in 1936 - author's mentions linked to entries in the linked data base of the Bibliothèque Nationale de France (BnF).   The user (Figure 2) loads a TEI text with annotated <placeName> or <personName> tags, chooses which class of entities to process (places or nouns) and the system runs the disambiguation and linking algorithm against the given linked data base - here French DBpedia and/or BnF. Then external information is extracted from the source and used for generating a particular view of the text. The result is a summing up of the disambiguated locations (some place names may be non resolvable because they are absent from the linked data base) and a visualization. For locations the visualization consists in an interactive map that also takes frequency of mention into account. Coordinates are retrieved from DBpedia when available and the map can be zoomed in, up to the level of streets (see Figure 3 where some places in Paris have been identified in the text by Apollinaire), when relevant.  For persons (see Figure 4), portraits of authors are automatically downloaded and visualized.    Conclusion The conference presentation will demonstrate REDEN ONLINE, a web based tool that enables researchers to connect place names and person names in their texts to existing linked data sources. The underlying technology will also be explained, in particular its use of standard formats, such as TEI and RDF for the linking algorithm, and GeoJSON for the creation of the map. We will also argue in favour of our economicity approach, namely the choice of not embedding semantic information in the TEI, which enables the use of different databases and the production of ad hoc \"views\" of the document. It is well known that aggregation and visualizations techniques may \"assist the critic in the unfolding of interpretive possibilities\" (Ramsay, 2008) when analysing texts. This tool has been particularly designed for the study of literature and literary criticism; in the presentation examples of use will be given using ongoing research on Apollinaire, highlighting how the visual representation of the itineraries contained in the stories may be considered as a form of novel \"digital reading\" of the text.   Figure 2 The REDEN ONLINE interface, with a sample text from Apollinaire. Place names results are visible as a map visualization.     Figure 3 The REDEN ONLINE interface, with a sample text from Apollinaire. Zoom on Paris of place name results.    Figure 4 A visualization of authors mentioned in Thibaudet’s “Réflexions sur la littérature”, frequencies are displayed in parenthesis.    ",
       "article_title":"REDEN ONLINE: Disambiguation, Linking and Visualisation of References in TEI Digital Editions",
       "authors":[
          {
             "given":"Francesca",
             "family":"Frontini",
             "affiliation":[
                {
                   "original_name":"Istituto di Linguistica Computazionale \"A. Zampolli\" - CNR Pisa",
                   "normalized_name":"Institute for Computational Linguistics “A. Zampolli”",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/028g3pe33",
                      "GRID":"grid.503055.6"
                   }
                }
             ]
          },
          {
             "given":"Carmen",
             "family":"Brando",
             "affiliation":[
                {
                   "original_name":"Institut National de l’Information Géographique et Forestière - Paris",
                   "normalized_name":"Institut Géographique National",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/05jxfge78",
                      "GRID":"grid.424645.5"
                   }
                }
             ]
          },
          {
             "given":"Jean-Gabriel",
             "family":"Ganascia",
             "affiliation":[
                {
                   "original_name":"Labex OBVIL - LIP6 (Laboratoire d’Informatique de Paris 6) Université Pierre et Marie Curie and CNRS",
                   "normalized_name":null,
                   "country":"France",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-26",
       "keywords":[
          "french studies",
          "visualisation",
          "literary studies",
          "natural language processing",
          "linking and annotation",
          "maps and mapping",
          "data mining / text mining",
          "content analysis",
          "English",
          "text analysis",
          "geospatial analysis, interfaces and technology"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" With this contribution, I would like to discuss how multi-layered visualizations of epistolary networks can contribute to a better understanding of the circulation of illegal literature and confidential ideas between Catholic Tuscany and the Calvinist Dutch Republic. It questions how intellectual exchanges between these two regions maintained a balance between, on the one hand, the necessity to distribute (prohibited) books and to express controversial ideas and, on the other, social control and the need to avoid the objections of powerful political and religious institutions and individuals. This comparative analysis allows for a sharper focus on the differences and similarities on how intellectuals capitalized on opportunities in the social and religious structures to which they were connected. Indeed, they had to deal with the many tensions between the oppressive catholic environment of the court of Cosimo III and the Dutch Republic, already well known for its relative tolerance and freedom of printing (e.g. Touber, 2014).  These personal and societal conflicts forced scholars and booksellers to take strategic measures of secrecy and confidentiality, which in turn depended on what Mauelshagen (2003) also called “networks of trust”. If we pose the question how epistolary networks evolved, understanding changing relationships between people, one might provide insights into aspects of confidentiality. For instance, as relationships grew friendlier, correspondence grew in confidence and trust, while on the other hand one did not correspond with adversaries (Heuvel, et al. 2014). Other examples include studies of Lux and Cook (1998), who claimed that the success of the Dutch Republic depended on what Granovetter (1973) also called “weak ties” instead of central hubs. This implies the importance of intermediaries between communities for the faster distribution of ideas: if you have a particularly close friend from another community, you are more likely to introduce him to your other close friends whom you know that will trust you (Barabási, 2009: 55).  If we wish to verify these statements and to understand how intellectuals were able to overcome these confessional and social barriers, the analysis of multi-layered networks of correspondences provides a very interesting addition to archival research. Or in other words, the combination of methods for network analysis for distant reading of large sets of letters with close reading devoted to detect the role of secrecy and confidentiality in epistolary exchanges strengthens historical research. This means that qualitative analysis will uncover how social relations are represented and constructed, sometimes reinforced and sometimes transformed, which is enriched by traditional hermeneutic methods to focus on specific religious and personal features that have influenced those dynamics. It is important to consider that full data integration, in particular when dealing with early modern letters, is impossible for reasons of incompleteness, complexity and uncertainty in data. Therefore the focus should not be on analytical and statistical methods of network representations alone, but on approaches that allows us to handle, inquire and interpret these complex historical data. We do not need just networks as static representations, but also networks as interactive interfaces (Heuvel, et al. 2016). To this end, the software tool Nodegoat is used to bring together, explore and contextualise these epistolary exchanges. Nodegoat, differently from network visual-analytical tools such as Gephi, is built around data entry, management and curation processes ( http://mnn.nodegoat.net/viewer). It enables us to explore and to combine historical networks in various configurations, involving a diverse set of actors. This means that not only persons (scholars and booksellers) constitute relationships but also textual objects, like books with their dedicatees and introduction letters. For instance, the overlay of networks highlights those artefacts that played an important role in the establishment of contacts. If we look at their intersection with their function in the network, new opportunities may rise about how to link book dedications to strategies adopted by scholars. The importance of textual objects as participants in networks has also been stressed by Latour (2005). From the correspondence of the Florentine librarian Antonio Magliabechi, for example, it turned out that the Dutch microbiologist Antoni van Leeuwenhoek (1632–1723) chose to dedicate his work the  Arcana Naturae Detecta to Magliabechi. By using this strategy, Leeuwenhoek was able to benefit from Magliabechi’s extensive network for the distribution of his work in Italy. Overlaying different networks sheds light on the role of the  Arcana in the network of Magliabechi (fig. 1 and 2). For instance, the image illustrates those correspondences in which Magliabechi mentioned the publication of Leeuwenhoek, showing in this way the diffusion of his publication over time.      Fig. 1 A visualization of networks around the  Arcana Naturae Detecta of Leeuwenhoek (the right yellow node) and Magliabechi. Magliabechi, at the centre of this visualisation, is surrounded by other dedicatees (represented in red), their accompanying books (yellow) and the letters in which the  Arcana is mentioned (light blue). Hovering over the nodes and ties opens an overview with the different connections, and specifies the nature of the relationships (see fig. 2)      Fig. 2 Overview of the  Arcana Naturae Detecta, linked to the Short Title Catologue of the National Library of the Netherlands (KB)  Moreover, the analysis of relationships between people can provide insight between direct and indirect transfer of confidentiality via intermediaries. In the Mapping Notes and Nodes in Networks project (Álvarez and Heuvel, 2014) it appeared that the overlay of more networks shed light on the evolution of co-citation networks and introduction networks. As correspondents entered networks of epistolary exchange, they did so not in some ideal egalitarian society, where anyone could join simply by writing a letter, but in a world regulated by social norms and rules of etiquette. In short, letters of introduction were often necessary to be admitted into an epistolary network. Following the evolution of introduction letters alongside a citation network and determine whether a shift takes place in the number of intermediates between correspondents (revealing shrinking degrees of separation), could reveal the importance of introductions in the establishment of epistolary networks.   Furthermore, Nodegoat is used to bring and contextualise epistolary networks by means of data integration from various data resources such as the Short Title Catalogue Netherlands (STCN) and archival research from archives in the Netherlands and in Italy. The STCN has been queried in order to disambiguate objects and to enhance the interoperability of my data. For example, information on books in epistolary networks can be linked directly to the STCN, which allows me to map the unstructured data in the letters (as titles are often mentioned incomplete) to structured data. ",
       "article_title":"Exploring Networks Of Confidentiality And Secrecy In Early Modern Transconfessional Correspondences",
       "authors":[
          {
             "given":"Ingeborg",
             "family":"van Vugt",
             "affiliation":[
                {
                   "original_name":"Scuola Normale Superiore di Pisa, Italy",
                   "normalized_name":"Scuola Normale Superiore di Pisa",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/03aydme10",
                      "GRID":"grid.6093.c"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "cultural studies",
          "databases & dbms",
          "historical studies",
          "networks, relationships, graphs",
          "English",
          "italian studies",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Defining the “state of the art” in Digital Humanities (DH) is a really challenging task, given the range of contents that this tag covers. One of the most successful efforts in this sense has been the international blogging event known as “DayofDH” or “A Day in the Life of the Digital Humanities” project, promoted and sponsored by centerNet ( http://www.dhcenternet.org/), which has put together digital humanists from around the world to document once a year what they do (Rockwell et al., 2012). The websites of DayofDH were hosted in North America until 2015, when it was coordinated in Europe by LINHD ( http://linhd.uned.es), the Digital Innovation Lab, at UNED in Madrid. Participants belong to several countries around the world.  The relevance of DH in non-English speaking countries has been quick and important in the last decade, and especially important in the Spanish-speaking world (Spence and González-Blanco, 2014; González-Blanco, 2013; Del Rio Riande, 2014a; Del Rio Riande, 2014b; Galina et al., 2015). Technological projects for humanities have existed in the Spanish world for many years; however, the discipline called “Digital Humanities” arose in 2011 with the first meeting that originated the Spanish Digital Humanities Association, HDH. This relevance is reflected in the creation of a parallel version of the DayofDH in Spanish, the “DíaHD”, which was hosted by the UNAM in Mexico in 2013 and 2014 and converged in the last initiative at UNED transforming both blogging events into a bilingual version of the Day.  Although there have been general studies about the information on participation in those events (Priani et al., 2014), there has not been an automated data analysis using NLP (Natural Language Processing) or Big Data tools to extract and classify the relevant information gathered in blogs (Webb et al., 2004). More technical details about these aspects can be found in (Tobarra et al., 2014b). According to this, the main goal of this paper is to develop a dashboard that allows us to get more insight about interest topics and leaderships of this community during the period of time in which this event has been developed. With the “dashboard” word, we mean the analysis and presentation of results, not a tool. In this sense, the topic characterization process deals with the detection of the most relevant topics which are employed in the publication tools of these kinds of virtual communities (Tobarra et al., 2014a). In order to achieve our aforementioned objectives, this work is focused on the datasets corresponding to four years of DayofDH (2012, 2013, 2014 and 2015 editions), and the Spanish version of the event in DíaHD 2013. This work strives at showing the evolution and trends in the last four years in order to give account of the presence of the Hispanic communities in the field. The information of the Spanish 2014 edition has been discarded, as it is not any more available online due to technical problems at the organizing institution. All editions of DayofDH employ WordPress, which has an associated SQL database, including several general tables and a specific set of tables per blog, defined in the project and common to all editions. The CMS is combined with the Buddypress social plugin, which lets users register, create communities and forums and interact among them. For the last edition of the Day, LINHD included also the bilingual plugin WPML to make it available the possibility of including translation in Spanish and English. This feature was, however, just used for the general website and its blog entries. The data employed in this proposal has been obtained by using web scraping techniques (Fredheim, 2014) in the DayofDH websites for the previously mentioned editions. In particular, humanists’ blog data, and their associated posts in the website have been gathered in this phase. All information scrapped from blogs is public and accessible from the Internet and, also, they have been anonymized for ethical issues. For validation purposes, the conceptual information about the database schemas have been compared with the extracted dataset, concluding the extraction process has been satisfactory. Since the data obtained are huge enough to be efficiently processed, the use of big data techniques have been considered for this work ( http://social-metrics.org/analyzing-big-data-with-python-pandas/). In order to achieve the main goals of the project, all the information related to the textual content in DayofDH have been processed, so that the most significant tokens are selected. Then, these tokens have been characterized by two parameters; first, it has been used the direct frequency which characterize if a token is used regularly in all DayofDH blogs. Secondly, the inverse frequency of the token that give information of how significant the token is in the context of digital humanities in a semantic way.  These parameters have been used to observe the interest and evolution of the characterized tokens along the time, either in a global and individual way. The interest of the global analysis is to find how the knowledge has evolved during the years of the study. From the point of view of a personal analysis, the interest is to build individual profiles that show the main interest of the researchers in the humanist community. Finally, the leadership relations have also been explored by using disease propagation techniques in the generated social network, taken into account the different editions of the DayofDH. For instance, Fig. 1 shows the social network according to the amount of authors’ participations.    Fig. 1. Social network generated for 2012-2015 editions of DayofDH The resulting graphics and visualizations (Tobarra et al., 2014b) let users make a quick idea of how the DH focus has been moving and distributing across the time through the different Academies in the different countries, but also how topics and interests change from one country to another and it is strongly related to their perspectives and disciplines, which are not independent from their origin (as an example, see Figs. 2 and 3). This approach will enlighten future studies on DH perspectives with real and precise data on the current state-of-the-art on DH perception and its evolution. Data of the years 2009, 2010 and 2011 are not used at the moment, as the same information is not available through web scraping. They will be incorporated to this study as a future work.    Fig. 2. Interest topics for 2012-2015 editions of DayofDH    Fig. 3. Interest topics for 2013 edition of DíaHD  Acknowledgements  This work is part of next research projects funded by MINECO and leaded by the professor Elena González-Blanco; Research Europe Action EUIN2013-50630: Digital index of European poetry (DIREPO) and FFI2014-57961-R; and the Digital Humanities Innovation Lab: Digital edition, Linked data, and Research virtual environment for Humanities, in addition to the European project ERC-2015-STG-679528 POSTDATA. Authors would also like to acknowledge the support of the research project (2014I/PPRO/031) from UNED and Banco Santander, and a local project (2014-027-UNED-PROY). Furthermore, we thank both the Region of Madrid for the support of E-Madrid Network of Excellence (S2013-ICE2715) and the Spanish Government for the support of SNOLA Network of Excellence (TIN2015-71669-REDT). ",
       "article_title":"Researchers’ perceptions of DH trends and topics in the English and Spanish-speaking community. DayofDH data as a case study.",
       "authors":[
          {
             "given":"Antonio",
             "family":"Robles-Gómez",
             "affiliation":[
                {
                   "original_name":"Spanish University for Distance Education, UNED",
                   "normalized_name":null,
                   "country":"Spain",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Elena",
             "family":"González-Blanco",
             "affiliation":[
                {
                   "original_name":"Spanish University for Distance Education, UNED",
                   "normalized_name":null,
                   "country":"Spain",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Salvador",
             "family":"Ros",
             "affiliation":[
                {
                   "original_name":"Spanish University for Distance Education, UNED",
                   "normalized_name":null,
                   "country":"Spain",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Gimena",
             "family":"Del Rio Riande",
             "affiliation":[
                {
                   "original_name":"CONICET, Universidad de Buenos Aires",
                   "normalized_name":"University of Buenos Aires",
                   "country":"Argentina",
                   "identifiers":{
                      "ror":"https://ror.org/0081fs513",
                      "GRID":"grid.7345.5"
                   }
                }
             ]
          },
          {
             "given":"Roberto",
             "family":"Hernández",
             "affiliation":[
                {
                   "original_name":"Spanish University for Distance Education, UNED",
                   "normalized_name":null,
                   "country":"Spain",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Llanos",
             "family":"Tobarra",
             "affiliation":[
                {
                   "original_name":"Spanish University for Distance Education, UNED",
                   "normalized_name":null,
                   "country":"Spain",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Agustín C.",
             "family":"Caminero",
             "affiliation":[
                {
                   "original_name":"Spanish University for Distance Education, UNED",
                   "normalized_name":null,
                   "country":"Spain",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Rafael",
             "family":"Pastor",
             "affiliation":[
                {
                   "original_name":"Spanish University for Distance Education, UNED",
                   "normalized_name":null,
                   "country":"Spain",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-25",
       "keywords":[
          "digital humanities - multilinguality",
          "digital humanities - pedagogy and curriculum",
          "digital humanities - diversity",
          "digital humanities - institutional support",
          "digital humanities - nature and significance",
          "digital humanities - facilities",
          "data mining / text mining",
          "content analysis",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Founded in 1922 by the League of Nations upon observation that the pacification of Europe may benefit from a better collaboration between scientific elites, the  International Committee on Intellectual Cooperation (ICIC) is responsible for coordinating the restructuration of knowledge circulation. Bringing together leading researchers at the height of their career, such as Albert Einstein, Marie Curie and George Hale, chaired by Henri Bergson, the Committee weaves a complex network between transnational scientific institutions and societies, congresses and individuals (Pernet, 2014).   This paper proposes an analysis of the work and functioning of the organization between 1919 and 1927 by setting up a database containing metadata of thousands of documents contained by the ICIC funds (United Nations Archives, Geneva). Visualized as a network of 3.200 people (tens of thousands of relationships), this work provides a new understanding of the internal organization of the Intellectual Cooperation, as well as completely new insights about its relations with the rest of the scientific and diplomatic world. In particular, we will show the necessity to compare the “micro” structure of relationships as mapped by the archive with the “macro” formal structure of the institution. Do the thousands of documents, in a  distant reading approach (Moretti, 2013), confirm the internal organization of the League of Nations or do they show individuals/communities that bypass the official hierarchy?   As an opening to an epistemological debate, this research questions the relationship between the researcher, the database and its sources: are the metadata of an archive corpus usable information, regardless of their unique qualitative content? More technically, it also addresses the issue of data visualization and modeling in the historical sciences.   Sources Initially launched in 1922 as a consultative commission, the ICIC quickly mobilized the major part of the  International Bureaux section secretariat (upon its stabilization a few years later as a permanent commission), resulting in the production of very vast archives. In the period covered by our study, 1919-1927, which can be qualified as the start-up years of the dynamics of intellectual cooperation in Europe and the World, the funds contain 27.000 documents, mostly internal and external correspondence, about the main missions of the commission: university relations, bibliography coordination, educational matters and various enquiries.    Figures 1-3. From a relational database to a network. The relational database links documents with their agents (Fig. 1), a relation that is then mapped as a 2-mode network (Fig. 2). By projecting the documents on the agents, the 1-mode network of co-occurrencies is fully exploitable (Fig. 3).    Methodology We are particularly interested in individuals who are personally concerned by the documents. Firstly, we indexed all the documents by creating a relational database (fig.1) of all \"agents\" (senders and receivers). In order to analyze the co-occurrences of agents in the same document, the database, displayed as a 2-mode network (fig.2), is projected onto a 1-mode network (fig.3). Each of the 3.200 agents are connected to its co-occurring by an edge whose weight reflects the intensity of the relationship.   Data analysis and visualization This paper presents the final result of years of manual indexing (intermediary results have already been presented as case-studies in Grandjean, 2015 and Grandjean, 2014). The complete graph (fig.4) displays 38.600 co-occurrences between 3.200 agents of the complete set of documents from 1919 to 1927. The size and color of the nodes are proportional to the number of appearances of the individuals in the index. The size and color of the edges are proportional to the number of co-occurrences of the two people they bind.  Beyond an apparently low visual intelligibility, due to the amount of information and the dataset complexity, such a graph already makes the measurement of its mathematical properties possible (centrality measures, as detailed by Koschützki et al., 2005 or Newman, 2010). Developed as an interactive online visualization, it provides a global view and a more instinctive navigation in the archive directory. This type of graph is necessary in order to observe what happens at the margins of the institution (and thus also to understand the geography of the object: what is central and what is peripheral). However, as so often in network analysis, the core is so dense that we can not distinguish the edges and therefore makes a more advanced visual analysis impossible. Here comes the challenge of readability: how to show that elements are not only connected horizontally to other elements by maintaining a macro-structure that does not always correspond to the natural organization of the agents?  As many opportunities to play with scale exist (Brailly and Lazega, 2012), we chose to flatten the institutional organization on the relational structure (fig.5). Hence our research question: do the scientists, diplomats and senior officials - who constitute the network of intellectual cooperation - structure their relationships in coherence with the organization of their institution? Or, are they the ones that determine the links that their institutions maintain together? Forced distribution of nodes under an administrative “geography” make it easier to read the edges between groups. We also note that this is a way to provide a spatial distribution that does not vary over time, and thus allows the study of several successive moments of the network, without losing the mental map. This system is also suitable for superimposing prosopographical information.   Figure 4. The network of the Intellectual Cooperation. 38.6K co-occurrences of 3.200 agents of the documents in the League of Nations’ Intellectual Cooperation archives (1919-1927, 27K documents).    Figure 5. Untangling the network. Same network as Fig.4 but mapped according to the institutional structure of the League of Nations, introducing a 3rd dimension.    Perspectives It is often at the periphery of the network that the most interesting personal trajectories may be found. As such, this display allows us to discover and highlight the thematic affinities of some of the privileged interlocutors of the ICIC: government delegates, heads of international scientific organizations or partners seeking asylum under the authority of the League of Nations. The consistency of many internal networks can be evaluated: is the plenary commission a clique (cluster where each and every node is connected to all the others)? Are the expert sub-committees coherent communities? And are these communities created by a well defined group of documents or by a heterogeneous collection of correspondences on various themes? Pushing further the distant reading, we will also see that the macro-analysis of the institutional level reveals structures that were not clear at the individual level, showing the need for a constant back-and-forth between the scales. This will also be an opportunity to recall that network analysis is a modeling process that does not relieve the researcher of the consultation of the archival documents themselves. From a quantitative approach, we return to the qualitative: the structural organization of a network is definitely a “qualitative, morphological” information (Moretti, 1999; 68) derived from the quantitative compilation of individual relationships.  ",
       "article_title":"Archives Distant Reading: Mapping the Activity of the League of Nations’ Intellectual Cooperation",
       "authors":[
          {
             "given":"Martin",
             "family":"Grandjean",
             "affiliation":[
                {
                   "original_name":"University of Lausanne, Switzerland",
                   "normalized_name":"University of Lausanne",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/019whta54",
                      "GRID":"grid.9851.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016",
       "keywords":[
          "visualisation",
          "data modeling and architecture including hypothesis-driven modeling",
          "historical studies",
          "knowledge representation",
          "archives, repositories, sustainability and preservation",
          "English",
          "networks, relationships, graphs"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" One of the most discussed issues in Biblical studies in the past 15 years is the history of Biblical Hebrew. Standard works on this issue assume that one can distinguish between Archaic Biblical Hebrew, Early Biblical Hebrew and Late Biblical Hebrew (Saenz-Badillos; 2004, Hurvitz; 2013). This position has been challenged in a number of recent publications, in which the authors state that the variation that can be found in Biblical Hebrew is better explained by assuming that there have been different styles of Biblical Hebrew (Young, Rezetko and Ehrensvärd, 2008) in use throughout the biblical period, which is roughly the whole first millennium BCE. A complicating factor in the research on the biblical texts is that we know relatively little about their transmission in the centuries after their composition. In general the manuscript used for research on Biblical Hebrew is the Codex Leningradensis, which was created in 1008/1009 CE. There exist older manuscripts of the Hebrew Bible; by far the most important ones are those found in the Qumran Caves which can be dated to the beginning of the Common Era, but many of these manuscripts survive in a fragmentary state. Many studies on the diachrony of Biblical Hebrew are concerned with Hebrew vocabulary. These have resulted in long lists of early lexical items that were supposed to be replaced gradually by late alternatives. These late alternatives can often be identified as loans from languages like Aramaic and Persian (Young, Rezetko and Ehrensvärd, 2008). One of the problems of studying vocabulary as a gauge of linguistic change is that the vocabulary could have been manipulated easily during the process of transmission. Scribes could change words, thereby consciously archaizing the language of a text. In order to solve this problem, we study syntax instead of vocabulary. Forming sentences takes place on a less conscious level than choosing words, and therefore this is a better way of studying continuity and change in the history of Biblical Hebrew. In our study we investigate the use of prepositions accompanying a whole range of verbs. In the literature on linguistic variation in Biblical Hebrew the use of prepositions and other function words in various contexts has been studied before, but this has always been done in a very restricted way. Sometimes only a few biblical texts had been studied or the data had been extracted from one manuscript exclusively (Hornkohl, 2014:218-38). In our research we will focus on verbs of motion and on stationary verbs. In the former category we find verbs like בוא (bōʔ, to come), עלה (ʕālā, to go up) and יצא (yāṣā, to go out), in the latter we find verbs like ישב (yāšav, to sit) and עמד (ʕāmaḏ, to stand). These verbs have in common that in most cases they have a locative as complement, which is often introduced by a preposition (Oosting, Dyk and Glanz, to be published). It is known that various prepositions can be used with a given verb and this variation can be found in parallel texts in the Codex Leningradensis, within specific biblical books and between different manuscripts (Kutscher, 1974). The use of function words like prepositions is well known in authorship attribution (Argamon and Levitan; 2005, Garcia and Martin; 2007, Segarra, Eisen and Ribeiro; 2013), but in the case of ancient religious texts, detecting the author of a text is a controversial issue. Not only could texts have been adapted during the transmission of the complete text, also the composition of a text may have had a long history. Therefore we do not try to find the supposed author of a text, but based on the study of prepositions accompanying verbs of motion we would like to find out what is the main factor of the variation in the use of these prepositions. Is it related to diachronic development of the Hebrew language or to the way the texts were transmitted or both or are there still other options? We investigate these issues by comparing the thousands of instances of prepositions accompanying verbs of motion and stationary verbs in: 1. different books in the Codex Leningradensis (Genesis, Exodus, etc.) 2. different manuscripts (e.g. compare Isaiah in the Codex Leningradensis with the Great Isaiah Scroll) 3. parallel texts. This kind of research can only be conducted in a proper way if the textual data is in place, not only for the research proper, but also for those who want to reproduce this research later on. Therefore we base our research on the Amsterdam Hebrew Text Database (Van Peursen et al., 2015). This database is Open Access and can be downloaded from DANS, a national research archive in the Netherlands. Without downloading, the material in the database can be accessed through the website SHEBANQ (https://shebanq.ancient-data.org and Roorda, 2015b). Here the text of the Codex Leningradensis can be browsed, and while doing so, the user has access to a wealth of annotations that represent linguistic information and additional observations. In particular, there is an extensive set of cross-reference annotations between virtually all parallel passages (Roorda, 2015a). With the help of clustering techniques and entropy calculations we are investigating the challenge of linguistic variation in Biblical Hebrew and the transmission of the biblical texts. Results of others (Hornkohl, 2014:218-38, Rezetko and Young, 2014:380-83) and ourselves show that this approach leads to significant progress in our understanding of these issues. The relevance of this research for digital humanities in general is that it explores challenges such as working with ancient languages of which we have only fragmentary evidence and (religious) texts with a long history of composition and transmission. While there is a lot of literature on these issues in traditional studies, it is clear that digital methods of research have not been pursued to their full potential yet. ",
       "article_title":"Linguistic Variation In The Hebrew Bible: Digging Deeper Than The Word Level",
       "authors":[
          {
             "given":"Martijn",
             "family":"Naaijer",
             "affiliation":[
                {
                   "original_name":"Vrije Universiteit Amsterdam, Netherlands, The",
                   "normalized_name":"VU Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/008xxew50",
                      "GRID":"grid.12380.38"
                   }
                }
             ]
          },
          {
             "given":"Dirk",
             "family":"Roorda",
             "affiliation":[
                {
                   "original_name":"Data Archiving and Networked Services",
                   "normalized_name":"Data Archiving and Networked Services",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/008pnp284",
                      "GRID":"grid.500519.8"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-26",
       "keywords":[
          "visualisation",
          "corpora and corpus activities",
          "data mining / text mining",
          "historical studies",
          "linguistics",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The aim of this paper is to discuss how the new scholarly publishing models proposed by the Digital Humanities, if properly executed, may also serve to increase geopolitical diversity in the field. If not, they may well replicate vicissitudes of the traditional scholarly system and perpetuate the current deficiencies. The scholarly communication and publishing system currently in place is a highly complex and international structure that has over the past few decades come under increasing criticism as scholars debate its effectiveness, in particular in relation to new possibilities enabled by digital technologies. At the centre of these debates is the fact that scholarly publishing is both a communicative and collaborative practice that is vital for knowledge construction, as well as being an integral part of the academic reward system and therefore a essential part of both power and prestige within academia. It is directly linked to activities such as hiring, tenure, assignment of grant money, to name a few.  Researchers from periphery countries are sorely underrepresented in the global scholarly publishing system. They have relatively little participation in “international journals” where we find a dominance of publications from researchers in developed nations. Scientific production is measured in a number of ‘core journals’ that are determined by indexing services that tend to favour publications from certain regions of the world and published in English. This leads to the invisibility of research produced in periphery countries as “structural obstacles and subtle prejudices that prevent researchers in poor nations from sharing their discoveries with the industrial world and with each other” (Gibbs, 1995). The work being done in periphery countries does not participate on the global stage. Fiormonte (2015) analyzed Digital Humanities literature and found a strong predominance of citations to publications in English and articles about English speaking institutions and projects. In terms of knowledge construction it is important to note: “how the values of the Western intellect traditions are reflected in the conventions and practices of academic communities and their communications; how mainstream journals and their publishing practices are congenial to the interests of center knowledge while proving recalcitrant to periphery discourses; and how academic writing/publishing functions are an important means of legitimating and reproducing center knowledge” (Canagarajah 2002). As such we find that there is a marginalization of peripheries in the production of knowledge and the impact of the research.  The predominance of knowledge production from a handful of countries has important consequences, in particular the Humanities that require multilingual, multicultural heterogeneous environments if they are to fully represent the wide spectrum of human diversity. The Digital Humanities is a community that not only represents itself as collaborative and open (Spiro, 2012) but also sees itself as potentially transformative of the Humanities: “The tension between the digital humanities and the academic establishment is multifaceted and involves institutional hurdles to doing interdisciplinary and collaborative work, need for space and technological infrastructure, tenure systems not adapted to digital production and publications, and the need for non-faculty experts and corresponding career paths (Svensson, 2012). In this sense Digital Humanities in on the periphery of academia, seeking validation of the types of digital scholarship it is developing against established power structures and recognized and suitable forms of what is deemed ‘valid scholarship’.  DH scholarship is produced in a variety of formats that are not necessarily monographs or journal articles that make up the traditional scholarly publishing system. Datasets, web pages, digital scholarly edition, textual markup and visualizations, to name a few, are part of DH production and the community has focused on how to certify them as valid outputs and forms of communicating knowledge. New forms of publishing, peer review and career paths are part of DH literature as it challenges the traditional ways of producing, disseminating, validating and certifying knowledge through the types of scholarly output it is producing.  For the Digital Humanities the possibility of new modes of communication and publishing have been fundamental in its construction. Initial work concentrated on digitizing and publishing texts online, what Davidson (2012) refers to as Humanities 1.0. The first attraction of online publishing is making available material that is of difficult access and/or dispersed geographically (Priani, 2015). For others an important feature, with the relative low cost compared to publication on paper, was the possibility of making lesser-known materials available, such as non-canonical texts (Earhart 2012). From the periphery the possibility of electronic publishing offered a way of getting information published and noticed. Many Open Access projects, which focus on journal publishing, have worked towards this (Alperin, Fischman and Willinsky, 2008). Since then however, it has become clear that the Internet provides the opportunity to change the way we think about publishing and what types of outputs can be considered valid forms of communicating knowledge. This in turn has led to discussion on how we can validate and certify this production. In order to change the system however, it requires “substantative rethinking (…) of the ways those faculty do their work, how they communicate that work, and how that work is read both inside and outside the academy” (Fitzpatrick, 2011). If we consider that the traditional scholarly publishing system has systematically excluded research from periphery countries, there is an opportunity, as we work towards new types of publishing and communication systems, to find ways of being deliberately inclusive. Although not referring specifically to periphery research Davidson (2012) idea of Humanities 2.0 which is “distinguished from monumental, first generation data-based projects not just by interactivity but also by an openness about participation grounded in a different set of theoretical premises, which decenter authority and knowledge”, can be applied. Many (Fiormonte, 2012; Liu, 2012; Rodríguez, 2012; Clavert, 2013; Dacos, 2013; Risam, 2015) have argued that DH must reflect more on the nature of the digital medium and the technologies that are being employed as well as addressing issues related to geo-linguistic diversity in the community. At the same time Digital Humanities is also a community about building and creating (Ramsay, 2011). If DH is indeed a transformative motor of academia, then reflecting from a critical perspective on the new types of digital scholarship that we are proposing is indispensable. We could propose new models that adequately incorporate digital scholarly output from countries on the periphery that are left out of the global publishing system within the traditional scholarly publishing model. If DH is proposing and fighting for new types of scholarly publishing, then should we not seek to build a model that takes this into consideration?  It is not possible of course to resolve this in a single conference presentation. The aim of this paper is to bring this subject to the table and to initiate a discussion in the different ways that this can be addressed. It is important to invest more in understanding the effects of the new types of publishing that we are advocating for as well as the digital infrastructures, primarily publishing platforms that we are developing and/or using. Discussing the implications of what we are building, the methods and structures we are using for communicating and publishing as well as the languages and materials we are prioritizing as part of the necessary self-reflection on what we are and what we do. As we advocate for new types of scholarship and we discuss new forms of peer review, certification, validation, publication and dissemination of these new types of publications we must make sure we do not incorporate tacit assumptions about the role and validity of periphery scholarship if not we shall inevitably continue to replication long-held prejudices and marginalization.  ",
       "article_title":"New DH Publishing Models and Geopolitical Diversity",
       "authors":[
          {
             "given":"Isabel",
             "family":"Galina Russell",
             "affiliation":[
                {
                   "original_name":"Instituto de Investigaciones Bibliográficas -Universidad Nacional Autónoma de México - UNAM, Mexico",
                   "normalized_name":"National Autonomous University of Mexico",
                   "country":"Mexico",
                   "identifiers":{
                      "ror":"https://ror.org/01tmp8f25",
                      "GRID":"grid.9486.3"
                   }
                }
             ]
          },
          {
             "given":"Ernesto",
             "family":"Priani Saisó",
             "affiliation":[
                {
                   "original_name":"Facultad de Filosofía y Letras, Universidad Nacional Autónoma de México - UNAM, Mexico",
                   "normalized_name":"National Autonomous University of Mexico",
                   "country":"Mexico",
                   "identifiers":{
                      "ror":"https://ror.org/01tmp8f25",
                      "GRID":"grid.9486.3"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-05",
       "keywords":[
          "digital humanities - multilinguality",
          "digital humanities - diversity",
          "digital humanities - institutional support",
          "English",
          "publishing and delivery systems",
          "multilingual / multicultural approaches"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Humanities scholars have experimented with the potential of different text mining techniques for exploring large corpora, from co-occurrence-based methods to sequence-labeling algorithms (e.g. Named entity recognition). LDA topic modeling (Blei et al., 2003) has become one of the most employed approaches (Meeks and Weingart, 2012). Scholars have often remarked its potential for distant reading analyses (Milligan, 2012) and have assessed its reliability by, for example, using it for examining already well-known historical facts (Au Yeung, 2011). However, researchers have observed that topic modelling results are usually difficult to interpret (Schmidt, 2012). This limits the possibilities to evaluate topic modeling outputs (Chang et al., 2009). In order to create a corpus exploration method providing topics that are easier to interpret than standard LDA topic models, we propose combining two techniques called Entity linking and Labeled LDA; we are not aware of literature combining these two techniques in the way we describe. Our method identifies in an ontology a series of descriptive labels for each document in a corpus. Then it generates a specific topic for each label. Having a direct relation between topics and labels makes interpretation easier; using an ontology as background knowledge limits label ambiguity. As our topics are described with a limited number of clear-cut labels, they promote interpretability, and this may help quantitative evaluation.  We illustrate the potential of the approach by applying it to define the most relevant topics addressed by each party in the European Parliament’s fifth term (1999-2004).  The structure of our work is as follows: We first describe the basic technologies considered. We then describe our approach combining Entity Linking and Labeled LDA. Based on the European Parliament corpus (Koehn, 2005),   http://www.statmt.org/europarl/  we show how the results of the combined approach are easier to interpret or evaluate than results for Standard LDA.     Basic technologies  Entity Linking Entity linking (Rao et al., 2013) tags textual mentions with an entity from a knowledge base like DBpedia (Auer et al., 2007). Mentions can be ambiguous, and the challenge is to choose the entity that most closely reflects the sense of the mention in context. For instance, in the expression Clinton Sanders debate, Clinton is more likely to refer to DBpedia entity Hillary_Clinton than to Bill_Clinton. However, in the expression Clinton vs. Bush debate, the mention Clinton is more likely to refer to Bill_Clinton. An entity linking tool is able to disambiguate mentions taking into account their context, among other factors.    LDA Topic Modeling Topic modeling is arguably one of most popular text mining techniques in digital humanities (Brauer and Fridlund, 2013). It addresses a common research need, as it can identify the most important topics in a collection of documents, and how these topics are distributed across the documents in the collection. The method’s unsupervised nature makes it attractive for large corpora. However, topic modeling does not always yield satisfactory results. The topics obtained are usually difficult to interpret (Schmidt, 2012, among others). Each topic is presented as a list of words. It generally depends on the intuitions of the researcher how to interpret these tokens in order to propose concepts or issues that these lists of words represent.   Labeled LDA An extension of LDA topic model is Labeled LDA (Ramage et al., 2009). If each document in a corpus is described by a set of tags (e.g. a newspaper archive with articles tagged for areas like “economics”, “foreign policy”, etc.), Labeled LDA will identify the relation between LDA topics, documents and tags, and the output will consist of a list of labeled topics.    Our approach Labeled LDA has shown its potential for fine grained topic modeling (e.g. Zirn and Stuckenschmidt, 2014). The method requires a corpus where documents are annotated with tags describing their content. Several methods can be applied to automatically generating tags, e.g. keyphrase-extraction (Kim et al., 2010). Our source for tags is Entity linking. Since entity linking provides a unique label for sets of topically-related expressions across a corpus’ documents, it can help researchers get an overview of different concepts present in the corpus, even if the concepts are conveyed by different expressions in different documents.  Our first step is identifying potential topic labels via entity linking. Linked entities were obtained with DBpedia Spotlight (Mendes et al., 2011). Spotlight disambiguates against DBpedia, outputting a confidence value for each annotation.   Spotlight outperforms other systems when corpus entities often correspond to common-noun mentions like  democracy, vs. proper-noun mentions (e.g.  Greenpeace). See Cornolti et al., 2013 and Usbeck et al., 2015.    Annotations whose confidence was below 0.1 were filtered out. We also removed too general or too frequent entities (e.g. Country or European_Union)  We then rank entities' relevance per document with tf-idf (Jones, 1972), which promotes entities that are salient in a specific subset of corpus documents rather than frequent overall in the corpus. Finally, we select the top five entities per document as per tf-idf. These five entities are used as labels to identify, with Labeled LDA, the distribution of labeled topics in the corpus.    Experiments and Results Using the Stanford Topic Modeling Toolbox,   http://nlp.stanford.edu/software/tmt/tmt-0.4/  we performed both Standard LDA (k=300) and Labeled LDA (with 5 labels)   Each document (party) is labeled with 5 entities. Some entities are shared across parties. For the 125 parties, this gives 300 distinct labels. This corresponds to k=300 topics in Standard LDA.  on speech transcripts for the 125 parties at the European Parliament (1999-2004 session). The corpus contains 125 documents, representing one party each. Documents were tokenized and lemmatised; stopwords were removed. DBpedia entities were detected with Spotlight and ranked by tf-idf, as described above. We present the outputs of Labeled LDA with entity labels (EL_LDA) for three parties, compared to both Standard LDA and to the top-ranked entities for each party (by tf-idf). In each case, we show topics with relevance above 10%. Results for the remaining parties are available online.   https://sites.google.com/site/entitylabeledlda     Figure: Linked entities (tf-idf-ranked), standard LDA topics and EL-LDA topics for speeches by Les Verts (France).    Figure: Linked entities (tf-idf-ranked), standard LDA topics and EL-LDA topics for speeches by the Conservative Party (UK).    Figure: Linked entities (tf-idf-ranked), standard LDA topics and EL-LDA topics for speeches by Partido Nacionalista Vasco (Spain).    Discussion Labeled LDA combines the strengths of Entity Linking and standard LDA. Entity Linking provides clear labels, but no notion of the proportion of the document that is related to the entity. Standard LDA’s relevance scores do provide an estimate to what an extent the topic is relevant for the document, but the topics are not expressed with clear labels. Labeled LDA provides both clear labels, and a quantification of the extent to which the label covers the document's content. An advantage of Labeled LDA over Standard LDA is topic interpretability. Consider the UK Conservative Party's topics. In each standard LDA topic, there are words related to the concepts of  Industry and  Business in general, and some words related to the UK appear on the first topic. However, in each topic, some other words (e.g.  government, directive, decision, measure, health, consumer) are related to other concepts, like perhaps  Legislation or  Social policy. A researcher trying to understand the standard LDA topics is faced with choosing which lexical areas are most representative of each topic: is it the ones related to  Industry,  Business, and the UK, or is it the other ones? The clear-cut labels from Labeled LDA are more interpretable than a collection of words representing a topic.  The Labeled LDA topics may be more or less correct, just like Standard LDA topics. But we find it easier to evaluate a topic via questions like \"is this document about  Industry,  Business and  the UK, in the proportions indicated by our outputs?\" than via questions like \"is this document about issues like  house, british, amendment, market, industry, government, (and so on for the remaining topics)\"?  The topics for French party Les Verts illustrate Labeled LDA’s strengths further. Most of the Standard LDA topics contain some words indicative of the party's concerns (e.g.  environment or  development). However, it is not easy to point out which specific issues the party addresses. In Labeled LDA, concrete issues come out, like  Genetically modified organism.  Topic label  Development aid shows a challenge with entity linking as a source of labels. Occurrences of the word  development have been disambiguated towards the entity  Development_aid, whereas the correct entity is likely  Sustainable_development. These errors do not undermine the method’s usefulness. Efficient ways to filter out such errors exist; this is conceptually similar to removing irrelevant words from Standard LDA topics. However, we need to be aware of and address this challenge.   Regarding Partido Nacionalista Vasco (Basque Nationalist Party), the Standard LDA topic misses the word  basque, which is essential to this party. Labeled LDA identifies  Basque people as a dominant concept in this party’s interventions.     Outlook Our method performs Labeled LDA using Entity Linking outputs as labels. Its main advantage is providing a specific label for each topic, that improves topic interpretability, and can simplify human evaluation of topic models.  More evaluation is needed to fully assess the approach. We will consider two possible complementary evaluations: first, a crowdsourced task where participants evaluate the coherence of Labeled LDA topics with the corpus documents. Second, an assessment of our topics by political science experts. We’re mostly interested in evaluating the approach for diachronic comparisons.  ",
       "article_title":"Entities as topic labels: improving topic interpretability and evaluability combining Entity Linking and Labeled LDA",
       "authors":[
          {
             "given":"Federico",
             "family":"Nanni",
             "affiliation":[
                {
                   "original_name":"Data and Web Science Group, University of Mannheim",
                   "normalized_name":"University of Mannheim",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/031bsb921",
                      "GRID":"grid.5601.2"
                   }
                }
             ]
          },
          {
             "given":"Pablo",
             "family":"Ruiz Fabo",
             "affiliation":[
                {
                   "original_name":"LATTICE Lab, École Normale Supérieure, France",
                   "normalized_name":null,
                   "country":"France",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "information retrieval",
          "natural language processing",
          "data mining / text mining",
          "content analysis",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Second World War on the Semantic Web Data about wars is typically heterogeneous, distributed in the data silos of the fighting parties, multilingual, and often controversial depending on the political point of view. It is therefore hard for the historians to get a global picture of what has actually happened, to whom, where, when, and how. We argue that Semantic Web and Linked Data technologies are a very promising approach for modeling, harmonizing, and aggregating data about war history. Our goal is to make it possible, for both historians and laymen, to study history in a contextualized way where linked datasets enrich each other. The paper presents the in-use WarSampo   “Sampo'' is a magical artifact in Finnish mythology that brought good fortune to its owner.  system, where massive collections of heterogeneous data about the (Finnish) history of the Second World War are harmonized using an event-based approach, and provided as a Linked Open Data service for applications to use. As a use case, a semantic portal WarSampo providing six different perspectives to the war based on events is presented.  There are several projects publishing data about the World War I on the web, such as Europeana Collections 1914–1918   http://www.europeana-collections-1914-1918.eu  , 1914–1918 Online   http://www.1914-1918-online.net  , WW1 Discovery   http://ww1.discovery.ac.uk  , Out of the Trenches   http://www.canadiana.ca/en/pcdhn-lod/  , CENDARI   http://www.cendari.eu/research/first-world-war-studies/  , Muninn   http://blog.muninn-project.org  , and WW1LOD (Mäkelä et al., 2015). War history makes a promising use case for Linked Data because war data is heterogeneous, distributed in different countries and organizations, and written in different languages (Hyvönen, 2012). Using metadata, also different opinions and conflicting information about the war can be represented.  Many websites also publish information about the World War II, the largest global tragedy in human history, such as the World War II Database   http://ww2db.com   to name just one. However, such portal data is typically meant for human consumption, and there are only few works that deal with machine readable data about the WW2 for applications to use (Collins et al., 2005; de Boer et al., 2013).  Our work contributes to the related research above by initiating and fostering large scale LOD publication of WW2 data on the web, based on event-based data modeling. The idea is to publish Linked Open data, aggregated from distributed data silos, for Digital Humanities applications to use. In our case study, the data is related to the Finnish Winter War 1939–1940 against the Soviet attack, the Continuation War 1941–1944, where the occupied areas of the Winter War were temporarily regained by Finns, and the Lapland War 1944–1945, where the Finns pushed the German troops away from Lapland. We first present and discuss the data modeling approach developed for the WarSampo LOD service. After this an application of the data, the WarSampo semantic portal, is presented where events are linked to related resources, such as photos, persons, and historical places. In conclusion, lessons learned are discussed and directions for further research pointed out.   The data service: modelling war events as Linked Data     Table 1. Central datasets published and linked in WarSampo.   Data The project deals initially with the datasets presented in Table 1. The casualties data (1) includes data about the deaths in action during the wars. War diaries (2) are digitized authentic documentations of the army unit actions in the frontiers. Photos and films (3) were taken during the war by the troops of the Defense Forces. The Kansa taisteli magazine (4) was published in 1957–1986; its articles contain mostly memories of the men that fought on the fronts. Karelian places (5) and maps (6) cover the war zone area in pre-war Finland that was finally annexed by the Soviet Union. Organization cards (7), written after the war, document events of military units during the war. War time events (8) extracted from various publications include, e.g., battles and political incidents. The data, over 5 million triples in total, has been transformed into RDF from database dumps, spreadsheets (CSV), and by applying OCR to documents. Named Entity Recognition (NER) techniques were used to link texts to data, e.g., to identify and disambiguate persons and places in the magazine articles and captions of the photos. In addition, new datasets are planned to be included in the system, such as the Finnish Broadcasting Company YLE’s audio and film material recorded during the war or related to it (“Living Archive”), and a database of prisoners of war.   Metadata models  CIDOC CRM   http://cidoc-crm.org   is used as the harmonizing basis for modeling data, with events providing the semantic glue for data linking (Doerr, 2003). Our data model for WW1, presented in (Mäkelä et al., 2015), is used as the metadata model to start with. The model and data is documented at the data service   http://www.ldf.fi/dataset/warsa/  .    Domain ontologies The data is annotated using a set of domain ontologies, including: 1) an ontology of the troops and their hierarchies, 2) persons with their ranks and roles, 3) place ontology of historical places, 4) event ontology of battles, politics, and other war time incidents, 5) an ontology of time periods, and 6) a subject matter ontology. Ontologies on objects such as weapons, aircraft, and vessels remain topics of possible future work. All WarSampo datasets are available as Linked Open Data (LOD) at the “7-star” Linked Data Finland service   http://www.ldf.fi   (Hyvönen et al., 2014), based on Fuseki   http://jena.apache.org/documentation/serving\\data/   establishing the SPARQL endpoint, and with a Varnish Cache   https://www.varnish-cache.org   frontend for dereferencing URIs.     Application: perspectives to war history The idea of the WarSampo portal is to provide a variety of different perspectives to war history. There are six perspectives available: Events, Persons, Army Units, Places, Kansa taisteli Magazine Articles, and Casualties (Hyvönen et al., 2016). The idea is that the perspectives enrich each other based on data linking. Figure 1 illustrates the WarSampo Events perspective application to the WarSampo data. Events are displayed on a map, (a) in Fig. 1, and on a timeline (b) that shows here some events of the Winter War. When the user clicks on an event, it is highlighted (c), and the historical place, time span, type, and description for the selected event are displayed (d). Photographs related to the event (e) are also shown. The photographs are linked to events based on location and time. Furthermore, information about casualties during the time span visible on the timeline is shown alongside the event description (f), and the map (a) features a heatmap layer for a visualization of these deaths.     Figure 1. Event perspective featuring a timeline and a map.  The events can also be found and visualized through other perspectives. For example, in the Army Units perspective, the events in which a unit participated can be viewed on maps and in time, providing a kind of graphical activity summary of the unit. In the casualties perspective, military units of the dead soldiers are known, making it possible to sort out and visualize the personal war history of the casualties on historical maps that come from a yet another dataset in WarSampo. The WarSampo semantic portal   The application is in use at  http://sotasampo.fi.   was published Nov 27, 2015 at and has had tens of thousands of users. It is implemented solely on the SPARQL endpoint of the WarSampo LOD data service.    Discussion Our first experiments, as illustrated in Section 3, suggest that heterogeneous datasets of war history really can be interlinked with each other through events in ways that provide useful insights for the historians. We have also learned that even in the rural northern parts of Europe, massive amounts of WW2 data can be found. We have initially dealt with tens of thousands of people killed in action. However, there is also data available about hundreds of thousands of soldiers who survived the war. In addition to historians, WarSampo data is very interesting to the laymen, too: every soldier’s history is of interest at least to, e.g., his/her relatives. Managing the data, and providing it for different user groups, suggests serious challenges when dealing with, e.g., the war in the central parts of Europe, where the amount of data is orders of magnitude larger than in Finland, multilingual, and distributed in different countries. For example, solving entity resolution problems regarding historical place names and person names can be hard. However, it seems that Linked Data is a promising way to tackle these challenges. Our work   See the project homepage  http://seco.cs.aalto.fi/projects/sotasampo/en/.   is funded by the Ministry of Education and Culture and Finnish Cultural Foundation. Wikidata Finland project financed the alignment of the historical maps in WarSampo.   ",
       "article_title":"Publishing Second World War History as Linked Data Events on the Semantic Web",
       "authors":[
          {
             "given":"Eero",
             "family":"Hyvönen",
             "affiliation":[
                {
                   "original_name":"Aalto University, Finland",
                   "normalized_name":"Aalto University",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/020hwjq30",
                      "GRID":"grid.5373.2"
                   }
                }
             ]
          },
          {
             "given":"Erkki",
             "family":"Heino",
             "affiliation":[
                {
                   "original_name":"Aalto University, Finland",
                   "normalized_name":"Aalto University",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/020hwjq30",
                      "GRID":"grid.5373.2"
                   }
                }
             ]
          },
          {
             "given":"Petri",
             "family":"Leskinen",
             "affiliation":[
                {
                   "original_name":"Aalto University, Finland",
                   "normalized_name":"Aalto University",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/020hwjq30",
                      "GRID":"grid.5373.2"
                   }
                }
             ]
          },
          {
             "given":"Esko",
             "family":"Ikkala",
             "affiliation":[
                {
                   "original_name":"Aalto University, Finland",
                   "normalized_name":"Aalto University",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/020hwjq30",
                      "GRID":"grid.5373.2"
                   }
                }
             ]
          },
          {
             "given":"Mikko",
             "family":"Koho",
             "affiliation":[
                {
                   "original_name":"Aalto University, Finland",
                   "normalized_name":"Aalto University",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/020hwjq30",
                      "GRID":"grid.5373.2"
                   }
                }
             ]
          },
          {
             "given":"Minna",
             "family":"Tamper",
             "affiliation":[
                {
                   "original_name":"Aalto University, Finland",
                   "normalized_name":"Aalto University",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/020hwjq30",
                      "GRID":"grid.5373.2"
                   }
                }
             ]
          },
          {
             "given":"Jouni",
             "family":"Tuominen",
             "affiliation":[
                {
                   "original_name":"Aalto University, Finland",
                   "normalized_name":"Aalto University",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/020hwjq30",
                      "GRID":"grid.5373.2"
                   }
                }
             ]
          },
          {
             "given":"Eetu",
             "family":"Mäkelä",
             "affiliation":[
                {
                   "original_name":"Aalto University, Finland",
                   "normalized_name":"Aalto University",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/020hwjq30",
                      "GRID":"grid.5373.2"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-22",
       "keywords":[
          "semantic analysis",
          "visualisation",
          "internet / world wide web",
          "databases & dbms",
          "programming",
          "geospatial analysis, interfaces and technology",
          "knowledge representation",
          "English",
          "semantic web",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction University College London (UCL) owns a large corpus of the philosopher and social reformer Jeremy Bentham (1748-1832). Until recently, these papers were for the most part untranscribed, so that very few people had access to the corpus to evaluate its content and its value. The corpus is now being digitized and transcribed thanks to a large number of volunteers recruited through a crowd-sourcing initiative called Transcribe Bentham (Causer and Terras, 2014a, 2014b).  The problem researchers are facing with such a corpus is clear: how to access the content, how to structure these 30,000 files, and how to get relevant access to this mass of data? Our goal has thus been to produce an automatic analysis procedure aiming at providing a general characterization of the content of the corpus. We are more specifically interested in identifying the main topics and their structure so as to provide meaningful static and dynamic representations of their evolution over time.    Comparison with other works The exploration of large corpora in the Humanities is a known problem for today’s scholars. For example, the recent PoliInformatics challenge addressed the issue by promoting a framework to develop new and original research in text-rich domains (the project focused on political science but can be extended to any sub-field within the Humanities).  Specific experiments have recently been done in the field of philosophy, but they mainly concern the analysis of metadata, like indexes or references (Lamarra and Tardella, 2014; Sula and Dean, 2014). Different experiments have nevertheless involved an exploration of large amounts of textual data (see e.g. Diesner and Carley, 2005 on the Enron corpus) with relevant visualization interfaces (Yanhua et al., 2009).  In this paper, we propose to explore more advanced natural language processing techniques to extract keywords and filter them according to an external ontology, so as to obtain a more relevant indexing of the documents before visualization. We also explore dynamic representations, which were not addressed in the above-mentioned studies.    Corpus exploration strategy  The Text analysis module Different scripts have been developed to filter the corpus   For example, Bentham sometimes used French in his correspondence and these texts are eliminated via automatic language detection, since we focus on English in this experiment.  . Then documents are assigned a date whenever possible: Since the corpus mostly contains notes and letters, the first date mentioned in the document often refers to the date of the document’s composition (even if this assumption is of course not always true). A large number of documents cannot be assigned a date and are thus not used for the dynamic analysis of the corpus.   To index the corpus and identify meaningful concepts, we first tried to directly extract relevant keywords from the texts. However, traditional techniques like the use of tf-idf (Salton et al., 1983) and c-value (Frantzi et al., 2000) do not seem very efficient in our case. This is not too surprising: it is well known that texts are too ambiguous to provide a sound basis for a direct semantic extraction. Surface variations, the use of synonyms and hyponyms, linguistic ambiguity and other factors constitute strong obstacles for the task. We thus decided to use natural language processing techniques that provide relevant tools to overcome some of these limitations. The tools we employed are either web-based or possible to execute on a personal computer with average specs.  We tried to refine concept extraction by confronting the text with an external, structured database. We used DBpedia (Auer et al., 2007) as a source of structured knowledge (DBpedia is a database made of information extracted from Wikipedia). DBpedia is not a specialized source of information but this guarantees that the approach is not domain or author specific and could be easily used for other corpora. We then used the DBpedia Spotlight Web Service (Mendes et al., 2012; Daiber et al., 2013) to make the connexion between the corpus and DBpedia concepts. This leads to a much more fine grained and relevant analysis than possible with an entirely data-driven keyword extraction.  Based on the outputs of Spotlight, only concepts that occurred at least 100 times, and with a confidence value of at least 0.1 were kept. Spotlight outputs a confidence value between 0 and 1 for each annotation; a 0.1 threshold removes clearly unreliable annotations while maintaining good coverage. Tagging the full corpus with Spotlight (ca. 30,000 documents) took over 24 hours. We called the Spotlight service one document at a time; parallelizing the process can decrease processing time.   The visualization module Once relevant concepts are identified, one wants to produce relevant text representations so as to provide a usable interface to end users. We present here three different kinds of interfaces that show the possible exploitation of the analysis described above.  The corpus is first indexed in a Solr search index   https://lucene.apache.org/solr/  and accessible through a graphical end-user interface. It is possible to query the corpus by date, using Solr’s faceted search functions   https://wiki.apache.org/solr/SolrFacetingOverview  (see figure 1).     Figure 1: Search interface: users can search via year extracted from the text, which in most cases is the year of writing, allowing users to see texts (especially correspondence) in chronological order.   It is also possible to cluster together related keywords, so as to get access to homogeneous sub-parts of the corpora representing specific subfields of Bentham’s activity (see figure 2).    Figure 2: the main topics addressed in the corpus, based on clusters of concepts, showing the main concerns of Bentham's writings, which map closely onto established research areas in Bentham studies. The network was produced by Cortext; colours and fonts were reformatted in Gephi based on Cortext’s gexf-format export   https://gephi.github.io/    Dynamic maps are also possible, to see for example the evolution of the different topics addressed in the corpus over time (see figure 3).    Figure 3: A dynamic view of the corpus, computed with the Cortext plarform  (tubes layout), with the evolution of the main topics addressed over time      Scholarly benefits of these tools for the Transcribe Bentham project Since 1958, UCL's Bentham Project has been producing the new, critical edition of the “Collected Works of Jeremy Bentham”. The edition is expected to run to some eighty volumes, the thirty-third of which has recently been sent to the press. The “Collected Works” is based upon texts, which Bentham published during his lifetime, and unpublished texts, which exist in manuscript. It is a major task: UCL's Bentham Papers runs to some 75,000 manuscript pages, while the British Library's has a further 25,000 or so pages. About 40,000 pages have been transcribed to date and, while UCL's award-winning 'Transcribe Bentham' initiative has helped to significantly increase the pace of transcription, a great deal more work needs to be done. The first task in producing a volume of the “Collected Works” based upon texts in manuscript is to identify all the relevant pages. Bentham Project editorial staff use the Bentham Papers Database Catalogue, which indexes the manuscript collection by sixteen headings, including date, main heading, subject heading(s), author(s), and so forth. It is, however, entirely possible to miss relevant manuscripts using this method. The subject maps produced for this research promise to complement traditional Bentham Project methods; for instance, Bentham's work on political economy encompasses topics as varied as income tax to colonisation, and the subject maps will make it more straightforward to investigate the nexus between these, and other, subjects. The dynamic corpus view, showing the evolution of topics addressed over time, could also prove useful in editorial work as can be shown in two examples. First, an editor at the Bentham Project is currently working on Bentham's writings on convict transportation, though there is some confusion over when exactly Bentham first broached the topic. The dynamic corpus view could help to clear up whether it was only around 1802 when Bentham wrote about transportation, or if he had investigated the subject in any great detail during the 1790s. Second, Bentham became more radical as he aged, and several Bentham scholars have sought to identify the point at which Bentham abandoned his earlier conservatism and 'converted' to political radicalism, and representative democracy; an analysis of Bentham's language at the turn of the century would be instructive in helping clarify this matter.   Conclusion In this paper, we have presented a first attempt to give a relevant access to a large interdisciplinary corpus in the domain of philosophy, law and history. We have shown that using tools in concept clustering and visualization can provide an alternative way to navigate large-scale corpora, and confirm and visualise scholarly approaches to large scale textual corpora. Exploring how these tools can be effectively used with a corpus such as Bentham's indicates these methods are applicable to other sources as well. In the near future, we are planning to refine the linguistic analysis in order to give better representations of the textual content of the corpus. We are also planning experiments with end-user to evaluate in more details the solution and the visualisation techniques used so far in this project.    Acknowledgements We want to thank IDEX PSL (Paris Sciences et Lettres, ref. ANR-10-IDEX-0001-02 PSL) as well as the labex TransferS (laboratoire d’excellence, ANR-10-LABX-0099) for supporting this research. Lastly, we would like to thank the reviewers for their insightful comments on the paper.   ",
       "article_title":"Mapping the Bentham Corpus",
       "authors":[
          {
             "given":"Estelle",
             "family":"Tieberghien",
             "affiliation":[
                {
                   "original_name":"LATTICE-CNRS, France",
                   "normalized_name":null,
                   "country":"France",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Pablo",
             "family":"Ruiz Fabo",
             "affiliation":[
                {
                   "original_name":"LATTICE-CNRS, France",
                   "normalized_name":null,
                   "country":"France",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Frédérique",
             "family":"Mélanie-Bécquet",
             "affiliation":[
                {
                   "original_name":"LATTICE-CNRS, France",
                   "normalized_name":null,
                   "country":"France",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Thierry",
             "family":"Poibeau",
             "affiliation":[
                {
                   "original_name":"LATTICE-CNRS, France",
                   "normalized_name":null,
                   "country":"France",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Tim",
             "family":"Causer",
             "affiliation":[
                {
                   "original_name":"University College London, UK",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"Melissa",
             "family":"Terras",
             "affiliation":[
                {
                   "original_name":"University College London, UK",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "semantic analysis",
          "visualisation",
          "natural language processing",
          "corpora and corpus activities",
          "data mining / text mining",
          "knowledge representation",
          "content analysis",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Beyond simple annotation and visualization tools or expensive proprietary software, open access software for accessing and analyzing audio is not widely available for general use by the humanities community. Speech recognition algorithms in projects such as MALACH (Multilingual Access to Large spoken ArCHives) are often not built as Web-accessible interfaces for broader audiences. Analysis and visualization software such as PRAAT, which is used by linguists, and Sonic Visualizer, which is often used by music scholars, are desktop tools that typically allow users to focus on one file at a time, making project-sharing difficult for collaborative research and classroom projects. In bioacoustics, researchers use Raven (from the Cornell Lab of Ornithology) and Avisoft (expensive, proprietary software), which perform well with clean data from a single animal. Most of these tools are either not used in multiple domains or with large collections and none of them do well with noise or with multiple signals. As a result of these factors, humanists have few opportunities to use advanced technologies for analyzing large, messy sound archives. In response to this lack, the School of Information (iSchool) at the University of Texas at Austin (UT) and the Illinois Informatics Institute (I3) at the University of Illinois at Urbana-Champaign (UIUC) are collaborating on the HiPSTAS (High Performance Sound Technologies for Access and Scholarship) project. A primary goal of HiPSTAS is to develop a research environment that uses machine learning and visualization to automate processes for describing unprocessed spoken word collections of keen interest to humanists.  This paper describes how we have developed, as a result of HiPSTAS, a machine learning system called ARLO (Adaptive Recognition with Layered Optimization) to help deal with the information challenges that scholars encounter in their attempt to do research with unprocessed audio collections.   ARLO (Adaptive Recognition with Layered Optimization) Software ARLO was developed with UIUC seed funding for avian ecologist David Enstrom (2008) to begin exploring the use of machine learning for data analysis in the fields of animal behavior and ecology. ARLO software was chosen as the software we would develop through HiPSTAS primarily because it extracts basic prosodic features such as pitch, rhythm, and timbre that humanities scholars have called significant for performing analysis with sound collections (Bernstein, 2011; Sherwood, 2006; Tsur, 1992).   Filter Bank Signal Processing and Spectrogram Generation ARLO analyzes audio by extracting features based on time and frequency information in the form of a spectrogram. The spectrogram is computed using band-pass filters linked with energy detectors. The filter bank approach is similar to using an array of tuning forks, each positioned at a separate frequency, an approach that is thought to best mimic the processes of the human ear (Salthouse and Sarpeshkar). With filter banks, users can optimize the trade-off between time and frequency resolutions in the spectrograms (Rossing, 2001) by choosing a frequency range and ‘damping factor’ (or damping ratio), a parameter that determines how long the tuning forks ‘ring.’ By selecting these features, users can optimize their searches for a given sound. For these reasons,    Machine-Learning Examples and the ARLO API (Application Programming Interface) In ARLO, examples for machine learning are audio events that the user has identified and labeled. Audio events comprise a start and end time such as a two-second clip, as well as an optional minimum and maximum frequency band to isolate the region of interest. Users label the examples of interest (e.g., “applause” or “barking”). Other control parameters such as damping factor are also provided for creating spectrograph data according to optimal resolutions for a given problem. The algorithm described below retrieves the features of the tag according to the user's chosen spectra and framing size (e.g., two frames per second, each 0.5 seconds) from the audio file through the ARLO API.   ARLO Machine-Learning Algorithms: IBL (Instance-Based Learning) The ARLO IBL algorithm finds matches by taking each known classified example and “sliding” it across new audio files looking for good matches based on a distance metric. The average of the weighted training set classes determines prediction probability. The number of match positions considered per second is adjustable and is set to the spectral sample rate. In addition to simple spectra matching, a user can isolate pitch and volume traces, compute correlations on them, and weight the different feature types when computing the overall match strength. This allows the user to weight spectral information that might correspond to such aspects as pitch or rhythm. In the IBL algorithm, accuracy is measured using a simulation of the leave-one-out cross-validation prediction process described above.     Use Case: Finding Applause in PennSound Poetry Performances Humanities scholars have identified the sound of applause as a significant signpost for finding patterns of interest in recorded poetry performances. Applause can serve as a delimiter between performances, indicating how a file can be segmented and indexed. Applause can also serve as a delimiter between the introduction to a performance and the moment when a performance has ended and a question-and-answer period has begun, both of which indicate contextual information such as the presence of people who might not appear in traditional metadata fields (Clement and McLaughlin, 2015). A means for quantifying the presence of applause can also lead researchers to consider more in-depth studies concerning the relationship between audience responses and a poet’s performance of the same poem at different venues as well as the differing responses of audiences at the same venue over the course of a poet’s career or perhaps as a point of comparison between poets. Examples of these results are described below. For this use case, we ingested approximately 30,257 files remaining (5374.89 hours) from PennSound into ARLO. We chose 2,000 files at random, manually examined them for instances of applause, and chose one instance of applause per recording until we had an example training set of 852 three-second tags, including 582 3-second instances of non-applause (3492 0.5-second examples) and 270 3-second instances of applause (1620 0.5-second examples). Optimization for the IBL test went through 100 iterations. As a result of this optimization process, we used the following parameters for both tests: 0.5-second spectral resolution; 0.5 damping factor; 0.8 weighting power (for IBL); 600 Hz minimum frequency; 5000 Hz maximum frequency; 64 (IBL) and 256 (Weka) spectral bands; spectral sampling rate of 2 (i.e., half-second resolution).   Preliminary Results We first evaluated our models using cross-validation on the training data. Using the leave-one-out approach, the IBL classifier achieved an overall accuracy of 94.52% with a 0.5 cut-off classification threshold. After comparing 676 configurations, we found that the optimal approach was using IBL with Hann smoothing over 14 windows (7 seconds). The accuracy for this configuration was 99.41%.  In our initial analysis of classification data, we identified significant differences between measured applause durations for six poets, each with more than ten readings in the evaluation set. Table 6 presents the results of pairwise single-tailed Mann-Whitney (Mann and Whitney, 1947)  U tests of applause durations that have been predicted by our IBL classifier. The alternative hypothesis states that the performer in the left column tends to receive more applause than the corresponding one listed in the top row.  Results that are significant at the p<0.05 level appear in bold, with the counts and overall means of each set of observations provided in the right two columns. It appears, for instance, that the poet Rae Armantrout tends to receive more applause than either Bruce Andrews or Barrett Watten. These two differences remain significant when comparing \"seconds of applause per minute\" instead of total applause duration.    Table 1. P-values for Pairwise Directional Mann-Whitney  U Tests Between Six Poets' Applause Durations      Discussion and Future Work This is preliminary work in an ongoing attempt to create a virtual research environment for analyzing large collections of audio. These data warrant further scrutiny, however, since multiple factors might be skewing the results. First, recording technologies have changed over time and as a result some earlier recordings likely include more noise and thus more false positives. Second, editing practices and event formats can vary widely between venues and over time. Finally, recordings that are included in the PennSound archive represent curation decisions that may favor certain kinds of performers over others. Part of the challenge is to determine what use such analysis might serve for scholars in the humanities and in other fields. Some of the HiPSTAS participants have written about their experiences using ARLO in their research (MacArthur, 2015; Mustazza, 2015; Sherwood, 2015; Rettberg, 2015), but we are interested in feedback from multiple user communities including linguists and scientists who have recorded too much sound data for traditional forms of analysis and processing (Servick, 2014). Furthermore, the IBL algorithm produced promising results, but could be improved with more training data, for example. Or, in extended experiments in which users wish to increase the accuracy of the model, we could develop a voting mechanism on the predictions by comparing the models. Users could validate newly identified examples and include them as new training examples, building each model again on the new data. We are currently working on further testing the models and developing a means for these iterative approaches.   ",
       "article_title":"ARLO (Adaptive Recognition with Layered Optimization): a Prototype for High Performance Analysis of Sound Collections in the Humanities",
       "authors":[
          {
             "given":"Tanya",
             "family":"Clement",
             "affiliation":[
                {
                   "original_name":"University of Texas at Austin, United States of America",
                   "normalized_name":"The University of Texas at Austin",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00hj54h04",
                      "GRID":"grid.89336.37"
                   }
                }
             ]
          },
          {
             "given":"Steve",
             "family":"McLaughlin",
             "affiliation":[
                {
                   "original_name":"University of Texas at Austin, United States of America",
                   "normalized_name":"The University of Texas at Austin",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00hj54h04",
                      "GRID":"grid.89336.37"
                   }
                }
             ]
          },
          {
             "given":"David",
             "family":"Tcheng",
             "affiliation":[
                {
                   "original_name":"University of Illinois Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Loretta",
             "family":"Auvil",
             "affiliation":[
                {
                   "original_name":"University of Illinois Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Tony",
             "family":"Borries",
             "affiliation":[
                {
                   "original_name":"University of Illinois Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "data mining / text mining",
          "archives, repositories, sustainability and preservation",
          "English",
          "audio, video, multimedia"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Problem At present, three hieroglyphic scripts are encoded in the Unicode Standard: Egyptian, Anatolian, and Meroitic. The historic Mayan hieroglyphic script is not yet in Unicode, in part because its complex clustering poses special encoding problems. Besides the difficulties inherent in the Mayan script, writing a Unicode proposal for Mayan involves additional challenges, requiring significant time commitment and travel. Early meetings between with Unicode experts are a desideratum in order to identify the best technical approaches to handle Mayan text issues. Then the proposal needs to be written – complete with all the required details for the entire character repertoire. When the proposal goes before the two standards committees, the authors need respond to any questions, and make revisions to the proposal as needed. In addition, proposers need to commit to seeing the process through to the end, which can take between two to five years, or longer.  In short, the twin challenges of representing and displaying Mayan clusters on computers and writing a Unicode proposal (especially for anyone new to the process) present a huge hurdle.   Background on Mayan clustering Mayan signs appear in clustered glyph-blocks or \"collocations,” which could be modified by the masterful Mayan scribes with advanced “tools,” such as ligatures, conflation, infixation, superimposition, pars pro toto and full-figure variants (see Fig s. 1 and 2). The visual complexity of Mayan requires mechanisms that go beyond standard script encoding approaches.   Fig. 1 Infixation of the highlighted logogram K’AN (shown on left, infixed as a circle on the right)    Fig. 2 Conflation of the highlighted logogram MO’ (shown on the left, conflated on the right)    Solution A recent multidisciplinary collaboration established between UC Berkeley's Script Encoding Initiative (SEI) and the MAAYA Project aims to employ new methods combining linguistics, Mayan epigraphy, digital palaeography and computer vision to overcome some of the major challenges preventing the encoding of Mayan hieroglyphs in the Unicode Standard. Some of the strategies envisioned rely on already existing MAAYA-Project resources. Such resources include datasets with annotated database records for all individual glyphs and possible sign-combinations attested in Mayan hieroglyphic books and an advanced concordance functionality, that cross-references Mayan signs across existing glyph-catalogues (Gatica-Perez et al., 2014 and Hu et al., 2015). In addition, SEI's experience in overseeing the encoding over 70 of the world script systems means Mayan experts will have a front-line guide to help the proposal through the entire encoding process – providing Unicode expertise, assisting on the authoring and review of a proposal, and presenting the proposal at standards meetings. Having direct involvement of Unicode specialists means the proposal can draw on Unicode experience to identify successful methods that can adapted and expanded to account for the extraordinary variability of Mayan signs clustered in glyph blocks.   Specific encoding issues: Egyptian format characters In Unicode 8.0, the default display for the three encoded hieroglyphic scripts (Egyptian, Anatolian and Meroitic Hieroglyphs) is a linear listing of the characters, as shown by the example in Fig. 3.   Fig. 3 Current default display of Egyptian hieroglyphs in Unicode (Richmond, 2015)  According to the Unicode Standard, display of the characters in a non-linear manner (i.e., in clusters) should be handled by a higher-level protocol (Unicode Consortium 2015: 430-31, 437), and is outside the scope of Unicode. However, clustering is more typical of the layout for most hieroglyphic scripts. Although Egyptian hieroglyphs were encoded in Unicode in 2009, Egyptologists are currently prevented from interchanging data easily, because they have had to rely on non-standard software to handle character grouping. According to Richmond 2015, hieroglyphs need to be capable of being grouped together in plain-text - without proprietary software - in order to be truly useful. In 2015, a new proposal was put forward for three format characters that allows basic clustering (Richmond, 2015, see Fig. 4). The characters indicate placement of a character, either above another character or alongside it, or, for the third character, identify it as forming a ligature with the following character. The three characters are based upon the conventions of the Manual de Codage, which uses ASCII characters to indicate the placement of the hieroglyphs. (Note: Since the mirroring of glyphs is handled by line direction, as specified by markup or bidirectional characters, no format characters are not proposed for that.)    Fig. 4 Example showing the same characters as in Fig. 3, but with expected clustering, made possible with the format characters (Richmond, 2015)  The three Egyptian format characters were approved by one of the two standards committees in January 2016 (Unicode Consortium, 2016). The characters have been tested and shown to display as expected in the Universal Shaping Engine, a rendering engine in Windows 10 and recent versions of Android, Chrome, LibreOffice and Firefox. If the characters successfully complete the approval process, simple clustering in Egyptian hieroglyphs should be capable of plain-text representation in the future, meaning that scholars would not have to rely on an ad hoc, non-standard solution. At this point, Egyptian hieroglyphs appear to only require the three format control characters for clustering, and need no other mechanisms. Evidence from the MAAYA project suggests that Mayan will require three script-specific format characters that serve the same functions as Egyptian, and at least two additional characters: one for truncation and another for infixation (see Fig. 5).   Fig. 5 Expected clustering of Mayan hieroglyphs (adapted from Richmond and Glass, 2016)  Hence, it appears that Egyptian and Mayan can share a common model, using format characters for clustering (with at least two additional ones needed for Mayan). In addition, advances in text display of Egyptian hieroglyphs, which have been tested successfully on the Universal Shaping Engine, may well be applicable to Mayan.   Specific encoding issues: Ideographic Description Characters Unicode contains a mechanism to describe Chinese-Japanese-Korean (CJK) ideographs, called Ideographic Description Characters (IDCs). These characters are used to describe the layout of CJK characters, but is not used for rendering. The IDCs have been defined as capable of being extended to other scripts (Unicode Consortium 2015, pp. 679-80). (See Fig. 6.)   Fig. 6 Examples of Ideographic Description Characters (extended to Mayan hieroglyphs). Top row: Unicode Ideographic Description Characters (IDC); second row: Chinese IDCs (left) and Mayan descriptors (right); third row: Chinese examples; fourth row: Mayan script examples  In April 2015, Unicode experts met with the co-author, Mayan expert Carlos Pallán Gayol, and recommended the MAAYA project identify the structural patterns of Mayan characters, and use Ideographic Description Characters (IDCs) to describe them. Such a mechanism will help in defining Mayan characters (in Unicode terms), and help “unlock” the script.   Next steps At a meeting in January 2016 with the co-author Pallán Gayol, unicode experts noted progress on the encoding model issues and suggested the MAAYA project continue to define the base set of characters, identify the structural patterns of IDCs, and verify the number of format characters needed.   Expected results Encoding the Mayan hieroglyphs in the Unicode format will allow creation of vast open-access Mayan hieroglyphic text repositories and libraries, upon which advanced search and query functionalities relying i.e. on Optical Character Recognition (OCR) and text-mining could be applied. (Once a script is in Unicode, an OCR engine can be trained to read the script, though the text needs to be consistent). Thus, we argue that the ability to render any Mayan hieroglyphic text in an encoded digital format could impact on the overall accessibility, reproduction, visualization and long-term preservation of the sum of ancient knowledge recorded by the Mayan scribes on thousands of texts and inscriptions produced between ca. 250 B.C. and 1520 AD in Central America. It could also act as a model for the encoding of other hieroglyphic scripts of the Americas, including Aztec.   Funding This work was supported by the National Endowment for the Humanities [PR-50205-15] and by the DFG (Deutsche Forschungsgemeinschaft, German Scientific Association).  ",
       "article_title":"Unlocking The Mayan Script With Unicode",
       "authors":[
          {
             "given":"Deborah",
             "family":"Anderson",
             "affiliation":[
                {
                   "original_name":"UC Berkeley, United States of America",
                   "normalized_name":"University of California, Berkeley",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01an7q238",
                      "GRID":"grid.47840.3f"
                   }
                }
             ]
          },
          {
             "given":"Carlos",
             "family":"Pallán Gayol",
             "affiliation":[
                {
                   "original_name":"University of Bonn, Germany",
                   "normalized_name":"University of Bonn",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/041nas322",
                      "GRID":"grid.10388.32"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "information retrieval",
          "standards and interoperability",
          "archaeology",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction In this paper a range of maps, documents, data and archaeological finds are brought together in a historical GIS (Geographical Information System) to reconstruct different forms of labour and especially labour relations in existence in late medieval Northwest Europe. Starting from the Late Middle Ages, England and the Low Countries laid the groundwork for becoming two of the dominant powers in Europe and the world. The question is how this process of growth began. Often it has been coined with the rise of capitalism and the rise of wage labour as the predominant form of labour (e.g. van Bavel, 2007). The issue also touches on the Great Divergence debate. This debate centres around why and at what point in time Europe became the dominant continent in the world, surpassing the power and wealth of China, India, Japan and the Ottoman Empire (Pomeranz, 2000). England and the Low Countries play an important role in this debate and quite often the level of (real) wages is used as a determinant to study the emerging differences between Europe and other parts of the world (Allen, 2001). This opens up the question, however, how many people in a certain time and place actually worked for wages: an issue which questions the validity of real wages as (the only) measured variable (Lucassen, 2016).   Labour relations Labour relations can be defined as “for or with whom one works and under what rules” (Hofmeester et al., 2015). In the “Global Collaboratory on the History of Labour Relations, 1500-2000”, a collaboration of researchers from all over the world, study these labour relations and especially shifts between them. To effectively compare different parts of the world across time, a uniform way of entering and presenting data as well as a taxonomy has been created (Figure 1). One of the advantages of this taxonomy is that it encompasses the entire population of an area, working and non-working, forces researchers to think about female labour participation (even if it is omitted in their sources), and includes various forms of labour, from reciprocal labour to wage labour, self-employment or slavery. Studying societies other than modern ones can be especially difficult however. Lacking anything like modern census data, one has to be more creative to study different forms of labour across time and space and bring together a wide range of data and information. One may think about information on land use, vicinity of roads, waterways or coastlines, all proving modes of transportation, the vicinity of towns and monasteries, demographic density, documents related to taxation and many other. GIS is best equipped to present such combinations of different data, and is therefore at the core of the following two areas for which the labour relations are reconstructed.    Reconstruction England/Wales To come to a reconstruction of the labour relations in England and Wales a range of sources are combined, including the 1378-1381 Poll Tax records (Fenwick, 1998), the muster rolls of 1522, information on medieval markets (Keene et al., 2002; Keene and Letters, 2004), and GIS shapefiles of historical parish boundaries (Southall and Burton, 2004). From these sources regional variation in demography, market presence, and occupational structure can be extracted – albeit with many caveats. One of the main sources used for the reconstruction, however, are the tens of thousands of archaeological coin finds, by both amateurs and professionals. These are made available through the Portable Antiquities Scheme (PAS) website (The British Museum, 2013-2015) (Figure 2).  The principal assumption behind using the coin finds is that the presence of small denominations, valued at a day’s wage or less, can function as a determinant of wage labour (Lucassen, 2014). Used in combination with figures on mint output, the coin finds can therefore be used to study the relative presence of wage labour; as well as regional differences and developments over time.  As the PAS database was formed by different people over a long period of time, various issues caused by inconsistent or erroneous data entry had to be solved. Much effort has gone into cleaning (especially place names) in the Portable Antiquities Scheme database and supplementing the numerous missing geographical coordinates. The following step is linking the coin finds to the data mentioned earlier, a complicated process that has started but for which much still has to be done in the coming months. In the end, the combination of available data provides us with an extensive toolbox to reconstruct the labour relations in late medieval England and Wales and to study the mechanisms that influence these labour relations, causing regional and temporal variation.    Reconstruction Low Countries For the reconstruction of the labour relations on the other side of the North Sea, in the Low Countries a different approach has been chosen, although here too coin finds may be used in the end (De Nederlandsche Bank, 1997-2015). Starting point is the County of Holland. In 1494 and 1514 two sets of questionnaires were produced by the Burgundian/Habsburg rulers, intended to allocate a new round of taxes. The questionnaires had to be answered by representatives of all towns and heerlijkheden (a feudal administrative-judicial unit, precursor of modern municipalities) in Holland. They asked for information about the number of dwellings and parishioners; economic activities; how these economic activities developed in the past couple of years; land ownership; and tax-related issues. The questionnaires have been used to assess the state of the economy in Holland and study per capita growth (van Zanden, 2002).  Never before have these questionnaires been mapped, which, for instance, would allow to combine the information with land use, the vicinity of monasteries and modes of transportation, and study in more detail regional variance in the county and – because of the nature of the questionnaires – changes over time between 1477 and 1514. For this purpose, a historical GIS of the administrative-judicial boundaries in this and some neighbouring counties was created from scratch. A wide range of digitised historical maps from the sixteenth to eighteenth century were used, in combination to the historical atlas of the Netherlands drawn by Anton Beekman at the beginning of the twentieth century (Figure 3; results in Figure 4). The maps were georeferenced and combined with information on natural and/or current (sometimes unchanged) municipality borders to reconstruct the location of the historical boundaries.  One of the immediate advantages was that using this GIS map, silent omissions in the questionnaires became visible (e.g. certain heerlijkheden that were not mentioned explicitly in the sources). Moreover, the advantage of using the historical boundaries, instead of just looking at the location of the villages and creating Voronoi diagrams for instance also becomes clear, as the maps provide much more detail and display patterns more clearly: see Figure 5.  By combining the questionnaires and the newly created geographical information with assumptions on household size, child and female labour participation, information on religious houses (using Goudriaan and Stuyvenberg, 2008), and other information a tentative reconstruction is made on the full range of labour relations present in the County of Holland in the Late Middle Ages.   Conclusion In the end, the goal is to create a map that shows the presence and absence of various forms of labour and labour relations in the North Sea region. This allows us to study this part of the world, just before two new world powers emerged from here, and the role labour plays in this development. By using GIS (instead of displaying the information in tables for instance), the full potential offered by geographical information systems is made available: e.g. easily combining different forms of information, pinpointing developments to a certain point in time and space. Large-scale regional variation can therefore become a point of study, instead of hindering or difficult to grasp principle.   Figure 1. Taxonomy of the Global Collaboratory on the History of Labour Relations, 1500-2000.    Figure 2. Archaeological coin finds in England/Wales and the Netherlands (Northern Low Countries). Note that at the moment the finds in the Netherlands and England/Wales cannot be compared easily due to national differences in registration.    Figure 3. Excerpt of geographical sources: various 16th-18th century maps, historical atlases.    Figure 4. Result: Historical GIS of administrative-judicial units in Low Countries around 1500 (with churches; monasteries; present-day municipality boundaries).    Figure 5. Thought process and development of mapping medieval tax/population records in Holland: from Voronoi diagrams surrounding the towns and villages mentioned in the sources; to mapping these towns and villages; and finally locating and assessing the areas not explicitly mentioned in the sources.   ",
       "article_title":"Reconstruction of Labour Relations in the North Sea Region in the Late Middle Ages: Spatio-Temporal Analysis Using Historical GIS, Taxation Sources, and Coin Finds",
       "authors":[
          {
             "given":"Rombert",
             "family":"Stapel",
             "affiliation":[
                {
                   "original_name":"International Institute of Social History",
                   "normalized_name":"International Institute of Social History",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/05dq4pp56",
                      "GRID":"grid.450142.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-05",
       "keywords":[
          "medieval studies",
          "spatio-temporal modeling, analysis and visualisation",
          "maps and mapping",
          "archaeology",
          "historical studies",
          "English",
          "geospatial analysis, interfaces and technology"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Contemporary studies on social inequality often focus on income or status attainment, but neglect the fundamental underlying relationship between employer and employee, referred to as a 'labour relation'. This negligence of labour relations (for and with whom one works) was for a long time of no concern, as the employer-employee relationship was predominant in the period after World War II. However, the current fierce debate on the desirability of precarious work among the rising number of freelancers and independent contractors around the world, as well as the increased media attention and consumer awareness of 'forced labour', shows that employer-employee relationship is not a universal constant, but the result of a much broader historical context. Moreover the interest in the historical context of social inequality is now bigger than ever, judging by both the academic and media impact of recent publications (e.g. Clark, 2014; Piketty and Goldhammer, 2014; van Zanden et al., 2014). In order to describe and explain the historical context of shifts in labour relations and to recognize global connections between these shifts, a taxonomy of labour relations covering the past five centuries was devised by the participants of the Global Collaboratory on the History of Labour Relations (https://collab.iisg.nl/web/LabourRelations/; Figure 1). This collaboratory – hosted by the International Institute of Social History in Amsterdam (http://www.socialhistory.org) – has grown in the past decade to an online scholarly community of dozens of regional experts across the world and from various scholarly disciplines, such as social and economic historians, archaeologists and sociologists.  Using the taxonomy (Lucassen, 2013) (Figure 1), accompanying codebook and guidelines for data entry, the global collaboratory has gathered data on labour relations for more than twenty countries, using five temporal cross-sections (1500, 1650, 1800, 1900, [Africa: 1950], 2000) (Brown and van der Linden, 2010; Hofmeester and Moll-Murata, 2011; van der Linden, 2008; Lucassen, 2008). Databases of many more countries and regions are currently being prepared. For each country and cross-section a set of scholarly products are created that consists of a predesigned database with 1) population and demographic data and 2) details of the labour relations. A methodological paper explaining the choices made in the data collection for each country and time period accompanies the database. Both the database and methodological paper are verified before they are provided online as open access publication.   Figure 1. The taxonomy of labour relations, 2015  Until now, most of the data gathered are manually derived from aggregated sources, similar to contemporary occupational censuses, mainly for the period before 1800. The main aim of this paper is to present an alghorithm that was developed to automatically derive labour relations from digitized census materials. We therewith improve on the traditional way of working in the following ways. First, our alghorithm specifically applies to the biggest two projects that digitize census materials in the world NAPP and IPUMS, therewith providing a major contribution to the collection of labour relations for the post 1800 period and for the entire world.  A second advantage of the algorithm is that it provides the first alternative derivation of labour relations and thus can be used to test the reliability of the traditional derivation of labour relations. For many of the digitized censuses both original aggregated tables as well as the individual level data have been preserved, thus allowing for a reliability test of both methods.  The third advantage our algorithm provides is that is able to derive labour relations from individual level census data, thus allowing for more rigorous tests of hypotheses on labour relations. For until now, labour relations have just been gathered from aggregated census tables, only allowing for national comparisons and hypothesis testing. Also labour relations for the past two centuries have only been gathered for three cross-sectional years, while census data is available by decade from ca. 1850 onwards. By being able to attach labour relations to individuals, for the first time descriptive results on heterogeneity of labour relations within countries as well as over time will become available. Moreover, researchers will be much more able to zoom in on the characteristics related to changes in labour relations, such as individual characteristics (e.g. age, gender, education), household composition (e.g. extended family, sibling composition), and historical context of the municipality, region or state (e.g. level of development, political orientation) and therefore be able to provide more rigorous tests of hypotheses on changes in labour relations. For this purpose, an algorithm is created to allocate labour relations to each of the millions of individual records that are part of the IPUMS databases. This algorithm was first tested on the censuses of the United States between 1850 and 2010. While an earlier version was written in R, the current algorithm was written in SPSS syntax, a fourth generation programming language, as SPSS proved to be better equipped for our purpose and to handle the large file sizes (4-10 GB). In the end, the algorithm will be available to all users of the IPUMS databases. Starting from the total population, in each iterance a proportion of the records was allocated to a certain labour relation until all records were assigned. Different variables were used or combined, including age, class of worker, employment status, whether someone was considered in the labour force, their occupation, whether they lived on a farm. Here, the order of the different allocations is very important. Broadly speaking, the algorithm moves from the general to the detail. The end result is an enormous database of labour relations, both in the United States and the wider world. This allows us to study shifts in labour relations in much more detail than before (for example: Figure 2). As the census also includes numerous other information, including for instance place of residence, it is also possible to study in great detail geographical factors (Figure 3, or the role of education, gender, age, ethnicity, household composition, migrant status, wealth, and many other things.    Figure 2. Likelihood that one changes labour relation  Although the IPUMS project has done much effort already to harmonise the different census data, and although historical census takers were also much aware of the need to create uniform censuses both national and trans-national, the algorithm can easily be adapted to the specifics of each of the sources used by the Collaboratory. This means that changes in categories in the census, but also changes in the meaning of census category labels can be adapted to. A future goal is to create an updated version of the algorithm, that allows not only an allocation of a labour relation, but also provides a certainty value. This update is expected in the coming months. Also, in our paper, in addition to presenting the algorithm itself, we will provide a complete research cycle for the US census data from 1850-2013. We start by deriving the labour relations for each of the census years and show our census-specific adaptations of the algorithm to account for historic changes in census taking, such as changes in instructions or the meaning of census category labels. Next, we will describes shifts in labour relations in the US for the period under study using amongst others animations as depicted in Figure 3. Finally, we will derive and test hypotheses on acts that affect self-employment in the US, such as the Midwives Act, that forced women out of self-employment.   Figure 3. Still of an animation of changes in self-employment in the US, 1850-2013  ",
       "article_title":"Work In A Globalised World. Allocation Algorithm To Add Labour Relations To Digitised Census Data",
       "authors":[
          {
             "given":"Richard",
             "family":"Zijdeman",
             "affiliation":[
                {
                   "original_name":"International Institute of Social History",
                   "normalized_name":"International Institute of Social History",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/05dq4pp56",
                      "GRID":"grid.450142.6"
                   }
                }
             ]
          },
          {
             "given":"Rombert",
             "family":"Stapel",
             "affiliation":[
                {
                   "original_name":"International Institute of Social History",
                   "normalized_name":"International Institute of Social History",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/05dq4pp56",
                      "GRID":"grid.450142.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "linking and annotation",
          "databases & dbms",
          "other",
          "historical studies",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Figure 1. Page from a bundle of field notes, describing and depicting a mouse species. Source: Naturalis Biodiversity Center, Archief van de Natuurkundige Commissie voor Nederlands-Indië. Copyright: Public Domain Mark 1.0  This paper presents initial findings of the  research project Making Sense of Illustrated Handwritten Archives and demonstrates the recognition capabilities of the MONK artificial intelligence system developed since the early 2000s at the institute of Artificial Intelligence and Cognitive Engineering (ALICE) at Groningen University (Van der Zant et al., 2008; Van Oosten and Schomaker, 2014a, Van Oosten and Schomaker, 2014b). In a period of four years (Q1 2016 – Q1 2020), our research project aims to produce an innovative and user-friendly online environment that combines both image and textual recognition, and allows an integrated study of fragmented historical heritage collections.  Next to a short demonstration of MONK, we use this paper to outline how handwriting and image recognition helps to increase the value of illustrated handwritten collections by enriching and linking information that is now inaccessible and disconnected. By doing so it advances the state of the art in automated extraction, classification, and networking of knowledge from heterogeneous manuscript collections.  The MONK system uses shape-based feature vector methods that have very few assumptions concerning the content or style of the material. It avoids the traditional OCR approach (optical character recognition) which assumes that individual characters are essentially legible. That assumption only holds for a tiny fraction of handwritten material and a limited number of scripts. The only assumptions MONK makes are that pictorial and textual segments are separated by white spaces; and that the layout, of underlining, etc. in a specific document, is consistent throughout the document. In MONK, classification methods are used that allow for a fast bootstrap from single example instances (nearest-neighbor search) (Gast et al., 2013). With larger numbers of labeled examples, models can be computed, varying from nearest-centroids to support-vector machines and neural networks in a continuous learning process (Krizhevsky et al., 2012; Liu et al, 2015; Guo, in press). A challenging topic from the technical point of view is the relation between existing semantic knowledge (ontologies) and the statistically inferred semantics using Google’s  word2vec and current deep-learning neural networks. Can the underlying structure and style in a collection of a common and realistic size be detected by such algorithms? Can the proposed enrichment system profit from generally available text corpora? The processing power required by the proposed architecture is substantial. For this project, algorithmic optimization of the image processing and recognition system is necessary in order to create the necessary speed and flexibility of the system for use by non-technical end users. In order to tackle this challenge the consortium will make use of the combined knowledge and expertise of ALICE in Groningen, and the Leiden Institute of Advanced Computer Science (LIACS), where multiple supercomputers and high performance computing experts are present.  Because of its visual approach, MONK can handle the diversity of material that we encounter in our use case and in historical collections in general: text, drawings, and images. MONK also does not require a language model nor fully transcribed samples to quickly assess the contents of an archive page. The human-in-the-loop approach of MONK is currently ‘label’ oriented, but will be enhanced by providing the user and the system with ontologies for bootstrapping the learning process. The system will understand handwritten corpora to such an extent that the visual and textual content on individual pages is categorized, determined and networked to other pages in the archive and external sources. To construct training examples for MONK, biologists and historians of science will manually label documents to the machine learning software by means of a human in the loop approach. In addition, a crowdsourcing approach will be used to further expand this corpus of examples. Our consortium will here build on the expertise of ALICE and Naturalis Biodiversity Center, the Leiden-based National Museum of Natural History. Eventually, the computer-assisted recognition of words and visual information on a page will thus allow users to search, filter and group arbitrary archive items and enables connections with external databases. Last but not least, MONK lays the groundwork for full transcription of any handwritten-illustrated archival collection.    Figure 2. Drawing of Burro multicolor created in Buitenzorg, Java in 1827 by Pieter van Oort. Source: Naturalis Biodiversity Center, Archief van de Natuurkundige Commissie voor Nederlands-Indië. Copyright: Public Domain Mark 1.0  The central use case of our research project is the collection of the  Natuurkundige Commissie voor Nederlandsch-Indië (hereinafter NC). It is one of the top-collections of Naturalis. From 1820 to 1850, the NC charted the natural and economic state of the Dutch East Indies and returned a wealth of scientific data and specimens which are now stored in archives in the Netherlands and Indonesia. The collection comprises thousands of handwritten notes and drawings and tens of thousands biological and geological specimens. While these archival items have all been digitized, the individual pages in the notebooks, diaries and reports are not catalogued nor labeled separately. Many of the field notes combine different textual and visual elements on one page. Our short paper presentation is based on the processing of an initial set of understudied handwritten field notes which we carried out in early 2016. By doing so, we will demonstrate the efficiency of the MONK system and our approach.  Owing to the different ‘hands’ and languages used in the documents, links across handwritten field records and notes, drawings and specimens cannot be made in an efficient way. Our corpus contains material from at least seventeen different writers and the used languages range from German  (Kurrentschrift) to Latin, French, and Dutch. The labels of related historic specimens only provide very general information on collection localities and collectors. Hence, the typical use case of a scholar wishing to retrieve information on a certain species, person, drawing, or collecting locality is limited. Owing to its sheer dimension and its weak structure, it is impractical to disclose and network this archive manually. Its current inaccessibility hampers research into Southeast Asian natural history and the history of (scientific) knowledge production. Knowledge extracted from the documents will be structured and served as Linked Open Data. This will allow interlinking of content and also enable interoperability with other cultural heritage resources, for example, the physical specimens obtained during expeditions, or other historically significant data collections from the same area.   The multi-layered character of the material makes it a perfect use case for developing a technologically advanced and usability engineered digital environment for interpreting and connecting illustrated-handwritten collections. In our consortium data scientists from the Universities in Leiden and Groningen work closely together historians of science from the University of Twente and taxonomy experts from Naturalis. Fueled by an investment from BRILL publishers in a national funding scheme, this project will not only result in the disclosure of the NC archive, but will also enable the integrated study of underexplored scientific manuscript collections and archives in general.  ",
       "article_title":"Making Sense of Illustrated Handwritten Archives",
       "authors":[
          {
             "given":"Lambert",
             "family":"Schomaker",
             "affiliation":[
                {
                   "original_name":"ALICE, University of Groningen, The Netherlands",
                   "normalized_name":"University of Groningen",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/012p63287",
                      "GRID":"grid.4830.f"
                   }
                }
             ]
          },
          {
             "given":"Andreas",
             "family":"Weber",
             "affiliation":[
                {
                   "original_name":"STePS, University of Twente, The Netherlands",
                   "normalized_name":"University of Twente",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/006hf6230",
                      "GRID":"grid.6214.1"
                   }
                }
             ]
          },
          {
             "given":"Michiel",
             "family":"Thijssen",
             "affiliation":[
                {
                   "original_name":"BRILL, The Netherlands",
                   "normalized_name":null,
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Maarten",
             "family":"Heerlien",
             "affiliation":[
                {
                   "original_name":"Naturalis Biodiversity Center, The Netherlands",
                   "normalized_name":"Naturalis Biodiversity Center",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/0566bfb96",
                      "GRID":"grid.425948.6"
                   }
                }
             ]
          },
          {
             "given":"Aske",
             "family":"Plaat",
             "affiliation":[
                {
                   "original_name":"LIACS, Leiden University, The Netherlands",
                   "normalized_name":"Leiden University",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/027bh9e22",
                      "GRID":"grid.5132.5"
                   }
                }
             ]
          },
          {
             "given":"Siegfried",
             "family":"Nijssen",
             "affiliation":[
                {
                   "original_name":"LIACS, Leiden University, The Netherlands",
                   "normalized_name":"Leiden University",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/027bh9e22",
                      "GRID":"grid.5132.5"
                   }
                }
             ]
          },
          {
             "given":"Fons",
             "family":"Verbeek",
             "affiliation":[
                {
                   "original_name":"LIACS, Leiden University, The Netherlands",
                   "normalized_name":"Leiden University",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/027bh9e22",
                      "GRID":"grid.5132.5"
                   }
                }
             ]
          },
          {
             "given":"Michael",
             "family":"Lew",
             "affiliation":[
                {
                   "original_name":"LIACS, Leiden University, The Netherlands",
                   "normalized_name":"Leiden University",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/027bh9e22",
                      "GRID":"grid.5132.5"
                   }
                }
             ]
          },
          {
             "given":"Eulalia",
             "family":"Gasso Miracle",
             "affiliation":[
                {
                   "original_name":"Naturalis Biodiversity Center, The Netherlands",
                   "normalized_name":"Naturalis Biodiversity Center",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/0566bfb96",
                      "GRID":"grid.425948.6"
                   }
                }
             ]
          },
          {
             "given":"Katy",
             "family":"Wolstencroft",
             "affiliation":[
                {
                   "original_name":"LIACS, Leiden University, The Netherlands",
                   "normalized_name":"Leiden University",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/027bh9e22",
                      "GRID":"grid.5132.5"
                   }
                }
             ]
          },
          {
             "given":"Ernest",
             "family":"Suyver",
             "affiliation":[
                {
                   "original_name":"BRILL, The Netherlands",
                   "normalized_name":null,
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Bart",
             "family":"Verheij",
             "affiliation":[
                {
                   "original_name":"ALICE, University of Groningen, The Netherlands",
                   "normalized_name":"University of Groningen",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/012p63287",
                      "GRID":"grid.4830.f"
                   }
                }
             ]
          },
          {
             "given":"Marco",
             "family":"Wiering",
             "affiliation":[
                {
                   "original_name":"ALICE, University of Groningen, The Netherlands",
                   "normalized_name":"University of Groningen",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/012p63287",
                      "GRID":"grid.4830.f"
                   }
                }
             ]
          },
          {
             "given":"Rene",
             "family":"Dekker",
             "affiliation":[
                {
                   "original_name":"Naturalis Biodiversity Center, The Netherlands",
                   "normalized_name":"Naturalis Biodiversity Center",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/0566bfb96",
                      "GRID":"grid.425948.6"
                   }
                }
             ]
          },
          {
             "given":"Joost",
             "family":"Kok",
             "affiliation":[
                {
                   "original_name":"LIACS, Leiden University, The Netherlands",
                   "normalized_name":"Leiden University",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/027bh9e22",
                      "GRID":"grid.5132.5"
                   }
                }
             ]
          },
          {
             "given":"Lissa",
             "family":"Roberts",
             "affiliation":[
                {
                   "original_name":"STePS, University of Twente, The Netherlands",
                   "normalized_name":"University of Twente",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/006hf6230",
                      "GRID":"grid.6214.1"
                   }
                }
             ]
          },
          {
             "given":"Jaap",
             "family":"Van den Herik",
             "affiliation":[
                {
                   "original_name":"LCDS, Leiden University, The Netherlands",
                   "normalized_name":"Leiden University",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/027bh9e22",
                      "GRID":"grid.5132.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "information retrieval",
          "GLAM: galleries, libraries, archives, museums",
          "historical studies",
          "networks, relationships, graphs",
          "English",
          "publishing and delivery systems",
          "image processing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  As most of modern and pre-modern western writing systems explicitly represent division of the words in a sentence by spaces or breaks, it has been easy to use computers to analyze texts based on each word and its meanings. However, there are several modern and pre-modern writing systems that do not explicitly indicate word separation in texts; that is, all words in a sentence are contiguous. A major contemporary representative of this kind of writing is seen in the language system of East Asia. Moreover, a popular Japanese pre-modern writing system called kuzushi-ji (cursive style characters) had often been presented with undivided characters even in typesetting until the late nineteenth century (Fig. 1, Fig. 2). As the lack of word-separation has been evoking not only ambiguity but also multiple interpretations, it has formed an aspect of cultural richness in Japanese culture. However, as a result, Japanese texts have intrinsically presented difficulties: not only in the case of textual analysis but also in both manual and automatic transcription in the digital era. This presentation will discuss problems in these writing systems and the current situation of attempts to resolve them through the methods of digital humanities.    Fig.1: Typesetting printing ( Ise-monogatari (Tales in Ise). Saga-bon. 1608.  http://dl.ndl.go.jp/info:ndljp/pid/ 1287963/6  )      Fig. 2: Woodcut printing (Yamamoto Shunsho ed.  Eiri-Genji-Monogatari (Pictorial tales of Genji. 1654.  http://base1.nijl.ac.jp/~anthologyfulltext/ )    Difficulties of transcription  Recent Japanese texts do not have serious problem in case of OCR due not only to the separation of each character but also accuracy and clarify of its printing. However, it is difficult to OCR books printed even ten decades ago because of two points: most of them uses relatively complicated characters for OCR and parallel embedded small-font size texts (called ruby in HTML5) which explain pronunciation of a word, and are too close to the explained word to OCR (Fig. 3), even though they were printed by metal typesetting. More three decades ago characters were sometimes connected, and the writing style of characters were partially cursive (Fig. 4). Recently, some researchers are attempting to develop tools for recognition of kuzushi-ji not based on the shape of individual characters but by continuous shapes of characters. They have not yet reached the stage where they are able to transcribe all characters accurately, for both technical and intrinsic reasons, but the technology can nonetheless assist in reading such texts by showing candidates of characters (Fig. 5)   Hashimoto, Yuta, et al. The SMART-GS Project: An Approach to Image-based Digital Humanities.  Digital Humanities 2014:476-477. 2014.  . One of reasons why such kind of image recognition of a series of characters by machine is available in many cases is that many resources written by kuzushi-ji are woodcut printing, in which case the continuous cursive characters are more or less normalized within a single book.      Fig. 3: Ruby close to text body (Ohashi Matatarou.  Jituyou-ryouri-hou, A Guidebook to practical cooking). Hakubunkan. 1895.  http://dl.ndl.go.jp/info:ndljp/pid/849051/19      Fig. 4: Continuous characters (Ryusuitei Tanekiyo ed.  Shiranui-monogatari (Tales of Shiranui). Vol. 68a. Enju-dou. 1885.  http://dl.ndl.go.jp/info:ndljp/pid/ 884924/8 )      Fig. 5: A result of image search in SMART-GS   However, there are special difficulties presented when a needed character is not encoded in Unicode. It seems to be similar with the case of Medieval Unicode Font Initiative   http://folk.uib.no/hnooh/mufi/ , but the number of unencoded characters would be much more in the Japanese case included in East Asia culture. Especially, as Japanese culture has been involved with foreign cultures and developing them in its contexts, several writing systems are preserved in its cultural resources, including Kanji, Hiragana, Katakana, Hentaigana, and Siddham scripts. Siddham scripts were encoded in Unicode 8.0 with its variant characters by efforts of Script Encoding Initiative, international experts, and SAT project   Pandey, Anshuman. Proposal to Encode the Siddham Script in ISO/IEC 10646. ISO/IEC JTC1/SC2/WG2 N4294. 2012.  http://www.unicode.org/L2/L2012/12234r-n4294-siddham.pdf .  KAWABATA , Taichi, Toshiya SUZUKI, Kiyonori NAGASAKI and Masahiro SHIMODA. Proposal to Encode Variants for Siddham Script. ISO/IEC JTC1/SC2/WG2 N4407. 2013. http://std.dkuug.dk/JTC1/SC2/WG2/docs/n4407.pdf . Anderson, Deborah, et al. 2013-11-22 Siddham Script (梵字) Meeting @ Tokyo, JAPAN, Earth. ISO/IEC JTC1/SC2/WG2 N4523. 2013.  http://std.dkuug.dk/JTC1/SC2/WG2/docs/n4523.pdf .  . There are already 80,000 Kanji (CJK unified characters) registered, but thus number will continue to increase. Hentaigana (including over 200 glyph shapes) was proposed to the ISO committee on October 2015   ITSCJ SC2 Committee, IPSJ, JAPAN. Proposal of Japanese HENTAIGANA. ISO/IEC JTC1/SC2/WG2 N4674. 2015.  http://unicode.org/wg2/docs/n4674-Japan_Hentaigana_Proposal-a.zip .  . In order to make easy-use digital scholarly edition for Japanese texts, especially classics, this process will be continued.   While efforts of transcription, due to commoditization of digitizing textual materials in hi-resolution, digital image databases have also been grown in Japan. Especially, the National Diet Library in Japan has been addressing the publication of digitized collection including over 300,000 books--since over decades ago and recently stated that most of them are to be released in the public domain   http://dl.ndl.go.jp/ . And some institutes such as Kyoto Prefectural Library and Archives   http://hyakugo.kyoto.jp/  and the University of Tokyo Library   http://dzkimgs.l.u-tokyo.ac.jp/utlib_kakouzou.php  are publishing their digitized collections under open license. The Art Research Center in Ritsumeikan University and the National Institute of Japanese Literature   http://www.nijl.ac.jp/  have released many digitized textual resources under academic license. The latter institute plans to distribute parts of their contents under open license in this year in its new comprehensive digitization project   http://www.nijl.ac.jp/pages/cijproject/index_e.html . Needless to say, these are useful to enhance the convenience of humanities research. Especially, in Japanese contexts, many humanities researchers mention that validation of research results has been made much more efficient by the increased use of the digitized images.  Crowd sourcing transcription has recently emerged also in Japan. Transcribe JP project has been conducted as a SIG of the Japanese Association for Digital Humanities. It provides a Web service   Hondigi2014. http://lab.ndl.go.jp/dhii/omk2/  for transcription with Omeka and Scripto plugin. Moreover, it started a micro task crowd sourcing project on   翻デジ＠JADH×Crowd4U. http://www.jadh.org/transcribejp  October 2015 in cooperation with Crowd4U project   Crowd4U. http://crowd4u.org/en/ . Contributors can determine whether a character is exactly OCRed or not, comparing a candidate character with a piece of an image only by one click. The first experiment was finished in a much shorter time than we expected. Further results will be reported at the DH2016.    Difficulties in Word Separation  In spite of the difficulties of transcription, there are many digitized texts in Japanese. Aozora-Bunko   http://www.aozora.gr.jp/ , a public domain Japanese texts repository similar to the Gutenberg Project, provides over 10,000 texts on its Web site and GitHub. The National Institute of Japanese Language and Linguistics (NINJAL)   http://www.ninjal.ac.jp/  publishes several encoded historical Japanese texts with POS tags on Web and Web services of textual analysis on modern Japanese texts including 100 million words with POS tags each word in its original format. The SAT project   http://21dzk.l.u-tokyo.ac.jp/SAT/  also provides digital texts of Buddhist scriptures consists of 100 million characters mainly in Chinese and Japanese with some philological tags on Web.   The texts of NINJAL consists of separated words with POS tags, but most of the others do not use this method. Then, methods for textual analysis are common in Japan: The one is n-gram analysis regarded a character as one “n”. The other is developing tools for automatic separation of words sometimes with POS tagger, such as Mecab   http://taku910.github.io/mecab/ , Chasen   http://chasen.naist.jp/hiki/ChaSen/ , and Kuromoji   http://www.atilika.com/ja/products/kuromoji.html . These tools realize a high degree of precision, but sometimes produce erros. In this case, one has to manually correct the result of the tools if sharing exactly-processed texts is necessary. Moreover, even if a separation is not mistaken, it might support an interpretation in some cases. Such kinds of cases can also be occurred in word-separated corpora. This type of writing system includes such kinds of issues.     Rendering of texts In XML-formatted texts, suc has those maintained in TEI, JATS   http://jats.nlm.nih.gov/ , and so on representation of breaks in source XML files seems to be regarded a space as a separation between words in popular stylesheets. But in the case of non-separated texts, it causes problems such as unnecessary separation. The XSLT-processed Japanese text in fig.7 must exclude spaces between characters in spite of line-breaks in the XML source (fig.6). Conversely, as a Japanese semi-governmental open access journal system adopting JATS ignores line breaks even in English, the words are connected in the case of Fig. 6 and Fig.7. This problem seems to be recognized in ePub with solution in CSS according to the target language   http://www.idpf.org/epub/30/spec/epub30-overview.html  . While it must already be discussed even in contexts of DH because non-spacing texts have been generated in various time and place, the differences of treatment of the line-breaks in XML source files should be carefully treated regarding not only representation but also analysis of texts.        Fig. 6 An example for contrast of word separation in XML format    Fig. 7 An example for a result of XSL Transformation of the Fig. 6  In contexts of current DH, huge humanities resources have still been dormant. According to their awakening, these kind of issues should be gradually revealed and needed to be solved from both practical and abstract viewpoints. Through solving them earnestly under global communication, DH will come to better fruition.    ",
       "article_title":"Digital Humanities in Cultural Areas Using Texts That Lack Word Spacing",
       "authors":[
          {
             "given":"Kiyonori",
             "family":"Nagasaki",
             "affiliation":[
                {
                   "original_name":"International Institute for Digital Humanities, Japan",
                   "normalized_name":"International Institute for Digital Humanities",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/0454arg59",
                      "GRID":"grid.474291.d"
                   }
                }
             ]
          },
          {
             "given":"Toru",
             "family":"Tomabechi",
             "affiliation":[
                {
                   "original_name":"International Institute for Digital Humanities, Japan",
                   "normalized_name":"International Institute for Digital Humanities",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/0454arg59",
                      "GRID":"grid.474291.d"
                   }
                }
             ]
          },
          {
             "given":"Charles",
             "family":"Muller",
             "affiliation":[
                {
                   "original_name":"The University of Tokyo",
                   "normalized_name":"University of Tokyo",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/057zh3y96",
                      "GRID":"grid.26999.3d"
                   }
                }
             ]
          },
          {
             "given":"Masahiro",
             "family":"Shimoda",
             "affiliation":[
                {
                   "original_name":"The University of Tokyo",
                   "normalized_name":"University of Tokyo",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/057zh3y96",
                      "GRID":"grid.26999.3d"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "digital humanities - multilinguality",
          "digital humanities - diversity",
          "digitisation - theory and practice",
          "English",
          "asian studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Agents of Change: Women Editors and Socio-Cultural Transformation in Europe, 1710–1920 is a five–year humanities research project funded by a European Research Council ( ERC) Starting Grant (2015–2020), directed by Marianne Van Remoortel from the Department of Literary Studies at Ghent University, Belgium. It examines a neglected aspect of the social and cultural life in Europe in the modern period: the impact of women editors on public debate. From the 1700s on, European women actively participated in the cultural arena through the journals that they edited.  Agents of Change advances the hypothesis that periodical editorship enabled women editors to take a prominent role in public life and as a result influence public opinion and shape transnational processes of socio-cultural change. By examining how these processes unfolded in the press through practices of textual transfer both among women and in the larger publishing landscape,  Agents of Change will not only initiate a shift in our thinking about the participation of women in society and print culture, but also pave the way for pan-European research on the periodical press.   In order to trace these networks of intellectual exchange,  Agents of Change is using  NodeGoat, a web-based integrated data management, network analysis, and visualisation platform, developed by  Lab1100, a research and development company, based in The Netherlands. NodeGoat allows us to collaboratively gather our research data about women editors and their periodicals, and enables us to visualise and analyse the linkages (both biographical and bibliographical) between them. By gathering evidence to prove connections between people and publications across languages and state borders, we will be able to identify the dynamics of cultural prestige at work in Europe. For example, how knowledge and fashion radiated outward from a few trendsetting periodicals across the pages of myriad publications which translated, adapted, or reprinted them either in part or in their entirety. The data that we collect in NodeGoat will also be invaluable as a descriptive index of periodical editors, a role which traditional print culture studies has tended to overlook, especially when it comes to women periodical editors. For this reason, we are developing a web front-end that will act as a catalogue interface to make this descriptive index freely available online.  In order to fully capture these transnational networks of intellectual exchange it is important to strive for the most comprehensive coverage possible of the period and region at hand. Our multilingual and multidisciplinary team of six researchers will pay particular attention to practices of textual transfer (including translation, adaptation, reprinting and reviewing) across language boundaries and historical periods. However, six researchers cannot cover every language across the 1710–1920 period. In order to make our data as rich as possible, we will be inviting researchers from outside our research team to contribute missing data. We will develop an online workflow, based as far as possible on existing crowd-sourcing initiatives, for managing this community-sourced content. The workflow will enable the community-sourced data to be reviewed by the research team before it is added to the  Agents of Change dataset. An important aspect of the workflow will be to ensure that such contributors are properly credited for their work.  Alongside the collection of our research data, we are also requesting retrospective assignment of International Standard Serial Numbers (ISSN), a unique identification code for serial publications, for the periodicals which do not currently have one. These ISSNs will be used as stable identifiers for the periodicals we store within our database. Similarly, we are working on establishing authority records via the Virtual International Authority File (VIAF) for those women editors that do not have one yet. We will use use their VIAF IDs as stable identifiers within our NodeGoat dataset.  Our ultimate goal is to create a Virtual Research Environment (VRE), as an essential tool for establishing the research field of European Periodical Studies. The VRE will bring together primary sources and secondary literature, as well as the original scholarship that is produced as a result of the research data that we collect. Additionally, we would like the VRE to enable researchers outside the project team to contribute to the field, which we hope, step-by-step, will become a research community for European Periodical Studies.    We want to ensure that  Agents of Change becomes a sustainable research tool beyond the end of the project funding by working closely with the local digital humanities centre. The Ghent Centre for Digital Humanities ( GhentCDH) is an interdisciplinary centre facilitating digitally-enabled research in the arts, humanities and social sciences at Ghent University and beyond.  GhentCDH also plays an active role in the coordination of Belgium’s participation in  DARIAH, the Digital Research Infrastructure for the Arts and Humanities. Within the framework of DARIAH, Ghent University, along with the universities of Antwerp and Leuven, has received startup funding from the Research Foundation Flanders, to develop a Virtual Research Environment Service Infrastructure (VRE-SI).   The VRE-SI is being developed by focussing on the infrastructural needs of existing humanities research projects in Flanders and Belgium that have a ‘digital focus’. Now that the first year of the DARIAH-Flanders project has drawn to an end, the project team have gained a better understanding of how DARIAH Partner Institutions can sustainably support digital scholarship in the humanities. For example, at Ghent University, it has been identified that the establishment of a  digital humanities expert team including humanities researchers, library staff, IT professionals and digital humanities experts would help to institutionally embed digital humanities research support. The role of this interdisciplinary team is to both support the realisation of the digital humanities aspects of existing humanities research projects as well as providing advice and guidance in the development of new project proposals. To date, it has been identified that a missing element in the existing service provision is a  digital humanities scientific programmer, whose role is to combine an understanding of the humanities research questions with the skills of an IT professional to realise the tools and services needed. It is possible to use the DARIAH funding to temporarily recruit such a member of staff and to demonstrate the value of such a post to the Faculty Management Team, with the view to, such a position being structurally funded by the university, in the medium to long-term.   Considering the curation and management of the research data both during the project funding and beyond, is a further crucial aspect of the project. As it is intended that  Agents of Change will become a sustainable digital humanities research tool thriving beyond the fixed-term project funding, the establishment and implementation of a Data Management Plan (DMP) has been anticipated from the start of the project. The  Faculty Library of Arts and Philosophy, as a result of their  Arts and Humanities Research Data project, coordinated by their  LibraryLab, is providing support to researchers in the faculty in the development of DMPs. Within  DARIAH-BE, the intention is that every ‘DARIAH pilot project’, is strongly recommended to consider research data management from the outset. Finally, the GhentCDH is working closely with Lab1100 to explore how NodeGoat could be offered as a DARIAH-service. In the medium to long-term, the possibility of facilitating the development of an open source community around NodeGoat to further extend the environment for the needs of the digital humanities research, is being investigated.   The aim of this short paper is to firstly, to present some initial research results, based on the analysis of the data gathered by the  Agents of Change on tracing the networks of intellectual exchange across temporal, geographic and linguistic borders through women editors and their periodicals. Secondly, this paper will demonstrate how working together with the local digital humanities centre and participating in DARIAH is helping to facilitate  Agents of Change in becoming a sustainable digital humanities research tool that will thrive beyond the end of the fixed-term project funding.  ",
       "article_title":"Mapping European Periodical Counterpublics: Building a Sustainable Collaborative Framework for European Periodical Studies",
       "authors":[
          {
             "given":"Jasper",
             "family":"Schelstraete",
             "affiliation":[
                {
                   "original_name":"Ghent University, Belgium",
                   "normalized_name":"Ghent University",
                   "country":"Belgium",
                   "identifiers":{
                      "ror":"https://ror.org/00cv9y106",
                      "GRID":"grid.5342.0"
                   }
                }
             ]
          },
          {
             "given":"Sally",
             "family":"Chambers",
             "affiliation":[
                {
                   "original_name":"Ghent University, Belgium",
                   "normalized_name":"Ghent University",
                   "country":"Belgium",
                   "identifiers":{
                      "ror":"https://ror.org/00cv9y106",
                      "GRID":"grid.5342.0"
                   }
                }
             ]
          },
          {
             "given":"Marianne",
             "family":"Van Remoortel",
             "affiliation":[
                {
                   "original_name":"Ghent University, Belgium",
                   "normalized_name":"Ghent University",
                   "country":"Belgium",
                   "identifiers":{
                      "ror":"https://ror.org/00cv9y106",
                      "GRID":"grid.5342.0"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016",
       "keywords":[
          "spatio-temporal modeling, analysis and visualisation",
          "digital humanities - multilinguality",
          "bibliographic methods / textual studies",
          "literary studies",
          "gender studies",
          "digital humanities - institutional support",
          "databases & dbms",
          "data modeling and architecture including hypothesis-driven modeling",
          "archives, repositories, sustainability and preservation",
          "English",
          "multilingual / multicultural approaches",
          "crowdsourcing",
          "geospatial analysis, interfaces and technology"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Problem: towards a non-topical classification of weblog genres The existing typologies of weblog genres - both popular and academic - are based on the blog topic, e.g. cooking blogs, travels, business (cf. Morrison 2008) or its medium, e.g. vlogs, picture logs (cf. Herring et al. 2005). In order to go beyond topical distinctions, Maryl, Niewiadomski and Kidawa (2016) conducted an interpretive study on the sample of 322 popular Polish blogs. They adopted a new-rhetorical approach, basing on Carolyn Miller’s (1994) concept of genre as a social action, concentrating mostly on the blog’s communicative purpose and functions.  Following the principles of the grounded theory (cf. Lonkila 1999) the team interpreted those blogs and created an empirical-conceptual typology which entailed following genres: diaries (subjective, self-referential discourse), reflection (subjective discourse on universal matters), criticism (subjective and expert discourse on general issues), information (objective facts), filter (gateway to the existing web content), advice (subjective and expert instructions on particular issues), modelling (serving as a role model for readers) and fictionality (description of fictional events). Weblogs in the sample were coded by three separate coders with 69% average pairwise percent agreement and Cohen’s kappa of .622   The intercoder reliability was calculated with ReCal3, see (Freelon, 2010) . Such a moderate agreement could be attributed to the fact that the resulting genres are ideal types, and most of the actual blogs share features of more than one genre.  This subsequent study aims at supplementing this close-reading typology with a distant-reading perspective (Moretti 2013), based on selected tools for language processing and text clustering. We explore the style of those genres, adapting the definition proposed by Herrmann et al.: “Style is a property of texts constituted by an ensemble of formal features which can be observed quantitatively or qualitatively” (2015:41). We chose this approach due to its stress on mixed methods, as we are combining linguistic and literary criteria of selecting style markers to discriminate between blog genres (Leech & Short 2007,57-58). Current research in the field of computational literary genre stylistics focuses on Most Frequent Words (e.g. Schöch and Pielström 2014; Jannidis & Lauer 2014) or functional linguistic categories (or both) (e.g. Allison et. al 2011). Yet, this study applies similar methods to emergent and uncategorised forms of writing. The quantitative methods are incorporated into the qualitative research workflow in order to create a productive feedback loop.   Corpus The corpus of blogs was collected with the use of  BlogReader - an extension of a corpus gathering system developed in CLARIN-PL (Oleksy et al. 2014) on a basis of open components:   jusText  and  Onion  (Pomikálek, 2011). From the initial set analysed by Maryl et al., 250 blogs were selected for processing as being long enough and included clean text (comments were omitted). We intentionally left out blogs with exceptionally large or small amount of text in order to balance the sample. The selected subcorpus includes: Diaries (44 blogs), Reflection (12), Criticism (73), Information (10), Filter (11), Advice (59), Modelling (24), Fictionality (5), and 10 ‘Unblogs’, i.e. websites or portals using the label of blogs. Posts from the one blog were merged together into a single text document per a blog that was saved in the CCL corpus format (Broda et al., 2012).    Processing We followed the blueprint of stylometry to find groups of blogs, e.g. (Burrows 2002), (Stamatatos, 2009) or (Eder, 2011). Blogs were described by feature vectors whose initial values were frequencies of the selected elements. They were next filtered or transformed. The transformed vectors were clustered into a number of groups that could be presented as automatically identified blog types or compared with the original types. According to the criteria considered for the typology of blogs, we assumed that the interesting distinctions are not of semantic character. Thus we tried to define descriptive features that are not sensitive to the semantics of the blog contents. As a consequence, we have analysed features based on frequencies of lemmas, grammatical classes and sequences of grammatical classes. The brief description below will be elaborated in the presentation:  We have selected the 500 most frequent lemmas from the  Polish National Corpus (Przepiórkowski et al., 2012) and in the series of experiments on the corpus of novels we reduced it to 212 lemmas that did not trigger semantic grouping (e.g. filtering out most of nouns and verbs).  Grammatical classes (as defined in the  Polish National Corpus tagset) were recognised by  WCRFT morpho-syntactic tagger (Radziszewski, 2013).  Features were defined and extracted with the help of the  Fextor system (Broda et al., 2013).  Raw feature values were transformed by measures returning positive results for those features which contribute the significant amount of information to the document description.  SuperMatrix system (Broda & Piasecki, 2013) for Distributional Semantics was applied during the transformation.  Similarity of the transformed vectors were computed by the cosine and ratio measures. The first is not sensitive to the differences in the document lengths that was the case of the analysed collection. The ratio as a heuristic measure that is aimed at comparing how much information is shared by the two vectors:  ratio(V,U) = 2*sum( (Vi + Ui)/max(Vi, Ui) - 1) / (length(V ) + length(U ))   Clustering was performed by the  Cluto package for text data clustering (Zhao &  Karypis 2005). In addition,  Stylo package (Eder et al., 2013) for stylometry was used in experiments with visualisation of the possible blog clusters.   In order to understand the clusters better, most significant features for each cluster were identified and ranked. From several tests the Mann-Whitney U nonparametric test was chosen. For each feature its values in the documents of the given cluster were compared with its values in documents from the rest of the collection.   Experiments We have performed several experiments that can be divided into three main groups:   lexical level analysis, based solely on the selected most frequent lemmas and punctuation marks and aimed at testing whether those properties can serve as a basis for automated identification of the blog types;   lexico-syntactic level analysis featured grammatical classes in combination with the lexical features of the lexical analysis in order to assess whether blog styles result in syntactic properties. On both levels we set the expected number of clusters to 20, in order to give algorithm more ‘freedom’;  extraction of significant features for the blog types with the help of the Mann-Whitney U nonparametric test.    Discussion The generated groups represented relatively high average of clusters purity: 54%-60,4%, i.e. more than 50% blogs in a cluster are of same type. Entropy was higher than expected: 0.438-0.481, i.e. besides dominating types in clusters blogs of other types were scattered (especially smaller types). However, the obtained clusters did not match very well the qualitatively defined types. Lexical analysis combined with the ratio measure produced results that were closest to the qualitative types: entropy of 0.467 and 58% purity, see Figure 1. Yet, lexico-syntactic analysis (lexical features together with grammatical classes and bigrams) yielded better results: 0.438 of entropy and 60.4% of purity, see Figure 2. A slightly worse result: 0.481 of entropy and 54% of purity, was obtained with trigrams instead of bigrams - groups became too small and too specific.   Figure 1. Results of the lexical analysis (features: 212  selected frequent lemmas, punctuation marks), PMI weighting, the ration similarity and, graph clustering algorithm from Cluto    Figure 2. Results of the lexico-syntactic analysis (lexical features plus grammatical classes and bigrams), PMI weighting, ration similarity, graph clustering algorithm  Such genres as advice, criticism and, to certain extent, diaries and modelling were clustered together with others present in multiple clusters. It was caused by distinctive language features of those genres, especially of the advice, which employs instructional vocabulary, or criticism, due to its essayistic style with compound sentences and conjunctions reflecting logical reasoning. Diaries tend to use narrative language, whereas modelling blogs are clearly concentrated on expressing the author’s self. Those differences were further explored through the extraction of blog types’ significant features with the use of Mann-Whitney U statistic. The results were in line with the definitions of classes, but provided more detailed information about the linguistic cues in those genres, some of which are presented in Table 1. Table 1. Selected linguistic features of weblog genres (Mann-Whitney U)   Genre Linguistic features   Advice infinitive, passive adjectival participle, numerals, measurements (“about”, “large”, “small”)   Criticism subjective vocabulary: „I”, „mine”; conjunctions pointing to logical reasoning, e.g. “if”, “that”, “given”, “hence”, “but”   Diaries 1 st & 2 nd person; vocabulary: “self”, “to be”; specific words and verb forms pointing out to a narrative: “certain”, “there”    Fictionality past tense, 3rd person   Filter punctuation, substantives   Information impersonal verb forms, 3 rd person    Modelling interjections (e.g. “eh”), exclamation marks, 1 st & 2 nd person, vocabulary: “mine”, “thing”, “new”, “why”, “because”    Reflection 1 st & 2 nd person, vocabulary: “self”, “always”, “everything”      Conclusions This study showed how close readings (literary interpretative practices) and distant readings (computational approaches to genre analysis) could be integrated in a non-topical analysis of the emerging genres. The novelty of the presented approach lies in the fact that we do not aim at assessing existing genres but rather at developing tools and procedures for the analysis and classification of new genres. The automated methods are used not only to verify the qualitative findings, but rather to enhance them by pointing towards the attributes which might have been overlooked by human coders who were able to read only a sample of each of 332 blogs. The aim is not to cluster texts automatically but rather to support human interpretation in an integrated research design. Recurring problems with clustering genres other than advice could be attributed to the fact that individual blogs within one class may consists of posts which follow different genre conventions. Hence, further studies should explore the genre problem by comparing individual posts (rather than entire blogs) by different authors in order to find stylistic similarities.  ",
       "article_title":"Where Close and Distant Readings Meet: Text Clustering Methods in Literary Analysis of Weblog Genres",
       "authors":[
          {
             "given":"Maciej",
             "family":"Maryl",
             "affiliation":[
                {
                   "original_name":"Institute of Literary Research of the Polish Academy of Sciences, Poland",
                   "normalized_name":null,
                   "country":"Poland",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Maciej",
             "family":"Piasecki",
             "affiliation":[
                {
                   "original_name":"Wrocław University of Technology",
                   "normalized_name":null,
                   "country":"Poland",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Ksenia",
             "family":"Młynarczyk",
             "affiliation":[
                {
                   "original_name":"Wrocław University of Technology",
                   "normalized_name":null,
                   "country":"Poland",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "literary studies",
          "natural language processing",
          "interdisciplinary collaboration",
          "philology",
          "stylistics and stylometry",
          "content analysis",
          "linguistics",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Remediation 1.0.: “Printed database” Polish Literary Bibliography (PBL) is a specialized bibliography which aims to map the totality of literary and cultural life in postwar Poland. It references primarily literary works and literary scholarship, however its entries also cover the related literary critique, adaptations, theatre performances, cinematography, radio and television broadcasts, as well associated events such as conferences or awards. At the heart of PBL lies its subject classification which orders the entries to reflect the domains, hierarchies and entities of Polish literary world, i.e. its ontology in the classical sense. PBL has been developed since 1954 and today covers the period 1944-2001. For most of its history it has existed in print, however since 2000 the data has been collected in the  existing digital database which currently covers the period 1988-2001 what gives app. 600 000 records.  The vicissitudes of PBL remediations could be accurately captured through an urban planning metaphor. What definitely strikes every visitor to a large Moroccan city is a great contrast between  medina, the traditional old town with centuries-long history, and  Ville Nouvelle, new district built under the French Protectorate in the first half of the 20th century. The former reminds a maze with endless narrow streets, and buildings which are stuck densely next to each other with no visible order, whereas the latter is the essence of modern architecture with wide boulevards, large buildings and streets laid out in a grid pattern.   The current online database is quite exemplary for early bibliographical and cataloguing projects (in Poland as elsewhere) in that it is geared towards remediating the print form of the PBL instead of taking advantage of the new medium (cf. Antelman, Lynema and Pace 2006, 128). It is a tailor-made relational database developed in Oracle whose data model is built on a plethora of dataspaces for different types of records, accompanied by various catalogues of creators, contributors, associated institutions and subject headings. The former set reflects PBL’s main entities: literary works in monographs and journals, adaptations in cinematography, radio and television and associated events. The latter represents an early digital take on the index card catalog, the traditional tool of the bibliographer. Furthermore each record has a special markup in order to assure that its display at the frontend follows the structure of the paper edition.  The result of this remediation is a  medina-like database, very rich and complicated but not fit for modern uses. It makes perfect sense for people who built it, yet at the same time it is difficult to navigate by those lacking the local knowledge - be it a human or the machine. As it often happens with relational databases,   For a useful comparison of different approaches to data modelling, see van Hooland and Verborgh 2014: 11-70.  it does not comply with any of the common standards in terms of record structure or data formats, what eventually leads to serious problems with both preservation and interoperability of collected data.     Towards remediation 2.0. The aim of the research project we are currently pursuing (  Polish Literary Bibliography – a knowledge lab on contemporary Polish culture ) is to reestablish the PBL database project on Linked Open Data principles for its better reuse within and beyond the bibliographic domain (see e.g. Roszkowski 2013; Coyle 2010). However, we want to do better than the French colonizers of Morocco. The modernisation of PBL will be reflexive insofar as it will reconcile the OWL and the PBL’s unique ontology of the literary world expressed through the structure of its entries and metadata. The main task of the current phase of the project is development and application of the new data model. This task involves (1) the choice of vocabularies and ontologies and (2) rendering of the subject classification structure.  (1) Vocabularies and ontologies (in the narrow sense used in information science) are needed to disambiguate the RDF triples (subject-predicate-object expressions). Here we need to balance two criteria. First the vocabularies and ontologies must enable widest possible sharing in the data cloud. Second they must be granular and complex enough in order to reflect the PBL data model, since adding too many heterogeneous elements would be counterproductive. The above applies to both metadata elements and their values.  Whereas the choice of value vocabularies was rather straightforward, using the  geonames and  Virtual International Authority File (VIAF) for disambiguating geographical, personal and corporate names, the choice of the meta-ontology,   Our understanding of “meta-ontologies” (as opposed to value vocabularies) is analogous to what the W3C Library Linked Data Incubator Group calls “ Metadata Element Sets”. An additional feature of a meta-ontology is that it “ combines and organizes other ontologies to describe objects in a complex way.” See: Stahmer 2015.    or the vocabulary describing the metadata elements of the current PBL data model was much more difficult. It would be only natural to opt for one of the ontologies dedicated for describing bibliographic records, such as  Functional Requirements for Bibliographic Records (FRBR) and its  Resource Description and Access (RDA) and  Bibliographic Framework Initiative (BIBFRAME) vocabulary variants (cf. Coyle 2016). Indeed, both contain a crucial distinction between “works” (a certain intellectual creation as such, regardless its edition, format or medium) and “instances” (expressions and manifestations of this intellectual creation) which in PBL is paramount for referencing editions, adaptations and critiques of a literary oeuvre of a particular author. For example, a review of  Don Quijote refer to either Cervantes’ literary achievement in general or to the newest translation of the Spanish original into Polish.   However, the FRBR-based ontologies are either not well equipped to handle theatre, cinematographic, radio and television instances of literary works, or (as in the case of FRBRoo) too complex to be easily handled by metadata producers in their everyday practice (Coyle 2016, 153).   In case FRBRoo is in fact widely adopted by the community of practice, we will be definitely interested in mapping it to our data model.   Therefore, we opted for a solution that is more generic but robust enough - the schema.org ontology. However contestable due to its rather restricted vocabulary when it comes to describing books, this solution is not unprecedented in the bibliographic domain.   For list of library-specific extensions proposed by the Schema Bib Extend W3C Community Group, see bib.schema.org. WorldCat.org uses a subset of schema.org terms for Linked Data (  https://www.oclc.org/developer/develop/linked-data/worldcat-vocabulary.en.html ). Updates on schema.org developments are frequently announced on Richard Wallis’ blog dataliberate.com    This process of mapping is by no means mechanical. In many cases the PBL original methodology and the solutions of the first remediation entailed conceptual challenges, which will be addressed in more detail in our presentation. For instance, one needs to solve the tension between a minute bibliographic description on one hand, and the standard vocabulary on the other. Expressions entailing similar yet slightly different properties of the book such as “woodcut engravings”; “illustrations”; “drawings”; “reproductions”; “pictures”; “prints” need to be fit into the elements of the formal vocabulary of schema.org, properties such as “illustrator” and “artform.”  (2) The second challenge of the new data-model involves the PBL subject classification structure. Here the option of using one of the existing and well-established subject headings/authority files published as Linked Data, such as the Library of Congress or German National Library Subject Headings was rather out of question, given the methodological uniqueness of PBL. Instead we will strive to create our own Linked-Data ready classification scheme while at the same time providing a partial mapping to existing resources. To realize the scope of this challenge one needs to bear in mind that PBL has been an ongoing project for the last sixty years. During this time, not only literary life and its study have evolved (cf. the emergence of the online literary life, Maryl 2015), but also certain state entities disappeared (e.g. Yugoslavia or the Soviet Union). Given that the future database will be populated through retroconversion of the paper records in addition to the existing database records, we cannot take the current classification for granted, but also accommodate its historical evolution. A non-intrusive way to account for the historicity of PBL would be to add timestamps to subject headings. Whether a synthetic data-reconciliation layer is possible requires further analysis.   Conclusions In the concluding remarks we will concentrate on the expected benefits of translating PBL into LOD.   PBL datasets can be enriched through integrating other Linked Data collections (e.g. geographical data on places relevant to literary life). Data exchange protocols can be established between PBL and other bibliographies published as Linked Data. PBL data can be used for data-driven research in the humanities on such fields as reception history or transfer studies.  The methodology and the production pipeline developed in this project can be reused for retroconversion of other disciplinary bibliographies.  Acknowledgment This work was supported by Polish Ministry of Science and Higher Education through the National Programme for the Development of the Humanities (grant number: NR 0061/NPRH3/H11/82/2014).  ",
       "article_title":"Remediations of Polish Literary Bibliography: Towards a Lossless and Sustainable Retro-Conversion Model for Bibliographical Data",
       "authors":[
          {
             "given":"Maciej",
             "family":"Maryl",
             "affiliation":[
                {
                   "original_name":"Institute of Literary Research of the Polish Academy of Sciences, Poland",
                   "normalized_name":null,
                   "country":"Poland",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Piotr",
             "family":"Wciślik",
             "affiliation":[
                {
                   "original_name":"Institute of Literary Research of the Polish Academy of Sciences, Poland",
                   "normalized_name":null,
                   "country":"Poland",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "cultural studies",
          "literary studies",
          "data modeling and architecture including hypothesis-driven modeling",
          "ontologies",
          "digitisation, resource creation, and discovery",
          "English",
          "bibliographic methods / textual studies",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Some broad questions require the analysis of huge collections of texts. Other broad questions and many narrower ones require microanalyzing parts of texts. Some microanalyses are unproblematic: narrative structure and its relationship to chapter divisions can be studied simply by dividing texts into chapters. Analyzing narrative or dialogue only, or the relationships between these and chapter divisions, may be much more problematic, as may analyzing a novel that also contains letters, diaries, legends, and poetry. Some or all of these may be more appropriately analyzed separately or ignored. Difficulties multiply for multiple narrators whose narratives contain dialogue and subdivisions.  One of the most difficult tasks is analyzing the character dialogue in a novel. Burrows showed that the frequencies of the most frequent words in their dialogue can distinguish Jane Austen’s characters from each other (1987), but few scholars have followed his lead, at least partly because of the tedium and difficulty of separating character parts. McKenna and Antonia (1996) were an early exception, but most related work involves epistolary novels or multiple narrators, where the separation of parts is simpler (Stewart 2003; Rybicki 2006; Ramsay 2011; Balossi 2014; Hoover, Culpeper, and O’Halloran 2014; Hoover 2010, and forthcoming).) Consider the case of Sherlock Holmes. Perhaps, as Moretti argues, “Doyle owes his phenomenal success to his greater skill in the handling of clues” (2004, 48), but Holmes and Watson are also extraordinarily fascinating characters. Analyzing their voices for distinctiveness requires comparing them with his other characters. Because reliable results require substantial amounts of text, I focus here on the longest Holmes novel,  The Hound of the Baskervilles ( Hound, below).   Extracting the dialogue computationally still requires the tedious and error-prone manual separation of the character parts and identification of the speakers. Typically characters are too numerous to open separate dialogue files for all of them, and multiple files increase copying and pasting errors. Initial decisions about the handling of dialogue may also change, requiring painstaking re-editing. Instead, I introduce very simple markup that is then processed in “Analyze Textual Divisions,” an Excel spreadsheet with macros. The markup, powerful enough for texts with quite complex structures, is also simple, flexible, and customizable: <1> text division 1 <2> text division 2 <3> text division 3 <4> text division 4 [ ] Letter writer { } Letter addressee / speaker \\ speech marker > copy without processing ^ special character follows  For Wilkie Collins’s complex novel  No Name, with scenes containing chapters, which contain letters and other documents, the four divisions are “Scene”, “Chapter”, “Letter”, and “Document.” (The spreadsheet includes brief excerpts from this novel with mark-up.) Epistolary novels might use “Letter,” and others might use “Volume” and “Book.” For texts with multiple narrators and for plays “Narrator” and “Act” and “Scene” are obvious divisions. The top-level division, like the rest of the markup, can be modified. For  No Name, division one is defined as follows: div1name = “Scene”. Novels divided into books could use “div1name = “Book.” Alternatively, after the macro operates, the labels can be changed as desired.   Here is a truncated version of  No Name:     A “<1>” has been inserted to mark “THE FIRST SCENE.” as division one, and all lines in the first scene will be so labeled. In line two, “>” indicates that “COMBE-RAVEN, SOMERSETSHIRE.,” which seems like a scene-setting label, not narrative, should not be processed (epigraphs or poems might be treated similarly). In line three, “CHAPTER I.” marks division two. In line six, “/Norah” labels lines 6-8 as hers (the person addressed could, like a letter addressee, be marked with {}). In line seven, “\\she said” is a speech marker, categorized separately because they sometimes vary interestingly and because “she said” seems to me neither dialogue nor narration. In line eight, the quotation mark indicates dialogue. The blank ninth line changes the label from dialogue to narrative until marked otherwise. The beginning of chapter thirteen is marked similarly. Later in the novel, embedded letters are marked with “Letter writer” and “Letter addressee.” Finally, “^” must begin any line that would otherwise begin with “+”, “-”, or “=” (reserved characters in Excel). (Line-division can be changed instead, except where required line breaks force special characters to the beginning.) With the Analyze Textual Divisions spreadsheet and the marked-up text open in Excel, the macro processes the text line by line, producing the results below (the marked-up text and empty columns have been deleted). Each line gets a scene label, and, beginning in line three, a chapter label, and all the lines are numbered. Lines 4-5 are marked as narration, lines 6 and 8 as Dialogue, and line 7 as Marker, and the speaker is entered for lines 6-8 and 14-16. The processed text appears on the right with all markup removed. The text could be marked up in TEI and the character parts extracted with XSLT, but the markup here is much simpler and easier to learn, and the spreadsheet has advantages over XSLT. Excel’s built-in sorting function can handle several levels of sorting, for example, so that the dialogue can be sorted by type, scene, chapter, speaker, and line number, all at once. The unmarked processed text, after sorting, can be divided and analyzed however the analyst desires with plain-text tools. Sorting on the line number restores the original order for further analysis, and errors can be corrected in the original text, and the analysis re-run. (See my Excel Text-Analysis Pages at  http://wp.nyu.edu/exceltextanalysis/ for detailed instructions.) This method works especially well for short, simple texts like  Hound, with character parts too short to be analyzed by chapter; the dialogue can be marked with just speaker and speech marker characters, and > and ^.      To test the distinctiveness of the character voices in  Hound, I selected all character parts at least 1,500 words long, and divided longer parts into 1,500-word sections. Initial testing was disappointing. Although the sections of dialogue by Stapleton, Mortimer, and Watson grouped correctly, those by Baskerville, Barrymore, and Holmes did not, casting doubt on the distinctiveness of their voices. The section of Baskerville’s dialogue that groups with Barrymore’s, however, consists almost entirely of a conversation between the two, so that similarity of topic may skew the results. More significantly, the first six sections of Holmes’s dialogue consistently group correctly. The final two, which consist almost entirely of the final chapter, and which tend to group separately from all others, are Holmes’s explanation of the case to Watson. Nominally dialogue, this chapter is more like narration, a genre difference that is almost certainly responsible for the anomalous clustering. Removing the final chapter and sorting the lines of Baskerville’s dialogue in random order to blunt any topical or thematic effects produces the cluster analysis shown in Fig. 1, based on the 225mfw (most frequent words).   Cluster analysis is an exploratory statistical method that compares the frequencies of a set of words across a set of texts to determine which texts use those words at the most similar frequencies. The nearer to the left that sections join together into a single cluster, the more similarly they use the words. All sections in Fig. 1 group correctly by speaker, and several sections of Holmes’s dialogue are the most similar, and the results are correct across analyses based on the 125-325mfw. Doyle’s use of clues may have helped the Sherlock Holmes stories succeed, but the distinct character voices also seem likely to be a factor. (The analysis here uses Ward Linkage and squared Euclidean distance; the often-used complete linkage gives weaker results.)  Separating character dialogue can never be easy, but my spreadsheet makes it much easier. It also provides a versatility in comparing multiple kinds of textual divisions that may encourage more in-depth analysis of dialogue and characterization and enhance our understanding of how texts work.   Fig. 1: Character Dialogue in Hound (225mfw)  ",
       "article_title":"Microanalyzing Parts of Texts",
       "authors":[
          {
             "given":"David L.",
             "family":"Hoover",
             "affiliation":[
                {
                   "original_name":"New York University, United States of America",
                   "normalized_name":"New York University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0190ak572",
                      "GRID":"grid.137628.9"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-10",
       "keywords":[
          "literary studies",
          "stylistics and stylometry",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   The Canonical Text Services (CTS) protocol (Blackwell and Smith, 2014) offers the scholarly community a way to use URNs for referring to two categories of propositional objects commonly called texts: to their ideal representations,  works, and their specific realizations,  expressions (International Working Group on FRBR and CIDOC CRM Harmonisation, 2015). CTS URNs point to complete texts and their subdivisions. CTS has a potential to transform scholarly practices. It supports the migration of our interpretations and knowledge from print to digital. It also forces us to reconsider what exactly we are doing when we refer or cite. It could, eventually, integrate into our referring and citing machine-driven comparisons across multiple versions of texts.  The CTS protocol is currently implemented in two projects: the  Homer Multitext (Dué and Ebbott, 2015) and the  Perseus Digital Library (Crane et al., 1987-). Both focus on texts which have traditionally been considered classical. Centuries, in some cases millennia, of appreciation and careful study have provided us with slightly different, but well-established citation schemes for such texts, and the main challenge to CTS up to now has been to reproduce these schemes. We put the protocol to a new test, by applying it to a non-canonical corpus of Latin texts published in the digital collection  Croatiae auctores Latini, CroALa (Jovanović et al., 2009-).  CroALa collects and enables research of texts from a rich tradition of writing in Latin in Croatia. Latin was written through the Medieval and Early Modern periods up to modern times (our latest title is from 1984). The corpus includes a number of translations of Homeric poems into Latin, such as the partial one – an episode from the  Iliad - by Janus Pannonius (1447), and a complete  Iliad by Rajmund Kunić (1776). We wanted to connect  CroALa manifestations (digital editions) of these expressions to the  Iliad as work, thus making possible a connection to manifestations of its other expressions, published elsewhere – in our case, to the Greek editions published by the  Homer Multitext.   The process involved three stages: making the CroALa texts canonically referable through XML catalog records, validating and verifying the prepared editions, and establishing connections between editions prepared by different projects.       Homer Multitext have produced URNs for each line of the  Iliad as work; e. g. book 6, line 119 is described by  urn:cts:greekLit:tlg0012.tlg001:6.119.  A RDF triple connects this to URN of a line in the edition of the Venetus A codex (urn:cts:greekLit:tlg0012.tlg001.msA:6.119). Something similar is done for CroALa;  you can see t he work URN implied in the URN of a line our edition of Janus Pannonius’ Latin translation (which is an expression fragment):  urn:cts:greekLit:tlg0012.tlg001.croala-lat01:6.119.  The same work URN is also implied by a line in the edition of Rajmund Kunić's complete Latin expression of the  Iliad:  urn:cts:greekLit:tlg0012.tlg001.croala-lat02:6.119.  The referencing happens through CTS Text Inventory (CTS TI), an XML catalog file. There Janus Pannonius' text is described by the following fragment:      The TEI XML guidelines allow multiple ways of marking up text structures (Schmidt, 2014). Therefore the most important sections in the fragment above are the XPaths which describe locations of individual books and linesin our editions. Books and lines can be encoded in a different way, represented by different XML elements, but through CTS URNs we are still able to connect the corresponding points, just as we refer to the same verse in the  Iliad-as-work regardless of the fact that it is realised (printed or written) on different pages in different editions (Manifestation Product Types).  Our descriptions have also to be checked for correctness. Here the Homer Multitext project also can help; they have developed an excellent system for automated validation of editorial descriptions. We are adapting this to ensure that everything works in CroALa CTS, that no errors are introduced during the encoding process.Validation happens in a Virtual Machine which ensures that the entire process is replicable (Smith, 2015). But, since faithful replication of the process will only faithfully replicate systematic errors, a validation system was developed to assess our work in a different method, independent of how it was created. The system first tokenizes all of the words.  Parsley, a parsing machine for Latin morphology (Schmidt, 2015), checks that all tokens are valid Latin forms. Personal and geographic named entities do not parse automatically, so the system analyzes these separately. Named entities are checked for consistent markup and for compliance with our authority lists. The tokens that do not parse at either of these stages are analyzed by a researcher.   What has been validated has also to be verified as correct; our validation system ensures that all the tokens are acceptable Latin forms, but researchers have to ultimately decide whether the forms were correctly used.  Forms that are identified as being invalid are analyzed further for encoding errors, incorrect entries, problems with the parsing machine. One of recurring problems is that neo-Latin vocabulary is missing from the classical Latin dictionary used by the parser, so new words should be added to the lexicon. Even more numerous are neo-Latin forms that orthographically (sometimes morphologically too) violate classical Latin rules. Such forms have to be matched with the classical equivalents so that they can be accepted when the machine comes across these forms again. A similar approach would be required for all editions in which language differs markedly from the standard modern variant - e. g. for the Early Modern English as used in Shakespeare. Once we have a text referenced by canonical URNs and tested as validated and verified CTS, we can serve the URNs and retrieve them from wherever we want. Connecting different editions - for example, linking Croatian Latin translations of Homer to editions of manuscripts prepared by the Homer Multitext project - is then a question of aligning the two sets of URNs. These aligned sets will enable us later, for example, to display in parallel the texts served behind each of them. Though clear and simple in principle, the actual application of CTS to CroALa texts opened up a series of practical questions with certain theoretical implications. We mention only two. First, a text and its translation are not always in a 1 : 1 relationship. A verse of the original can be rendered by verse and a half, or by a half verse, in the translation; a description (\"Peleiades\") can be glossed (\"Achilles\"). This had to be taken into account during the process of editorial verification. We had to introduce additional checks for translation alignments and establish a procedure for marking places where translation \"shifts\" equivalents forward or back in the textual structure (Latin equivalent of a Greek word appears elsewhere in the sentence, and therefore may appear in a different line). Second, Croatian Latin translations of the  Iliad are expressions of the Homeric work, but at the same time they themselves are of potential interest as authorial works, and they themselves exist in multiple manifestations (Kunić's translation was published in Rome 1776, Venice 1784, Vienna 1784, Firenze 1831 and 1838). To enable detailed scholarly study of translation as a work, the system has to take into account this additional level of multiplicity too: not only  Homer Multitext, but also a  Kunić Multitext (with the same underlying work).  Among the grand visions of digital humanities there is a dream of a world - or a space - in which different digital editions, carefully prepared, annotated and interpreted, mesh easily together, thus providing an especially rich and detailed groundwork for further annotations and interpretations. This space of interchangeability is today attained only rarely and with difficulty. The level of difficulty can be significantly lowered, as shown by CroALa's successful implementation of CTS and its automated validation and verification processes. A further step towards interchangeability will be wider acceptance of a digital canonical reference system such as CTS. For this to happen, a series of applications and \"recipes\" for specific usage cases is needed. We hope to have offered one such recipe here.  ",
       "article_title":" Implementing Canonical Text Services in the Croatiae Auctores Latini Digital Collection  ",
       "authors":[
          {
             "given":"Neven",
             "family":"Jovanović",
             "affiliation":[
                {
                   "original_name":"Faculty of Humanities and Social Sciences, University of Zagreb, Croatia",
                   "normalized_name":"University of Zagreb",
                   "country":"Croatia",
                   "identifiers":{
                      "ror":"https://ror.org/00mv6sv71",
                      "GRID":"grid.4808.4"
                   }
                }
             ]
          },
          {
             "given":"Alexander",
             "family":"Simrell",
             "affiliation":[
                {
                   "original_name":"College of the Holy Cross, Worcester MA",
                   "normalized_name":"College of the Holy Cross",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05dwp6855",
                      "GRID":"grid.254514.3"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-05",
       "keywords":[
          "renaissance studies",
          "linking and annotation",
          "xml",
          "classical studies",
          "standards and interoperability",
          "philology",
          "scholarly editing",
          "translation studies",
          "multilingual / multicultural approaches",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Motivation In a relatively recent study on the needs of humanities faculty and students in using digital sources (Lindquist and Long 2011), two major issues were identified: 1) locating data relevant to a topic when online collections are distributed across institutions and systems; and 2) being able to explore the items found in context. In addition, problems were identified with crossing language barriers and with ambiguities and variants in names.  The CORE contextual reader is an application that uses natural language processing and Linked Data (Heath and Bizer 2011) techniques to address these issues in the context of close reading of primary source material In this, the application can be seen as a spiritual successor to the Magpie tool (Dzbor, Motta, and Domingue 2007), although the two share nothing concrete with each other.. Particularly, the CORE application has been designed to improve the user reading experience with texts in a domain not entirely familiar to them. Examples of this situation include a history student approaching a new topic through primary sources, or a layperson trying to make sense of law texts.     Figure 1. The contextual reader interface     The CORE user interface CORE supports contextualization in and understanding of unfamiliar documents by utilizing Linked Data reference vocabularies and datasets to identify entities in any PDF file or web page. For each discovered entity, CORE can then present configurable information sourced from these reference datasets on a mouse-over inside the web browser being used to read the document. Figure 1 shows this functionality in the context of reading a primary source document dealing with the First World War. The document, a scanned PDF, is shown in the interface on the left-hand side. Colored boxes highlight all of the entities identified by CORE. Here, the user has moused over “Captain Fryatt”, and the interface has brought up his picture and a short biography. Other examples of contextual information shown are word definitions for domain-specific vocabulary, maps showing the geographical context of unfamiliar places mentioned, and so on. If further information is needed, an entity can be clicked on to load more information and context into the pane on the right-hand side of the reader. In this pane, contextualization is further supported by visualizations, for example, locating the entity of interest temporally on a timeline and geographically on a map. Figure 2 shows these visualizations for an identified event, in this case the execution of Nurse Edith Cavell by the Germans in Belgium during WW1. At the top of the pane, the event is contextualized temporally among other war events. These are color-coded to differentiate: 1) important top-level wartime events sourced from the Imperial War Museum, 2) all events happening in the same timeframe, and 3) other wartime events happening nearby. Below the timeline, all of these events are presented on a map to give a geographical perspective. Clicking on any of the entities visualized loads the information pertaining to that entity into the contextualization pane, allowing further navigation of the context. In addition to providing more nuanced context, the right-hand pane of CORE also facilitates serendipitous discovery of further related content. Using the configured Linked Data vocabularies, CORE is able to extract relevant search terms for an entity of interest. These search terms can then be used to discover related content from configured endpoints, even if they support only simple text searches. In Figure 1, this functionality is seen on the right-hand side of the user interface. First, formally encoded metadata brings in another relevant primary source from the University of Colorado Boulder’s (CU-Boulder) WWI Collection Online   . Images of the burial of Captain Fryatt from Europeana   , on the other hand, are found not through formally encoded keywords, but rather a match on his name that appears in the textual description of the images.  Among the extracted terms used in the query are multilingual labels for places, variant names for actors and events, etc. Leveraging these terms enables CORE to cross language barriers and handle naming variations. To improve recall even further, the search term extraction can be configured to include terms for related entities, such as the actors participating in an event or the names of all villages in a particular municipality under investigation.  Because CORE is able to dynamically process most HTML and PDF content, any linked resource can be loaded into the contextual reader by clicking on it. This function facilitates endless browsing on a topic through thematic and contextual connections, regardless of from where the linked material comes.     Figure 2. Contextual visualizations for the shooting of Nurse Edith Cavell     System demonstrators In contrast to most other similar systems, a CORE instance can relatively easily (and always should be) configured for a particular domain, thus ensuring the contextual information provided is actually useful and interesting to the end-user.  To provide its services, CORE makes use of dynamic, configurable entity recognition, in which modular lexical analysis services are combined with SPARQL queries For technical details of the system, see (Mäkelä 2014).. This allows multilingual entity recognition against any vocabulary stored at a Linked Data endpoint. A configuration therefore consists of tuning the lexical analysis service to a particular domain and set of languages, as well as defining the endpoints and queries to be used in bringing in contextual information and related resources.  While in future the application is intended to be fully configurable using a web user interface, currently new instances must be configured from the source code, released under the MIT open source license at  . Thus far, three different demonstrators have been created For those technically oriented, the configuration files for these demonstrators can be perused at  . .  The first of these is the contextual reader for First World War primary sources available at  . For vocabularies, it draws on the WW1LOD dataset (Mäkelä et al. 2015), the vocabularies of 1914–1918 Online  http://www.1914-_1918-_online.net/ , the Europeana 1914–1918  http://www.europeana1914-_1918.eu/  thesaurus, the Out of the Trenches (Pan-Canadian Documentary Heritage Network 2012) and Trenches to Triples    vocabularies, and DBpedia (Lehmann et al. 2015). Repositories used for sourcing related content are CU-Boulder’s WWI Collection Online, WW1 Discovery   , Europeana, the Digital Public Library of America (DPLA)   , and The European Library (TEL)   .  To further demonstrate multilingual support as well as support for inflected languages, a second contextual reader has been configured to support the study of ancient Roman sources, be they translated into English or still in the original Latin. This installation is available at  . Here, ancient place names are located on maps through the Pleiades gazetteer of ancient places   , while information on entities like people and mythical characters mentioned in the texts is sourced from the English and Latin DBpedias. Targeted repositories are the Perseus Catalog   , the various Pelagios datasets   , and again Europeana, DPLA, and TEL.  The final demonstrator, aimed at supporting the reading of legal documents in the highly inflected language of Finnish, is available at  . In this case, the documents are drawn from, for example, the consolidated legislation    and the precedents of Finnish supreme courts    published by the Finnish Ministry of Justice. In addition to linking these distributed resources to one another, the application is able to bring in news articles    dealing with laws of interest published by Edita Publishing.  When reading documents containing precise legal terminology, the reader is supported by definitions from the legal terminology section of the Bank of Finnish Terminology in Arts and Sciences   , the Asseri vocabulary of the Ministry of Justice, the Edilex legal vocabulary from Edita, the Finnish law vocabulary from Talentum Publishing, and the legal terminology section of the Finnish DBpedia. In addition to Finnish, this reader has also been configured with limited support for Swedish, as Finland is a bilingual country.    Conclusions and future work The CORE contextual reader clearly demonstrates the potential of utilizing Linked Data vocabularies to bridge institutional silos and language barriers, even in situations where the structured metadata of the corresponding databases is lacking. On the other hand, the core mission of the tool is to support contextualization and understanding. While initial experience points both to significant overall support, as well as a marked increase in support with regard to less domain-configured alternatives (Csomai and Mihalcea 2007; Olango, Kramer, and Bouma 2009), a formal user evaluation of the reader remains to be conducted. This will be the natural next step for the project, and plans for testing the WW1 version of the reader are already underway.  At the same time, the CORE reader is currently seeing uptake in new contexts, most notably a project to unify disparate material related to the Finnish view of the Second World War (Hyvönen et al. 2015). Supporting these new contexts may require further development of components of the reader. For example, the Second World War material under study contains multiple distinct places and people with the same names. To properly handle these would require better support for disambiguation in the entity recognition component of the reader.   ",
       "article_title":" CORE - A Contextual Reader based on Linked Data  ",
       "authors":[
          {
             "given":"Eetu",
             "family":"Mäkelä",
             "affiliation":[
                {
                   "original_name":"Aalto University, Finland",
                   "normalized_name":"Aalto University",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/020hwjq30",
                      "GRID":"grid.5373.2"
                   }
                }
             ]
          },
          {
             "given":"Thea",
             "family":"Lindquist",
             "affiliation":[
                {
                   "original_name":"University of Colorado Boulder, United States",
                   "normalized_name":"University of Colorado Boulder",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02ttsq026",
                      "GRID":"grid.266190.a"
                   }
                }
             ]
          },
          {
             "given":"Eero",
             "family":"Hyvönen",
             "affiliation":[
                {
                   "original_name":"Aalto University, Finland",
                   "normalized_name":"Aalto University",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/020hwjq30",
                      "GRID":"grid.5373.2"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-29",
       "keywords":[
          "information retrieval",
          "semantic analysis",
          "visualisation",
          "natural language processing",
          "linking and annotation",
          "interdisciplinary collaboration",
          "internet / world wide web",
          "GLAM: galleries, libraries, archives, museums",
          "English",
          "publishing and delivery systems",
          "semantic web",
          "interface and user experience design",
          "text analysis",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction An appeal to strengthen DH research using audiovisual collections has been repeatedly articulated in the DH community. Some of these collections are digitized museum collections of heritage objects. Many of these serve as repositories (sometimes for internal inventory purposes), some of the collections follow the rules of building digital scholarly editions and provide tools of analysis/summarizing and, sometimes, sources for critical interpretation.  Little is known, however about digitization practices within Museums in Russia, with no prior research into the number of Russian museum web sites, the amount of their collections digitized or put online or their digitization procedures. International research on digitization in museums has a comparatively long tradition and there is a significant literature on statistical data about digitization, digital preservation and online access to cultural heritage across Europe (ENUMERATE, 2015) and Northern America. Reports on digitization success stories in these geographical areas have been published (see among others ENUMERATE, 2015; Clough, 2013; Olsen, 2015) but there is no prior research regarding digitization uptake and practices in Russia. This paper explores Russian museum digitization practices employing the standard method of surveying museums employed by the ENUMERATE project, allowing us to compare results to ENUMERATE. We also chose to augment our findings with the results from exploring the content of museum web sites to understand what parts of museum analogue collections are posted online. The paper seeks to answer a critical question about the size of digital collections as measured by the ratio of digital copies of unique museum objects to the number of unique objects from a museum collection (or the number of digital copies posted on the museum web site). We understand it very well that this can only serve as an imperfect proxy for digitization practices in Russian museums. However, in a situation when this is the only data that could be obtained, we judged it would be reasonable to start the discussion of digitization in Russian museums from this point.    Methods In the early stages of our project we adapted the ENUMERATE questionnaire to our goals of obtaining answers from museum staff. We posted a survey with twelve questions online in October 2014 and we sent letters to 440 museums with a link to the survey. The email addresses were obtained from Museums of Russia web portal.Museums of Russia web portal (http://www.museum.ru) includes detailed information on 3063 Russian museums, ranks their web sites, posts news, discussion threads and announcements for curators. Online since 1996, it was initiated by the State Darwin Museum and supported by the RF Ministry of Culture (see also Mikhailovskaya and Nasedkin 2002). The database lists 3063 Russian museums including data on the number of visitors per year, the year when a museum was established and the number of curators among its employees. Our sampling method was to choose 130 museums located in Moscow and Saint Petersburg and 310 museums located in provincial cities and smaller settlements. Each of the 80 administrative districts in the Russian Federation was represented by 3-6 museums with one or two of them belonging to the group with the number of visitors per year more than 50,000 people. The other two groups included small museums (the number of visitors per year was fewer than 15,000 people) and medium museums with the number of visitors between 15,000 and 50,000 per year. This gave us an appropriate sample of museums to begin to understand different museum digitization practices across Russia.   Our next step was to study the web sites of large provincial museums and medium-sized provincial museums for 58 administrative districts (116 provincial museums as a total). Nineteen web sites for large museums and 23 web sites for medium-sized museums in Moscow and Saint Petersburg were also studied. The number of digital images on museum web sites was compared with the number of unique objects in their collections as reported in the Museums of Russia database (Museums of Russia, 2015).    Results and discussion The response rate for the survey was a disappointing 6% (30 memory institutions completed the questionnaire and answered the most important question about the size of their digital collection). Such a response rate was very low compared to 30% response rate for the survey of library digitization projects in the USA in 2004 (Boock and Vondracek, 2006) and 51% for European cultural institutions in May 2007 and May 2009 (Poll, 2010). This result though is consistent with the finding that other Russian surveys tend to demonstrate low response rates with some studies reporting low level of trust to surveys among respondents (Kalinin, 2012), which has important methodological implications for those carrying out research within Russia.  In Table 1 we summarize the results for the survey of museum digital collections. Absolute average of the proportion of an analogue collection that was reported digitized for the 30 museums in the sample was 18,3%, in line with the results from ENUMERATE survey for 2012 (ENUMERATE, 2014).  Table 1. Distribution of parts of collection digitized (as reported by museums in our survey)   ratio of digital images to unique objects (%) number of museums in a sample as % of a number of museums in a sample   0 8 26.3   10 8 26.3   20 8 26.3   30 3 10   50 1 3.3   80 1 3.3   90 1 3.3   Tables 2, 3, 4, and 5 show the results for exploring the web sites of 158 Russian museums in the provinces and major cities to find out what parts of their analogue collections are posted online.  Table 2. Parts of museum analogue collections published online. Large provincial museums.    % of digital images representing museum collections online number of museums as % of large provincial museums in the sample   0 8 14   from 0 to 0,1 27 47   from 0,1 to 0,98 14 25   from 1 to 10 6 10   more than 10 2 3   Table 3 Parts of museum analogue collections published online. Medium-sized provincial museums.   % of digital images representing museum collections online number of museums as % of medium-sized provincial museums in the sample   0 13 22   from 0 to 0,1 11 19   from 0,1 to 0,98 21 36   from 1 to 10 9 15   more than 10 4 7        Table 4 Parts of museum analogue collections published online. Museums in Moscow and Saint Petersburg   % of digital images representing museum collections online number of museums as % of museums in the sample   0 6 14   from 0 to 0,1 10 23   from 0,1 to 0,98 15 35   from 1 to 10 11 26   more than 10 1 2        Table 5 Parts of museum analogue collections published online. Overall results for 158 museums in the sample.   % of digital images representing museum collections online number of museums as % of museums in the sample   0 27 17   from 0 to 0,1 48 30   from 0,1 to 0,98 50 31   from 1 to 10 26 16   more than 10 7 4        As shown in Table 5, a third of Russian museums in our sample publish less than 0,1% of their images online while another third of museums post digital images for a bigger part of their collection (but still less than 1%).  Large provincial museums are not enthusiastic about publishing their images online, with half of studied web sites demonstrating results which were lower than 0,1% of their analogue collections. Medium-sized museums show slightly better results, with a third of them displaying between 0,1 to 1% of their analogue collections online. They, however, have fewer objects to digitize and annotate. Museums in the two major cities seem more inclined to post their images online, with a quarter of museums publishing between 1% and 10% of their analogue collections on the Web.    Limitations Unfortunately, the survey was designed so that respondents only had an opportunity to choose between 0% and 10% options when describing the parts of their analogue collections being digitized. This may have left many respondents with collections in between these figures indecisive on what option to choose, deteriorated the quality and accuracy of the results and may have influenced the response rate. The low response rate is also problematic, but, when combined with our survey of practice still gives us interesting insights to a hitherto undocumented area. Future work will be needed to work with museum bodies on gathering further data.    Conclusion This work includes the survey results of thirty Russian museum digital collections to find out what part of their analogue collections is digitized. We also studied 158 museum web sites to count the number of digital images representing museum objects and to compare this number to the number of objects in the analogue collections. The average ratio of Russian museum digital collections in the sample of 30 museums compared to their analogue collections was 18,3% which is in line with the results from the ENUMERATE project (ENUMERATE, 2014), and aligns the work of Russian museums to the rate of digitization across Europe.  We have shown that half of large provincial museums in Russia publish an insignificant number of their images on the Web, and further work will allow us to establish why this is the case. Also (at a time when European Museums are being encouraged to adopt the Open Licensing Agenda), further work will pursue the opportunities of sharing digitized collections within Russian legislation.  ",
       "article_title":"Museum Digitization Practices Across Russia: Survey and Web Site Exploration Results",
       "authors":[
          {
             "given":"Inna",
             "family":"Kizhner",
             "affiliation":[
                {
                   "original_name":"Siberian Federal University, Russian Federation",
                   "normalized_name":"Siberian Federal University",
                   "country":"Russia",
                   "identifiers":{
                      "ror":"https://ror.org/05fw97k56",
                      "GRID":"grid.412592.9"
                   }
                }
             ]
          },
          {
             "given":"Melissa",
             "family":"Terras",
             "affiliation":[
                {
                   "original_name":"University College London, UK",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"Maxim",
             "family":"Rumyantsev",
             "affiliation":[
                {
                   "original_name":"Siberian Federal University, Russian Federation",
                   "normalized_name":"Siberian Federal University",
                   "country":"Russia",
                   "identifiers":{
                      "ror":"https://ror.org/05fw97k56",
                      "GRID":"grid.412592.9"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-05",
       "keywords":[
          "English",
          "digitisation - theory and practice"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Poetry in the Middle Ages changed as it was recopied, recited and passed along.  Mouvance is a term used by late Swiss medievalist Paul Zumthor to designate the high degree of instability in medieval text traditions. Zumthor qualifies this instability as an “interplay between variant readings and reworkings,” balancing both the textual, literary elements of written works with oral, performative ones (Zumthor, 1992; p.44). Twentieth-century text editing came to grips with this instability in large text traditions in different ways. For complex text traditions with a high degree of variance in medieval French, editors sometimes compiled one edition containing all the witnesses in successive chapters, as in the cases of  La vie de sainte Marie l’Egyptienne (Dembowski, 1977) and  L’Evangile de Nicodème (Ford, 1973) or they arranged them in a synoptic-style parallel edition as in the case of the fabliaux (Rychner, 1960). The alignment of such synoptic editions, although not discussed explicitly by the editors, was no doubt hand-ordered and its page layout was based on rough narrative equivalence of passages.  Cerquiglini has argued that, faced with such variance, the medievalist’s “analysis must be comparative, not archeological” (Cerquiglini, 1999; p.44). Such parallel print editions provide readers with a visual frame for comparative reading of variant texts, inviting exploration and giving insight into processes of textual change. There are a number of problems in text traditions with oral influence. There are not only different kinds of variance (single word/string variance, half-line or hemistich variance, transposition and reorganization of rhyming verse lines or interpolation of entirely new lines), but patterns of variance are also not uniform across a text, making the desired comparative visualization of texts difficult. Alignments can also mix and confuse kinds of variance. What are ways of visualizing textual variation in a digital environment? This is the question our paper intends to explore. Visualization strategies for historical text reuse vary according to the scale of the phenomenon and the nature of the texts involved (Franzini et al., 2015). Sophisticated visualizations for alignment exist at the micro-level, that is, at the level of the word such as the graph visualizations of TRAViz (Jänicke et al., 2015). They facilitate comparative readings of word-level variance in manuscript witnesses or translations. The text in such alignments is fully legible. A clean example of this can be found in the TRAViz alignments of English translations of the Bible (Figure 1), a textual use case in which units and sub-units of text are already commonly agreed upon by tradition (e.g. book, chapter, verse). On the other hand, solutions exist for macro-level text reuse, such as fingerprinting techniques (Jänicke and Geßner, 2015), creating distant patterns of textual similarity without showing the text (Figure 2).    Figure 1: Micro-level alignment of 24 English translations of Genesis 1:1 (Jänicke et al., 2015)      Figure 2: Macro-level fingerprint illustrates similarity between 24 English Bible translations (Jänicke and Geßner, 2015)    A critical discussion of the description and design of meso-level visualization of complex text traditions is missing. Not only are these text traditions large, but they are highly influenced by orality, that is, in Zumthor's words, they present a combination of textual and performative variance. Textual reworking at multiple scales (whole chunks of text, groups of lines, individual lines, sub-line strings) are challenges for both alignment and visualization. Our design attempts to translate the insights of Zumthor and Cerquiglini into an environment for visual exploration using two medieval text traditions: a tradition of short baudy tales known as the  fabliaux ,  and in versions of the well-known epic the  Chanson de Roland .   These two traditions have been chosen for the kinds of complex variance they exhibit. Our paper focuses on so-called meso-level alignments that visualize patterns of textual variance higher than the word and verse line level, and stress both legibility and human interaction in visualizing patterns.  Existing methods for text alignment in digital environments, generally speaking, favor relatively stable texts with small variance. The Versioning Machine accepts texts encoded “according to TEI's Parallel Segmentation method” and “interprets the encoding, parsing out the text into its constituent parts” (Versioning Machine, 2015). The authors of the Versioning Machine offer a sample alignment of a medieval “Prophecy of Merlin” (Figure 3). The line-to-line alignment has been encoded by the textual scholar. Similar lines are visually connected using customary mouse behavior, however, variance within the line or across lines is not visualized.     Figure 3: Two versions of the Prophecy of Merlin visualized in Versioning Machine     Another environment for the collation of raw text and visualization of textual differences is JuxtaCommons (Wheeles and Jensen, 2013). The example in Figure 4 uses two editions of Chretien de Troyes’ medieval romance  Perceval ,  one based on BnF, ms. français 12576 (Roach, 1959) at left, and the other Bern, Burgerbibliothek ms. 354 (Méla, 1990) at right. There is basic alignment of the verse line and minor lexical or dialectal variance, and mouse over in JuxtaCommons allows basic comparative reading of word- and string-level variants. In their out-of-the-box implementation, both tools allow  for easy  comparison of two versions of the text, although the Versioning Machine has been implemented for larger comparison sets.     Figure 4: Two editions of Chretien de Troyes’ Perceval visualized in JuxtaCommons    When complex text traditions containing more than just variant readings, but also interpolations, extra or missing lines or a significant amount of orthographic variance are collated automatically in JuxtaCommons, their results, however, are nearly illegible. Figure 5 shows two  witnesses  of  the same  old French  fabliau ;  the visual alignment, however, does an insufficient job at expressing the performative element of their  mouvance .     Figure 5: Two versions of a fabliau visualized in JuxtaCommons    Our design for a sufficient representation of  variance in  the   fabliaux  is shown in Figure 6. We use the intuitive quality of stream graphs (Byron et al., 2008) in order to support the analysis of aligned verses and to illustrate the transmission of one  fabliau   in four versions. The text editions are juxtaposed in columns in order to minimize edge crossings, in other words, we order the editions according to their similarity.     Figure 6: Four versions of a fabliau in our design     The visualization is available at      .  Clicking on a specific verse line produces a TRAViz micro-view of the line-level variance (Jänicke et al., 2015), whereas the larger meso-view of this portion of the  fabliau   allows patterns between and across verse lines to be clearly ascertained.  V ariance  in this genre  maintains prosody and avoids hypermeter;  mouvance   is characterized  here  by the interpolation of larger narrative multi-line fragments of text. The exception to this general rule is manuscript Harley 2253 in the column at far right where the narrative is reconstituted almost completely around sparse line re-use, illustrating what Rychner calls in the subtitle of his book “deteriorization”  [dégradation]  (Rychner, 1960).  Mouvance   occurs in multi-line “chunks,” visible in  highly legible streams of text re-use. This provides much more insight into textual transformation than reading Rychner’s hand-aligned synoptic edition, with the streams  here  drawing our attention to patterns of re-used text. In his edition, blank spaces and horizontal lines in the page layout effect a more uncertain alignment.   Another example of what we are calling meso-level alignment is the visualization of one stanza from the  Chanson de Roland   tradition contained in six manuscripts.  Figure 7 illustrates   laisse   1 of the Lavergne fragment, absent from Oxford, Bodleian Library manuscript Digby, 23. An interactive version of the alignment is available at      .       Figure 7: Six editions and one fragment of the Chanson de Roland in our design      To indicate how often lines recur across the whole manuscript tradition, we use streams colored  with  varying saturation. Highly saturated colors indicate frequently repeated passages, whereas less saturated colors indicate less repeated ones. Such a feature allows for a “consensus” visualization of the tradition at this meso scale. It is easy to see the more complex, transpositional variance of lines in the  Chanson de  Roland .  This compositional feature of French epic visualized here needs to be studied across the entire corpus of seven manuscripts and three fragments. It is perhaps on this point, however, that a principle of visualization clashes with the visual expectations of a medieval textual scholar. As in the above example, we ordered the  editions to reduce the number of crossing streams and to maximize legibility. This is potentially at odds with readers who expect to see temporality of manuscript dating represented in the visualization. More thinking needs to be done about the visual semantics of such a large tradition. In such a case, a perfect order that produces less clutter might be hard to determine, and we will be required to extend our proposed design.   We began with the alignment of the  fabliaux   and  Chanson de Roland   since they are traditions where editors have indexed alignment manually, either using page layout or by  numerical  cross referencing  of the stanzas .   As the example shown in Figure 8 illustrates,  lin es are not identical word for word, but rather our alignment has asserted basic diegetical equivalence.      Figure 8: Visualization of an aligned verse line in three versions of a fabliau        Figure 9: Alignment of an aligned verse among seven Chanson de Roland editions      In the example portrayed in Figure 9, performative  mouvance   is reflected in the recombination of words in the line.  In developing our research on this topic, we intend to pursue more granular alignment using computational means, down to the line and perhaps the hemistich. Based on the Relative Edit Distance of strings (Jänicke et al., 2015), we aim to determine spelling variants and rhyme sets automatically. Combined with an n-gram analysis, we will support the discovery and alignment of similar text passages. Due to the high degree of orthographic variance of the medieval French language, a purely computational approach might lead to a high number of false positive alignments. Therefore, we will design a visual analytics system that includes the human in the alignment process in order to configure appropriate settings that maximize the alignment of re-used text passages. This visual analytics process includes the supervised training of a classifier that supplies the parameters steering the alignment. Iteratively, computationally aligned text fragments are scored by the textual scholar according to their relevance. After each scoring session, the alignment will be recomputed taking the scholar's justifications into account.  In contrast to the manual alignment illustrated in the visualizations above (Figures 6-9), such a semi-automated process will potentially yield a very different picture in particular with respect to the degree of difference the human allows in string matching and the presence of shared n-grams required to align certain text passages. A further opportunity of this generic visual analytics approach is its straightforward adaptability to editions in other languages than medieval French. With  such a user-centered approach, alignment is not to be understood as a final product, but rather a process, for understanding variant text traditions, exploring their intricacies and supporting generation of hypotheses about textual behavior.  ",
       "article_title":"Visualizing Mouvance: Towards an Alignment of Medieval Vernacular Text Traditions",
       "authors":[
          {
             "given":"Stefan",
             "family":"Jänicke",
             "affiliation":[
                {
                   "original_name":"Institut für Informatik, Universität Leipzig, Leipzig, Germany",
                   "normalized_name":"Leipzig University",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/03s7gtk40",
                      "GRID":"grid.9647.c"
                   }
                }
             ]
          },
          {
             "given":"David Joseph",
             "family":"Wrisley",
             "affiliation":[
                {
                   "original_name":"American University of Beirut, Lebanon (Lebanese Republic)",
                   "normalized_name":"American University of Beirut",
                   "country":"Lebanon",
                   "identifiers":{
                      "ror":"https://ror.org/04pznsd21",
                      "GRID":"grid.22903.3a"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "medieval studies",
          "french studies",
          "visualisation",
          "folklore and oral history",
          "literary studies",
          "philology",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" In his insightful book  The Language of the Third Reich, Jewish linguist Victor Klemperer pointed out, “One tends to understand Schiller’s distich on a ‘cultivated language which writes and thinks for you’ in purely aesthetic and, as it were, harmless terms [...] But language does not simply write and think for me, it also increasingly dictates my feelings and governs my entire spiritual being the more unquestioningly and unconsciously I abandon myself to it.” Thus he observed and detailed the language change during and after the rise of Hitler and Nazism, kept an invaluable record for study of the impact of language on the mind.    Klemperer was by no means the only scholar who was alert to the influence of language. In Chinese context, when May Fourth intellectuals followed the flow of turning the classical Chinese into something more colloquial in late Qing period and advocated a new literature/cultural movement, which was at its heart a language revolution, the idea under the action was that one cannot separate the language habits from the operation of mind. They started the vernacular movement in magazines like  the New Youth, and hoped by modernizing language, they would empower the mind of the nation, modernize the culture, and eventually really modernize China, a task that 1911 revolution apparently did not fully succeed. The language revolution in early Republic was closely studied by many scholars. Our team also contributed by bringing in digital humanity methods, finding effective ways to tell the classical Chinese from modern Chinese by computer. It was when we analyzed the data of  The New Youth Magazine that we noticed something rarely discussed by modern scholars: the language changed significantly in Volume 8 of the magazine, published on the eve of the rise of Chinese Communist Party.    According to our study, by the end of Volume 7 (published in 1920), modern vernacular Chinese had almost completely replaced classical Chinese as the main written language. However, the language of Volume 8 seems to be a new breed, deeply influenced by the translation of Soviet language and full of political jargons. It seems that right after success of language revolution,  The New Youth Magazine immediately change the language again and promote a new kind of “revolutionary language” to advocate the Communism before the Chinese Communist Party was founded. Was there someone like Klemperer to observe the language change before the political turmoil? It called for further study. What we are doing here is to analyze the language turn in the late period of  The New Youth Magazine. For comparison, we draw in two other materials. First, the editorials of  United Daily News from 1951 to 1960, inherited the May Forth legacy and published in Taiwan before Taiwanese Modernist movement in 60s and Indigenized movement in 80s changing the written language violently. Second, the essays from the Chinese Communist Party’s  People’s Daily from 1971 to 1989, published in mainland China before the Chinese economic reform.   There are two types of data: one is structured data (with a high degree of organization) and the other is unstructured data. Most of the textual data are unstructured and quantifying them often requires certain knowledge about the application domain. We need to create a relational structure for the textual data before plugging classification methods. In particular, we use the notion of Exploratory Data Analysis (or EDA, proposed by famous statistician J.W. Tukey in 1977) to evaluate potential variables which can differentiate the language styles of Volume 7 and Volumes 8~11 in  New Youth Magazine. According to our previous study of writing style for Volumes 1~7, we found that the number of words, the number of different words (or vocabularies) and their distribution, in addition to function words in classical and modern Chinese, can be used to distinguish Chinese writing styles. We also include species diversity indices, such as Simpson index and entropy (or Shannon index).  Since there are more than 30 variables, we also consider data reduction methods, such as principle component analysis, to include fewer variables. Then, we apply classification methods (logistic regression) to judge the style of an article is close to Volume 7 or Volumes 8~11. Also, to avoid over parameterization (i.e., using too many unnecessary variables), we use cross validation to check whether the model is stable. The regression model is first built based on training data and then applied to the testing data. The fitting accuracies of training data and testing data are recorded separately, and these two numbers of accuracy should be close if the model is stable. For every simulation run, we randomly separate the training data and testing data into proportions of 90% and 10%, respectively. Based on 100 simulation runs, the regression model is fairly stable since it has very similar fitting accuracy (and small standard errors) for training and testing data. Next step, we apply the constructed regression model to the articles from two newspapers,  United Daily News and  People’s Daily. Both newspapers have about 500 articles and Table 1 shows their classification results. The  United Daily News is published in Taiwan and the Soviet Union should have little influence on this newspapers. As expected, only 10% articles are classified to the group of “Revolutionary Language.” On the other hand, more than 50% of articles from  People’s Daily are classified to “Revolutionary Language.” This matches to our anticipation, since the articles from  People’s Daily are published in 1971~1989, in which period China had close link with the Soviet Union.    　 No. of Article Classified to “Revolutionary Language”     No. of Article Proportion   United Daily News 550 57 10.36%   People’s Daily 534 308 58.68%   Table 1. Classifications of  United Daily News and  People’s Daily  Similar to the cross-validation for the articles from the  New Youth magazine, we can fit the regression model to the articles from two newspapers year by year. Figure 1 shows the yearly classification results. It seems that the results of  United Daily News are fairly stable and the yearly results lie around the average (red dotted line). The fitting results of  People’s Daily somehow show an inconsistence pattern. The average proportion of “Revolutionary Language” is 76.3% for the articles in 1971~1977 and is 39.2% in 1978~1989 (the overall average is the red dotted line).    Figure 1. Yearly Classifications of  United Daily News and  People’s Daily   The yearly classification results of  People’s Daily are interesting and very encouraging. The first stage of China’s reform and opening is between 1978~1989, under the formal leader Deng Xiaoping. China started to open to the outside world gradually since 1978, and it also triggers the economic blooming of China at the end of 20 th century. Figure 1 suggests a similar story. The proportion of “Revolutionary Language” (under the influence of Soviet Union) articles is larger before 1978 and has a radical change ever since 1978 until 1989, coincide with the years of reform and opening in modern China.  2015 marks the 100th anniversary of The  New Youth Magazine. By employing the digital humanity methods and focusing on the language change of the latest few volumes that so far rarely discussed by scholars, we hope to make some new contribution to the study of both The  New Youth Magazine and the writing style of modern Chinese language. Our study shows that the styles of articles from  United Daily News are close to that of Volume 7 in  New Youth Magazine, while those from  People’s Daily have about equal probability for being classified to Volume 7 or to Volumes 8~11. This implies that the Chinese writing styles of Taiwan and mainland China are different and the writing style of mainland China is likely to be influenced by the Soviet language.     Figure 2. Predicted Results of  United Daily News and  People’s Daily   Also, the change of “Revolutionary Language” in 1978 seems to indicate a new possible writing style. Judging from the proportion of “Revolutionary Language,” those of 1978~1989 lie between “Revolutionary Language” and  United Daily News, and it seems that the writing style of China turned into a new direction in 1978~1989 (namely, Reform China). Figure 2 shows the predicted results (after kernel smoothing) of all articles, with a value near 1 indicating style closer to “Revolutionary Language” and vice versa. If we treat  United Daily News and 1971~1977  People’s Daily as two extremes, then 1978~1989  People’s Daily is somewhat in between (or a mixture of) these two extremes. It would be interesting to explore the writing style in China since 1978. For example, we can further compare the writing styles of China according to the following three periods, 1978~1989 (first stage), 1989~2002 (economic blooming), and 2002~today (modern China).  ",
       "article_title":"From Language Revolution to Revolutionary Language",
       "authors":[
          {
             "given":"Ching-Syang Jack",
             "family":"Yue",
             "affiliation":[
                {
                   "original_name":"National Chengchi University, Taiwan, Republic of China",
                   "normalized_name":"National Chengchi University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/03rqk8h36",
                      "GRID":"grid.412042.1"
                   }
                }
             ]
          },
          {
             "given":"Li-Hsing",
             "family":"Ho",
             "affiliation":[
                {
                   "original_name":"National Tsing Hua University, Taiwan, Republic of China",
                   "normalized_name":"National Tsing Hua University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/00zdnkx70",
                      "GRID":"grid.38348.34"
                   }
                }
             ]
          },
          {
             "given":"Wen-Huei",
             "family":"Cheng",
             "affiliation":[
                {
                   "original_name":"National Chengchi University, Taiwan, Republic of China",
                   "normalized_name":"National Chengchi University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/03rqk8h36",
                      "GRID":"grid.412042.1"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "data mining / text mining",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Computational stylistics often analyzes style variation, including chronological change and the dialogue or narration of multiple characters or narrators (Craig 1999; Stamou 2008; McKenna and Antonia 1996; Stewart 2003; Burrows 1987; Hoover 2003, 2007). Here I suggest it is sometimes desirable to omit parts of texts, or to randomize a text to mask some variation, either analyzing parts of texts in random order or truncating such parts to create a word list based on equal amounts of text (see also Burrows, 1992).  Analyzing Doyle’s  The Hound of the Baskervilles, shows that two sections of Holmes’s dialogue and one of Baskerville’s fail to cluster correctly, but both failures likely result from intra-textual variation that should  not influence the analysis. Holmes’s outlier sections are a retrospective explanation more like narration than dialogue. This genre difference disrupts Holmes’s otherwise strongly consistent voice. I have similarly suggested ignoring the final “summing up” chapter of  The Waves when analyzing its six narrative voices (Hoover forthcoming). Baskerville’s problematic section is largely a conversation with Barrymore, which may cause their voices to merge and their sections to cluster.  Special pleading in the face of apparent failure seems potentially illegitimate, but removing Holmes’s retrospective “dialogue” leaves the rest consistently clustered. Furthermore, Holmes’s dialogue from other stories clusters perfectly with his normal dialogue from  Hound, so that inter-textual consistency supports an intra-textual argument. Retesting  Hound with Baskerville’s dialogue randomly sorted clusters his three sections separately from Barrymore’s section, which forms its own cluster. But such randomizing to dampen intra-textual variation seems questionable. Will less distinctive voices cluster with these homogenized parts? Will randomization produce specious clustering of sections that are not “really” consistent in style?  Consider the nine fairly distinct main narrators in Collins’s  The Moonstone (Hoover, Culpeper, and O’Halloran, 2014). Here, randomizing the parts greatly improves accuracy, correctly clustering all sections by all narrators over a wide range of analyses. Normal analysis of Faulkner’s  As I Lay Dying clusters Darl’s and Vardaman’s sections correctly and clusters all of Tull’s sections with Cash’s section over a wide range of analyses (Hoover, 2010), but randomizing each character’s narrative correctly clusters all the characters, even when divided into much shorter sections.  What about less distinct character parts? Randomization only slightly improves results for the letter writers in  The Coquette (an epistolary novel), showing that randomization does not always produce artificial consistency. Jake and Brett from Hemingway’s  The Sun Also Rises are memorable characters, yet their dialogue does not cluster very consistently in standard analyses, never approaching the success with Collins’s or Faulkner’s narrators . Figure 1 shows the most accurate clustering, based on the 700 MFW (most frequent words); all other analyses are weaker. Randomizing the character parts produces the completely correct results shown in Fig. 2 for analyses based on the 400-800MFW. (Cluster analysis is an exploratory statistical method that compares the frequencies of a set of words across a set of texts to determine which texts use those words at the most similar frequencies. The further to the left that two or more texts form a cluster, the more similar they are.) Randomization transforms the poor character separation in a standard analysis of James’s  The Ambassadors into a clear separation of all except Bilham, in analyses based on the 500-800MFW. The same is true for  Jane Eyre, though Jane’s dialogue seems problematic, as her story ranges from childhood to adulthood in five different settings. Many more texts will have to be analyzed to determine why randomization produces varying results and where it is appropriate.    Fig. 1. Character Dialogue in The Sun Also Rises–Standard Analysis, 700MFW    Fig. 2. Character Dialogue in The Sun Also Rises–Randomized Parts, 700MFW   Dracula reacts similarly. A standard analysis, shown in Fig. 3, fails to cluster all the sections correctly. In the middle of the large lower cluster is a problematic mixed cluster containing Van Helsing’s single section and the final sections from Mina, Harker, and Seward. These sections narrate the race to capture Dracula at the end of the novel, and this provides an opportunity to push the question of randomization further. Because the  Dracula narratives range widely in length, the full word list is skewed away from Lucy (4,400) and Van Helsing (5,200) and toward Mina (22,000), Harker 1 (19,000), Harker 2 (14,000) and Seward (35,000). A word list based on the first 6,000 words from the randomized narratives of these five characters (half from each of Harker’s) clusters the narrative much more accurately and clearly distinguishes the narratives of Lucy and Van Helsing (see Fig. 4).    Fig. 3. Six Dracula Narrators–Standard Analysis, 900MFW    Fig. 4. Six Dracula Narrators–Word List Based on 6,000 Randomized Words, 900MFW  Harker’s, Mina’s, and Seward’s final sections remain clustered in Fig. 4, however. Analyzing these and Van Helsing’s in shorter sections, using a word list based on the equalized randomized parts, again improves results. Seward’s and Van Helsing’s sections cluster separately, as do Mina’s second, third, and fourth; only her first section clusters with Jonathan’s. Thus the voices remain relatively distinct, though inflected by the effects of the narrative structure. Because Mina’s wayward section is mainly a memo, its failure to cluster with the sections from her journal may also be explicable. Further analysis of the sub-genres of  Dracula should provide additional insights into intra-textual variation.  Consider now a very different kind of text posing different problems–a collection of nearly 400 high-stakes writing exams administered in U.S. high schools. These very short texts (128-1307 words) in multiple genres were written in response to a wide variety of prompts from multiple states (Jeffrey 2010; Jeffrey, Hoover, and Han 2013). This makes testing for the characteristic vocabulary of high- and low-scoring texts very challenging. Here I use a variant of Zeta, which identifies words used consistently by one group and avoided consistently by another (Burrows, 2006; Craig and Kinney, 2009; Hoover, 2013). Because the entire vocabulary is being tested, a normal word list seems appropriate. I also combine the low-scoring texts into one large text and the high-scoring texts into another, randomize the lines of each, and then compare analyses based on sections of the combined texts with analyses based on the combined and randomized texts. Holding out twenty high- and twenty low-scoring sections for testing, I use the eighty-six remaining sections for training. The combined texts distinguish high and low test texts fairly well (Fig. 5), but the combined-randomized texts greatly improve the results (Fig. 6). (The vertical and horizontal axes record the percentage of word types in each section that are characteristic of low-scoring and high-scoring texts, respectively.)    Fig. 5. Combined High- and Low-Scoring Exit Texts    Fig. 6. Combined-Randomized High- and Low-Scoring Exit Texts  Certainly (intra-)textual variation helps to create memorable characters and narrators, and it is often crucially linked to description or narrative action. Some kinds of variation, however, can mask important kinds of consistency and unity, and a reasoned argument can be made for ignoring some sections of texts, or for analyzing them in randomized form. Alternatively, or in addition, word lists based on truncated and randomized sections also often improve results. Though the value and legitimacy of such transformations and truncations will need further study, they seem a promising line of research. ",
       "article_title":"Textual Variation, Text-Randomization, and Microanalysis",
       "authors":[
          {
             "given":"David L.",
             "family":"Hoover",
             "affiliation":[
                {
                   "original_name":"New York University, United States of America",
                   "normalized_name":"New York University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0190ak572",
                      "GRID":"grid.137628.9"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-15",
       "keywords":[
          "literary studies",
          "english studies",
          "stylistics and stylometry",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The study of an individuals’ personality traits is a new line of research that emerged only recently, primarily through the investigation of dialogue systems and weblogs (Gill et al., 2012; Konstantopoulos, 2010; Mairesse and Walker, 2006; Mairesse and Walker, 2007). This paper proposes a novel application for personality classification by leveraging on cognitive computing research and by exploiting the poetic production of theatrical plays. More specifically, this research is circumscribed to the analysis of Shakespeare’s tragedies, which offer a rich spectrum of characters for a detailed and in-depth study. This research does not aim at introducing the innovative technological aspects of personality classification in cognitive computing but rather at employing such technology in the study of English literature, and analyse some implications that arise from the results. Before presenting the core of this research, we outline the psychological theory of the “Big Five,” its implementation as well as its extension in IBM Watson. Successively, in Section 2, we introduce the data used, the method applied and the results obtained in our analysis. In Section 3, we discuss some useful applications and extensions for literature scholars. Finally, we conclude by presenting some considerations for future research.   IBM Watson and the Big Five personality insight Cognitive computing originated in the early 60s, however, it has been improved dramatically in recent years, achieving significant success through the launch of IBM Watson in 2010. IBM Watson simulates human cognitive systems by implementing advanced natural language processing, information retrieval, knowledge representation, automated reasoning, and machine learning technologies to the field of open domain question answering (Ferrucci et al., 2010; Ferrucci, 2012). Among human cognitive activities, one of the most employed is the capability to understand and forecast other people’s personalities. As described in the Big Five theory (Norman, 1963) formulated by scholars in psychology, each individual presents a different aptitude in identifying characteristic patterns of thinking, feeling and behaving in others. The Big Five theory is the main theory on which IBM Watson has been built; it distinguishes between five broad dimensions underlying an individual’s personality, namely openness, conscientiousness, extroversion, agreeableness, and neuroticism.  Openness mirrors the level of scholarly interest, imagination and an inclination for oddities.  Conscientiousness is the propensity to be reliable, to show self-control and act obediently.  Extroversion comprises vitality, positive feelings, confidence, amiability and the propensity to look for incitement in the organization of others.  Agreeableness is the inclination to be merciful and agreeable as opposed to suspicious and adversarial towards others.  Neuroticism is the propensity to encounter obnoxious feelings, for example, outrage, uneasiness, wretchedness, and powerlessness.  Next to the Big Five personalities, IBM Watson takes into account the concept of “Needs,” which are described by the literature as universal needs shared by all human beings (Ford, 2005; Kotler and Armstrong, 2010). Along with Big Five and Needs, IBM Watson takes into account the psychological concepts of Values which are defined as “desirable, trans-situational goals, varying in importance, that serve as guiding principles in people’s lives” (Schwartz, 2006). As mentioned on the IBM Watson website . accessed on October 25 2015  “Schwartz summarizes five features that are common to all values: (1) values are beliefs; (2) values are a motivational construct; (3) values transcend specific actions and situations; (4) values guide the selection or evaluation of actions, policies, people, and events; and (5) values vary by relative importance and can be ranked accordingly.”    Method and Results Our objective consists in comparing the personality of the main characters of three Shakespeare’s tragedies in a positive versus negative sentimental context. To establish these contexts we use the work of Nalisnik et al. (Nalisnick and Baird, 2013) on sentiment analysis to divide the characters of the play into two groups. The first group is composed of those characters towards which the main character expresses mainly positive sentiments; the second group comprises those characters towards which the main character expresses mainly negative sentiments. In practice, we extracted each instance of continuous speech from the plays . XML versions provided by Jon Bosan:  accessed on October 15 2015 . The groups divisions take in account the sentiment valence and the minima of sings that the IBM Personality Insights needs to produce significant results and then assumed that each speech act by one speaker was directed towards the character that spoke immediately before him. We used this assumption to replicate Nalisnik’s data but, as he pointed out in his paper, “This assumption does not always hold; it is not uncommon to find a scene in which two characters are expressing feelings about someone off-stage.”. We retrieved the sentiment valences for each main character . The tables provided by Nalisnik in  accessed on October 15 2015 , reported in the following tables:       Hamlet’s Sentiment    Valence Sum   Guildenstern  31    Polonius  25    Gertrude  24    Horatio  12    Ghost  8    Marcellus  7    Osric  7    Bernardo  2    Laertes  -10    Phelia  -5    Rosencrantz  -12    Claudius  -27        Othello’s Sentiment   Valence Sum   Iago  71    Cassio  38    Brabantio  27    Duke of Venice  24    Montano  7    Desdemona  -1    Lodovico  -4    Emilia  -10      >Table 1: Tables representing the sentiment valence sum, and the used groups for each main character. Positive scores stand for a character’s positive attitude towards others and negative scores stand for a negative attitude  Macbeth’s Sentiment   Valence Sum   Murderer 1  22    Banquo  16    Duncan  8    Angus:  7    Macduff  5    Which 3  5    Which 1  1    Young Siward  -4    Lennox  -11    Seyton  -20    Lady Macbeth  -39       By assembling all continuous speech sequences directed to the characters in the different groups, we created two text groups: the first group contains all lines of the main character towards the others when expressing positive sentiments, the second group includes those speech sequences characterized by a negative sentimental connotation. We performed this task for three main characters, more specifically Hamlet, Othello and Macbeth. In Table , the positive and negative word counts for these text assemblies are plotted.  Table 2: Word counts per characters of positive and negative sentimental context   Positive  Negative   Hamlet  6655  3455    Othello  3424  2407    Macbeth  1293  1852    Finally, we used IBM’s personality insight service to create personality profiles for each of the main characters based on the positive and negative texts. Figures 1, 2 and 3 represent the distribution of needs and values with respect to the positive or negative sentimental valence. Points on line represent independence with respect to the latter. For needs and values falling below this line, the distribution is characteristic for the expression of negative sentiments. We also created personality graphs with respect to the groups. Although they are calculated as percent of a given facet (as indicated by the x axis), the facets vary between low and high values. Thus, the bars in the graph begin at 50% and can either be low (to the left) or high. A score of 50% signifies that the facet is balanced.     Figure 1: Hamlet’s scores for Needs, Values and the Big Five categories      Figure 2: Othello's scores for Needs, Values and the Big Five categories      Figure 3: MacBeth's scores for Needs, Values and the Big Five categories     Some Insights and Discussion A quantitative approach to character clearly generates a comparatively large amount of linguistic data. The question is, how are we to use this data? What can such an approach offer Shakespeare studies and the humanities more broadly? How does the data we have generated contribute to the critical conversation in a sub-field like character criticism (Hazlitt, 1845; Bradley, 1992; Desmet, 1992; Yachnin and Slights, 2009) which has such a long and distinguished history. There are two answers to this question. First, our analysis shows in a more concrete and detailed way than ever before, the close relationship between character and language, something easy to forget in the context of a representational practice like theatre which is so dependent on non-linguistic features, such as gesture, costume, and stage properties. Playgoers, however, do not just see character; they also hear it. And in the modern humanities classroom, they read it. Accordingly, words play a significant role is crafting what we would now call the “personalities” of Shakespeare’s stage. Our approach offers a new means of isolating and analyzing these verbal features of character. The second way in which our work contributes to Shakespeare studies has to do with something our data does not tell us. To understand what we mean by this, consider for a moment why it is that the words associated with some characters generate a personality profile that anyone familiar with the plays knows does not quite fit? Why, for example, does Hamlet have a verbal data set that makes him seem much more serious and honor-driven than he actually is It is because the technology we are using is not capable of accounting for context and therefore cannot detect things like irony and wordplay, two things that are extremely important components of the way language articulates character and personality. While this may be viewed as a methodological weakness from the perspective of information technology, it is a great strength from the perspective of the humanities. For it is precisely at those moments that the data fails to deliver coherent results that we are forced to ask compelling questions about language and art: why do words that seem to mean one thing have the opposite effect on stage? What is the relationship between the literal and implied meanings of words in the constitution of dramatic character? In the end, this facet of our contribution to Shakespeare studies illustrates something important not just about our project specifically, but also about the digital humanities generally: the value of applying computational technology to literary texts lies not in the promise of “better data” or irrefutable “facts,” but rather in the way such technology returns us again and again to the fundamental humanist questions that help us understand how literature and art work . This idea has been advanced influentially in the work of Jonathan Hope and Michael Witmore. See their blog, Wine Dark Sea  http://winedarksea.org. .    Future Research In this paper we introduced novel techniques based on cognitive computing to get an understanding of characters in Shakespeare’s plays. This research explores newway of computer-assisted methods for the investigation of literature. Nonetheless, some technical issues need to be overcome in order to improve the quality of this new methodology. First, an improved methodology to outline the sentiment polarity needs to be developed. Second, the low representativity of IBM’s personality model needs to be enhanced in order to catch literary phenomena in sources such as Twitter, Wikipedia, and other such corpora. Thus, recreating a personality insight service based on a literary work corpus would surly enhance the results of our method.  ",
       "article_title":" How IBM Watson Can Help Us Understand Character in Shakespeare: A Cognitive Computing Approach to the Plays.  ",
       "authors":[
          {
             "given":"Mattia",
             "family":"Egloff",
             "affiliation":[
                {
                   "original_name":"University of Lausanne, Switzerland",
                   "normalized_name":"University of Lausanne",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/019whta54",
                      "GRID":"grid.9851.5"
                   }
                }
             ]
          },
          {
             "given":"Davide",
             "family":"Picca",
             "affiliation":[
                {
                   "original_name":"University of Lausanne, Switzerland",
                   "normalized_name":"University of Lausanne",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/019whta54",
                      "GRID":"grid.9851.5"
                   }
                }
             ]
          },
          {
             "given":"Kevin",
             "family":"Curran",
             "affiliation":[
                {
                   "original_name":"University of Lausanne, Switzerland",
                   "normalized_name":"University of Lausanne",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/019whta54",
                      "GRID":"grid.9851.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-01",
       "keywords":[
          "creative and performing arts, including writing",
          "literary studies",
          "english studies",
          "digital humanities - nature and significance",
          "content analysis",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The Wired! Lab for digital art history and visual culture at Duke University comprises a group of faculty, staff, and students engaged in applications of visualization methods to studies of material culture and art, architectural, and urban histories. Members of the lab collaborate to develop critical digital research employing 3D modeling, mapping, and database tools. Art historians and digital humanists work together to integrate both digital and art historical methodologies in lab courses and projects. In the Wired! classroom’s collaborative teaching model, a digital humanist takes on a significant role in both course planning and implementation. She works with instructors, graduate assistants, and librarians to redesign syllabi and assignments for preexisting departmental courses that incorporate not only digital tools but also critical methods. She then attends class meetings to familiarize herself with courses’ art historical content; she delivers workshops on digital concepts and tools; and she works with instructors and students to establish project workflows, to troubleshoot technical issues, and to critique student work. For students, this kind of collaboration can provide opportunities to make intellectual connections across two modes of inquiry as they apply digital methodologies to art historical topics. For instructors, this collaboration can enrich pedagogical practice as digital methods present different possibilities for student engagement. While some educators have focused on pedagogical challenges such as, “How does one teach students the digital tools to address a wide variety of projects without neglecting traditional discipline-specific issues of research formulation and data collection?”, (Johanson and Sullivan et al., 2012) the Wired! Lab’s digital pedagogy focuses on only the digital knowledge required for a specific topic. This approach ensures that students intentionally engage art historical content via digital methods, prioritizing quality of digital interventions over quantity while also addressing very practical issues of scalability within a disciplinary context. In this presentation, I will examine two cases in which Wired! Lab instructors and a digital humanist collaborated to design and implement project-based undergraduate courses. These examples will demonstrate how the different teaching teams worked in tandem to create these learning experiences and will discuss benefits and challenges of these pedagogical collaborations. I will also situate the Wired! Lab’s pedagogical work within the larger digital humanities and digital art history ecosystem.  Introduction to Art History  In Spring 2015, Professor Caroline Bruzelius implemented a team-based teaching approach for her Introduction to Art History course. Together, we worked with a graduate assistant and librarian to redesign the survey course and student projects. We all attended class meetings, we each taught aspects of the course, and we assessed student projects as a group. Combining our variant expertise, we created a course in which students employed a digital humanities approach to performing art historical critical analyses of spatial, temporal, and cultural relationships that influenced the movement of raw materials and cultural objects across ancient and medieval Western and Mediterranean societies. The digital tools we chose to use in the course were Neatline, a spatiotemporal exhibit builder, and Omeka, a collection management system in which Neatline operates. We implemented Neatline first for visualizing the syllabus and second for developing students’ visual narratives concerning specific pre-modern art historical objects and materials.   Fig. 1. Neatline syllabus for Introduction to Art History  The interactive visual syllabus (Fig. 1) introduces students to the course narrative: its units and lectures are shown in time and space accompanied by contextual maps, specific geospatial points of reference, and other relevant multimedia including hyperlinks to important objects’ museum pages, lecture slides, and supplementary videos. The interactive visual syllabus makes explicit temporal, spatial, and cultural relationships that effected the development of art practices across pre-modern societies. Presenting the syllabus in Neatline also familiarized students with Neatline’s affordances and interface in preparation for creating their own Neatline projects, in which they used critical understandings of spatiotemporal narrative to develop cohesive art historical arguments concerning particular pre-modern objects, their making processes, political influences, and economic and environmental impacts.   Visualizing Venetian Art Dr. Kristin Lanzoni’s upper level course on early modern Venetian art also employed a collaborative teaching model. She and I worked together to develop a syllabus and project in which students studied course material through processes of visualization. Students spent the semester not only learning about Venetian art, history, and culture but also working together to model a Venetian palace no longer extant and to design an immersive visual narrative about the palace’s political and cultural significance (Fig. 2). The students worked with tools ranging from Adobe Photoshop to SketchUp to Unity3D to visualize the palace.   Fig. 2. Model of a Venetian palazzo created by students in Visualizing Venetian Art  As the semester progressed, Dr. Lanzoni and I worked closely with the students to troubleshoot a number of problems that arose as students strove to translate historical evidence into an historically informed 3D model. These issues stemmed from both primary sources, which give conflicting visual evidence for the palace’s scale and appearance, and digital tools, which present challenges for modeling non-rectilinear structures and force compromises with regard to levels of detail. Students had to make joint decisions regarding model and narrative designs based on both historical research and the digital tools’ affordances and limitations. In both courses, students gained understandings of art historical topics through digital visualization processes. Wired! Lab teaching teams facilitate these types of learning experiences by combining their expertise in course design and implementation. While in the survey course, students were asked to create individual visual narrative projects, guided by an art historian, a graduate student, a librarian, and a digital humanist, the students in Visualizing Venetian Art worked together on a single topic from which their learning about early modern Venice radiated outward. While some digitally-inflected Wired! courses ask students to work individually, other courses model collaboration not only in the teaching but also in the learning. Overall, the majority of Wired! courses are not “Introduction to Digital Humanities” but rather art history courses redeveloped to integrate specific digital approaches that directly support course-specific content and disciplinary methods. The integration of a digital humanist in Wired! art history courses ensures that students’ digital projects are informed not only by disciplinary knowledge but also by critical approaches to digital methodologies.  ",
       "article_title":"Wired!: Collaborative Teaching & Critical Digital Making In An Art History Classroom",
       "authors":[
          {
             "given":"Hannah L.",
             "family":"Jacobs",
             "affiliation":[
                {
                   "original_name":"Wired! Lab for digital art history & visual culture, Duke University, United States of America",
                   "normalized_name":"Duke University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00py81415",
                      "GRID":"grid.26009.3d"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-28",
       "keywords":[
          "visualisation",
          "digital humanities - pedagogy and curriculum",
          "teaching and pedagogy",
          "maps and mapping",
          "art history",
          "virtual and augmented reality",
          "historical studies",
          "knowledge representation",
          "English",
          "interdisciplinary collaboration",
          "geospatial analysis, interfaces and technology"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Preparing sources for historical research usually requires making many heterogeneous collections digitally accessible and linking them to compose a multi-faceted and multi-layered resource that supports both distant reading and close reading forms of analysis. In the lifecycle of historical information – a model introduced in 2004 - the Dutch DH-experts Boonstra, Breure and Doorn emphasize three points that should be kept in mind by e-science experts and researchers alike to keep historical information systems alive and useful: durability, usability and modeling (Boonstra et al., 2006: 22).  Timbuctoo, developed at the Huygens ING, offers a system that makes it possible to model and store heterogeneous data but also incrementally enrich and link the data. Furthermore, it also offers facilities to document the provenance of all data as well as all steps in data editing, extraction and linking data. These features are vital for historical research in which researchers need to be able to exert 'source criticism' and go back to the original source or data at all times (Ockeloen et al., 2013). We will demonstrate the solution Timbuctoo offers with the Migrant, Mobilities and Connection project as a use case, because of its complex and multiple links to datasets from a myriad of cultural heritage institutions (archives, libraries and museums) on several levels. The main focus of the Migrant project is on the life courses of the migrants. Starting from the limited core data from a connection between (digitized) Dutch emigrant cards and Australian immigration files, the life courses will be elaborated using in depth-analysis of these and other collections. It is important to note that for the purpose of the Migrant project that life courses not only comprise dates and birth, marriage, migration, education and employment but also extend to the interactions of migrants with all sorts of institutions in the Netherlands and Australia and their representatives. The database therefore enables us to analyse and compare the evolution of a multitude of social networks (Arthur et al. (submitted); Van Faassen, 2014a, 2014b).   Migration files as a multi-layer resource The Migrant, Mobilities and Connection project focuses on the Dutch-Australian post World-War II migration from the Netherlands to Australia. Like all migrants these 180,000 people have left many traces in different cultural heritage collections ranging from (supra) government archives to the photo and memorabilia collections of the migrant families themselves and anything in between. These collections are dispersed over different countries. A lot of the collections are available in a digital form or will be digitized in the future, but like all historical collections they contain partially structured information and partially unstructured information that needs to be made accessible for further analysis. In elaborating the data we will use a variety of methods ranging from computer assisted data extraction and linking of a large collection of life events to hand editing of handwritten registration cards and personal migrant files. From an analytic perspective all these collections and edited data can be seen as different layers that need to be accommodated by the data store in which they are kept (Hoekstra and Nijenhuis, 2012).   Timbuctoo Timbuctoo is a data repository system aimed at humanities research with the aim of linking together datasets containing structured information concerning people, places and organisations without actually merging them to facilitate scientific analysis and discussion. To accomplish this it defines a number of primitive types that describe entities that all researchers agree on, such as the afore mentioned persons, places, organisations as well as works, languages, concepts and events. Each research project that makes use of the repository can extend the primitive types with extra fields. On top of that the repository has the ability to store multiple viewpoints on the same entity. In this way, researchers become aware of the different or sometimes even conflicting assumptions about entities, fueling scholarly debates in a conceptual way. Timbuctoo also support versioning and provenance. To make it clear on which information the results are based every change made to the data should have information who made the change, when the change was made and for what reason the change was made. The user interface, analysis and visualization are completely separated from the storage of the data. All services are coupled using REST (Fielding, 2000) APIs. The software is freely available under an open source license (GPL 3) and is published on Github.   Information extraction During the project data will be added, edited and analysed continuously. As indicated above, at the beginning of the project the data consists of migration information contained in cards, files and information of governance agencies. Apart from the core information already available in a simple database, the cards and files contain much more information that must be digitized to be able to use it for analysis. In the course of the project, a lot of other materials from archives, libraries and other collections from different cultural heritage institutions will be added to the Timbuctoo database. Some of the information will be structured, but most is contained in typed or handwritten files and in images. The aim of information extraction is to extract structured information from unstructured information. For the elaboration of this wealth of materials we will use an eclectic mix of editing and information extracting methods. Previously, hand editing was the only option for these types of materials, but in light of the amount of material that will be collected, all computer assisted information extraction that is possible will contribute to the database and help analysis.  To automate this process the data needs to be stored in such a way that a context can be build. The computer can search for patterns and suggest links to data already present in the network or calculate statistics to point out interesting or unusual things in the dataset. To begin with, an algorithm needs to be made to link the Dutch and Australian records together. Note that the system should not actually merge the records. Data about persons can be linked together based on (for example) familyname, year and place of birth and indeed all other types of structured information available such as migration date, migration scheme, ship with which they travelled or still very different data depending on the source and the context. Other examples of computer assisted data extraction include the recognition of certain keywords in the facsimile or transcriptions. Researchers can add or edit information manually through the user interface, either manually or using algorithms. Automatic information extraction should suggest relations between different records or entities, but never actually enforce those changes on the researcher. We started out to build Timbuctoo with the problem of the large variety of heterogeneous data sources that our institute produced in the course of a hundred years of classical and some twenty-five years of digital source editing and publishing. The use case of the Migrant, Mobilities and Connection project, with data about thousands of personal migrant stories scattered all over the world and its myriad of policy files on national and supranational levels recorded in different datasets, demonstrates the different features of Timbuctoo. First, Timbuctoo is used as a repository where researchers as well as automatic tools such as parsers store all the heterogeneous data and the relations between it. Since all the data is versioned and provenance information is recorded, it is always clear where the data originates from. Second, Timbuctoo serves as a data source for researchers and parsers. Named entity recognition tools can use all the available names of places, organisations and persons as training data. Researchers can do queries on certain properties and do statistical analysis on the results to either find outliers or confirm or refute a hypothesis on a larger scale in more varied ways than previously possible. Finally, being a graph database, Timbuctoo is a research tool that enables researchers to infer indirect relations from the numerous direct relations in the repository. This makes it possible to perform complex queries and conduct network analysis and visualization. These three features combined enable researchers to discover unexpected phenomena that can lead to new research and methodological questions.  ",
       "article_title":" Bringing Migration Data Into Context Using Digital Computational Methods  ",
       "authors":[
          {
             "given":"Ronald",
             "family":"Haentjens Dekker",
             "affiliation":[
                {
                   "original_name":"Huygens ING, Netherlands, The",
                   "normalized_name":"Huygens Institute for the History of the Netherlands",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04x6kq749",
                      "GRID":"grid.450092.a"
                   }
                }
             ]
          },
          {
             "given":"Rik",
             "family":"Hoekstra",
             "affiliation":[
                {
                   "original_name":"Huygens ING, Netherlands, The",
                   "normalized_name":"Huygens Institute for the History of the Netherlands",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04x6kq749",
                      "GRID":"grid.450092.a"
                   }
                }
             ]
          },
          {
             "given":"Marijke",
             "family":"van Faassen",
             "affiliation":[
                {
                   "original_name":"Huygens ING, Netherlands, The",
                   "normalized_name":"Huygens Institute for the History of the Netherlands",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04x6kq749",
                      "GRID":"grid.450092.a"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-02",
       "keywords":[
          "information architecture",
          "information retrieval",
          "cultural studies",
          "historical studies",
          "archives, repositories, sustainability and preservation",
          "English",
          "networks, relationships, graphs"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The state of the art  Burrows’ Delta is one of the most successful algorithms in computational stylistics (Burrows 2002). A series of studies have proven its usefulness (e.g. Hoover 2004, Rybicki & Eder 2011). There are two essential steps in Burrows’ Delta. The first is to standardize the relative frequencies of words in a document-term-matrix through a  z-score   transformation. In the second step, the distances between all texts are calculated. For each word, the difference between the  z-score   of the word in one and the other text are calculated. The absolute values of the differences are added for all words taken into account. The usual interpretation is that the smaller the sum, the more similar two texts are stylistically, and the more likely it is that they have been written by the same author.   Despite the fact that Burrows’ Delta is as simple as it is useful, there is still a lack of a good explanation why the algorithm works so well. Argamon (2002) has shown that the second step in Burrows’ Delta is equivalent to taking the Manhattan distance between two points in a multi-dimensional space. He suggests, among other things, using the Euclidean distance instead. An empirical test of his proposals has shown, however, that none of them lead to an improvement in performance (Jannidis et al. 2015).    Figure 1: Illustration of the distance between two texts made up of just two words   Smith and Aldrige (2011) have suggested to use the cosine of the angle between the document vectors for the second step, as is customary in Information Retrieval (Baeza-Yates & Ribeiro-Neto 1999:27). The Cosine variant of Delta (Delta Cos) outperforms Burrows’ Delta (Delta Bur) in many different settings and has the advantage of not showing the drop in performance typical ofother Delta variants when large numbers of MFW are used (Jannidis et al. 2015). The question now is why Delta Cos is so much better than Delta Bur and other variants, that is, in what way Delta Cos captures the authorship signal more clearly than other variants of Delta.   Of decisive importance for our further analyses was the insight that using the Cosine Distance is equivalent to a vector normalization in the sense that (in contrast to Manhattan and Euclidean Distance) the length of the vector does not play a role for the calculation of the distance (see figure 1). Previous experiments have shown that an explicit, additional vector normalization also substantially improves performance of the other Delta measures (Evert et al. 2015).   Hypotheses Having discovered that impact of the normalization effect, we have developed two empirically testable hypotheses:  (H1) Performance differences are caused by single extreme values, so-called outliers. These are particularly large positive or negative  z-scores specific to single texts rather than all texts of a single author. As the Euclidean distance should be more sensitive to single extreme values than the Manhattan distance, this hypothesis would explain the comparatively bad performance of Argamon’s “Quadratic Delta” Delta Q. The positive effect of vector normalization originates from the reduction of outlier amplitudes (“outlier hypothesis”).  (H2) The author specific “style profile” manifests itself more in the qualitative combination of word preferences, i.e. in the pattern of over- and under utilization of vocabulary, rather than in the actual amplitude of  z-scores. A text distance measure is particularly successful in authorship attribution if emphasizing structural differences of author style profiles without being too much influenced by actual amplitudes (“key-profile hypothesis”). This hypothesis explains directly why vector normalization results in such impressive improvements: it standardizes the amplitudes of author profiles in different texts.     New insights  Corpora For the experiments in this paper, we use three similarly composed corpora in German, English and French. Each corpus contains 25 different authors with 3 novels each, thus 75 texts in total. The corpora have been described in Jannidis et al. (2015). Due to space issues, the following section will only present our observations on the German corpus. The results for the corpora in both other languages show only small deviations and also support our findings.   Experiments To further investigate the role of outliers and thus the plausibility of H1, we complement Delta Bur and Delta Q with additional variants based on the general Minkowski distance (for  p ≥ 1):       We generally name these distance measures L p-Delta. The specific case  p = 1 equals the Manhattan distance (L 1-Delta = Delta Bur),  p = 2 the Euclidean distance (L 2-Delta = Delta Q). The higher the value for  p, the larger the influence of single outliers on L p-Delta.  Fig. 2 compares four different L p distance measures (for p=1, √2, 2, 4) with Delta Cos. The method of comparison is the same as in Evert et al. (2015): 75 text are automatically clustered in 25 groups according to Delta distances; clustering quality is estimated with the adjusted rand index (ARI). An ARI of 100% signifies perfect author recognition whereas a value of 0% shows that the clustering is entirely random. The performance of L p Delta obviously decreases with increasing  p. Additionally, the robustness of the measures also decreases with an increasing number of MWF used. As already reported in Jannidis et al. (2015) and Evert et al. (2015), Delta Bur (L 1) consistently outperforms Argamon’s Delta Q (L 2). Especially if many features, i.e. a large number of MFW is considered, high p values result in low performance. Delta Cos is more robust than other variants and achieves almost perfect attribution success (ARI > 90%) over a wide range of the MFW.     Figure 2: Clustering quality of different Delta measures as a function of the number of the MFW considered   Normalizing the feature vectors to length 1 improves the quality of all Delta measures significantly (fig. 3). In this case, Argamon’s Delta Q is identical to Delta Cos: the red line is completely covered by the green one. The other Delta measures (Delta Bur, L 1.4-Delta) now reach about the same quality as Delta Cos. Only L 4 Delta, which is especially prone to outliers, falls short considerably. These results seem to support H1.     Figure 3: Cluster quality of various Delta measures with length-normalized vectors   A different approach to limit the influence of outliers is to truncate extreme  z-scores. To do so, we set all | z| > 2 to +2 or –2, depending on the original  z-scores’s sign. Fig. 4 shows the effects of various normalizations on the distribution of the feature values. Vector length normalization (lower left) produces only slight changes and practically does not reduce the number of outliers at all. Pruning large  z-score values only affects words with above-average frequencies (upper right).      Figure 4: Distributions of feature vectors for all 75 texts, using vectors of 5000 most frequent words. The table shows the distribution of the original  z-scores (upper left), the distribution after length-normalizing the vectors (lower left), the distribution after clamping outliers with | z| > 2 (upper right) and a ternary quantization to the values –1, 0 and +1 (lower right). The red curve in the lower left graph shows the  z-scores before normalization; the direct comparison shows the normalization has only minimal effect and almost does not reduce outliers. The thresholds for the ternary quantization,  z < –0.43 (–1), –0.43 ≤  z ≤ 0.43 (0) and  z > 0.43 (+1), have been selected such that in an ideal normal distribution, a third of all feature values would fall into each of the classes –1, 0, and +1.     As Fig. 5 shows, this manipulation improves the performance of all L p Deltas considerably. However, its positive effect is noticeably smaller than that of vector normalization.     Figure 5: Cluster quality after clamping outliers, i.e. feature values with | z| > 2 have been replaced with the fixed values –2 or +2, depending on  z-score’s sign   With these differing effects of the normalizations on outlier distributions and Delta results, H1 cannot be upheld. H2 is supported by the good results of vector length normalization. However, on its own, it cannot explain why clamping outliers leads to a considerable improvement as well. To examine this hypothesis further, we created pure “key profile” vectors that only discriminate between word frequencies that are above average (+1), unremarkable (0), and below average (–1; cf. Fig. 4, lower right).    Figure 6: Cluster quality with ternary quantization of the vectors in frequencies that are above average (+1,  z > 0.43), unremarkable (0, –0.43 ≤  z ≤ 0.43), and below average ( z < –0.43)   Fig. 6 shows that these key profile vectors perform remarkably well, almost on par with vector normalization. Even the especially outlier-prone L 4 Delta reaches a quite robust clustering quality of more than 90%. We interpret this observation as giving considerable support to hypothesis H2.     Discussion and perspectives H1, the outlier hypothesis, has been disproven as the vector normalisation hardly reduces the number of extreme values and the quality of all L p measures is still considerably improved. On the other hand, H2, the key profile hypothesis, has been confirmed. The ternary quantification of the vectors shows clearly that it is not the extent of deviation resp. the size of the amplitude, but the profile of deviation across the MFW which is important. Remarkably, the measures behave differently if more than 2000 MFW are used. Almost all variant show a decline for a very large number of features, but they differ in when this decline starts. We suppose that the vocabulary in those parts is less specific for an author than for topics and content. Clarifying such questions will require further experiments.   ",
       "article_title":" Outliers or Key Profiles? Understanding Distance Measures for Authorship Attribution  ",
       "authors":[
          {
             "given":"Stefan",
             "family":"Evert",
             "affiliation":[
                {
                   "original_name":"Universität Erlangen-Nürnberg, Germany",
                   "normalized_name":"University of Erlangen-Nuremberg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00f7hpc57",
                      "GRID":"grid.5330.5"
                   }
                }
             ]
          },
          {
             "given":"Fotis",
             "family":"Jannidis",
             "affiliation":[
                {
                   "original_name":"University of Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          },
          {
             "given":"Thomas",
             "family":"Proisl",
             "affiliation":[
                {
                   "original_name":"Universität Erlangen-Nürnberg, Germany",
                   "normalized_name":"University of Erlangen-Nuremberg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00f7hpc57",
                      "GRID":"grid.5330.5"
                   }
                }
             ]
          },
          {
             "given":"Thorsten",
             "family":"Vitt",
             "affiliation":[
                {
                   "original_name":"University of Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          },
          {
             "given":"Christof",
             "family":"Schöch",
             "affiliation":[
                {
                   "original_name":"University of Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          },
          {
             "given":"Steffen",
             "family":"Pielström",
             "affiliation":[
                {
                   "original_name":"University of Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          },
          {
             "given":"Isabella",
             "family":"Reger",
             "affiliation":[
                {
                   "original_name":"University of Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "french studies",
          "literary studies",
          "english studies",
          "german studies",
          "stylistics and stylometry",
          "data mining / text mining",
          "authorship attribution / authority",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Many researchers, from the humanities and other domains, have a strong need to study resources in close detail. Nowadays more and more of these resources are available online. To make these resources discoverable, they are described with metadata. These metadata records are collected and made available via central catalogues. Often, resource providers want to include specific properties of the resource in the metadata. The purpose of catalogues will be more generic and addresses a broader target audience. It is hard to strike the balance between these two ends of the spectrum with one metadata schema, and mismatches can negatively impact the quality of metadata provided. The European CLARIN infrastructure (CLARIN ERIC, 2016b) was confronted with this specific problem, and designed a solution based on a flexible mechanism to build resource specific metadata schemas, potentially using domain, community or project specific terminology, out of shared components and semantics. This paper introduces this approach and the infrastructure built for it, which is applicable to any domain with the same needs.   Component Metadata In the Component Metadata (CMD) Infrastructure (CMDI) (CLARIN ERIC, 2016c; Broeder et al., 2012) the metadata lifecycle starts with the need of a metadata modeler to create a dedicated metadata profile for a specific type of resources, e.g., speech recordings (e.g., HZSK, 2015) or historical letters (e.g., Roorda, 2013). The modeler can browse and search a registry for components and profiles that are suitable or come close to meeting the requirements at hand. A component groups together metadata elements that belong together and can be potentially reused as a group in a different context, e.g., a location or a language description. Components can also group other components, e.g., the actor component can contain the general location component. The CMDI Component Registry (CLARIN, 2016a) already contains many of these general components. And these can be reused as they are or be adapted, i.e., add or remove some metadata elements and/or components. Also completely new components can be created to model the unique properties of the resources under consideration. All the needed components are combined into one profile that is specific to the type of resources, e.g., a profile for a speech recording (see Figure 1). Components, elements and values in this profile are linked to a semantic description, e.g., a concept, to make their meaning explicit. This feature allows the use of community specific terminology, while still creating a shared semantic layer that can be exploited by generic tools. Finally, metadata creators can create records for specific resources that comply with the profile relevant for the resource type, and these records can be provided to local and global catalogues. Notice also that CMD leaves the final responsibility of how heavy a metadata description should be to the modeler: it is perfectly possible to create a lean profile, resulting in lightweight records and combine it with a full-text index of the resources for discovery.   Figure 1: Example CMD profile (p), components (c) and elements (e)  The Component Metadata approach is currently being standardized by ISO Technical Committee 37. And the first part (ISO 24622-1, 2015) of this family of standards has been released and specifies the model just described.   Component Metadata Infrastructure CLARIN built an infrastructure (see Figure 2) around the approach described in the previous section. This infrastructure is open source and provides many tools, which can readily be reused by other communities.   Figure 2: An overview of tools and roles in CMDI   CMDI toolkit (CLARIN, 2016b): A set of XML schemas and transformations that implement the workflow from validating component and profile specifications to conversion into profile specific XML schemas used to validate specific CMD records.   Component Registry (CLARIN, 2016b): A registry storing the profiles and components created by the community for reuse. It also provides an editor for the creation of new profiles and components and derivatives of existing ones. The backend also provides REST services based on the toolkit, i.e., serves the XML schema representation of a CMD profile.   Concept Registry (CLARIN, 2016c; Schuurman et al., 2015): A SKOS-based registry storing the communities widely accepted concepts and their relations, which form the general semantic network overlaying the specific metadata profiles (Durco and Windhouwer, 2014).   CMD editors and forms: Various general CMD editors have been developed, e.g., the desktop tool Arbil (The Language Archive, 2016a) and the online editor COMEDI (CLARINO, 2016). Also dedicated forms for specific profiles, e.g., the CMDI maker (CLARIN-D, 2016), which can be inspiring.   Repository systems: Many CLARIN centers have deployed and configured generic repository systems to store their resources accompanied by CMD records. LINDAT (UFAL, 2016) and The Language Archive (2016b) have done so as well and released their solutions as general CMD-capable repository systems.   OAI harvester (The Language Archive, 2016c): CMD records can be harvested with any OAI harvester, but this harvester is special in that has easy provisions to add transformations to CMD that enable the harvesting of other metadata formats.   Catalogues: Faceted browsing is a suitable and commonly applied method for exloring vast collections based on some key properties. The CLARIN Virtual Language Observatory (VLO) (CLARIN, 2016d) is such a browser. Although the front-end is rather CLARIN specific, the VLO importer in the back-end, which takes the harvested CMD records, determines the facets and their values and stores these in a SOLR index, is generic because the facet mapping is highly configurable and exploits the shared semantic layer. The Meertens Institute has developed an alternative faceted browser (Meertens Institute, 2016) for CLARIN-NL. The importer of this browser does not take any configuration and dynamically creates facets for any semantically different context it finds in the CMD records.   Convertors: CMDI is currently XML oriented, but other representation formats are possible. The CMD2RDF service (CLARIN-NL, 2016) provides a RDF representation to link CMD records with the world of Linked (Open) Data. Also convertors for other metadata formats, e.g., MODS or OLAC, to CMD are available.   Although this list is far from complete it shows that the CMD Infrastructure is a thriving and versatile software ecosystem. Also many parts of it are configurable, which makes it adaptable to other domains.   Lessons learned by CLARIN When CLARIN started development on the CMD Infrastructure in its preparatory phase many things were not clear yet and a lot of flexibility was needed. Common and reusable components and concepts still had to be specified. This has lead to situations where sometimes several alternatives have coexisted for a long time in the CMD ecosystem, which can make it hard for users, especially novices, to select which one to use. It is better to prevent this kind of confusion. CLARIN advises new communities that start using the CMD Infrastructure to first setup a set of basic recommended or even obligatory components and concepts, so a stable generic core is available to the community to extend with their specific needs.   Future work CLARIN keeps on working actively on the CMD Infrastructure, with the next version, i.e., CMDI 1.2 (Goosen et al., 2015) currently under development. The following topics will be addressed by this update:  Lifecycle management of components and profiles. Connection to vocabulary services. Annotation of profiles and components with hints targeted at tools.  Also improvement of the metadata quality is an ongoing process. The ongoing CLARIN-PLUS project (CLARIN ERIC, 2016a) includes the design and development of tools and and a workflow that can be used used by metadata curators for quality assessment and curation of CMD records (King et al., 2015).   Conclusions CLARIN has built a highly flexible and versatile infrastructure for metadata that is able to meet both the generic needs of catalogues and the specific needs of resource providers. The fruits of these efforts are ready to be picked by any community experiencing the same needs. CLARIN is happy to share their experiences as well as the sometimes hard learned lessons.  ",
       "article_title":"Flexible Community-driven Metadata with the Component Metadata Infrastructure",
       "authors":[
          {
             "given":"Menzo",
             "family":"Windhouwer",
             "affiliation":[
                {
                   "original_name":"Meertens Institute, Netherlands, The",
                   "normalized_name":"Meertens Institute",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/05kaxyq51",
                      "GRID":"grid.450081.8"
                   }
                }
             ]
          },
          {
             "given":"Twan",
             "family":"Goosen",
             "affiliation":[
                {
                   "original_name":"CLARIN ERIC",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Jozef",
             "family":"Misutka",
             "affiliation":[
                {
                   "original_name":"Institute of Formal and Applied Linguistics, Charles University in Prague",
                   "normalized_name":"Charles University",
                   "country":"Czechia",
                   "identifiers":{
                      "ror":"https://ror.org/024d6js02",
                      "GRID":"grid.4491.8"
                   }
                }
             ]
          },
          {
             "given":"Dieter",
             "family":"Van Uytvanck",
             "affiliation":[
                {
                   "original_name":"CLARIN ERIC",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Daan",
             "family":"Broeder",
             "affiliation":[
                {
                   "original_name":"Meertens Institute, Netherlands, The",
                   "normalized_name":"Meertens Institute",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/05kaxyq51",
                      "GRID":"grid.450081.8"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "information architecture",
          "digital humanities - facilities",
          "archives, repositories, sustainability and preservation",
          "English",
          "publishing and delivery systems",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Augmented Reality (AR) superimposes geolocated digital text and images on a real-time view of the world.  Museums, urban spaces and cultural heritage sites are using AR to virtually restore and enhance historical displays and environments on smartphones or tablets. We have created an AR application that combines dynamic, interactive content, including realistic 3D models, video, animation, game, and website portal, to immerse a diverse public in the history and culture of an important and well-trafficked setting: Flushing Meadows-Corona Park (FMCP), a 1255-acre urban park in Queens, NY.  Littered with dramatic remnants of two World’s Fairs, FMCP is steeped in a largely-forgotten history as the site of the 1939/40 and 1964/65 World’s Fairs (NYWF), the first UN General Assembly, activities of European colonists, and early habitation by Algonquian speaking Native Americans. Additionally, the park served as the backdrop for the novel The Great Gatsby and as a site for an interesting natural history. Our public venture partner, the Queens Museum, is situated within FMCP, in the most diverse immigrant neighborhood in New York State. In 2014  the population of Queens was composed of 49.1% Caucasian, 28% Latino, 25.8% Asian, and 20.8% African American. 47.8% of households are headed by individuals born outside the United States (http://quickfacts.census.gov/qfd/states/36/36081.html). By harnessing the multimodal possibilities of AR, we have engaged an ethnically, generationally and educationally diverse audience in the cooperative - and critical - exploration of history and culture. The application will provide users two complementary experiences: an intergenerational learning game which enables children and adults to be partners while learning about the history of FMCP, and a guided tour that explores the two World’s Fairs through comparison of their common, recurrent themes such as futurism, technological optimism, citizenship, progress, race and nationalism. Through these modalities, the application supplies a greatly enhanced destination experience to users of all ages and backgrounds, whether they are families from the surrounding neighborhoods, Queens Museum goers or tourists to New York City.  The project is informed by interdisciplinary humanities scholarship, including World's Fair history, anthropology, theme park studies, cognitive psychology, AR mobile design,and game design for STEM and cultural learning (http://srealserver.eecs.ucf.edu/chronoleap/).  I.  Intergenerational Game Based upon the activity Geocaching, an intergenerational learning game is under development to provide children a scaffolded way to learn about the history and heritage of FMCP. Geocaching is a scavenger hunt-style game, which sends individuals or teams searching an environment for hidden physical items. By making the caches virtual, there are no limitations on items and avoid impacting park operations.  An adult led by the child must surmount the challenge of locating virtual geocaches, while exploring the past of FMCP.  Teams will also drill virtual core samples in the park and examine them to find geocached items within. Game play proceeds as users are provided clues and instructions guiding them to a physical locations. For example,  a clue might read “Find the ancient column” (referring to the extant Roman Column of Jerash, presented to NYC by Jordan’s King Hussein at the close of the 1964-65 NYWF) (Wingfield, 2011). Player’s devices notify them when reaching the location and are then provided new GPS coordinates. The device enables them to find the location, as well as see the landscape of the park as it appeared in the past and find the virtual artifact. Artifacts are stored by the application for future examination. This process continues for a set of artifacts – each artifact being one piece of a puzzle. In geocaching tradition we provide a final reward at the end. Another feature entails the collection of virtual pieces of artifacts from the core-sample’s strata, which are eventually reassembled at the relevant location. Examples of artifacts which might be found and assembled by users include the Videophone displayed at the Bell Telephone Pavilion during the 1965/65 Fair, the first television displayed at the RCA Pavilion during 1939/40 Fair, and a Mastodon from the Pleistocene Era. The application can provide supplemental content (e.g. video, audio and images), questions that open up further pathways for thought, and fun quizzes that challenge a user’s knowledge and memory.   As children have yet to develop a mastery of most subjects, they can often learn better when they are provided support from a familiar adult (Vygotsky, 1978). Such support, commonly known as scaffolding, is essential for them to integrate new information into their base of knowledge.  This dovetails well with Lave and Wenger (1991), where learning is a communal, interactive activity where less experienced individuals learn from master practitioners.  Our app provides an intergenerational pathway, where the adult can both support learning information presented and share associated information from their own life. A child will likely to engage with media prompts when with an adult (Takeuchi et al., 2011). Participatory mediation of learning, where adults and children collaborate, sharing a dialogue stimulated by the geocaching activity offers the potential for learning in such application (Clark, 2011).  This could be extended to encompass the sharing of deep-rooted memories and experiences for the adult, while also enabling children to share their experiences.   II.  The Deep Experience This modality targets individuals who are interested in advanced content and interpretation for deeper, critical understanding of the 1939/40 and 1964/65 Fairs at Flushing Meadows, remnants of which form a dramatic backdrop. World's Fairs were typically experienced as spectacles of culture, industry, and technology. (Benedict,1981; Geppert et al., 2005; Greenhalgh, 2011; Corn and Horrigan, 1984; Garn et al., 2007; Rydell, 1984, 1993; Rydell and Gwinn, 1994; Rydell et al., 2000)  However, they are also environments that shaped popular understanding of society’s past, present and future, linked through enduring narratives of citizenship, progress, technological optimism, consumerism, race and nationalism (Hollengren et al., 2014; Gordon, 2005; Marchand ,1991,1995; Samuel, 2007; Tirella, 2014; Winner, 1991). This mode enables users to critically engage with the changing social themes and narratives that tie the Fairs together by sending users to cognate, virtual pavilions. Designed as a curated tour starting at the Queens Museum, visitors are guided through the park viewing models of Fair pavilions, and learning about common themes through archival images, audio and documentary footage, both within the application itself and on the associated website. Examples include:  Landscapes of the Future The connected themes of progress, technology, commerce and utopia – as expressions of “the future in the present” – have always dominated in World’s Fairs (cf Land, 2011).  Visitors are directed to the interstate highway utopia of Futurama at the General Motors pavilion, designed by Norman Bel Geddes (Marchand, 1991; 1995), and also to Futurama II, its updated version in the later Fair, in which “General Motors set out to reveal how technology would conquer the harshest environments for the betterment of humanity” (Walters, 2014:467).  Visitors will be able to view archival footage of both exhibits, read contemporary accounts and see original designs.   Protests at the Fair National and global conditions did not reflect the optimistic themes of the Fairs, nor did Fairs go uncontested by groups who were excluded from self-representation and employment.  The 1964/65 NYWF was designed as a virtual bubble away from the emergent civil rights, free speech and anti-war movements. Visitors are invited to explore the CORE (Congress of Racial Equality) protests of 1964 (Peneda, 2014; Jacoby, 1998; Tirella, 2014; http://www.democracynow.org/2014/4/25/protesting_the_1964_world_s_fair) and to see them come alive at the site of the Unisphere (still extant), through  digital overlay of archival footage (see for example http://timetraveler.berlin).  Visitors will also access additional material such as pamphlets, flyers and photographs, and hear audio of speeches and meetings.   Racial Representation: two Africas Both Fairs contained pavilions that represented “Africa” as a project of defining, organizing and displaying people and culture. (Lorini, 1999; Rydell, 1999).  Our comparison of these exhibitions highlights the increased agency of the emergent African Republics of 1964/65 to represent themselves as modern members of the family of nations with professional dance performances, art exhibit, displays of industry (cf Benedict, 1993; Lukas, 2007).  In contrast, the Great Britain pavilion of 1939/40 contained “natural history” dioramas of traditional scenes in the life of its colonized subjects  (http://www.1939nyworldsfair.com/worlds_fair/wf_tour/hall_of_nations/great_britain, https://archive.org/stream/ldpd_11290477_000#page/n1/mode/2up). The static diorama was a key visual strategy that distinguished colonized Africans from its white Commonwealth citizens, also on display.  Visitors will be able to compare the 1964/65 professional dance performances with those that took place at the 1939/49 Midway where they were framed as exotic, marginal entertainments. The interpretive experience provides an associated website that contains contemporary accounts, academic essays, archival media, and exhibition content.  The website will also provide an opportunity to upload personal oral histories of the Fairs (cf Anderson, 2003) that will prompt them to critically reflect on the relationship of the historic Fairs and contemporary life.                     ",
       "article_title":" An Augmented Reality Mobile Application for Intergenerational Learning and Critical Connection Experiencing The Histories of Flushing Meadows Park ",
       "authors":[
          {
             "given":"Tamar",
             "family":"Gordon",
             "affiliation":[
                {
                   "original_name":"Rensselaer Polytechnic Institute, United States of America",
                   "normalized_name":"Rensselaer Polytechnic Institute",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01rtyzb94",
                      "GRID":"grid.33647.35"
                   }
                }
             ]
          },
          {
             "given":"Lori",
             "family":"Walters",
             "affiliation":[
                {
                   "original_name":"Institute for Simulation and Training, University of Central Florida",
                   "normalized_name":"University of Central Florida",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/036nfer12",
                      "GRID":"grid.170430.1"
                   }
                }
             ]
          },
          {
             "given":"Rob",
             "family":"Michlowitz",
             "affiliation":[
                {
                   "original_name":"Institute for Simulation and Training, University of Central Florida; Institute for Simulation and Training, University of Central Florida",
                   "normalized_name":"University of Central Florida",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/036nfer12",
                      "GRID":"grid.170430.1"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "games and meaningful play",
          "digital humanities - pedagogy and curriculum",
          "digital humanities - diversity",
          "mobile applications and mobile design",
          "digital humanities - nature and significance",
          "virtual and augmented reality",
          "historical studies",
          "knowledge representation",
          "anthropology",
          "English",
          "multilingual / multicultural approaches",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Background In the last decades, quantitative linguistics (following exact and social sciences) has developed several statistical methods providing an insight into measurable phenomena of natural language. Although to a lesser extent, it also applies to the analysis of diachronic changes. Obviously, the so-called philological method in historical linguistics (unlike historical comparative and internal reconstruction methods) was always a kind of “corpus linguistics”, which means that a linguist studying a given period of a language investigated, via close reading, available written records. Consequently, the text was usually treated as a (mistrustful) informant. The implication of this attitude is that in principle, a single attestation of a linguistic fact in a text was considered a strong evidence. Paradoxically, it is synchronic corpus linguistics that changed the overly conservative approaches to diachrony. The most significant here is the shift from purely qualitative to quantitative argumentation. Certainly, the availability of machine-readable corpora allows for much more sophisticated quantitative analysis these days. A significant drawback of many of the quantitative methods applied so far is a tacit assumption that the researcher knows in advance which elements of the language are subject to change. In other words: the method of, say, plotting and inspecting the trend for a given phenomenon may be applied only to verify hypotheses stipulated earlier by traditional (that is qualitatively oriented) diachronic linguistics. A real challenge, however, is to develop such a method that would allow to trace chronological change in the language without a prior knowledge which linguistic features are responsible for the change. Promising results may be expected using some of the stylometric techniques based on the statistical analysis of style, especially the so-called multidimensional methods. The combination of stylometry and historical linguistics is not an entirely new idea. The problem of automatic recognition of relative chronology of texts was recently addressed by Stamou (2008; 2009), Štajner and Mitkov (2011), Popescu and Strapparava (2013), Štajner and Zampieri (2013), Zampieri et al. (2015). We shall note, however, that the first who sought to solve the question of chronology of texts via their stylistic features was Lutosławski (1897). Stylometric methods are particularly efficient when applied to frequencies of function words (or, the most frequent words). However, an interesting question arises what if we disregard words and examine grammatical features instead? Obviously, the usage of archaic vs. modern inflected forms alone will differentiate texts written in two distinct (yet still close) periods. What is less obvious, however, is whether processing solely POS-tags, i.e. grammatical labels, can show the dynamics of language change. Note that the sequences of POS-tags are a good approximation of syntax, even if they cannot replace parsing (Hirst and Feiguina, 2007; Wiersma et al., 2011). To scrutinize the above research question, we performed a number of stylometric tests using different (tailored) methods and different combinations of lexical and grammatical features’  n-grams.    Chronology at a glance Standard stylometric methods are aimed at tracing differences between (groups of) texts. They proved to be successful in detecting a predominant stylistic signal, which in most cases is the authorial voice. However, when the number of analyzed texts is high enough, the emerging authorial groups (clusters) tend to form larger lumps reflecting the existence of other stylometric signals, such as genre, gender or chronology. This phenomenon can be observed very clearly when bootstrap consensus network – an enhanced version of cluster analysis (Eder, 2015) – is applied.    Figure 1: Stylometric network of similarities between 333 English texts   In Fig. 1, a network of an exemplary corpus of 333 English texts (De Smet, 2005) covering the period 1700–1930 is shown. The network was produced using most frequent words as predictors. One can notice a clear split into three distinct areas of the network that is due to a strong genre signal. However, despite the overwhelming division into novels, non-fiction, and drama, an additional chronological signal – represented by a transition from green (the earliest works) to red (the latest works) is fairly noticeable within each of the three sub-groups. Networks for other style markers (word  n-grams, POS-tag  n-grams) showed a similar behavior.    Modeling stylistic drifts Certainly, the general picture revealed by the above network is by no means satisfactory, at least from the perspective of historical linguists. In particular, one would like to know how to pinpoint the observed chronological transition, in terms of identifying interpretable trends and/or breaks. The idea discussed in this section addresses this problem by combining multivariate stylometry with linear regression models.  Multidimensional scaling is a way of compressing (or projecting) a highly complex space into its simpler, usually two-dimensional, representation. Even if such a procedure always involves some loss of information, it is believed to reveal actual differences between samples. Now, since the technique allows to reduce the original space into an arbitrary number of dimensions, one can squeeze the data into just one dimension. This single MDS score can be plotted against the timeline, in order to test if any correlation between the two variables exist. The more diagonal is the shape of the plotted points, the higher the correlation.    Figure 2: Multidimensional Scaling of 333 English texts (250 most frequent word 3-grams), compressed into one dimension and plotted against the timeline   In Fig. 2, some correlations between the timeline and the MDS values are fairly visible with a naked eye. However, when the results are scrutinized using a standard linear regression model  y i =  x i β 1 +  β 0 +  ε (where  β 1 and  β 0 are parameters of the model, and  ε is a random effect), their correlations become even more obvious. The estimated model (a dashed line in Fig. 2), is formulized as  ŷ i = 0.272 *  d i – 499.94 +  ε, where  d i denotes the  i-th text’s date of publication. In terms of the  p value, the model is statistically significant ( p < 0.01); however, the goodness of fit as represented by the adjusted  R 2 value is rather poor ( R 2 = 0.06), due to the overwhelming genre signal hidden in the dataset. When one splits the corpus into three genres and analyzes them separately, however, the explanatory power of the model is far higher.    Supervised classification and the timeline One of the most interesting aspects of language development – overlooked in a vast majority of the existing studies – is the question of the dynamics of linguistic changes. Presumably, one should expect epochs of substantial stylistic drift followed by periods of stagnation, rather than purely linear trends.     Figure 3: A sequence of Nearest Shrunken Classification tests on 333 English texts: cross-validated results for different vectors of most frequent POS-tag 2-grams   To assess this issue, we apply an iterative procedure of automatic text classification. First, we formulate a working hypothesis that a certain year – be it 1750 – marks a major linguistic break. We divide the text samples into the  ante and  post subsets, according to particular texts’ publication date. Next, we randomly pick a number of train and test samples representing the both classes ( ante and  post), and we train a supervised classifier. We perform a standard classification, and record the cross-validated accuracy rates. Then we dismiss the original hypothesis, in order to test new ones: we iterate over the timeline, testing the years 1755, 1760, 1765, 1770, ... for their discriminating power. The assumption is simple here: any acceleration of linguistic change will be reflected by higher accuracy scores.  In Fig. 3, the classification accuracy rates for the aforementioned corpus of 333 English texts were shown (POS-tag 2-grams, NSC classifier). As one can observe, the scores obtained for the period 1750–1850 are only slightly higher than the baseline, betraying no revolutionary changes in this period. Later, however, the stylistic drift accelerates, reaching 70% of correctly recognized test samples.   Conclusions in this paper we used a set of tailored stylometric methods to assess the question of language change over time. Our chosen techniques proved to be useful indeed; the further research will focus on tracing the very linguistic features that were responsible for the observed change. However, an important question has to be asked here: is it a change of Saussurean  langue what we track with our approach, or rather the change of  parole. Obviously, if texts written earlier can be separated from texts written more recently, they must share some features common for a given stage of language development. However, it is not clear if an observed change is due to, say, literary taste of the epoch or, if we face an actual change in the system here. Theoretically, the former and the latter are possible, as well as both answers together. It is also very likely that the change takes place in between: in the  norm in the sense proposed by Coseriu (1958). Still there are no means to answer this question with any stylometric method, what for a linguist might be seen as a drawback. However, the proposed method informs the linguist about the fact of change, which takes place not only in lexis but also in syntax; about the speed of change and, above all, about the points where this speed accelerates.    Acknowledgements This research is part of project UMO-2013/11/B/HS2/02795, supported by Poland’s National Science Centre.  ",
       "article_title":" Historical Linguistics’ New Toys, or Stylometry Applied to the Study of Language Change  ",
       "authors":[
          {
             "given":"Maciej",
             "family":"Eder",
             "affiliation":[
                {
                   "original_name":"Institute of Polish Language, Polish Academy of Sciences; Pedagogical University, Krakow",
                   "normalized_name":"Pedagogical University",
                   "country":"Mozambique",
                   "identifiers":{
                      "ror":"https://ror.org/0331kj160",
                      "GRID":"grid.442441.3"
                   }
                }
             ]
          },
          {
             "given":"Rafał",
             "family":"Górski",
             "affiliation":[
                {
                   "original_name":"Institute of Polish Language, Polish Academy of Sciences; Jagiellonian University, Krakow",
                   "normalized_name":"Jagiellonian University",
                   "country":"Poland",
                   "identifiers":{
                      "ror":"https://ror.org/03bqmcz70",
                      "GRID":"grid.5522.0"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-15",
       "keywords":[
          "natural language processing",
          "stylistics and stylometry",
          "authorship attribution / authority",
          "linguistics",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Probably best known as the author of  En attendant Godot /  Waiting for Godot, Samuel Beckett was not only a bilingual playwright, but also a poet, translator, essayist and novelist. Notably his prose fiction is the focus of this contribution, in which we use quantitative methods to delineate a periodization of Beckett’s œuvre. In art studies in general, there is a tradition of distinguishing an ‘early’ and ‘late’ period in an artist’s work, sometimes with a distinct ‘middle’ period in between. The late Beethoven sonatas are a good example, or the early Rembrandt’s ‘smooth’ style versus the rough paint surfaces of the late Rembrandt. Nevertheless, it is often difficult to determine exactly when an author’s work moves from, say, the ‘early’ to the ‘middle’ stage.  In Beckett studies, we find a similar pattern of periodization, ending with the ‘late style’ (Gontarski, 1997). Peter Boxall (2015: 34) problematizes the idea of periodizing Beckett’s œuvre, but admits that it is hard not to parcel it into a beginning, a middle and an end: an early period up to and including the novel  Watt, written during WOII; a rich middle period up to and including  The Unnamable; and the later, ‘halting’ prose after the latter text. Numerous critics have proposed alternative periodizations, resulting in almost as many different periodizations as the number of critics that devised them. We therefore investigated what a non-human ‘interpreter’ would come up with as a periodization by means of stylometry.   In this paper, we apply methods from stylometry to the problem of periodizing Samuel Beckett’s prose. A novelty is that we restrict it to function word frequencies, which are a common object of research in stylometry, but which have hardly been considered in Beckett studies. An exception is Banfield, who suggested a four-phase evolution in Beckett’s oeuvre, partially on the basis of linguistic arguments. Our approach is related to ‘stylochronometry’ (Stamou, 2008), in which a text’s writing style is studied as a function of its date of composition, in accordance with recent research into the stylistic development of individual authors, such as Henry James (Hoover, 2014), W. B. Yeats (Forsyth, 1999) or Jack London (Juola, 2007).   Preprocessing Here, we analyze Beckett’s prose fiction, in both French and English. We removed all non-authorial paratexts and only considered lower-case, alphabetical character strings. In the software repository accompanying this contribution ( https://github.com/mikekestemont/beckett), we present a tabular overview of the materials we collected, including the publication dates of the editions we used, the text’s length, etc. (Note that this repository also holds high-resolution versions of our plots, which will be much more readable on screen.) In terms of chronology, we focus on the moment when Beckett started working on a text in either language (Van Hulle et al., 2015).  We defined a relevant list of function words by extracting an initial list of the 300 most frequent words (MFW) from each corpus. From this list, we have manually removed non-grammatical words, which might correlate too strongly with the topic of particular texts. We refrained from removing common auxiliary verbs or personal pronouns, because they are interestingly tied to a text’s narrative perspective, as will become clear below. After this procedure, we were left with 162 function words for the English corpus and 169 for the French, the frequencies of which were scaled using  z-scores.    Preliminary Analyses We carried out an exploratory analysis of the material, using principal components analysis (PCA, first two dimensions plotted in Figs. 1-2 for 3,761-word slices). This unsupervised procedure does not yet integrate any chronological information in the analyses, which will help establish whether Beckett’s œuvre might have a ‘natural’ chronological structure with respect to writing style. In Figs. 1-2, the horizontal spread is dominated by a threefold clustering, with some of Beckett’s earlier works clustering in the far left. The loadings reveal that these works are characterized by a high frequency of words related to a third-person narrative perspective ( he,  she,  his,  has, etc.). To the far right, we find a tight sample cloud corresponding to some of Beckett’s post-war works, such as  The Unnamable and  Texts for Nothing. These texts can be characterized by the use of first-person pronouns ( I,  me) in combination with impersonal pronouns such as  it and  there, which suggests that these texts focus on the relation between an ‘I’ and his non-personal surroundings.  Texts from the in-between period (such as  Molloy and  Watt) also hold the middle in the horizontal distribution of samples. In both languages, samples from later works jump out with respect to the vertical dimension – e.g.  Worstward Ho in English, which is characterized by a rich mix of fairly abstract words, with an indeterminate semantics. Both scatterplots horizontally create an opposition between earlier and later writings in Beckett’s oeuvre, focusing on an opposition between a first-person and a third-person perspective. In the vertical dimension, both analyses reveal on a vocabulary shift, in Beckett’s late writings, towards a more abstract and indeterminate vocabulary.    Figure 1: English corpus PCA    Figure 2: French corpus PCA    Chronological Analyses The previous analyses were ignorant of the diachronic structure of the data: samples from Beckett’s early works could just as easily cluster with later writings. This prevented us so far from identifying clear turning points in Beckett’s career. Variability-based Nearest Neighbour Clustering or VNC (Gries et al., 2012) is a method for the diachronic analysis of corpus linguistic data. VNC aims to identify distinct temporal stages, by pinpointing the main turning points. In traditional cluster analyses, each node can be freely combined with any other node in the tree, thus potentially scrambling the original chronological order of the data. VNC adds the constraint that only consecutive nodes, immediately adjacent in time, can form new clusters. This restriction enables analyses in which the chronological structure of the data is reflected in the top branches of trees, representing the main diachronic stages in the data. We have run VNC on our data (Figures 3-4, Euclidean distances, Ward’s linkage), which resulted in clearer insight into the chronological structure of Beckett’s œuvre. The English prose, displays a clear initial cluster of Beckett’s earliest two novels,  More Pricks than Kicks and  Dream of Fair to Middling Women, which lack a French translation. Otherwise, the structures of Figs. 3-4 run remarkably parallel.  Murphy and  Watt constitute the second chronological cluster of works, together with the  Nouvelles. Only at a higher level is the former group paired with the cluster consisting of  Mercier and Camier,  Molloy and  Malone Dies. Interestingly, the original French versions of the  Nouvelles are joined with the next cluster, whereas the English versions are joined with the previous cluster, which indicates a different status of this collection in both languages. The last major branch for both languages holds the tight clade representing  The Unnamable,  Texts for Nothing,  How It Is and the series of shorter late works. In the English tree,  Worstward Ho occupies a fairly pronounced position, emphasizing its special status.    Figure 3: English corpus VNC    Figure 4: French corpus VNC  The VNC analysis (supplemented by a more complex, bootstrapped analysis, which we do not describe in this abstract) generally supports the periodization of Beckett’s oeuvre into an early, middle and late ‘cluster’. In English these periods would cover, firstly, Beckett’s early works,  More Pricks than Kicks and  Dream of Fair to Middling Women; secondly the mid-career works, ranging from  Murphy to  Malone dies; and thirdly, a series of later works starting with  The Unnamable. From the French prose, a similar periodization arises – although it clearly reflects the absence of any translated counterparts for  More Pricks than Kicks and  Dream of Fair to Middling Women.  Interestingly, our analyses invariably point to the beginning, rather than the end, of  L’Innommable /  The Unnamable as a major stylistic turning point, thus breaking up the unity of the so-called post-war ‘trilogy’. Additionally,  From an Abandoned Work / D’un ouvrage abandonné – notwithstanding its short length – also often emerged as a major watershed. The question, however, is whether this result is to be interpreted as a turning point leading the way for Beckett’s later, experimental works, or as a sort of ‘re-turning point’, marking a temporary relapse into the idiom of the  Nouvelles after  L’Innommable. This might be a valuable pointer for further research, since this particular text does not seem to have played a significant role in the periodization debate so far. Additionally, it turns out to be more difficult in English than in French to model the transition from a young to a middle Beckett (also in the bootstrapped experiments). This result possibly reflects the fact that the original, English version of  Watt was written relatively early (during the war), whereas its translation was made much later. In any case, this particular result offers grounds for a re-examination of the difference in evolution between Beckett’s French and English prose production, and in particular the role of  Watt as a transitional novel.   ",
       "article_title":"Stylochronometry and the Periodization of Samuel Beckett’s Prose",
       "authors":[
          {
             "given":"Dirk",
             "family":"van Hulle",
             "affiliation":[
                {
                   "original_name":"University of Antwerp, Belgium",
                   "normalized_name":"University of Antwerp",
                   "country":"Belgium",
                   "identifiers":{
                      "ror":"https://ror.org/008x57b05",
                      "GRID":"grid.5284.b"
                   }
                }
             ]
          },
          {
             "given":"Mike",
             "family":"Kestemont",
             "affiliation":[
                {
                   "original_name":"University of Antwerp, Belgium",
                   "normalized_name":"University of Antwerp",
                   "country":"Belgium",
                   "identifiers":{
                      "ror":"https://ror.org/008x57b05",
                      "GRID":"grid.5284.b"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "french studies",
          "literary studies",
          "english studies",
          "stylistics and stylometry",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Stylometry based on quantitative analysis of linguistic features such as most frequent words, lemmata, or parts of speech, is a time- and research-proven tool in authorship attribution and plagiarism detection, and is now also used in more general literary studies as part of the distant reading revolution. It has been particularly successful in grouping long texts by their authors in both supervised and unsupervised machine learning tests – and the appeal of this material to stylometrists is understandable in that novels are easily available and easily definable chunks of linguistic (and literary) material, and, despite rumors on the death of the author, most readily associated by the general reader with a single creative figure. And when they are not, discovering the fingerprints of more than one hand in collaborative works is another favorite pastime of stylometrists. While perhaps equally avidly studied, the authorship signal in drama is often more problematic. This is probably why the most famous question, that of Shakespearean authorship, is so complex and so hotly contested – as evident, for instance, in a fairly recent debate (Craig and Kinney, 2009; Vickers, 2011; Hoover, 2012). Other difficulties in this genre include the “codification of … literary discourse” in certain literary periods and the fact that the same authors might write drama both in prose and in verse (Schöch, 2013, 2014); also, it may be supposed that, as dramatists create their characters through dialogue, there is a more or less conscious effort on their part to differentiate their style. This last phenomenon has also been researched in novels (Burrows, 1987) and translations of novels and drama, and the results could be equally problematic (Rybicki, 2006, 2007, 2008). Even more distortion may be expected in a somewhat similar genre, that of film and TV dialogue – and its textual reflection in intralingual subtitles and interlingual translations. The final shape of filmic speech ascribed to a given screenwriter can be influenced by other agents, such as directors or actors. It can be further transformed in the process of intralingual subtitling, especially that performed by “fansubbers”, who do not necessarily reflect the exact dialogue spoken onscreen. This becomes even more of a problem in the case of interlingual translations, which by nature condense (subtitles, voice-over) or rework (dubbing) the original message, being at times anonymous versions of questionable quality (“fansubs”). Quantitative methods have already found their way into audiovisual translation research (Pérez-González 2014; Baños  et al., 2013), as exemplified by such projects as Pavia Corpus of Film Dialogue, used to examine sociolinguistic and pragmatic features of dubbed Italian (Freddi and Pavesi, 2009), or Forlixt1, a multimodal corpus which helps to investigate the interplay of verbal and non-verbal semiotics of the film (Valentini, 2006, 2008).  In comparison with the above attempts, our research was done on a specialized corpus of historical films and TV (mini)series. This choice was based on the assumption that the subgenre has unique characteristics which find reflection in film dialogue: namely, it authenticates the represented reality and it also adheres to the codes of realism existing in particular countries. This, in turn, made us look for the same phenomena in (English) originals and (Italian and Polish) translations, since translated dialogue, too, is shaped by stylistic necessities of the genre, culture-specific images of the past dominant in the target context, but also by norms and conventions of audiovisual translation in a given language/culture/country. The exact composition of the corpus is given in the table below:   Original Polish voice-over Polish “official” subtitles Polish fansubs Italian dubbing Italian “official”subtitles Italian fansubs    The Tudors Season 1 (2005)  + +  + + +    The Tudors Season 2 (2007)  + + + + + +    Elizabeth I (2005) (miniseries)   +   +    Elizabeth (1998) + +  + + +   Elizabeth. Golden Age (2007) + +  + + +    The Other Boleyn Girl (2008)   +  + + +    The Private Lives of Elizabeth and Essex (1939)    + + + +   Anne of a Thousand Days  +  +      Wolf Hall (2015)       +   From the point of view of film and audiovisual translation studies, our research project explores the concept of film/television genre and its distinctive features, focusing on the functions of film dialogue and linguistic/stylistic strategies used by screenwriters to fulfil them (Kozloff, 2000; Jaeckle, 2013). The first stage of our investigation consisted in a contrastive stylometric analysis of the extended Anglophone corpus, composed of both historical and non-historical film scripts, in order to verify our preliminary hypothesis about the genre-related specificity of film dialogue. We proceeded, then, to the analysis of parallel corpora of scripts in all available translations into Polish (voice-over, official and amateur subtitles) and Italian (dubbing, official and amateur subtitles). All this was done with several quantitative methods previously developed and used on other textual material, i.e. literary texts. In particular, frequencies of words from various frequency strata were compared between texts in each of the languages studied using the Delta procedure (Burrows, 2002). The analyses were performed with  stylo (Eder et al., 2013), a package for R, the statistical programming environment (R Core Team, 2014), later also postprocessed with Gephi network analysis software (Bastian et al., 2009).  On the basis of these tests several observations could be made. As concerns the screenwriters, they tend to adapt the dialogues to the requirements of historical genre and the presented epoch. This is visible in Fig. 1, where the authorial signal seems to disappear whenever a given writer worked on two films and/or series set in different eras or belonging to a different, i.e. non-historical genre.   Figure 1. Network analysis diagram of historical and non-historical scripts.  As concerns audiovisual translations, we arrived at rather unexpected conclusions. We compared versions of individual episodes of TV series and films in Italian (dubbing, subtitles) and Polish (voice-over, subtitles) by analyzing frequencies of single words and part-of-speech 5-grams; the latter measure was a rough approximation of syntax (Górski et al., 2014). As far as Italy is concerned, we noticed an astounding uniformity of style regardless of technique, be it subtitles or dubbing: translations of individual episodes of TV series and films clustered together in analyses of both word and part-of-speech frequencies. By contrast, Polish subtitles and voice-over scripts of the same episodes clustered together for single-word frequencies, while the latter formed their own clusters in part-of-speech 5-gram analysis. This is shown in Fig. 2 and 3.   Figure 2. Bootstrap consensus tree for most frequent words in Polish subtitle and voice-over translations in a corpus of Elizabethan-era films and TV series.    Figure 3. Bootstrap consensus tree for most frequent part-of-speech 5-grams in Polish subtitle and voice-over translations in a corpus of Elizabethan-era films and TV series.  Obviously, the similarities between dubbing and subtitles in Italian may stem from the fact that the latter are based on the former. However, the fact that even amateur subtitles, which usually are published before the release of the dubbed version, show a considerable affinity to dubbing, demonstrates high normalization of the formal language used in Italian historical films and television series.  All these results confirm our preliminary hypothesis that film genre influences the strategies of creating film dialogues and their translations. Although we believe that stylometric and computational analysis should not be the end in itself, it seems invaluable in audiovisual translation studies, encouraging closer qualitative analysis of the original and translated scripts. It invites further investigation of such issues as:  the importance of cultural norms and conventions in film translation; significant intercultural differences in translation strategies used by subtitlers; complex relations between dubbing and subtitles, official and amateur subtitles, voice-over and subtitles cultural development of written / spoken language in a given country and the salient stylistic trends in audiovisual translation.  Acknowledgements This research is part of project 2013/11/B/HS2/02890, supported by Poland’s National Science Center. ",
       "article_title":"Stylometry on the Silver Screen: Authorial and Translatorial Signals in Film Dialogue",
       "authors":[
          {
             "given":"Agata",
             "family":"Hołobut",
             "affiliation":[
                {
                   "original_name":"Jagiellonian University, Krakow, Poland",
                   "normalized_name":"Jagiellonian University",
                   "country":"Poland",
                   "identifiers":{
                      "ror":"https://ror.org/03bqmcz70",
                      "GRID":"grid.5522.0"
                   }
                }
             ]
          },
          {
             "given":"Jan",
             "family":"Rybicki",
             "affiliation":[
                {
                   "original_name":"Jagiellonian University, Krakow, Poland",
                   "normalized_name":"Jagiellonian University",
                   "country":"Poland",
                   "identifiers":{
                      "ror":"https://ror.org/03bqmcz70",
                      "GRID":"grid.5522.0"
                   }
                }
             ]
          },
          {
             "given":"Monika",
             "family":"Woźniak",
             "affiliation":[
                {
                   "original_name":"Sapienza University of Rome, Italy",
                   "normalized_name":"Sapienza University of Rome",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/02be6w209",
                      "GRID":"grid.7841.a"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-02",
       "keywords":[
          "film and cinema studies",
          "stylistics and stylometry",
          "authorship attribution / authority",
          "translation studies",
          "multilingual / multicultural approaches",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" When an earthquake struck Nepal in 2015, the band One Direction sent a tweet encouraging their fans to donate to relief efforts. This one tweet was retweeted a few times, but quickly lost in a flood of other tweets about One Direction’s tour. Simultaneously, an Indian Hindu extremist politician flooded his Twitter stream with rumors that Christian missionaries were coercing conversions from Nepalis in exchange for humanitarian aid. Additionally, an Indian religious group mixed substantial numbers of tweets about a movie they had released with tweets about their relief efforts in Nepal. These are just a few of the users who engaged with the disaster from a distance: they had different motives for tweeting about the disaster, and different levels of engagement with it. We call these users “onlookers:” they tweet about a disaster, but are not directly affected by it. This paper analyzes onlooker behavior: it argues that onlookers who will send a few tweets can be predicted by their interests, while onlookers who will tweet heavily about it have few, if any, shared interests. We show that onlookers who primarily tweet about entertainment topics and news topics are likely to mention the disaster, yet send few tweets about it. On the other hand, onlookers who tweet substantially about a disaster after it happens are difficult to identify before the disaster occurs because they do not share common interests aside from the disaster.  Background Natural disasters often attract significant attention on Twitter, both by those affected and those who are distant. A substantial amount of research has explored how social media causes users to engage with political, social, and humanitarian problems; however, opinions on social media’s effectiveness—whether it causes users to donate money, stay informed, or participate in campaigns—are mixed. Some argue that displaying concern in social media is more about acquiring social capital than effecting change (Shulman, 2009; Gladwell, 2011; Morozov, 2012; Morozov, 2014), while a Pew Research Center survey finds that social media does create change (Raine et al., 2016). Additionally, many have argued that social media was important though not essential to protests in Egypt (Mazaid, 2011; Tufekci and Wilson, 2012) and other nations (Raine et al., 2016; Shirky, 2011). One analysis found that charities’ use of social media does not increase donations (Malcolm, 2016), while another finds that certain tweeting strategies do (Gasso Climent, 2015) although tweets may not raise awareness about the charity’s causes (Bravo and Hoffman-Goetz, 2015). Where all these studies concur is that social media enable a substantial amount of discourse about crises. The question we explore is how to predict how much attention Twitter users pay to crises: social media presents the opportunity for a user to send a single retweet about a disaster—as many One Direction fans did—or to sustain interest by following other users and sending many tweets about the event over a period of time. Additionally, there is little question that social media is useful for those directly affected by disasters. A substantial amount of research finds that social media helps first responders (Regalado et al., 2015; Dugdale et al., 2012; Omilion-Hodges and McClain, 2016; Burns, 2015; Xiao et al., 2015; Kaewkitipong et al., 2016; Meng et al., 2015; Madianou, 2015; Palen, 2008). In fact, specialized algorithms have been developed for that purpose (Pohl et al., 2013a; Pohl et al., 2013b; Platt et al., 2011). Little work examines users who tweet about disasters at a distance, however, despite the large numbers of such users. We examine these onlookers because they produce a large amount of the tweets about humanitarian crises.   Method We use quantitative text analysis to identify tweets about the earthquake, to cluster onlookers based on shared interests, and to derive a correlation between onlookers’ interests and the number of tweets they sent about the earthquake. This section outlines our methods. To attain a broad sample of onlookers, we gathered a dataset of over 5 million tweets sent by around 15,000 users in the three weeks following the Nepal earthquake. We harvested the data from the Twitter REST API by searching for any tweets that mentioned the word “Nepal” from April 24 to May 8. We randomly selected 15,000 users from this set and harvested all of their tweets sent between April 24, 2014 and May 5, 2015. We attempted to capture only English-speaking users to increase the likelihood that we would capture users not directly affected by the earthquake, but we still captured some users who tweeted in multiple languages. This left us with roughly 11,000 onlookers. To determine how many tweets a user had sent about the earthquake, we trained a Naive Bayesian classifier using MALLET (McCallum, 2002) on a set of 100 onlookers’ tweets (totaling about 30,000 tweets), marking them as either quake-related or not. We applied the trained classifier to the remainder of the dataset to count each user’s quake-related tweets. Spot checking showed this technique had acceptable accuracy. To find shared interests, we used Latent Dirichlet Allocation (Blei et al., 2003), treating all of a user’s tweets as one document. We ran LDA with MALLET with various numbers of topics, and settled on 80. These topics represented a broad span of themes: greetings, news, entertainment, technology, plus four topics directly related to the earthquake. We then looked for connections between onlookers by building an edge list of shared topics, creating a weighted edge between two onlookers if over 25% of both onlookers’ tweets consisted of a shared topic. The edge weight was the product of their affinities to that topic. Using Gephi (Bastian et al., 2009), we then ran a weighted Louvian modularity algorithm (Blondel et al., 2008) over this onlooker graph to generate communities of users.   Analysis This experiment resulted in 21 communities of onlookers being identified. The communities were labelled using the strongest topics in each.    ID Label Average Number of Tweets Users Average Quake-Related Tweets   0 Foreign language (Spanish) 419 882 9   1 Japanese Music 403 135 5   2 Greetings 326 199 5   3 Portuguese/Fifth Harmony 710 658 7   4 News Media 977 11 1   5 News Media 652 1312 22   6 News/Politics 600 1236 26   7 Indonesia 386 416 8   8 Foreign language (unknown) 495 312 5   9 Unclassified 372 589 25   10 Dera Sacha Sauda 1732 54 205   11 News about Russia 780 30 18   12 Shopping 1153 226 11   13 Greetings 476 1010 11   14 Greetings 439 1347 5   15 Science and animals 521 108 13   16 One Direction 388 1085 10   17 Foreign Language (Italian) 553 47 14   18 TV/Music 598 722 5   19 Music Videos 649 14 30   20 Nepal 393 748 125   After pruning out the foreign language communities in the dataset and some that were difficult to classify (Communities 0, 7-9, and 17), we can further group these onlookers into three broad interest groups: Casual Users (Communities 1-3, 12-14, 18, and 19), News and Pundits (4-6, and 13), and Engaged Users (20). We divided these subgroups based on the topics that were strongest in each, but these subgroups correlated with the number of quake-related tweets that each sent. They are described in the table below.   Category Proportion Quake-Related Tweets/Week Topic Affinities   Casual Onlookers 46% 3 Entertainment, greetings   News Onlookers 25% 6 News and politics   Engaged Onlookers 12% 10 Nepal    Casual Onlookers. Onlookers in these communities showed high activity but low engagement with the disaster, sending an average total of three quake-related tweets a week. Their primary topics of discussion were entertainment, or greetings and positive sentiments. This is the largest group.   News Onlookers. These accounts are either the accounts of professional news outlets or amateur pundits. We find low average quake-related tweets in this group as well: users sent an average of six relevant tweets per week. News outlets generally moved from one topic to another quickly, and pundits only sustained interest in the topics that appealed to them.   Engaged Onlookers. This group sent the most quake-related tweets of all users; the strongest LDA topics in this group were two “Nepal earthquake” topics. However, users in this community had few other topics in common with each other.  This breakdown suggests a model for predicting the number of tweets onlookers send about events. There will be roughly three categories of onlookers: Casual Onlookers, News Onlookers, and Engaged Onlookers. Casual Onlookers will consist of roughly 50% of onlookers, and will send only a few tweets over the first three weeks. Membership in this group is predicted by an interest in entertainment topics. The number of News Onlookers will be half the size of the Casual Onlookers, but they will be roughly twice as engaged. An onlookers’s affinity to this group will be predicted by a general interest in news. Finally, the Engaged Onlookers will send 10-20 times as many tweets as the Casual Onlookers, and will comprise slightly over 10% of onlookers. This group sends the most tweets about an event, but membership in this group cannot be predicted from their preexisting interests.   Conclusion We find that it is easy to predict shallow engagement with a disaster on Twitter, but difficult to anticipate sustained interest. Onlookers who tweet about entertainment are likely to pass on at least a few messages about donating money because entertainers are likely to post these messages, and fans are likely to retweet them. On the other hand, onlookers who tweet more about an event are likely to have preexisting interests that intersect with a particular aspect of the disaster, but the relevant interests are hard to predict because doing so would require knowing the nature of the disaster ahead of time. For example, to know the Hindu extremist would tweet about rumors of coerced conversions in Nepal, we would have to predict a crisis that would produce such rumors.  Additionally, we acknowledge that our model is derived from a single case study. As such, we treat it as provisional pending further experiments. We hope to confirm this model with future work.  ",
       "article_title":" What Do Boy Bands Tell Us About Disasters? The Social Media Response to the Nepal Earthquake  ",
       "authors":[
          {
             "given":"David Lawrence",
             "family":"Shepard",
             "affiliation":[
                {
                   "original_name":"UCLA, United States of America",
                   "normalized_name":null,
                   "country":"United States",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Takako",
             "family":"Hashimoto",
             "affiliation":[
                {
                   "original_name":"Chiba University of Commerce, Japan",
                   "normalized_name":"Chiba University of Commerce",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/02qn0vb48",
                      "GRID":"grid.443770.3"
                   }
                }
             ]
          },
          {
             "given":"Tetsuji",
             "family":"Kuboyama",
             "affiliation":[
                {
                   "original_name":"Gakushuin University, Japan",
                   "normalized_name":"Gakushuin University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/037s2db26",
                      "GRID":"grid.256169.f"
                   }
                }
             ]
          },
          {
             "given":"Kilho",
             "family":"Shin",
             "affiliation":[
                {
                   "original_name":"Hyogo University, Japan",
                   "normalized_name":"Hyogo University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/024pdem44",
                      "GRID":"grid.462295.e"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "natural language processing",
          "data mining / text mining",
          "English",
          "social media"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Background Online education has been advocated as the ultimate way of democratizing knowledge, but recent research indicates that there are reasons for concern. As the Allen & Seaman 2014 report underlines, 66% of higher education institutions report that online education remains critical to their long-term strategy while 74% of chief academic officers consider the learning outcomes for online courses to be ‘as good as or better’ than traditional face-to-face courses. But “despite this confidence in online education, researchers continue to report ‘compromised quality in online courses’ as one of the concerns of faculty, administration, and the general public” (Kidder, 2015; Selingo, 2014). In the landscape of online teaching, MOOCs (Massive Open Online Courses) have received much attention in both academic and popular publications  (Bayne and Ross, 2015; Bulfin et al., 2014; Clara and Barbera, 2013) despite the fact that they are not representative of the diverse modalities of online teaching. Siemens (2012) makes a useful distinction between xMOOCs (behaviorist MOOCs) and cMOOCs (connectivist MOOCs). The former emphasizes “a more traditional learning approach through video presentations and short quizzes and testing” with a focus on “knowledge duplication”, whereas the latter focus on “knowledge creation” (Siemens, 2012). Along the same lines, Ozturk recently reported that new variations of MOOCs have emerged becoming more market oriented “aligning with instructivist, cognitive, and behaviourist pedagogy” (Ozturk, 2015). Moreover, the financial model of the MOOCs raises questions about the audience for and motivations behind this method of teaching (Ozturk, 2015; Manjoo, 2015). Conscious of this present situation, the #dariahTeach project (funded by an Erasmus+ Strategic Partnership) is developing a  network based in seven partner countries exploring the production, dissemination, and promotion of high quality, dynamic, extensible, localisable, and integrated educational materials for the digital humanities specifically tailored for third level education. It is adopting a cMOOC philosophy which focuses on ‘creation, creativity, autonomy, and social networked learning’ (Siemens, 2012) to provide pedagogical content that can be easily integrated into diverse teaching and learning situations.  A key consideration in the design of the platform is interoperability between courses/modules (and units within those modules) since DH draws on a wealth of methods and tools from a variety of disciplines. Moreover, it is envisioned that these modules will be used beyond the DH community as the societal impact of a culturally-driven digital transition grows opening up new ways of collaborating on productive theory and critical thinking (Hayles, 2012). Thus a goal of #dariahTeach is to develop rich educational materials that 1) instructors in the growing number of Digital Humanities programmes can use as appropriate to their own institutional settings and learning outcomes; 2) instructors in other disciplines can draw on and; 3)  students who are not at institutions that have DH expertise can use to develop the skills and methods, as well as understand the theoretical basis, to engage in digital humanities and humanities research. The project team is currently developing the infrastructure and design of the modules based on the production of five modules: Introduction to Digital Humanities, Text Encoding, AudioVisual Media and Multimodal Literacies, Retrodigitizing Dictionaries, and Ontologies and Knowledge Management. This paper will present the results of preliminary research carried out through an extensive study of user requirements, as well as desk research on module and platform design informed by a workshop in Belgrade funded by the Digital Research Infrastructure for the Arts and Humanities (DARIAH) on developing open educational materials.    Analysis of User Requirements The design and the implementation of a successful platform-based learning environment  melds concepts from psychology, education, and human-computer interaction. Poor interface design can become a serious obstacle to the learning outcome, as it may slow the process down and impose cognitive obstacles. To this end, a qualitative analysis and interpretation of online teaching practices and recommendations in the DH domain and the elicitation of corresponding user requirements was based on a series of semi-structured interviews with experienced instructors of online courses within Europe. Findings of the user requirement process are a key component of the development of the #dariahTeach platform. These indicate that the platform needs to cater for the following needs: be adaptable to different learning methodologies; allow for persistent roles; provide an API or advanced forms of web services so that new unforeseen components can be added to the environment; support  ad hoc groupings and grouping of materials across modules and units; allow for both synchronous and asynchronous collaboration and communication and enable user customization.   Module Design #dariahTeach modules are designed as building blocks tailored to the exigencies of teaching situations in different educational and cultural contexts, allowing  for localization and adaptation (via translation, subtitles, domain-specific examples etc.). By offering examples of and encouraging further adaptation of training materials to specific linguistic/cultural contexts, #dariahTeach will dispel any notion that the use of ICT methods leads to abstract representations of culturally impoverished outputs.  It is important to stress two levels of translatability of module design: 1) translatability and adaptability of the language of instruction; and b) selectability, translatability and adaptability of primary sources and materials that are used in instruction. This means that an English-language module on Text Encoding, for instance, is localizable both in terms of the instructional narrative, as well as the kind of texts that are used to exemplify the taught principles and methods of text modeling: different genres (poetry, prose, drama) but also language (Latin, Greek, Serbian, Dutch etc.)  Our “Introduction to DH” module will also not attempt to impose a single pedagogical narrative on what is a constantly evolving and highly diverse, interdisciplinary field. Instead, our Introduction to DH is based on a micromodular, polycentric approach: a collection of mutually-linked, cross-referenced, metadata-rich short videos that shed light on DH as a community of practice from multiple perspectives without creating a false sense of uniformity.   Platform Design Modules will be made available via an online portal/ web application based on existing solutions. This paper will explore the decision tree in adopting a solution including whether to use a well-established Content Management Systems (eg Drupal, WordPress, Joomla) with Learning Management System plugins and appropriate customizations or the use of a customizable Learning Management System (such as  Moodle or Blackboard). Considerations feeding into the decision tree include the platform being open source, freely available, well documented and customizable with plugin development support;   support for multilinguality; an embedded xml editor; collaboration and interaction functionalities (eg chats, forums and wikis); test and assessment functionalities; extended search functionalities for available metadata (mapped to Dublin Core and LOM to facilitate sharing and support interoperability and reusability (Roy et al., 2010); and copyright attribution and licence management functionalities.    Conclusion The paper will conclude with longer-term prospects for the project. Oversight of #dariahTeach will be maintained after the grant has ended by a General Editor and Editorial Board under the oversight of the DARAH’s Research and Education Competency Centre.  ",
       "article_title":"#dariahTeach: online teaching, MOOCs and beyond",
       "authors":[
          {
             "given":"Susan",
             "family":"Schreibman",
             "affiliation":[
                {
                   "original_name":"Maynooth University, Ireland",
                   "normalized_name":"National University of Ireland, Maynooth",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/048nfjm95",
                      "GRID":"grid.95004.38"
                   }
                }
             ]
          },
          {
             "given":"Agiatis",
             "family":"Benardou",
             "affiliation":[
                {
                   "original_name":"Athena Research and Innovation Center in Information Communication & Knowledge Technologies, Greece",
                   "normalized_name":null,
                   "country":"Greece",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Claire",
             "family":"Clivaz",
             "affiliation":[
                {
                   "original_name":"Swiss Institute of Bioinformatics, Switzerland; University of Lausanne, Switzerland",
                   "normalized_name":"University of Lausanne",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/019whta54",
                      "GRID":"grid.9851.5"
                   }
                }
             ]
          },
          {
             "given":"Matej",
             "family":"Durco",
             "affiliation":[
                {
                   "original_name":"Oesterreichische Akademie der Wissenschaften, Austria",
                   "normalized_name":null,
                   "country":"Austria",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Marianne",
             "family":"Huang",
             "affiliation":[
                {
                   "original_name":"Aarhus Universitet, Danemark",
                   "normalized_name":"Aarhus University",
                   "country":"Denmark",
                   "identifiers":{
                      "ror":"https://ror.org/01aj84f44",
                      "GRID":"grid.7048.b"
                   }
                }
             ]
          },
          {
             "given":"Eliza",
             "family":"Papaki",
             "affiliation":[
                {
                   "original_name":"Athena Research and Innovation Center in Information Communication & Knowledge Technologies, Greece",
                   "normalized_name":null,
                   "country":"Greece",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Stef",
             "family":"Scagliola",
             "affiliation":[
                {
                   "original_name":"Erasmus Universiteit Rotterdam, Netherlands",
                   "normalized_name":"Erasmus University Rotterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/057w15z03",
                      "GRID":"grid.6906.9"
                   }
                }
             ]
          },
          {
             "given":"Toma",
             "family":"Tasovac",
             "affiliation":[
                {
                   "original_name":"Belgrade Center for Digital Humanities, Serbia",
                   "normalized_name":null,
                   "country":"Serbia",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Tanja",
             "family":"Wissik",
             "affiliation":[
                {
                   "original_name":"Oesterreichische Akademie der Wissenschaften, Austria",
                   "normalized_name":null,
                   "country":"Austria",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016",
       "keywords":[
          "semantic web",
          "digital humanities - multilinguality",
          "project design, organization, management",
          "teaching and pedagogy",
          "digital humanities - pedagogy and curriculum",
          "multilingual / multicultural approaches",
          "encoding - theory and practice",
          "ontologies",
          "digitisation, resource creation, and discovery",
          "scholarly editing",
          "digitisation - theory and practice",
          "knowledge representation",
          "English",
          "audio, video, multimedia",
          "interface and user experience design"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Writing in  Literary and Linguistic Computing, Julianne Nyhan et al argue that “without a better understanding—a more appropriate term might be “body of interpretations”—of the near and distant history of computing in the humanities, we are condemned to repeat the revolutionary trope  ad infinitum.” (Nyhan, Flinn, and Welsh, 2013). Willard McCarty, amplifying this, writes that “rather than hypnotizing ourselves with supposedly unprecedented marvels, we must learn to see computing in its historical and social contexts, and so be able to ground modelling in something larger than itself. For computing to be  of the humanities as well as  in them, we must get beyond catalogues, chronologies, and heroic firsts to a genuine history. There are none yet.” (McCarty, 2008).   Susan Hockey wrote in  Companion to Digital Humanities that “humanities computing has a very well-known beginning,” by which she means the decades-long collaboration between Father Roberto Busa and IBM to create a concordance of the work of Thomas Aquinas (Hockey, 2004). This is the heroic first, which Busa, writing in the forward of that same volume, summed up with admirable brevity: “During the World War II, between 1941 and 1946, I began to look for machines for the automation of the linguistic analysis of written texts. I found them, in 1949, at IBM in New York City.” _ This narrative is familiar to many of those working in digital humanities today, and has become openly accepted as the standard historical background for, first, humanities computing and, subsequently, the digital humanities writ large.   This presentation aims to upset this easy narrative by re-situating the history of one type of digital humanities project—Early English Books Online—as one chapter in an overall history of a technological humanities. That history—the history of a technological humanities—the story of how academics have deployed technology to better understand human creations, especially in textual form—or to understand and explore texts—did not begin in 1949. The creation of digital humanities, radiating outward from those early years, is surely part of the larger story of how technology and text have come together and drifted apart over many centuries. I claim that there is a great deal more continuity in the apparatuses, in the knowledge infrastructure of the humanities than we put forward in our “official” histories. In our search for a neat disciplinary history, we elide technology as a whole with the digital electronic computer. Busa’s project likely does represent the beginning of one type of computational textual processing. It bears remembering, however, that his goal was to create a concordance, a type of reference tool and interface in existence in Western Europe since at least the 13th century. What is the history of  this type of textual processing in the intervening six hundred years?   Instead of a history of tools for textual work beginning with the rise of humanities computing and moving forward to the present day, I hope to juxtapose a different narrative, one that troubles the rhetoric of a textual digital humanities that arises from a clear break with what came before. I hope to, perhaps polemically, test the boundaries of histories of digital humanities by considering an equally technologically sophisticated pre-digital humanities. Such a reframing opens many avenues of inquiry, including a consideration of Linked Open Data in the context of cooperative cataloguing practices from the 19th & 20th centuries, or contemporary textual analysis tools such as Voyant alongside the imposing machinery of an electromechanical Hinman Collator. This presentation, however, will highlight particularly those technologies of textual reproduction developed prior to the oft-quoted originary moment of 1949. Drawing on the history of Early English Books Online (EEBO), I argue that while a  computational humanities may indeed be limited to the last half-century, the  technological humanities—in both materialist and cultural senses—have a much longer history.   ProQuest introduces the resource on their front page:  From the first book printed in English by William Caxton, through the age of Spenser and Shakespeare and the tumult of the English Civil War, Early English Books Online (EEBO) will contain over 125,000 titles listed in Pollard and Redgrave's Short-Title Catalogue (1475-1640), Wing's Short-Title Catalogue (1641-1700), the Thomason Tracts (1640-1661), and the Early English Tract Supplement - all in full digital facsimile from the Early English Books microfilm collection. _   This text was current as of summer 2015 and is available in cached form. Since that time, Proquest has altered the front page description of EEBO to the following:  Early English Books Online (EEBO) contains digital facsimile page images of virtually every work printed in England, Ireland, Scotland, Wales and British North America and works in English printed elsewhere from 1473-1700 - from the first book printed in English by William Caxton, through the age of Spenser and Shakespeare and the tumult of the English Civil War.  Strangely, this newer version eliminates reference to the Early English Books microfilm Collection, as well as collapsing a number of distinct early modern collections of content into what might be called the EEBO brand. See the current version of <http://eebo.chadwyck.com/home> and a cached version <https://web.archive.org/web/20150905141338/http://eebo.chadwyck.com/home> from September 2015.   In practice, this means that users are able to view the metadata for a given text; view page images of the original, early modern books in TIFF or PDF format; and, where available, view a full text transcription of the volume that are derived from the EEBO - Text Creation Partnership. Efforts to microfilm early English books began in 1931, intensified as World War II loomed, and continue today. Digital images of these microfilmed documents were made (and are still being made) available online first in 1998. The printed  Short Title Catalogue (itself published in 1926) has determined what objects were photographed and, subsequently, scanned and put online _(EEBO).  The history of EEBO crosses multiple media, was directly impacted by global war, involves private companies and public universities, and is both analog and digital. To bracket EEBO (or EEBO-TCP) as a only a digital project impoverishes our understanding of how digital technologies have impacted the reproduction, preservation, and use of texts in humanistic scholarship. To write the full history of the early English books project, Early English Books Online, the Early English Books Online Text Creation Partnership is to engage in an act of disciplinary archaeology, one that forces digital humanists to grapple with the pre-digital origins and ideologies that inflect contemporary digital resources undergirding scholarship. EEBO is a microcosm through which one body of interpretations of digital humanities might be seen.  As much as it is a history, this presentation is also engaged in answering claims by Alan Liu and Tara McPherson, amongst others, that digital humanities has chosen disciplinarily to disengage from socio-critical questions. _   For work by Liu on this topic, see “Where is the Cultural Criticism in the Digital Humanites?,” published in  Debates in the Digital Humaniteies, ed Matthew K. Gold <http://dhdebates.gc.cuny.edu/debates/text/20>. For McPherson work on UNIX and ideologies of race, see “Why are the Digital Humanities So White? or Thinking the Histories of Race and Computation” in the same volume <http://dhdebates.gc.cuny.edu/debates/text/29>.   Thinking through the history of EEBO is one way to approach the digital humanities as a discipline tied to war-driven technological development; the uneasy relationships between private-sector providers and our shared cultural heritage; or the many varieties of labour that are imbricated within the knowledge infrastructures humanists use day in and day out. _   It is worth noting that one of the very few publications to deal with the EEBO set of projects in this way was published in  Literary and Linguistic Computing (now  Digital Scholarship in the Humanities). See Diana Kichuk, “Metamorphosis: Remediation in  Early English Books Online (EEBO)” (2007) 22 (3): 291-303. DOI: http://dx.doi.org/10.1093/llc/fqm018. Kichuk’s efforts have helped establish a historical framework for this discussion; this presentation seeks to contextualise the facts she has brought together and extend their relevance into discourses about DH as a whole.   Blending media analysis, historical perspectives, and in-depth knowledge of humanities research tools, I hope to question the boundaries of what we consider digital humanities to be, how we write our histories, and how we move forward.  ",
       "article_title":"Early English Books in Context: Towards a History of the Technological Humanities",
       "authors":[
          {
             "given":"Daniel James",
             "family":"Powell",
             "affiliation":[
                {
                   "original_name":"University of Victoria, Canada",
                   "normalized_name":"University of Victoria",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04s5mat29",
                      "GRID":"grid.143640.4"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-08",
       "keywords":[
          "renaissance studies",
          "history of Humanities Computing/Digital Humanities",
          "media studies",
          "digitisation - theory and practice",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The emergence and growing popularity of Linked Open Data (LOD) offers researchers a new range of possibilities when it comes to publishing datasets online (Hyvönen 2012, Oomen et al 2012); indeed not only does the success of LOD greatly facilitate the process of making scholarly data accessible and to a wider community but it also permits the enrichment of individual datasets by linking them to the other datasets available on the so called Linked Open Data Cloud.   The advantages of Linked Open Data for teachers, academics and students in the humanities are obvious and are indeed manifold. However there is currently a paucity of linked open datasets in fields such as philology and literary studies, and in particular of datasets that deal with classical languages such as ancient Greek, Sanskrit, and Latin. This seems strange given the rich abundance of surviving works, of both a religious and secular character, that exist in those languages. A salient consideration here relates to the fact that even when such works have been digitised and made available in a format such as TEI-XML, a format which renders the structure and content of such texts more amenable to computer processing, the conversion of these resources into the Resource Data Framework (RDF), the standardised data model that underpins the Semantic Web, is not always straightforward.    In this article we describe ongoing work in the conversion of an important 19th century Ancient Greek resource the Liddell-Scott-Jones Lexicon, into RDF, part of a wider program of work that has been recently initiated at CNR-ILC in converting historical lexicons in languages such as Greek, Latin and Arabic into Linked Open Data.     Background The Liddell-Scott-Jones lexicon (LSJ), or to give it its original title “A Greek-English lexicon”, is a bilingual ancient Greek-English dictionary which since its first edition was published in the mid-nineteenth century has come to be regarded as amongst the most authoritative of modern day lexicographic resources dealing with the ancient Greek language, indeed it has the reputation of a standard in the field. As a result of its popularity the LSJ has been made available in a number of different versions differing in terms of the number of entries and the amount of data which they contain. For the work described in this paper we are using an abridged version of the LSJ which was originally published as “An Intermediate Greek-English lexicon,” but which is more colloquially known as the “Middle Liddell” (ML).  Fig. 1 shows the lexical entry for the adjective ἀληθής  (alēthēs) in the ML. Entries in the ML are structured into different nested (sub-)senses, and each of these senses contain references (usually just the name of an author) attesting the use of the word as described in the sense’s gloss.       Fig 1. An entry from the “Middle Liddell”   There were a number of motivations for choosing the LSJ as a starting point of our work into converting legacy lexical resources into linked data: aside of course from the question of its historical importance and continuing influence in the field of philology. Firstly we felt that given the lack of ancient Greek lexical resources in linked open data -- at the time of writing the Linguistic Linked Open Data cloud (Chiarcos et al 2011), that part of the LOD cloud that deals with linguistic data, contains no ancient Greek datasets -- there was an obvious necessity to ensure a presence on the cloud for a language that is absolutely foundational to the history of Western civilisation. Additionally there was also the challenge of converting a legacy resource like the LSJ which in its published form, and even in an abridged version like the ML, manages to condense a significant amount of lexical information in a relatively short amount of text, into linked data. In order to represent this information in the RDF model, and to stay close to the spirit of the Linked Open Data movement, a lot of what was implicit in the original text had to be teased out and rendered explicit. Finally, one very important practical reason for choosing the ML was the fact that the conversion of the ML into XML using the TEI dictionary guidelines had already been carried out and made freely available under a creative commons license by the Perseus project (Crane et al 2013). This obviously saved us the trouble of digitizing the text ourselves and meant that we could work from a source file that was already annotated for lexical entries, senses, translations, etc. In Fig 2 below we present the TEI-XML encoding of the ML entry for ἀληθής from the Perseus XML version of the ML which we used as our source dataset.    Fig 2. The TEI-XML encoding of the ML entry for ἀληθής.   The TEI-XML encoding for each entry already contains most of the information which we wish to represent in RDF marked up, and so the actual processing of the dataset was fairly straightforward.  The part of the conversion which, however, did call for some thought was the use of the lemon model to structure the RDF translation.     Translating the Middle Liddell in RDF using lemon    For the conversion of the TEI-XML version of the ML we decided to use the lemon model for publishing lexicons in RDF (McCrae et al 2011, McCrae et al 2012). lemon has by now become a de facto standard for converting lexicographic resources into RDF and has been used to convert a number of important lexical resources such as Wordnet (McCrae 2014), UBY (Eckle-Kohler et al 2014), Wiktionary (McCrae et al 2012), and Parole/Simple/Clips (Del Gratta et al 2015).  The Linked Open Data movement emphasises the re-use of general vocabularies and models in order to ensure semantic interoperability between datasets. And so given the widespread use of lemon in converting lexical resources into RDF, and given the lack of a more specific alternative specifically tailored for lexicographic resources like the ML, we decided to use it as the framework for our conversion. However using lemon for the conversion has not been without its challenges. One of the primary difficulties rests in the fact that lemon was originally intended as an onomasiological model, that it is, it was designed with the perspective in mind of enriching an already existing ontological or conceptual resource with linguistic information (Cimiano et al 2013). However in our case we started out with a very rich lexical resource but without any particular pre-ordained ontological or conceptual datasets in mind to which to link it. In fact we are not currently using the lemon:reference relation to link our dataset to others. The ML in particular and the LSJ more generally represent lexical resources that have a specific and relatively complex way of encoding information and that contain a lot of philological and historical data alongside or in addition to “pure” semantic information. Therefore in in order to ensure a faithful translation we had to define a number of new classes and relations in addition to those in lemon. In what follows below we will briefly describe (most of) the additional classes and properties which were introduced in order to model the ML and which together make up the lemonLSJ module. Fig. 3 below is a diagram showing classes and properties in the lemonLSJ module and their relation to some of the main classes. The new class lemonLSJ:Gloss represents the written text associated with each sense; the object relation lemonLSJ:gloss then links elements of this class to lemon senses. The lemonLSJ:usage relation links a sense to an ontological resource describing where that sense was used, e.g., by linking to an author or work where that sense can be found.We made the decision not to include CTS URNs describing particular works or copora, since this data is usually not included in the LM. This data is however available in the Perseus version of the LSJ (though not the ML) and we plan to include it when we come to convert the LSJ into RDF. We have also added the relations lemonLSJ:senseChild, and lemonLSJ:senseSibling in order to represent the nesting of subsenses in ML entries.    Fig. 3 The lemonLSJ module.   In Fig 4 we present an excerpt of the lemonLSJ RDF-Turtle encoding of the ML entry for ἀληθής with only two of the attached senses represented.  Note that we have linked the second of the word senses in Fig 4 to instances in the VIAF dataset (in this case the entries for Herodotus and Homer).    Fig. 4 Excerpt of the RDF encoding of an ML entry.   The diagram in Fig. 5 shows how the nesting of senses is treated in the RDF encoding.    Fig. 5 Sense diagram.     Conclusions and future work In this paper we have briefly described ongoing work in the conversion of an ancient Greek-English lexicon into RDF. Currently we are manually checking the RDF triples resulting from our conversion scripts and we also plan to additionally link the senses to Princeton Wordnet synsets by checking similarity of the glosses to synset glosses. The resulting dataset will soon be made available as an RDF dump together with other lexical resources of ILC CNR.   Acknowledgments First of all we would like to thank Perseus project for converting the data and for making it freely available.  Thanks also to the organisers of the mylider 2015  Summer Datathon, during which some of this work was carried out, for their advice.  ",
       "article_title":" Converting the Liddell Scott Greek-English Lexicon into Linked Open Data using lemon  ",
       "authors":[
          {
             "given":"Fahad",
             "family":"Khan",
             "affiliation":[
                {
                   "original_name":"Istituto di Linguistica Computazionale \"A. Zampolli\", Italy",
                   "normalized_name":"Institute for Computational Linguistics “A. Zampolli”",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/028g3pe33",
                      "GRID":"grid.503055.6"
                   }
                }
             ]
          },
          {
             "given":"Francesca",
             "family":"Frontini",
             "affiliation":[
                {
                   "original_name":"Istituto di Linguistica Computazionale \"A. Zampolli\", Italy",
                   "normalized_name":"Institute for Computational Linguistics “A. Zampolli”",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/028g3pe33",
                      "GRID":"grid.503055.6"
                   }
                }
             ]
          },
          {
             "given":"Federico",
             "family":"Boschetti",
             "affiliation":[
                {
                   "original_name":"Istituto di Linguistica Computazionale \"A. Zampolli\", Italy",
                   "normalized_name":"Institute for Computational Linguistics “A. Zampolli”",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/028g3pe33",
                      "GRID":"grid.503055.6"
                   }
                }
             ]
          },
          {
             "given":"Monica",
             "family":"Monachini",
             "affiliation":[
                {
                   "original_name":"Istituto di Linguistica Computazionale \"A. Zampolli\", Italy",
                   "normalized_name":"Institute for Computational Linguistics “A. Zampolli”",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/028g3pe33",
                      "GRID":"grid.503055.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "classical studies",
          "philology",
          "ontologies",
          "digitisation, resource creation, and discovery",
          "lexicography",
          "linguistics",
          "English",
          "semantic web"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction  3D modeling is transforming scholarship and teaching. In 2004, when Taliban militia destroyed the Buddhas of Bamiyan (one, the tallest Buddha in the world), a team of researchers from the Swiss Federal Institute of Technology responded by generating 3D models of these lost Buddhas (Grün et al., 2004). Indeed, 3D and virtual reality (VR) have sparked scholarly interest because VR projects, such as the Buddhas and Rome Reborn (an international effort to generate a virtual surrogate of the ancient city), allow for the modeling of inaccessible or lost historic spaces and objects and the interactive testing of scholarly hypotheses (Dylla et al., 2010).  Now, with advances in digital imaging and replication, 3D models have proliferated. They can be easily downloaded from sites such as SketchFab, Smithsonian X3D, and NASA 3D Resources. Moreover, scholars can publish them in a new journal:  Digital Applications in Archaeology and Cultural Heritage. However, this proliferation raises questions about best practices for viewing, archiving, and interacting with these complex digital assets. The primary interface for 3D remains the flat, relatively small, computer screen—miniscule when compared to a 53-meter Buddha.   One solution is to provide an immersive experience through VR, enabling viewers to share space with 3D artifacts. Such an interface, however, presents further problems. VR systems tend to follow one of two models: 1. The model of Rome Reborn, in which a virtual system houses one surrogate of a physical space, or 2. A laboratory, such as Stanford University’s CAVE system, in which sophisticated and costly VR equipment is employed for research. Can a virtual ecosystem be developed that not only makes uploading, hosting, and viewing VR assets easy but also is cost efficient and yet provides a robust, flexible, and accessible environment to meet divergent needs across a university and beyond? In this presentation, we will discuss our answers to these questions derived from building a VR ecosystem housed in the library of the University of Oklahoma: the Oklahoma Virtual Academic Laboratory (OVAL). OVAL is a fully functioning VR environment. It is designed to deliver ease in uploading and engaging 3D assets, especially for non-technical users. Through digital immersion, multiple scholars and students simultaneously encounter VR assets, moving through and around and rotating and resizing them. Such engagement represents a paradigmatic shift in the computer interface: scholars and students are no longer limited by a flat monitor and instead share the virtual space with their objects of study.  Furthermore, OVAL encourages enhanced experiential learning (Hermon and Kalisperis, 2011; 60-61). It gives students and scholars opportunities to engage digital materials beyond their normal reach (such as interacting with atomic structures or cultural heritage artifacts). And, when partnered with museums, medical centers and other stakeholders, OVAL makes VR accessible to a wide scope of people, institutions, and industries. Thereby, OVAL offers a new model of accessible VR.    Challenges Building a VR ecosystem for a whole university poses daunting challenges. For OVAL, the overriding hurdle was cost. To accommodate scholars and students across the disciplines, building multiple, multi-million dollar VR labs was not feasible. Therefore, cost constantly reigned-in decisions but rarely reigned-in performance.  There were also challenges for choosing compatible hardware and software. To keep costs reasonable and minimize spatial requirements, we opted to have users seated while immersed in OVAL, with head, upper body, and hands tracked and imported into the VR environment. This reduces major challenges to selecting a software platform to develop the VR environment, graphics processing unit (GPU), head mounted display (and its software), hand-gesture sensor (and its software), computer, and chair assembly. When full-body sensing hardware and software become available at a reasonable price, we will explore incorporating them into OVAL.  For building the VR environment, perhaps the most important choice is selecting the software platform. Currently, game engines (software platforms for generating 2D and immersive 3D environments) are the best option. To select an appropriate game engine, we established five criteria:  Minimally priced or free during development of VR environment Compatible with multiple platforms, including desktop, mobile, and Web Compatible with multiple headsets, such as Oculus Rift, HTC Vive, Google Cardboard, Gear VR, and Microsoft HoloLens Active and robust developer community Extensive online documentation    Our solution Our criteria limited our choices to two main video game engines: Unity3D and Unreal. Although, since our initial choice, a number of major companies (Google, Amazon, etc.) have developed game engines, we still champion Unity3D because of its low cost and robust developer community, which attests to its capability and compatibility. Part of Unity3D’s appeal is one of our main criteria: strong cross-platform compatible. Currently, it is compatible with 21 different operating systems, including Apple, Windows, Linux, and a variety of mobile devices. Furthermore, Unity3D provides strong online documentation, including tutorials.  For software sub-systems, choices had their complexities, and we will discuss them more fully in the presentation. They included Photon Unity Networking for networking the headsets; Oculus runtime software to support a camera with two “eyes”; and the LeapMotion SDK for hand-tracked interactions. These choices allow OVAL to preserve embodied interaction with hand and upper body tracking (leaning in produces a closer look at an object, and hand movements control features such as scaling and rotating objects) (Shapiro, 2014). Selecting hardware was less complicated. Available headsets are surprisingly limited. Our only real choice was Oculus Rift. Another headset, the HTC Vive, is set for release soon. Most headsets are developed and sold as part of a complete VR system.  For the chair-assembly, a unique on-campus resource simplified our choice. The University of Oklahoma houses a high-powered physics fabrication lab. We worked with them to develop a custom railed-chair assembly (ergonomically designed for a 360° range of motion). This railed-chair allows the computer to reside under the chair and out of the way. For a robust virtual environment, the computer contains a GeForce GTX 980 graphics card. It delivers a 75-frames/second refresh-rate (the human eye generally resolves 25 frames/second), insuring an instantaneous visual experience when manipulating 3D objects or when turning one’s head.  Finally, by integrating networking software into OVAL, a shared VR experience can occur across a range of clients. All changes made on a master workstation—including scale, rotation, lighting, and background imagery—are immediately transmitted to all co-participants, regardless of their physical location. In a classroom environment, for example, this means that students automatically see what the teacher sees. But this also allows OVAL to become a worldwide network. To facilitate such a network, all 3D models are uploaded via a public Dropbox, which immediately syncs with all OVAL clients. This means that all uploaded 3D asset are available to all OVAL clients. For a shared VR experience, each client only needs a short set of instructions concerning file names and how to manipulate them during a session.    Research and teaching In our presentation, we will also discuss ongoing uses of OVAL at the University of Oklahoma and explore their implications. Despite its recent completion, OVAL has already had extensive use. Undergraduate biology students have analyzed the atomic structure of hemoglobin and oxyhemoglobin. Architecture faculty has analyzed student projects for unseen flaws pertaining to safety and accessibility of interior spaces. The Sam Noble Museum of Natural History has uploaded their recently discovered  Aquilops Americanus skull into the OVAL system for curators and researchers. Art History faculty has begun analyzing sculpturally significant 3D scans for preserving what was once ephemeral art. A budding partnership with the Medical Imaging Facility has demonstrated how CT-to-OVAL workflows facilitate mammographic research. Finally, Bill Endres has begun to develop guided, immersive tours of the St Chad Gospels, an 8 th-century illuminated manuscript.     Conclusion  The rapid production of 3D models makes having VR systems available for their viewing a pressing concern. 3D models of massive structures, such as the large Buddhas of Bamiyan, highlight the limitations of interacting through a computer screen. OVAL provides one cost-efficient solution. In our next phase, we plan to add collaborators and make OVAL available. We are also interested in hosting 3D assets in an archive-quality database. However, the most effective and efficient means of doing these has yet to be determined. We are looking forward to presenting at DH 2016 and conversing about possibilities for OVAL and the wide-ranging opportunities for research and teaching through VR.  ",
       "article_title":"OVAL: A Virtual Ecosystem for Immersive Scholarship and Teaching",
       "authors":[
          {
             "given":"Bill",
             "family":"Endres",
             "affiliation":[
                {
                   "original_name":"University of Oklahoma, United States of America",
                   "normalized_name":"University of Oklahoma",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02aqsxs83",
                      "GRID":"grid.266900.b"
                   }
                }
             ]
          },
          {
             "given":"Matthew",
             "family":"Cook",
             "affiliation":[
                {
                   "original_name":"University of Oklahoma, United States of America",
                   "normalized_name":"University of Oklahoma",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02aqsxs83",
                      "GRID":"grid.266900.b"
                   }
                }
             ]
          },
          {
             "given":"Will",
             "family":"Kurlinkus",
             "affiliation":[
                {
                   "original_name":"University of Oklahoma, United States of America",
                   "normalized_name":"University of Oklahoma",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02aqsxs83",
                      "GRID":"grid.266900.b"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "English",
          "virtual and augmented reality"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Can the digital humanities offer an alternative to traditional modes of canon formation? This paper argues that quantitative methods can both enrich our understanding of the way canons are formed and help us create more flexible and interactive alternatives to traditional canons. Over the last year, I’ve built a tool for generating dynamic literary canons and placed it in public beta at  http://www.metacanon.org. Metacanon measures the canonicity of literary works by calculating the number of times they are mentioned in scholarly journals and using an algorithm to assign a uniform score to each work based on this data. While this certainly does not amount to a measure of aesthetic value or \"greatness,\" it does offer a concise snapshot of the body of literary works that are most discussed by scholars. Currently, it only covers twentieth century American fiction, but future versions will be expanded to include other genres, periods, and nationalities. For scholars, this will provide a tool for quickly measuring the relative centrality or obscurity of particular works as well as a tool for measuring how canons change over time. For students and the general public, it will offer a far more inclusive, flexible, and interactive alternative to the fairly predictable greatest books lists that currently act as arbiters of literary value outside of academic circles.   In the wake of Pierre Bourdieu’s Distinction (1979), literary studies has developed a nuanced critical apparatus for rethinking the role played by the canon and canonicity in the perpetuation of cultural capital. Our current scholarly common sense insists that far from reflecting aesthetic value, canons actually create this value socially, often thereby reinforcing hegemonic cultural values and hierarchies. And yet, even as we know this, the actual collection of texts that is consistently taught, written about, and by extension canonized remains relatively stable. By examining the frequency with which particular works are mentioned in various scholarly networks, Metacanon creates an accessible representation of this trend and in doing so introduces a greater level of transparency into the dominant allocation of literary values. In doing so, this project is similar in some respects to work being done by Mark Algee-Hewitt and Mark McGurl at the Stanfard Literary Lab, although using different means and with different ends in mind. Whereas Algee-Hewitt and McGurl have produced a master list of 350 twentieth century novels by combining several “found lists” supplemented by a survey of scholars working in the field of postcolonial literature, Metacanon uses an approach that takes advantage of a wider array of harvested data drawn from thousands of journal articles. This reflects the very different goals of this project. Rather than producing a necessarily limited corpus suitable for datamining, Metacanon is primarily intended as an exhaustive but flexible representation of the canon. This allows for a much larger interactive list, but I‘ve also found that the Metacanon list is much more diverse in terms of gender (and likely ethnicity) than the McGurl and Algee-Hewitt list, even though they consciously aimed to create a corpus that would be more representative than most standard lists. What this indicates is that although most publicly available “greatest books lists” tend to over-represent white men in their construction of literary value, scholars themselves tend to work on a much more diverse array of literary texts. In other words, there is already a working canon in existence that is much more diverse and representative than the standard lists and surprisingly more so even than Stanford’s intentionally varied list; it’s just that this working canon isn’t generally available in an accessible, objectified form. Metacanon takes the first steps toward producing this more accessible form, even as it integrates flexibility and transparency into its framework. The current version of Metacanon (0.6) is limited to twentieth century American fiction. As such, however, it is the most comprehensive relational database of American fiction from this period. Of course, there are more extensive listings of American literature available (for example, the Chadwyck-Healey Bibliography of American Literature). However these offer no way to easily distinguish between highly canonical works and more obscure works. In essence this forces readers looking for a definitive list of American fiction to choose between the unwieldiness of comprehensive bibliographies and the partiality of much shorter “greatest books” lists and standard field lists. What makes Metacanon unique is that it harnesses digital technology in order to offer both the expansiveness of a comprehensive bibliography while at the same time measuring the relative centrality or obscurity of each particular work.  This digital framework also allows users to become active participants in the construction of the canon rather than merely passive recipients. For example, one user might choose to see a list of the most canonical novels published between 1970 and 1979, or even more interestingly the most canonical novels of the 70s according only to data from the 80s or 90s. Another user might choose to see a list consisting only of science fiction written by women. A third user might choose to alter the algorithm to calculate canonicity based only on citations in a single journal. Contrary to the widespread fear that digital or quantitative approaches to literature are fundamentally opposed to nuance and flexibility, Metacanon demonstrates that this need not be the case, at least in so far as questions of canon formation are concerned. While most of what is written above concerns Metacanon’s value as a public humanities initiative and as an aid to students, this project has growing implications for literary scholarship more broadly. One of the most fascinating lines of inquiry in the digital humanities today is the use of quantitative textual analysis to trace the connections between literary form and reception over time. For example, scholars like Richard Jean So, Hoyt Long, Ted Underwood, and many others have used digital text mining to demonstrate relationships between particular formal features of literary texts and the social categories that govern their movement through the world, such as attributed aesthetic value and genre. As the precision of Metacanon’s measurement of canonicity improves, researchers could use this data along similar lines to ascertain connections between literary form and canonicity over time. ",
       "article_title":"Metacanon.org: Digital Humanities and the Canon",
       "authors":[
          {
             "given":"Nathaniel Allen",
             "family":"Conroy",
             "affiliation":[
                {
                   "original_name":"Brown University, United States of America",
                   "normalized_name":"Brown University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05gq02987",
                      "GRID":"grid.40263.33"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-01",
       "keywords":[
          "digital humanities - pedagogy and curriculum",
          "literary studies",
          "concording and indexing",
          "english studies",
          "databases & dbms",
          "corpora and corpus activities",
          "other",
          "data mining / text mining",
          "English",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Since the 1980s, under pressure of the definite ‘disappearance’ of the eye witness generations of WWII, a wide variety of initiatives has been undertaken to capture eye witness memories for the future. Multiple oral history collections have been created throughout the western world, in which ten thousands of interviews and life stories have been preserved on audio and video. Parallel to the quest for individual war memories to collect and preserve, there has been an increasing effort to transmit these memories onto younger generations. In my dissertation, I have referred to this process as the pedagogization of memory, with which I point at the transmission of WWII memories onto younger generations as a crucial way of giving meaning to the past, and, therewith, in creating and sustaining identities (Hogervorst, 2010, Proske 2012, Macdonald, 2013; 200). Both practices respond to, and express, a perceived shift from war memory towards war history, although in different contexts of historical culture (Erll 2011; Assmann, 1999, 2006). It is through the digital revolution that, at the beginning of the 21st century, both of these practices have become intertwined; the shift from memory to history is accompanied by, or expressed by, a shift from collecting and preserving to disclosing digitized interviews for a wider audience, also for education (Bothe/Lücke, 2013; Barricelli, 2009, 2010). My paper presents the first findings of a case study of my postdoc research project, that addresses the central question how online portals to digitized WWII eye witness testimonies are used in educational contexts – both formal education in history or civics, and heritage educational projects. The case study focuses on secondary school history teachers’ conceptions of digitized video testimonies as educational resource. Therefore, a group was composed of teachers interested in WWII, which was asked to explore different online interview collections. Through participatory observation and individual interviews with participating teachers, data was collected on expectations, desires and experiences regarding the use of digitized video interviews in history classrooms. The analysis focused on two different themes: (1) Participants’ conceptions of differences and similarities between live and digitized testimonies as educational resource; (2) Their experiences using specific online portals to interview collections. The latter will be the focus of my paper.  Participants were asked to explore two different online portals to different interview collections: Getuigenverhalen.nl (‘eyewitness stories’) and IWitness. Getuigenverhalen.nl is a Dutch portal hosted by the Netherlands Institute of War- Holocaust- and Genocidestudies, giving access to about 500 quite recently conducted video interviews (2007-2010), all in Dutch. The interviews address multiple WWII related topics such as resistance, daily life, persecution, and forced labor. The majority of these interviews is searchable at fragment-level as the transcripts and time-based key words that have been attributed are indexed and aligned with the video. The other portal is IWitness, the worldwide educational program of the USC Visual History Archive in Los Angeles. Through IWitness, in which a selected part (1,500 interviews) of the Shoah interview collection is made available online in an open, but supervised community of teachers and students. Twelve of these interviews are in Dutch. The video interviews can be watched and searched. Moreover, a video-editing tool enables users to select, annotate, and share video fragments, or to combine them with other fragments, photographs or information from the built-in encyclopedia. Because of IWitness’ theoretical fundament in constructivism as a learning theory, and the fact that it actively invites teachers (and students) to create their own learning materials with/within the program, it is unclear whether this program can be easily implemented in non-American education systems and practices.  In Dutch history education, oral history is not a common practice; neither as a source of information that pupils learn to assess, nor as a practice in which pupils are trained. Regarding WWII however, there is a modest tradition of inviting eyewitnesses in classrooms, mostly in the weeks prior to the yearly commemorations of the liberation in May. Video interviews are hardly being used in Dutch history education at this point. There seems to be a transition period, in which institutions disclose their interview collections for still undefined audiences, which are mostly unaware of the existence of such collections and their online accessibility, and continue current practices. It is this transition that is the background of my postdoc research project. Other case studies focus on video interviews in WWII exhibitions and educational projects. The aim of the postdoc research project is, first, to gain insight in contemporary historical culture, and specifically in the effects of the digital in transmitting and appropriating war memories across generations. For instance it would be important to know whether there is some kind of digital ‘streamlining’ of testimonies with specific features considered to be suitable for educational purposes. The same goes for the eye witnesses as culturally constructed figure, that is already very familiar to us through the numerous films, documentaries, news reports, and exhibitions (Kansteiner, 2015; Keilbach, 2012; Gries, 2012). Which characteristics of the narrators are perceived as necessary or relevant for letting students learn from their testimonies, and why exactly? And how about the perceived historical realism and authenticity of the testimonies – are we perhaps more critical when we do not encounter the narrators in person?  The second objective is to explore and think through the needs of the educational and heritage field as users of digitized oral history collections. This corresponds to research such as the CLARIAH project Oral History Today, that has made the numerous oral history collections in the Netherlands available for scholarly research, in close cooperation with both computer technologists and scholars in the humanities. (Scaglioa et al., forthcoming; Kemman et al., 2013; De Jong et al., 2011). It is relevant to expand the gained knowledge about the scholarly uses of oral history collections, examining how digital technology can be applied to turn interview collections into a network of knowledge relevant to multiple audiences, including teachers and students. ",
       "article_title":"Live/Life Stories. The Uses Of Digital War Testimonies In Educational Contexts",
       "authors":[
          {
             "given":"susan",
             "family":"hogervorst",
             "affiliation":[
                {
                   "original_name":"Open Universiteit Nederland, Erasmus University Rotterdam, Netherlands, The",
                   "normalized_name":"Erasmus University Rotterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/057w15z03",
                      "GRID":"grid.6906.9"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-07",
       "keywords":[
          "digital humanities - pedagogy and curriculum",
          "teaching and pedagogy",
          "cultural studies",
          "media studies",
          "user studies / user needs",
          "historical studies",
          "scholarly editing",
          "content analysis",
          "archives, repositories, sustainability and preservation",
          "English",
          "audio, video, multimedia",
          "interface and user experience design"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" For the last five years I’ve taught a course on “computer science for historians” at University Lyon3 Jean Moulin. The course includes 20 hours in ten sessions and has attracted twenty to forty students each year. It addresses history students during the first semester of their master studies: they start at this stage with information collection in archival sources and bibliography, which they can later exploit to write their master's thesis. Thus, the aim is to provide them with methods and digital tools for modeling and storing information, and then for subjecting it to interrogation, visualization and analysis. This is a great challenge because many students still use paper for taking notes when they analyze historical sources and are not used to working with software that is not completely self-installing. Furthermore, students will receive from their tutors all kinds of research subjects, from Ancient to Modern history, and they often want to analyze quite complex information which one cannot store in a simple spreadsheet. For this reason, the pedagogical challenge is also a challenge for the digital humanist: how can the students be provided with a generic and flexible information system of ready use for their research but sophisticated enough to store any sort of data? In this paper I will treat some theoretical and practical aspects of the digital information system I devised to cope with this problem, and will present some issues raised by recourse to the information system that concern both students and teacher. The manuscript of the course is publicly available on a  dedicated website  (all websites were accessed on 30 October 2015). : anyone interested can download the tools I developed and test the methods proposed to the students, or employ them for their own teaching. As I teach in French the documentation, interfaces, etc. are written in this language. The information system I propose in the course combines the experience acquired in developing the  symogih.org project  Documentation about the project and links to its the different components are provided on the project main website :   . , a collaborative platform for storing and sharing structured historical data, with the method of semantic annotation of texts adopted in our  platform for digital editions   .  in accordance to the Text encoding initiative's  guidelines (TEI). These data production practices, both as structured data and encoded texts, must be radically simplified to cope with the pedagogical need exposed above and this requires working on a high level of abstraction.     Figure 1   The first component of the information system provided to the students is a relational database designed using a generic data model  Cf.  . Databases are used in historical research both at individual and project level. See, e.g., Gast, Leugers and Leugers-Scherzberg, 2010; Alerini and Lamassé, 2011; Cellier and Cocaud, 2001; Erickson, 2013. The novelty of the method proposed by the symogih.org project, and in my course, is the abstraction level allowing to treat any kind of historical information. . In the center of the model (Figure 1), the object class, having the same sense as the \"Endurant\" class in DOLCE  DOLCE : a Descriptive Ontology for Linguistic and Cognitive Engineering:  . , or the \"Persistent Item (E77)\" class in CIDOC-CRM  CIDOC Conceptual Reference Model (CRM):  . , comprises individual actors, institutions, places, concepts, etc. about which students will be collecting information. The function of this class is to provide an identifier for each individual, in turn characterized by one or more names, a time span of existence, a type and an accurate textual definition. The database also allows treatment of some basic associations between objects defined in a class \"system parameter\" – a typical component of a generic data model – whose instances are predefined by the teacher. This simplifies the use of the database by students and guides them in their first steps of data production, but if needed parameters can be extended to other kinds of relationships. A simple PHP interface is added to facilitate data capture.  The database is implemented using PostgreSQL because this open-source database provides extended features in datatype treatment (namely XML) and comes along with a procedural language (PL/pgSQL) allowing data treatment in a SQL context without having to learn a different programming language  . . The teacher can thus write predefined functions to help the student prepare, transform and code the data before further treatment. A spatial extension is also available (PostGIS) which permits working with geo-referenced data if needed  . . PostgreSQL is therefore a kind of \"Swiss Army knife\" for historical data storage and treatment.  If the \"Objects\" (\"Endurants\" or \"Persistent items\") are identified in the database, where then is collected information about them? According to the  symogih.org semantic data model   . , a \"Knowledge Unit\" is an atomized portion of information that expresses a relationship among objects situated in space and time, established on critical analysis of documents. The class \"Knowledge Unit\" is therefore equivalent to the \"Temporal entity (E2)\" class in CIDOC-CRM or \"Perdurant\" class in DOLCE: “An endurant lives in time by  participating in some perdurant(s). For exemple, a person, which is an endurant, may participate in a discussion, which is a perdurant”  Cf. Masolo et al., 2003: 14..   In former years, \"knowledge units\" were also stored in the database, as the \"objects\" are presently   . . Pedagogical experience has shown that the degree of abstraction required for modeling information in form of structured data is generally too steep for training digital historians, although some students used this method with ease. The newly proposed information system comprises therefore a second component which consists in a text encoding method using some specific TEI tags and attributes. These allow semantic text encoding: \"knowledge units\" can be directly annotated into the text, thus marking up named entities with the database identifiers of the related objects and then encoding their properties and relationships in the text with specific tags and attributes   Some pages of the symogih.org project's user manual provide the encoding specification for XML/TEI semantic annotated texts using the symogih.org ontology: .  This method was presented at the TEI 2015 conference in Lyon, cf. Beretta 2015. The Special Interest Group Ontologies in the TEI Consortium is devoted to this approach. See the GIS Ontologies wiki :   and Eide, 2014-2015. A similar approach is represented by Jordanous, Stanley and Tupman, 2012.  .  But this method raises the question of the XML editor to adopt for text semantic encoding, meaning the addition of a further software component to the workflow of data production providing XML schema validation and also tools for querying the encoded text. XML text encoding is more suitable and I prefer it for PhD student and researcher training, but this demands a supplementary specific instruction that it impossible to provide in the limited master's course time. I therefore conceived a way of semantically tagging the text in a simple text editor or word processing program using curly brackets instead of angle brackets and replacing XML-attributes by predefined codes. This method is described on the  course wiki that also furnishes instructions for using regular expressions for proper encoding  . . Regular expressions are then used in a PL/pgSQL script in the database to transform the curly brackets and their content into real XML tags and attributes: the encoded tag “{en2ai_10}Johannes Kepler{/en}” becomes “<en type=“ai” ref=“2” ana=“10”>Johannes Kepler</en>” (belonging to a course-specific namespace). This transformation allows storage of the encoded text in a PostgreSQL XML field and consequently benefit of the full power of the XPath and SQL queries, and programming capabilities of PL/pgSQL, to extract information from the texts.  The workflow of data production and treatment ends with the phase of data analysis and visualization. For this purpose I adopted the R software that can be directly connected to a PostgreSQL database and provides many useful libraries. For instance, a former student produced data about relationship between persons attested by medieval charters that can be used for network analysis (Figure 2).     Figure 2   The students can send the teacher a dump of their database and formulate the research questions that the latter will transcribe into SQL, XPath or procedure language queries for extracting data, before sending this back to the students. Building upon these examples the students can themselves adapt the queries and scripts to new research questions. A wiki dedicated to each student's project can be created to document the specific workflow of each research project: it is not public but it is accessible to all other students participating in the process of data production and analysis. The students can use the results of data analysis and visualization to formulate new research hypotheses or they can integrate them into their master's thesis. In this paper I will present the essential conceptual and technical aspects of the whole workflow and consider three major advantages of this pedagogical approach for the disciplinary domain of digital history. First, students gain the experience of managing a workflow going from installation and personal practice on a solid community maintained open-source software, to reflection on data modeling concerning their own research agenda, to collaborative data and project management through a wiki, to an introduction in data mining and visualization techniques. Secondly, the abstraction level of the data model and text encoding practice proposed to the students implicitly introduces them to knowledge management and data production according to present-day standards like CIDOC-CRM and  Text encoding initiative: from this perspective historical knowledge is modeled as a graph of objects situated in time and space and linked to the texts from which they derive. Thus —and this is the third advantage— the course acquaints students with the basic principles of linked data and of semantic text encoding, introducing them to the concepts and practice of resource sharing and data curation: the datasets I use for the exercises come from the French national library (BNF) SPARQL endpoint and DBPedia, and the texts from Wikipedia. In a final part, I will discuss the issues that this pedagogical approach raises for master's students in history.  ",
       "article_title":" From Index Cards to a Digital Information System: Teaching Data Modeling to Master's Students in History  ",
       "authors":[
          {
             "given":"Francesco",
             "family":"Beretta",
             "affiliation":[
                {
                   "original_name":"CNRS - Université de Lyon, France",
                   "normalized_name":"University of Lyon System",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/01rk35k63",
                      "GRID":"grid.25697.3f"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "information architecture",
          "digital humanities - pedagogy and curriculum",
          "linking and annotation",
          "xml",
          "data modeling and architecture including hypothesis-driven modeling",
          "databases & dbms",
          "GLAM: galleries, libraries, archives, museums",
          "encoding - theory and practice",
          "ontologies",
          "historical studies",
          "knowledge representation",
          "query languages",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The Web is now deeply integrated into contemporary culture, and scholars interested in current phenomenon cannot afford to ignore it, however, collecting data from the web is not easy. The web is based on a mix of continually changing technical standards which make creating an archival copy of a web site for scholarly reference very difficult. Without such a copy there is no way for future scholars to question the interpretations we make today or reinterpret the phenomenon in light of new evidence tomorrow. Researchers and organizations, such as the Internet Archive, are attempting to preserve portions of the web for future retrieval, but much of the web disappears quickly. A 2014 study of web links in scholarly papers found that 1 out 5 scholarly papers contains links to web URLs which no longer function or no longer exist (Klein et al., 2014). The need for humanists to recognize the value of web archives to historical research is especially acute. Researchers cannot engage recent cultural histories and ignore the culture of the web (Milligan, 2012). The challenge of web archiving is especially acute for rapidly changing stories which track specific events. Discussions about controversial topics, such as GamerGate,   This paper is the result of a larger project investigating the discourse surrounding GamerGate, an internet controversy about feminism and gaming, which grew dramatically in 2014. The paper presents some of the methods used by our research group to study GamerGate. For a brief non-academic explanation of GamerGate see Hathaway, 2014  take place across multiple websites, use multiple forms of media, and occur in very different discourse communities. Underneath the different social worlds gathered around online communities there is an incredibly diverse set of technological platforms which require customized strategies for tracking and collecting data. This paper will:   Describe the key challenges for researchers wishing to collect just-in-time archives of web based cultural phenomenon. Put current challenges in an historical context of differing goals between web developers and web archivists. Propose some social and technical solutions to improve the situation, and Introduce a set of tools to help researchers engaged in these areas.   Challenges Dynamic changes in online content present one of the unique challenges for gathering contemporary web discourse. Most internet users are familiar with the constantly updating nature of Twitter and other social media platforms. Social media platforms present a challenge for web archivists because of their technological structure and commercial ownership. The speed of updates on social media requires specialized tools to download, especially in large quantities. A researcher needs deep technological knowledge of these tools and the application programming interface (API) provided by the website in order to build a reliable and useful corpus. On the legal side, the terms of service affect the types of information that can be gathered by researchers and how that data can be analyzed or shared with other researchers. Other commercial sites, such as news media web sites, often host comment threads where internet users can post their opinions on the topics covered in the main story. It is relatively trivial to download the main content of a news story posted on the web but collecting the comment stream may present a challenge because the comments may be hosted by another website service or may be displayed dynamically as a user scrolls further through a web page. In such cases the default web archiving tools may not be sufficient. Web discussion forums present yet another technical challenge. Researchers often frame their questions about web phenomena by describing a topic that they wish to study. But the architecture of the web is built around the key idea of a web site, a particular set of files which may include many different types of media including text, images, and video, and is hosted by a particular business, institution, or individual. These web sites are identified by the Uniform Resource Locators people type into their browsers in order to navigate to a web page.  The tools used to archive the web are built on this technical background for dealing with URLs, APIs, REST, RSS, and other interfaces which human beings do not usually interact with. In the language of web archive software the unit of research is the seed, or base URL, from which data can be harvested. For the researcher the unit of work is the topic. Negotiating between these two conceptions of how online research should work is a major social challenge for any type of internet research. Researchers and web users just want to see the content, but automating the collection of that content means reproducing a complicated software experience which has gradually been built by web developers and web browsers over the past 25 years. Humanities researchers have traditionally relied on stable or slowly changing content. Efforts by humanities scholars have been made to adapt to the changes in digital content represented by the web. Some universities have set up web labs for collecting and analyzing web data. One key task of these labs has been building subcollections from the overall web in order to further the study of particular topics (Arms et al., 2009). One of the key insights from our work is the need to continue building strong collaborations between multiple fields. Libraries and the Internet Archive need input from digital humanists in order to understand their research questions and digital humanists need to understand the technological challenges of web archiving in order to collectively design systems which will help future researchers. The web, however, is constantly changing at multiple levels, ranging from the technology used to deliver content, the processes of creating content, commenting on content, and the distribution of information. Archiving the web for humanities research calls for changing the conceptual image of stable sources, collaborating with new communities, and adopting new technologies.   Solutions The implication of the technological treadmill described above is that it becomes more and more difficult for a single researcher to adequately collect the web. There are two potential solutions to this problem: technological and social. Computer scientists are working to build better web archive software which can integrate with social media in order to reduce the amount of administrative overhead needed to collect information on particular topics.   Some of these research groups are located at the  Center for the Study of Digital Libraries at Texas A and M;  Web Science and Digital Libraries Research Group at Old Dominion; and the  Digital Library Research Laboratory at Virginia Tech.   These tools will automate the selection of web sites to be archived, removing some of the human intervention needed to curate web materials. But simplifying the data gathering process today may make future explanations of the context of a collection more technical. For now researchers are dependent upon a mix of tools, often customized for specific uses, and mixing open source and commercial software.  In our research project we used a combination of open source tools, subscription services, and customized API calls. For gathering data from Twitter we used a program called twarc.   Github repository at https://github.com/edsu/twarc  Customizations were made to improve the performance of the tool for our uses, which was tracking specific hashtags. The Twitter scraper was initially installed on a laptop belonging to a member of the research team, but when the number of tweets became too large for a laptop the program was moved to a cloud server provided by Compute Canada. The data from Twitter was stored in JSON and then transformed using standard libraries into files which could be analyzed for most frequent Twitter posters, most frequent URLs, and most frequent hashtags. Data from web pages was collected using the Archive-IT subscription service   Web site https://archive-it.org/  provided by the Internet Archive and the wget   Web site https://www.gnu.org/software/wget/  command line tool. Some specific websites, such as 4chan and 8chan, required the development of custom API interfaces to download material from relevant chat boards. Additional programs to download comments from YouTube are currently being tested. We plan to document our recipes for using these different tools on methodi.ca,   Web site http://methodi.ca/  the methods commons for text analysis.  Archiving the web involves many different institutions and disciplines. The largest players are the Internet Archive and various national libraries; the Internet Archive operates as a non-profit and has the most comprehensive collection of digital materials from the web. Unfortunately, the Internet Archive collections are not primarily built for researcher access and can be especially difficult to work with if you are investigating topics which cover multiple URLs or lengthy time periods. Any research project using their collections requires significant human labor. Libraries and museums can step in to fill some of the gaps by using services such as Archive-IT, which provides more curatorial control over the collection development process and also has a more robust search interface. In order to improve these tools, humanists will need to build connections with other disciplines, such as information and library science, computer science, and archival studies. Only by working together and extending our disciplinary horizons can we build the collections which current and future digital humanists can use to study our current era. One final social issue of importance are the legal and ethical implications of gathering large amounts of data from the web. We will not discuss these issues in great depth in this paper but they do need to be acknowledged because they constrain some of the actions which can be taken in web data gathering. In our project on GamerGate we have looked closely at the ethical implications of sharing data gathered from social media. The dataset we shared online includes an appendix on ethical issues related to data sharing.   Rockwell, G., Suomela, T., 2015, \"Gamergate Reactions\", http://dx.doi.org/10.7939/DVN/10253 V5 [Version]    ",
       "article_title":"Curating Just-In-Time Datasets from the Web",
       "authors":[
          {
             "given":"Todd",
             "family":"Suomela",
             "affiliation":[
                {
                   "original_name":"University of Alberta, Canada",
                   "normalized_name":"University of Alberta",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0160cpw27",
                      "GRID":"grid.17089.37"
                   }
                }
             ]
          },
          {
             "given":"Geoffrey",
             "family":"Rockwell",
             "affiliation":[
                {
                   "original_name":"University of Alberta, Canada",
                   "normalized_name":"University of Alberta",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0160cpw27",
                      "GRID":"grid.17089.37"
                   }
                }
             ]
          },
          {
             "given":"Ryan",
             "family":"Chartier",
             "affiliation":[
                {
                   "original_name":"University of Alberta, Canada",
                   "normalized_name":"University of Alberta",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0160cpw27",
                      "GRID":"grid.17089.37"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "internet / world wide web",
          "corpora and corpus activities",
          "digitisation, resource creation, and discovery",
          "content analysis",
          "archives, repositories, sustainability and preservation",
          "English",
          "social media"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1 Introducing the Taalportaal The Taalportaal project aims at the development of a comprehensive and authoritative digital scientific grammar for Dutch and Frisian, the two official (oral) languages of the Netherlands, in the form of a virtual language institute. The Taalportaal is built around an interactive knowledge base of the current grammatical knowledge of Dutch and Frisian. The Taalportaal’s prime intended audience is the international scientific community, which is why the language used to describe the language facts is English. The Taalportaal provides an (almost) exhaustive collection of the currently known data relevant for grammatical research, as well as an overview of the established insights about these data. This is an important step forward compared to presenting the same material in the traditional form of (paper) handbooks. For example, the three sub-disciplines syntax, morphology and phonology are often studied in isolation, but by presenting the results of these sub-disciplines on a single digital platform and internally linking these results, the Language Portal contributes to the integration of the results reached within these disciplines.  Technically, the Taalportaal is an XML-database, organized as DITA-topics (cf.  https://en.wikipedia.org/wiki/Darwin_Information_Typing_Architecture), that is accessible via the Internet using any standard internet browser. Organization and structure of the linguistic information are reminiscent of, and to a certain extend inspired by, Wikipedia and comparable online information  Sources (but without the anarchy of Wikipedia). The project is a collaboration of the Meertens Institute, the Fryske Akademy, the Institute of Dutch Lexicology and Leiden University, funded, to a large extent, by the Netherlands Organisation for Scientific Research (NWO) (Landsbergen et al. 2014). Besides the grammar modules, the portal contains an ontology of linguistic terms (recast recently in the CLARIN Concept Registry (Schuurman (2015), cf.  https://www.clarin.eu/ccr) and an extensive bibliography. As of January 2016, the first release of the Taalportaal is online via  http://www.taalportaal.org.     2 Enriching the Taalportaal with links to linguistic resources The Taalportaal database has been enriched with links to on-line linguistic resources. Links between a descriptive grammar and a linguistically annotated corpus are valuable for various reasons. Illustrating a given construction with corpus examples may help to get a better understanding of the variation of the construction and the frequency of these variants, as well as give insight into the lexical items that occur most often in the pertinent construction. Corpus data may also convince a reader that a given variant really occurs in (well-formed) text. Finally, corpus data may also yield occurrences of constructions judged ungrammatical by the authors of the descriptive grammar for reasons such as prescriptivism or theoretical bias. The possibility of enriching a grammar with links to on-line linguistic resources is thus a unique selling point of digital grammars vis-a-vis old school paper grammars (van der Wouden et al. 2015). Luckily, there is no lack of linguistic resources for Dutch that are useful for this purpose (Frisian is a slightly different matter), e.g. the (syntactically annotated part of the) Corpus of Spoken Dutch (manually verified syntactic annotation for 1M words of speech) (van der Wouden et al. 2002b; Schuurman et al. 2003), the Lassy Small treebank (manually verified syntactic annotation for 1M words of text from various genres) and the Lassy Large treebank (700M words of text, automatic syntactic annotation by means of the Alpino parser (van Noord 2006, van Noord et al. 2013) are all suitable corpora for our project. The first two resources provide high-quality data for a limited amount of text, while the last resource provides wide-coverage, but noisy, data. All treebanks follow (with minor modifications) the same annotation standard (van der Wouden et al. 2002a).  2.1 Automatic links We have investigated the feasibility of generating links to linguistic resources automatically (van der Wouden et al. 2015). As the Taalportaal texts are in XML format, linguistic examples, linguistic terms, etcetera are marked as such and can be “harvested” as such. Example sentences have been selected and translated into queries for corpus tools such as GrETEL (Augustinus et al. 2013), linguistic terms and lexical items that were highlighted, ended up being automatically linked to resources such as an etymological database such as the etymologiebank (van der Sijs 2010) the large (historical) dictionary WNT (De Vries & te Winkel et al. 1864–1998) or to a section in an on-line version of the Dutch reference grammar ANS (Haeseryn et al. 1997).   2.2 Intelligent links Next to these automatic links, the linguistic data is also enriched with tailor-made links to corpus data (van der Wouden et al. 2015). For this, student assistants with a considerable linguistic schooling have read the linguistic texts, interpreted them and translated their content into queries that address the corpora that seems most fit to them, documenting their choices and considerations.  The web-based corpus query tool PaQu (Odijk 2015,  http://zardoz.service.rug.nl:8067/xpath) is our first tool of choice for executing treebank queries. The PaQu interface helps the user to formulate XPATH-queries; it returns matching sentences in the selected corpus, with the option to display the matching nodes in the syntactic dependency graph. It also displays the query being executed along with a brief description. Queries are dynamic, i.e. the user can switch between treebank corpora, or substitute a given lexical item by an alternative. Furthermore, users can select up to three attributes (i.e. lemma, part of speech, dependency relation, etc.) of matching nodes to obtain a frequency distribution of the attribute values. Advanced users can also modify the XPATH query as they see fit. Integration of queries into the electronic version of the SoD will be done by adding links (in the form of an icon) to paragraphs and examples for which queries are available. (GrETEL (Augustinus et al. 2013,  http://nederbooms.ccl.kuleuven.be/eng/gretel) is another a corpus query tool that supports the same XPATH query language as PaQu, but that also provides support for example based query construction, a feature that might be particularly useful to non-expert users.)    2.3 First results After completion of approx. 1.000 queries that cover the syntactic parts on complementation and modification of adjectives and adpositions, we have learned that creating suitable queries for a given fragment from the SoD requires creativity and careful experimentation, tuning, and documentation (cf. van Engeland & Meertens 2016). Construction of queries is far from deterministic, that is, different annotators have different opinions concerning the most suitable query for a given example or phenomenon. In a surprisingly high number of cases, there are mismatches (in constituent structure, in part-of-speech) between the presentation in the grammar and the treebank annotation. While this makes the development of queries harder, it also underlines the value of the current project: by systematically exploring the way various linguistic examples are annotated in the treebank, we provide a starting point for further corpus exploration for users that have a general linguistic interest but who are not necessarily experts on Dutch treebank annotation.  The manually verified treebanks almost always provide sufficient examples of basic word order patterns for queries that are not restricted to a specific adjective or preposition. For queries that search for a specific lexical head or for less frequent word order patterns, the Lassy Large treebank usually has to be used. In that case, users must be prepared to see also a certain number of false hits. However, there are also examples in the Taalportaal descriptions that cannot be found even in a 700M word corpus. The conclusion that such word orders are not found in the language would be too strong, but it might be a starting point for further research (e.g. does this construction occur only in certain registers or discourse settings?) or for an alternative analysis (e.g. do these cases really involve adjectives?).    3 Extending the Taalportaal: Afrikaans Only recently, South Afrika has started building a virtual language institute Viva! ( http://viva-afrikaans.org/) that aims at developing a digital infrastructure for Afrikaans. Among its goals are study and description of the Afrikaans language and development of comprehensive tools and resources for written and spoken Afrikaans, including digital dictionaries and corpora; language advice is also supplied. Part of the Viva portal is a comprehensive grammar of Afrikaans, which is based on the Taalportaal architecture, and will be part of the Taalportaal infrastructure.   ",
       "article_title":"Taalportaal: A New Tool For Linguistic Research",
       "authors":[
          {
             "given":"Ton",
             "family":"van der Wouden",
             "affiliation":[
                {
                   "original_name":"Meertens Instituut, Netherlands, The",
                   "normalized_name":"Meertens Institute",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/05kaxyq51",
                      "GRID":"grid.450081.8"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "digital humanities - multilinguality",
          "morphology",
          "cultural infrastructure",
          "linguistics",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Tracing the \"origins of pictorial species\" (to borrow Larry Silver's turn of phrase) has long been an interest of art historians. (Silver, 2006) The emergence of distinct genres of painting (e.g. dedicated landscapes or still-lifes) in the sixteenth and seventeenth centuries in Europe, and in the Netherlands in particular, has proven especially fascinating. Historians of art and economics have hypothesized that, by specializing in standalone still-lifes, landscapes, or so-called \"genre scenes\" of everyday life, painters may have reaped two advantages: an opportunity to distinguish themselves in the uniquely competitive art market in the sixteenth- and seventeenth-century Netherlands; and the ability to efficiently paint similar compositions over and over again. (Chong, 1987; Montias, 1988) But would professional printmakers also have adopted this specialization strategy? Or did the medium, which was often put to use making reproductions after other artists' designs, instead favor etchers and engravers willing to render the works of a wide variety of artists? Existing case studies present conflicting evidence. How can we test this question at scale? While Silver only invokes speciation as a metaphor, ecology may offer a useful quantitative model for thinking about genre specialization. A common measurement of species diversity (Shannon's diversity metric) can be used to characterize artists' relative specialization or diversification in genre, thus allowing us to gain a broader perspective on printmakers' specialization or diversification strategies. I will first demonstrate how this index can detect Dutch painters' trend towards genre specialization from a database of paintings seventeenth-century Dutch household inventories, and a comprehensive database of Dutch paintings in modern-day museum collections. I will then use it to test whether or not we can detect similar results in a database of prints maintained by the Rijksmuseum.  Methodology Whether looking at the diversity of species within an ecosystem, or the variety of different industries within a state, diversity measures have to account for two dimensions:  Categorical: How many discrete classes are observed? Allocation: How even is the distribution of units among categories?  Shannon's measurement of diversity (     D   s    ), a widely-used metric, captures both of these dimensions of diversity.   Shannon's diversity index      D   s     is defined as the negative sum of the proportions of every class size within the population multiplied by their logged equivalents, where      n   i     is the number of observations in class    i  , and    N   is the total population size:       D   s   = - ∑     n   i     N   l n     n   i     N      For the original derivation of Shannon's diversity, see (Shannon and Weaver, 1949); for the R implementation of this formula, see (Oksanen et al., 2015).  Originally developed to characterize entropy in information transmission, this metric of diversity has been applied to the studies as diverse as ecological diversity, economic specialization, and racial segregation. (Gibbs and Martin, 1962; Ottaviano et al., 2003) To measure whether specialization or generalization was more favored by painters and printmakers, each artist's oeuvre is treated as a \"population\" with a single diversity score calculated per artist. By this measure, a population whose members are distributed evenly across several different species/categories will have a higher diversity index than a population whose members are largely concentrated in just one category.    Data   Figure The number of unique artists and artworks represented in each dataset, subdivided by birth year  This study is based on two sources of information about paintings, and one source for prints. The first, a modern resource, is the  RKDimages database compiled by the Rijksbureau voor Kunsthistorische Documentatie.   https://rkd.nl/en/explore/images   This catalog of Dutch and Flemish artworks extant in collections around the world contains approximately 13,000 dated and attributed paintings that have each been tagged with a series of keywords (on average between 6 to 7 keywords per painting) describing their subject matter.   These keywords have been assigned by hand by researchers at the RKD, building on the index cards typewritten by Hofstede de Groot (1863--1930) that have served as the foundation of the RKD's current digital databases.  The scale of the RKD database makes it unfeasible for the individual researcher to manually categorize each artwork into a single broad subject category. Therefore, I identified clusters of artworks that shared groups of keywords though community detection on a constructed graph where each object was connected to others based on shared RKD subject keywords.   On the community detection algorithm, see (Blondel et al. ,2008); implemented in R by (Csardi and Nepusz, 2006).  I then checked the resulting groups manually to confirm that they did, in fact, corresponded relatively well to common genre categories. The resulting groups roughly encompassed: 1) portraits, 2) still lifes, 3) landscapes, 4) religious paintings, and 5) a looser array of other works that featured multiple figures (generally genre scenes or history subjects).  Because the surviving paintings in the RKD database are a biased proxy of the  actual patterns of paintings produced in the seventeenth century, it is crucial to compare the trends derived from the RKD's modern database against contemporary archival records. The Montias Database of 17th Century Dutch Art Inventories, maintained by the Frick Art Reference Library, contains information on household inventories from Amsterdam that were recorded between 1575 and 1700.   http://research.frick.org/montias/home.php   Of these inventories, 1153 contained at least two paintings The Montias Database has 86 different subject headings, which have been manually grouped into the same general set of subject headings that we used for the RKD database.   The number of subject headings detailed by Montias are small enough that it was feasible to manually generate a concordance between the 86 original subject headings and the ten subject headings used by Van der Woude in his study of the same database: \"old testament\", \"new testament\", \"other religious\", \"mythology-allegory\", \"history\", \"landscape\", \"genre\", \"still life\", \"portrait\", \"animals\", \"other\", and \"unknown\"; (van der Woude, 1991).  The MDI describes 34,147 paintings, of which 26,349 (about 77% of the total) have an identified subject (the rest are labeled \"unknown\"), with 4,377 of those described paintings (about 13% of the total) attributed to a specific artist.   This limited level of description common in collection inventories from the seventeenth century. While it was common to describe the subject of the painting and its size, notaries generally did not make an attribution of an artwork unless its painter was well-known enough that its attribution would have impacted the painting's monetary value.  The Montias inventories are also an imperfect reflection of seventeenth-century painting production, being biased towards rich collectors, mostly in Amsterdam, who died with outstanding debt. However, if both the modern and contemporary datasets reveal similar patterns in specialization, this would strengthen the case for claiming that a trend towards specialization existed historically. (De Vries, 1991:259--260.)  Unfortunately, there are virtually no seventeenth-century inventories that catalog individual prints. Instead, we rely solely on the surviving prints in the collection of the Rijksmuseum   https://www.rijksmuseum.nl/  : an imperfect source, though one that is also unparalleled in its coverage of known surviving prints from this period. The Rijksmuseum has classified their artworks based on the ICONCLASS system for tagging iconography in European art   http://iconclass.org/  , and this has also been mapped to the same broader categories used in the Montias database. Multiple impressions of the same print have been roughly disambiguated by removing prints with a duplicate engraver, title, and dates. This study is also only considering reproductive prints, so prints made by engravers or etchers after their own designs are excluded from this analysis.    Results   Figure The oeuvre diversity ranges of painters (Montias and RKD datasets) and printmakers (RKM dataset) born at different points between 1500 and 1700  We find that, although both the Montias and RKD paintings datasets show wide variation, with both highly specialized painters and highly diversified ones, both datasets reflect an increasing number of specialized painters born after 1600, as shown by a decreasing median oeuvre diversity. On the other hand, the median oeuvre diversity of printmakers in the Rijksmuseum dataset remains consistent during the entire period of study. This confirms the widely-held hypothesis that an increasing number of Dutch painters defined a niche for themselves by specializing in a particular genre. The results also appear to support the previously-unexamined hypothesis that reproductive printmakers instead favored making prints after a wide array of artworks; printmakers who did define highly specialized niches appear to have been the exception, rather than the norm. So what subjects did these specialists prefer? Prolific specialized painters overwhelmingly favored landscapes: of those artists in the bottom diversity quartile (i.e. the 25% most specialized painters in the Montias database), almost 85% of their paintings are described as landscapes, followed in a distant second by still-life paintings. Landscape was a genre that was both highly conventionalized - it was easy to produce endless variations on the same general set of topographical motifs - and also amenable to a very efficient technique - a landscape could be rendered in broad brushwork with a limited palette and still be an aesthetic success. (Goedde, 1997) Still-life paintings in this period were also, by in large, the purview of specialists. They may comprise a much smaller share of the total number of paintings in the Montias database because, unlike landscapes, the aesthetic effect of still lifes was often dependent on the painter's mimetic skill and illusionistc finish - not a technique conducive to speedy production. On the other hand, of those printmakers who  did specialize in particular genres of prints, we do not find a single dominant theme. Rather, a few specialties rise to the top: \"news\" prints depicting current events, architectural illustrations, and allegorical or biblical series prints. Those printmakers who did specialize (Table 1) did not rely on prints as their main means of support; many were specialist painters who happened to produce prints as well. But these printmakers were outnumbered by those professional printmakers (Table 2) who were willing and able to render reproductions after a wide variety of artists. This flexibility could have presented an attractive insurance policy for print publishers, who had to continually react to the demands of a quickly-moving market for artistic prints and illustrations, while also appealing to the seventeenth-century function of prints as encyclopedic sources of knowledge. (MacGregor, 1999:395)    artist name works div subjects   Abraham Dircksz Santvoort 123 0.74 topographical views, history prints   Allaert van Everdingen 113 0.77 landscape, animals   Adriaen van Ostade 85 0.67 genre, low-life   Isaac Vincentsz van der Vinne 82 0.23 heraldry   Reinier Nooms 72 0.58 seascape   Cornelis Dusart 57 0.76 portraiture   Theodoor van Thulden 52 0.68 antiquity, mythology   Cornelis Pietersz Bega 36 0.62 genre   Anthonie Waterloo 35 0.67 landscape   Table 1: The most productive Dutch and Flemish specialist printmakers (those falling below the 45th diversity percentile). Note that the count of \"works\" treats print series as a single work   artist name works div subjects   Jan Luyken 2,047 1.64 bible scenes, seascape, genre, historical, architecture, titlepages   Caspar Luyken 454 1.74 bible, landscape, historical, genre, maps   Aegidius Sadeler 238 1.80 landscape, portraiture, allegory, mythology, religious   Jacob Matham 229 1.73 allegory, mythology, portraiture, biblical   Hendrick Goltzius 228 1.80 biblical, portraiture, allegory, antiquity, mythology, landscape   Crispin van de Passe (I) 203 1.80 moralizing allegories, portraiture, devotional, botanical, biblical   Johannes Wierix 215 1.54 portraiture, biblical, allegory, genre, mythology, devotional,   Abraham Bloteling 188 1.72 landscape, genre scenes, portraiture, mythological   Cornelis Bloemaert (II) 180 1.69 saints & other religious, biblical, portraiture   Raphaël Sadeler (I) 156 1.57 devotional series, biblical, allegory, mythology, titlepages   Table 2: The most generalist Dutch and Flemish printmakers (those falling above the 85th diversity percentile)  ",
       "article_title":"If Paintings were Plants: Measuring Genre Diversity in Seventeenth-Century Dutch Painting and Printmaking",
       "authors":[
          {
             "given":"Matthew",
             "family":"Lincoln",
             "affiliation":[
                {
                   "original_name":"University of Maryland, United States of America",
                   "normalized_name":null,
                   "country":"United States",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "data modeling and architecture including hypothesis-driven modeling",
          "art history",
          "GLAM: galleries, libraries, archives, museums",
          "data mining / text mining",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Karl Barth's  Church Dogmatics (Barth, 1969-80) is widely considered to be one of the most influential works of Christian theology since the Reformation, and within it Barth's doctrine of election is considered a decisive contribution to modern theology (Webster, 2004: 1, 88, 93; von Balthasar, 1992: 174). Briefly, the doctrine of election, as a theological topic, describes the manner of God’s salvific work, particularly how God determines those who will be saved. Barth’s genius was to inscribe election within the relationship of the Father and the Son, Jesus of Nazareth, so that God is both the electing God and the elected human, and the humanity of Jesus is elected even while the divinity of Jesus is condemned. Over the past two decades, theologians have engaged in a rich questioning of the significance of Barth's doctrine of election for his own theology. His elaboration of the doctrine occurs in the document sections, traditionally referred to as paragraphs,   Barth’s  Church Dogmatics are broken down into volumes, part-volumes, paragraphs, and sections. Paragraphs are the primary organizing unit, though, and each constitutes a coherent thematic whole.    numbered 32 through 35 (of a total of 73 plus a fragment), with paragraph 33 being the primary location for Barth's innovative reworking of the doctrine of election. Some scholars, such as Bruce McCormack (McCormack, 2000, 2010) and Paul Jones (Jones, 2011), contest that election is a turning point in Barth's theology, decisively shaping the remainder of the  Dogmatics to the point that some formulations after election are incompatible with formulations made prior to the doctrine of election, particularly within the context of Barth’s Trinitarian ontology. Others, such as George Hunsinger (Hunsinger, 2000, 2008) and Paul Molnar (Molnar, 2002, 2006), argue that while the doctrine of election is the heart of the  Dogmatics, it is a part of a consistent and coherent whole, and does not mark an incompatibility between what comes before and after.   This paper engages the question of the significance of the doctrine of election, as elaborated in paragraphs 32 through 35, to the whole of the  Church Dogmatics through algorithmic approaches. It suggests that if a portion of a corpora strongly determines the rest of the corpora after its appearance, there will be textual traces, such as changes in word frequencies and common semantic groupings, that can be detected through computational analysis. It approaches the corpora, consisting of the entire  Church Dogmatics, including prefaces and forewords written by Barth as well as his unfinished fragments that have been published as the final volume of the  Dogmatics, though a variety of analytic techniques.    The initial explorations are conducted through topic modeling in order to discover hidden thematic structure in texts (Blei, 2012). Using Mallet, we first run topic models on different collections of paragraphs, from 15 to 30 topics, to discern the thematic structure of the entire corpora, noting especially those topics that seem definitively about the doctrine of election. Given the hypothesis that the  Dogmatics from paragraph 36 on is determined by the theme of election in a way that the paragraphs leading up to paragraph 32 are not, we break the corpus into paragraphs 1 through 31, 32 through 35, and 36 through 73 plus the fragment that Barth was writing before his death. We then run topic models with the number of topics ranging from 15 to 30, looking for the presence of topics indicating the doctrine of election. We also run similar models for the entire corpus minus paragraphs 32 through 35 in order to see whether election would appear as a theme in the  Dogmatics without the presence of the paragraphs explicitly committed to explicating the doctrine. Examining the results, we find that election fails to surface as a topic at most levels of granularity when paragraphs 32 through 35 are not included. We find that at all levels of granularity in which the topics are meaningful and coherent, election does not appear as a topic in the corpus consisting of paragraphs 36 through 73, plus the fragment, and we offer an interpretation for why this is the case based on the rhetorical strategy that Barth employs throughout his lengthy work.   In his text,  Barth, Webster notes that Barth’s, ‘preferred method of exposition, especially in the  Church Dogmatics, is frustrating for readers looking to follow a linear thread of argument. Commentators often note the musical structure of Barth’s major writings: the announcement of a theme, and its further extension in a long series of developments and recapitulations, through which the reader is invited to consider the theme from a number of different angles and in a number of different relations” (13). Barth frequently circles around his topics, returning again and again to various doctrines through different paragraphs, in each case attempting to approach in such a way as to show something new.       Fig. 1: Graph of topic distributions across the Church Dogmatics. Each column is one paragraph  Our topic models not only provide data for interpretation, but also supply a vocabulary for focusing further computational analysis. Based on words we determined to be distinctive to the theme of election, we examine overall frequency of key terms across the whole corpora, tracking the rise and fall of language specific to election. We also use term frequency-inverse document frequency (tf-idf) to examine which terms are particularly characteristic of individual paragraphs, paying attention to words typically associated with election (Kilgarriff, 2001; Garside, 2000). In a similar vein to that of our topic models, we determine mean tf-idf values for all features (words) in the broken down corpus, consisting of the same three chunks as determined above, at the levels of unigrams, bigrams, and ngrams (n=1-3). Looking especially at the bigram and ngram results, we do see shifts in the importance of some features that fit the hypothesis that Barth’s doctrine of election determines the rest of his work. If the proponents of this thesis are correct, there should be an increase in reference to Jesus Christ due to a stronger Christological shift, and a greater sense of the humanity of Jesus due to the election-based eternal identification of the Son, the second Person of the Trinity, with the historical human Jesus of Nazareth. We do see a rise in the importance of ‘jesus’ in the unigram set, and in the ngram set see the bigram ‘jesus christ’ appear in the election and after election corpora. In the bigram set, we see ‘elected man’ appear in the election set, which in Barth’s paragraph 33 references Jesus, and interestingly find “man jesus” as the fifth most characteristic bigram of the after election corpora. In relation to the text, we interpret this as an indication of the increased importance of the humanity of Christ in the election and after election corpora.     Fig. 2: Top 15 tf-idf weights at unigram level    Fig. 3: Top 15 tf-idf weights at bigram level    Fig. 4: Top 15 tf-idf weights at ngram(n=1-3) level  Based on these results, we explore two conclusions. While our analysis of frequencies and tf-idf values does seem to support the hypothesis that Barth’s doctrine of election is a determining point in the  Dogmatics, the connection between high value features and a substantial conceptual shift is difficult to determine, especially when the conceptual shift regards that ontology underlying theological developments in many doctrinal loci. Our topic models likewise were inconclusive in identifying shifts in the corpus that could be attributed to the paragraphs on election. We suggest that Barth's style of writing, which notoriously circles around and repetitively approaches topics from different angles, though with a traditional theological vocabulary, proves resistant to current algorithmic approaches in textual analysis.  ",
       "article_title":"Testing the Doctrine of Election: A Computational Approach to Karl Barth’s Church Dogmatics",
       "authors":[
          {
             "given":"Christopher Scott",
             "family":"Bailey",
             "affiliation":[
                {
                   "original_name":"Scholars' Lab, University of Virginia, United States of America",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          },
          {
             "given":"Eric",
             "family":"Rochester",
             "affiliation":[
                {
                   "original_name":"Scholars' Lab, University of Virginia, United States of America",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "theology",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Since 2014, some of the countries that were formerly belligerent of the Great War – most particularly France and UK – have organised a series of commemorations of the First World War, known as the ‘Centenaire’ (France) or the ‘Centenary’ (UK). We can assume that there is a strong link – that cannot let a historian indifferent – between those commemorations, collective memory and historical studies. Though studies about collective memory are numerous since the famous works of the French sociologist Maurice Halbwachs (Halbwachs, 1950), few of them are examining how collective memories are being expressed – maybe even transformed – on social networks on-line. In the case of the Centenary of the First World War, a set of questions can be asked: What is the on-line echo of the commemoration of the centenary of the 1st World War? What is the behaviour of Memorial/Heritage Institutions about the 1st World War on Twitter? How do they transmit information about the Centenary? Is there an influence of the English predominance on Twitter about the Centenary on how non-english-speaking twitter accounts are considering the 1st World War? Are there specific subjects that are discussed on-line? Which ‘temporalities’ are present in tweets when Twitter users speak about the Great War on-line? Though we are not yet able to respond to all those questions, we’ll use our database of tweets in order to answer them at least partially. Indeed, since the 1 st April 2014, around 1.5 millions of tweets containing a hashtag (keyword) linked to the 1st World War were written by over 350 000 Twitter accounts in several languages (mainly English and French). Twitter is a good field to analyse relationships between history and collective memory, memorial institutions and citizens, historians and a wide non-academic audience. We started to explore this database (which is still expanding): we intend to show how a historian can collect, analyse and interpret those tweets, using Digital Humanities methodologies and software in order to answer questions about collective memory of the First World War online.   Tools and Methodologies  We are using 140dev, a PHP open source script within a LAMP environment to collect tweets through the Twitter streaming API    http://140dev.com/ (accessed 4 March 2016).   . The tweets are then stored in a MySQL database. Diverse information (tweets and their metadata, hashtags, user information, mentions, retweets) about those tweets can easily be extracted through SQL queries. Those queries can also be used to extract different kind of relations: between tweets, between Twitter users or even between hashtags ( ie if a Twitter user mentioned or retweeted another twitter user, if two users wrote the same hashtags, etc). Concerning privacy, we respect the Twitter API Terms.  To analyse tweets, we are using mainly two sets of methodologies/software: social network analysis and network visualisations (with Gephi: mention, retweets or hashtags are considered a link); text analysis through the theory of the  mondes lexicaux (Reinert, 1993) as it is implemented in the IRaMuTeQ software (Ratinaud and Dejean, 2009)    http://www.iramuteq.org/ (accessed 4 March 2016) - Interface de R pour les Analyses Multidimensionnelles de Textes et de Questionnaires. IRaMuTeQ is a free software based on python and R. It is available in French, English, German and Spanish (interface and analyses).   . The combination of both tools and methodologies has been described by (Smyrnaios and Ratinaud, 2014). IRaMuTeQ, thanks to time-stamped metadata, can also help us working on temporalities. Indeed, clusters that are defined by this software can be projected in time: we can know, day-by-day, the most used kind of tweets.    IRaMuTeQ works in dividing the corpus in small segments of text (around 40 words). In our case each segment is a tweet and each tweet is also a text.   It helped us, for instance, finding that French fallen soldiers are not described with the same words the 11 th of November in comparison to the rest of the year.  The methodologies and tools that remain to be found for this research concern temporalities – even if IRaMuTeQ has helped us answer some question on time. There are several temporalities that are expressed in this corpus: the constant feed of information that is the nature of Twitter; the temporality of each twitter user; the temporality of the Centenary (which is different from one country to the other, and from the Great War temporality); and the temporality of the War itself.   First results  Language English is overwhelmingly present in this corpus. Around 10% only of the collected tweets are not in English. Among those 10%, French is largely in majority and German almost absent, even though German hashtags are collected. The fact that Twitter is an English-based social network does not explain fully this disequilibrium between English and other languages. The Memorial institutions' communication policies on Twitter are better factors to explain it.  The decentralized communication policy of British memorial institutions (the BBC and all its Twitter accounts or the Imperial War Museum for instance) is obviously more efficient than the French centralized communication policy of the  Mission du centenaire. French WW1-related museums do not have Twitter accounts or do have one but do not follow twitter implicit rules such as the use of a general hashtag like #ww1 or the French #pgm.    British and French are not commemorating WW1 the same way The most striking difference between the French corpus and the English one is the fact that both linguistic areas do not commemorate the Great War the same way. There are two major differences between both countries:  French are mainly remembering the soldiers ( Poilus). British citizens are remembering soldiers, but also battles.  The French are focusing on the end of the war, the Armistice, on the 11th November. The British are focusing on the way they entered the war.    English public history and French history amateurs Thanks to the Network visualisations, this corpus also helps understand how public history is present in Britain, in contrary to France where it just begins to appear. The presence of amateurs of history in the French corpus also shows that French historians are not on twitter, in contrary to amateurs who, next to the  Mission du Centenaire, are structuring discussions about the First World War on Twitter.     Conclusion  Comparing multilingual corpora To compare our two main corpora (the French one and the English one) that can be extracted from the database, we had to use the two main pieces of software the same way on both corpora and then to ‘humanly’ compare the results. We could not find any tools able to compare two corpora that are in different languages.   Distant reading / Close reading This research project shows that, for historians, it is still important to keep a direct link with each single primary source, as some information can be learned from the interpretation of single tweets. Though methods used in this research are dealing with Franco Moretti’s notion of  distant reading (Moretti, 2007), it proved strategic to be able to go back to every single tweet. The software used, if metadata are kept all along their use, allow this.    Twitter and the rest of the web Why Twitter? The fact that the Twitter API, though sometimes very unstable, is very convenient to use is one of the criteria of this choice. Is it really pertinent in terms of research? Shouldn't we have broader sources? How to extrapolate the project's results to other on-line social networks? Last but not least, the difficulty to anticipate hashtags to be collected might introduce biases in our research.   Future of this research project The question of ‘temporalities’ and their imbrications (the temporality of Twitter / the temporality of users / the temporality of the commemorations / the temporality of the First World War itself) should be the next step of this research. But, as it will require the use of Named Entity Recognition, extending our research to places will be possible as well.   ",
       "article_title":"#ww1. The Great War on Twitter",
       "authors":[
          {
             "given":"Frédéric",
             "family":"Clavert",
             "affiliation":[
                {
                   "original_name":"Université de Lausanne, Switzerland",
                   "normalized_name":"University of Lausanne",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/019whta54",
                      "GRID":"grid.9851.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "data mining / text mining",
          "historical studies",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction We present an easy-to-use web-based user interface which allows scholars working on manuscripts to assess the usefulness of automatic document image analysis (DIA) methods when incorporating automatic processes into their workflows. In contrast to existing web interfaces (Clausner, Pletschacher, and Antonacopoulos, 2011; Embach et al., 2013), this interface allows the user to directly upload images of the manuscript of interest without any registration. Thus, a fast assessment of a variety of algorithms and DIA processes can be performed. DivaServices is not a specialized tool for one specific use-case, but a collection of tools for several tasks in different use cases.  With this web interface we build on our previous initiative (Würsch, Ingold, and Liwicki, 2015) to provide access to a wide range of DIA methods to the research communities in Computer Science and the Humanities. While the existing DivaServices are already useful to integrate state-of-the-art DIA methods into new research applications it is still difficult for researchers with little programming experience to estimate the capabilities of the offered methods. For example, it is not easy to know which binarization, text line segmentation, or OCR method and which parameters would work best on a given manuscript.  In order to overcome this shortcoming, we present a web application that allows interacting with all offered methods. Users are able to upload their own images, perform experiments on them and have the results visualized. With this, researchers in the Humanities should get a better understanding on what the methods developed in the Computer Science community are able to achieve. The other way round, researchers from the Computer Science will have their methods exposed to a much broader range of data and can gather feedback to further improve the methods. This feedback loop should enhance communication between the two communities such that future methods can target the respective needs even better. DivaServices-Spotlight is built in a highly modular way, providing Graphical User Interface (GUI) blocks for various kinds of input and output parameters. The web interface is therefore automatically updated when new methods are added without the need of any further ado. Furthermore, based on our continuous open source support, this tool is available under a LGPL v2.1 license and the source code can be downloaded from github.   Available at: https://github.com/DIVA-DIA/DIVAServices-Spotlight     DivaServices-Spotlight DivaServices-Spotlight   Available at: http://divaservices.unifr.ch/spotlight  is a web application that allows user to upload their own image data, perform experiments and investigate the results. This should help in deciding whether an algorithm can help solving a particular problem or not, and which parameters are best for the data at hand. Figure 1 provides an example of such an experiment where the highlighted area (left) is segmented into text lines (right) and visualized for the user.    Figure 1 Example of an executed experiment using DivaServices. The highlighted region (left) is segmented into separate text lines (right) and visualized for the user.  In this part we give an overview of how the different user interface works and provide an example on how to perform a workflow.   The User Interface From the welcome page the three main parts of DivaServices-Spotlight are available: Images, for uploading and manipulating input images; Algorithms, for executing methods; and Results, for accessing computed results.   Images Via the “Images” link the user can view his already uploaded images (Gallery) or upload new images (Upload). In its current version all uploaded images are automatically converted into the PNG format and users can upload a maximum of ten images at the same time. Users get the possibility to apply various pre-processing steps onto their image. It is possible to crop an image to a specific size and values such as  brightness, contrast, and  saturation can be adjusted. Performing these pre-processing steps can lead to better results of varying methods.    Algorithms On the “Algorithm” page, all currently available methods are listed (c.f. Figure 2). When selecting “Apply” on a given method, the user is asked to select one of his uploaded images.    Figure 2 The “Algorithms” page provides an overview of all available methods with a short description of what they can be used for. Using the “Apply” button one method can be used.   On the page for a specific algorithm the user then has to specify input for this method. The input elements are created automatically based on the specifications of the method. For certain input elements a method can also specify ranges of possible values. The input of the user is validated and error messages are displayed should the input be not in a valid range.    Figure 3 Different input types and validations. DivaServices-Spotlight offers automatic generation of input blocks for different types of inputs (a) like numbers, strings, and selection. Automatic validation (b) ensures that the user input is within ranges specified by the method.   Figure 3 (a) shows how various input blocks are generated by DivaServices-Spotlight. Currently it is possible to generate blocks for the following elements: strings (textual data), numbers, selection (one of multiple), and checkboxes. In Figure 3 (b) validation of input elements is visualized. When the user inputs data that is not valid for the given input type (e.g. text data for numbers) an error message is displayed and the user cannot execute the method.  Furthermore, an algorithm can specify that a user needs to select a region within the image. This is needed by methods which want to only work on a subset of the image and can speed up the runtime, as well as the quality of the results of a method (e.g., of text line detection). DivaServices-Spotlight allows for drawing the following selections onto an input image: rectangle, polygons, and circles. These regions are drawn using the mouse. Rectangles and circles can be created using a simple click and drag operation. Polygons are created through manually creating every point of the polygon and clicking near the start point to close it. After creating the various highlighters, they can be edited (e.g., a single point of a polygon can be moved to a new location after creation). The various highlighters are visualized in Figure 4.    Figure 4 The different selection methods; rectangle (left), polygon (center), and circle (right).  Once the user has entered necessary parameters and selected a region on the image (if needed) the execution can be started using “Submit”. The user is notified of the process at the top of the page that shows more information when clicked on with the mouse (Figure 5 (a)). Once the execution is finished, again the user is notified by a small balloon that pops up in the top right corner (Figure 5 (b)). Also, the counter behind the “Results” link in the menu navigation is increased (Figure 5 (c)).   Figure 5 Notifications shown to the user about the current status of an execution (a), when an algorithm finishes (b), and the number of available results (c).    Results The “Results” page provides the user an overview of all available results. Using the “+” button on a specific result will show him the computed result. On the left side the user sees the input image as well the used parameters and on the right side the user gets a visualization of the results.   Figure 6 Results of a text line segmentation method. User input (left) is shown together with the computed results (right). Below the images is the JSON information a programmer would receive when calling the methods on DivaServices directly.  Figure 6 provides an example of a detailed result. The user input is shown (left) with the computed result (right). The image view can be manipulated (dragging, and zooming) to get a better view of certain areas. Below each image is the JSON information that is sent to and received from DivaServices. This information should help programmers to see with what kind of information they have to deal with should they decide to integrate that method into another application.    Using DivaServices-Spotlight for Designing DIA Workflows  We provide an example how DivaServices-Spotlight can be used to design a full workflow. The aim is to build a system that takes an input image and performs OCR on the segmented text lines. For this we need to perform three steps: binarization, text line segmentation, and OCR recognition.  Using the “Save Image” functionality on the result page we save the result image after each step. Figure 7 (a) – (d) show results at each stage using a combination of available methods. Parameters or even method could be changed at each step in order to find the best suited combination.   Figure 7 Results at different stages in the workflow. The input image (a) is binarized (b), segmented into text lines (c) and processed using an OCR algorithm, leading to its digital representation (d).   Once a researcher is satisfied with the results on a small scale, he could then integrate that workflow into his application by directly invoking the methods on DivaServices using his programming language of choice.     Conclusion With DivaServices-Spotlight we provide a web application to interact with all available methods hosted on DivaServices. Researchers can run small scale experiments to experience the possibilities of the different algorithms. Furthermore, the application provides developers with the necessary information they would need to use the methods outside of DivaServices-Spotlight and integrate them into other applications.   ",
       "article_title":"DIVAServices-Spotlight – Experimenting with Document Image Analysis Methods in the Web",
       "authors":[
          {
             "given":"Marcel",
             "family":"Würsch",
             "affiliation":[
                {
                   "original_name":"University of Fribourg, Switzerland",
                   "normalized_name":"University of Fribourg",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/022fs9h90",
                      "GRID":"grid.8534.a"
                   }
                }
             ]
          },
          {
             "given":"Michael",
             "family":"Bärtschi",
             "affiliation":[
                {
                   "original_name":"University of Fribourg, Switzerland",
                   "normalized_name":"University of Fribourg",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/022fs9h90",
                      "GRID":"grid.8534.a"
                   }
                }
             ]
          },
          {
             "given":"Rolf",
             "family":"Ingold",
             "affiliation":[
                {
                   "original_name":"University of Fribourg, Switzerland",
                   "normalized_name":"University of Fribourg",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/022fs9h90",
                      "GRID":"grid.8534.a"
                   }
                }
             ]
          },
          {
             "given":"Marcus",
             "family":"Liwicki",
             "affiliation":[
                {
                   "original_name":"University of Fribourg, Switzerland",
                   "normalized_name":"University of Fribourg",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/022fs9h90",
                      "GRID":"grid.8534.a"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-01",
       "keywords":[
          "software design and development",
          "programming",
          "English",
          "image processing",
          "interface and user experience design"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The relationship between action and identity is a significant element of understanding the way that characterization functions within literary works; many memorable characters are in part defined by their actions. This link between character and action raises the question of whether  specific types of characters, or subjects, are consistently associated with certain types of action. Our project seeks to address this question by looking at the relationship between elements of a subject’s identity and the actions associated with that subject. Our research builds off of work begun by the University of Nebraska-Literary Lab that explores the relationship between behavior and gender in the 19 th century novel. The research begun by the Lab attempts to situate questions of gender and agency within the context of 19th century notions of propriety; is the Victorian valorization of passive women and active men reflected in novels from the period?   This project adds on to our initial foray into questions of gender; what is at stake is still very much a question of the allocation of agency. This avenue of research revolves around the question of when and why inanimate objects fill the subject position in sentences. This research also queries whether certain types of characters behave differently from others: what do kings do that peasants do not?  Our project examines the agency associated with male, female, human, and non-human actors by studying the different types of verbs used in conjunction with different types of subjects. This research explores the question of whether or not certain types of subjects  behave differently in our corpus, and if so, in what ways and to what effect.    Methodologies The initial foray into the study of gender and genre performed by the University of Nebraska-Literary Lab relied on POS tagging and used an R programming script to extract the ﬁrst pronoun that it encountered, along with the ﬁrst verb that followed this pronoun, and entered each as a relationship into a data frame.R is a statistical programming language often used in text analysis research and authorship attribution studies The male pronouns “him,” “his,” “he,” and “himself,” and the female pronouns “she,” “her,” “hers,” and “herself” were extracted. Thus, in the following sentence, the pronouns “she” would be extracted and grouped with the verb “walked.” After dinner, she walked outside. This approach was also our initial model for extracting non-human actors. For example, in the following sentence, we could similarly extract the pronoun “it” and the verb “howled.”  The wind was fierce; it howled into the night.  However, such an approach has several shortcomings. The first of which is that multiple verbs associated with a single subject in a sentence are not extracted. The second, is that this method only captures pronouns.  Instances of personification, which often rely on nouns rather than pronouns, are ignored by this model. In order to solve these issues, we turned to the Stanford Dependency Parser, a tool that provides a representation of grammatical relations between words in a sentence. For example, in the sentence “The wind is dancing and howling,” the parser would extract two subject verb pairs, “wind, dancing” and “wind, howling.” The output looks as follows: det(wind-2,The-1) nsubj(dancing-4,wind-2) nsubj(howling-6,wind-2) aux(dancing-4,is-3) root(ROOT-0,dancing-4) cc(dancing-4,and-5) conj:and(dancing-4, howling-6)  Using the parser allowed us to collect subjects that were not pronouns and allowed us to correctly associate multiple verbs with a single subject. It also allowed us to easily collect gender data, since we could simply collect any nsubj pair that contained a gendered pronoun.  While the parser does identify subject and verb pairs, it does not differentiate between human and non-human subjects. To differentiate between these subject types, we created a script that allows us to extract non-human agents and the verbs associated with them by ignoring sentences in which the subject is a gendered pronoun, a proper name, or a title. In performing our research, we realized that human subjects were indicated by either a pronoun (such as he), a proper name (such as Mary), or a title (such as the priest). If a subject did not fall into one of these three categories, the subject was most likely a non-human entity.  Ignoring nsubj groupings in which one of the words is a gendered pronoun was straightforward. In order to block proper names, we ran the Stanford Named Entity Recognizer on the texts in order to create a list of proper names from the corpus. We then ignored nsubj groupings that contained one of these names. Finally, in order to ignore titles, we created a dictionary of titles derived from vocabulary lists for non-native english speakers. These lists contained titles such as “captain,” professions, such as “baker,” terms signalling family relationships, such as “mother,” and general terms for human agents, such as “girl.” We then recorded each subject-verb relationship that did not contain one of these three categories into a data frame. However, in a separate script, we also used this list of titles to extract nsubjs that contain any of these titles. Our process allows us to use our program to assess the frequency of recurring syntactical relationships, essentially counting the number of times each verb is associated with male, female, human, and non-human subjects.    Observations The initial results observed by the Nebraska-Literary Lab in their study on gender indicate that certain verbs were strongly associated with male characters while different verbs were strongly associated with female characters. Continuing this research, Matthew Jockers and Gabrielle Kirilloff confirmed these results in their work, which used the Stanford Dependency Parser in the manner discussed above. Jockers and Kirilloff found that a verb can be used to predict the gender of the pronoun associated with it, with 89% percent accuracy. Given the high degree of accuracy obtained from this analysis, we can conclude that within our corpus of 19th century fiction, authors chose to portray male and female characters differently by associating them with divergent groups of verbs. This result is not surprising, especially given the way in which ideas about proper behavior differed for males and females within 19th century society.For a helpful discussion of the gender stereotypes that existing in the 19th century, please see: Welter, B. (1966). The Cult of True Womanhood: 1820-1860. American Quarterly 18(2). Clark, A. (1995). The Struggle for the Breeches: gender and the making of the British working class. Berkeley, CA: University of California Press. Gilbert, M. and Gubar, S. (1979). The madwoman in the attic: the woman writer and the nineteenth-century literary imagination.  New Haven, CT: Yale University Press. These works were influential in our understanding of 19th century notions of gender, behavior, and propriety. However, this result still has several far-reaching implications, one of which is that “actions,” or verbs, are in fact an important part of creating and determining character. One of the shortcomings of the analysis on gendered pronouns and verbs is that it does not take into account other aspects of character identity. A princess and a witch may perform the same actions, but the implications are radically different. Similarly, certain types of characters may be associated with verbs typically associated with the opposite gender; though both are male, clerics and soldiers are no doubt associated with different actions. The data we extracted is a first step toward broadening this work; our extraction of specific subjects (such as wife, soldier, cleric) allow us to more closely look at character identity. In querying our data, our results thus far support the findings on gendered pronouns and verbs. For example, the verb “wept” was found to be strongly associated with female pronouns. In examining specific types of actors, we found that “women,” “mothers,” and “woman” were the most frequent actors associated with “weep,” “weeping,” and “weeps” respectively.    Future work Our initial foray into our corpus has produced a wealth of data; at this stage our next step is to organize and query this data, asking more specific questions about the relationship between subjects and actions. For example, we have hypothesized that instances of objects performing actions occur more often in certain genres, specifically the Gothic.This hypothesis arose from both our own close reading of certain texts within our corpus and previous scholarship on the appearance and use of personification in the Gothic novel. For insight into the scholarly understanding of personification and the Gothic novel, please see the chapter on the Gothic novel in: Parrinder, P., Nash, A. and Wilson, N. (2015). New Directions in the History of the Novel. New York: St. Martin's. Over the coming months we intend to begin studying whether the actions associated with male, female, human, and non-human subjects are associated with specific genres. This type of analysis is challenging, largely because of the difficulties associated with collecting accurate Genre data. Genres are not rigid categories and many works participate in multiple genres.  In addition to exploring the effects of genre, we also intend to more thoroughly examine the types of non-human and human agency we are extracting. Man-made objects, objects found in nature, animals, and supernatural beings are just a few of the types of non-human agency we have observed. We would like to begin exploring and categorizing these differences in an attempt to better understand our data. Because the Stanford Dependency Parser allows us to look closely at syntactic relationships, we also intend to expand our research to encompass the objects of actions, essentially asking, who is doing what  to whom. This question has important implications for studies of gender and character identity.    ",
       "article_title":"Who’s Doing What?: Examining The Relationships Among Subjectivity, Agency, and Syntax In The 19th Century Novel",
       "authors":[
          {
             "given":"Jonathan Yu",
             "family":"Cheng",
             "affiliation":[
                {
                   "original_name":"University of Nebraska-Lincoln, United States of America",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          },
          {
             "given":"Gabrielle",
             "family":"Kirilloff",
             "affiliation":[
                {
                   "original_name":"University of Nebraska-Lincoln, United States of America",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "genre-specific studies: prose, poetry, drama",
          "literary studies",
          "gender studies",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The advent of digital infrastructures for arts and humanities research calls for deeper understanding of how humanists work with digital resources, tools and services as they engage with different aspects of research activity: from capturing, encoding, and publishing scholarly data to analyzing, visualizing, interpreting and communicating data and research argumentation to co-workers and readers. Digitally enabled scholarly work, and the integration of digital content, tools and methods, present not only commonalities but also differences across disciplines, methodological traditions, and communities of researchers. A significant challenge in providing integrated access to disparate digital humanities (DH) resources and, more broadly, in supporting digitally-enabled humanities research, lies in empirically capturing the context of use of digital content, methods and tools. This paper presents recent and ongoing work on the development of NeMO, an ontology of digital methods in the humanities, and its deployment for the development of a knowledge base on scholarly work.  Several attempts have been made to develop a conceptual framework for DH in practice. In 2008, a project funded by the UK’s Arts and Humanities Research Council, the  AHRC ICT Methods Network, based at King’s College, London, developed a taxonomy of digital methods in the arts and humanities. This was the basis for the classification of over 200 digital humanities projects funded by the AHRC in the online resource arts-humanities.net. This taxonomy was subsequently modified by Oxford University as the basis for the classification of digital humanities initiatives at the University ( Digital Humanities at Oxford). Other initiatives to build a taxonomy of Digital Humanities include  TaDiRAH and  DH Commons. From 2011 to 2015 the European Science Foundation funded the  Network for Digital Humanities in the Arts and Humanities (NeDiMAH). This Network was established to develop a better understanding of the practice of DH across Europe, and ran over 40 activities structured around key methodological areas in the humanities (digital representations of space and time; visualisation; linked data; creating and using large scale corpora; and creating editions). Through these activities, NeDiMAH gathered a snapshot of the practice of digital humanities in Europe, and the impact of digital methods on research. A key output of NeDiMAH is  NeMO:  the NeDiMAH Ontology of Digital Methods in the Arts and Humanities. This ontology of digital methods in the humanities has been built as a framework for understanding not just the use of digital methods, but also their relationship to digital content and tools. The development of an ontology, rather than a taxonomy, stands in recognition of the complexity of the digital humanities landscape, the interdisciplinarity of the field, and the dependencies that impact the use of digital methods in research.   NeMO was developed by the Digital Curation Unit (DCU), IMIS-Athena Research Centre, in collaboration with NeDiMAH, as a conceptual framework capable of representing scholarly work in the humanities, addressing aspects of intentionality and capturing the diverse associations between research actors and their goals, activities undertaken, methods employed, resources and tools used, and outputs produced, with the aim of obtaining semantically rich structured representations of scholarly work. It is grounded on earlier empirical research through semi-structured interviews with scholars from across Europe, which focused on analysing their research practices and capturing the resulting information requirements for research infrastructures. Its intellectual foundations lie in earlier work of the DCU on conceptualizing and modelling scholarly activity in the arts and humanities, conducted within the  Preparing DARIAH, DYAS / DARIAH-GR, and  EHRI projects, and manifested in the Scholarly Research Activity Model (SRAM), an ontological representation of scholarly information activity drawing from cultural-historical activity theory and process modelling, and compatible with CIDOC’s Conceptual Reference Model ( CIDOC CRM, ISO 21127:2006).   Architecturally, NeMO adopts a three layer structure, spanning from abstract/general to concrete/special concepts, to provide a flexible framework suitable to the multidisciplinarity of DH. Its top tier concepts (Actor, Activity, Object) provide a general reasoning frame, and function as semantic links to reference ontologies such as CIDOC CRM. These abstract notions are specialized in the second layer by way of domain-specific concepts covering every aspect of scholarly work:  Methods employed in activities of various degrees of complexity or taught in  Courses,  Tools used,  Information Resources taken as input or produced as output,  Groups/Organizations or  Persons participating in various roles,  Goals addressed,  Topics covered, etc. Furthermore, in this second layer, several semantic relations capturing the context of the aforementioned core concepts allow for modeling scholarly work through four complementary perspectives: (1) Process-related, centered around the concept of  Activity and capturing temporal and spatial aspects; (2) Methodological, centered around the  Method concept and capturing \"how\" aspects; (3) Agency-related, centered around the  Actor and  Goal concepts and capturing \"who\" and \"why\" aspects; and (4) Resource-related, centered around the  Information Resource concept and covering \"what\" aspects of scholarly work.   Ιn the third layer of NeMO, fine-grained notions supporting domain-specific detailed descriptions are represented as specializations of second layer concepts. Respective vocabularies are organized as SKOS thesauri. More specifically, controlled vocabularies of lexical terms are structured hierarchically under the concepts of  ActivityType, MediaType, InformationResourceType, TopicKeyword, ActorRole, SchoolOfThought and  Discipline, which are specializations of the  Type concept of the second layer, and are used for characterization/classification in parallel to ontological classification. The role of these taxonomies is, thus, twofold: (1) as a vocabulary of terms that can be used for flexible tagging of the objects of interest; (2) as entry points for the alignment, or mapping, of terms from NeMO to terms from other existing taxonomies. The latter enables integration with related work, as well as effective use of these taxonomies as documentation instruments or entry points for content in NeMO knowledge bases. For instance, the  ActivityType taxonomy is organized in five hierarchies roughly corresponding to Unsworth's \"cholarly primitives\", and offers a flexible tagging system for modelling the intentionality of actors, scope adherence of activities, or purpose of use of tools and methods. On the other hand, mappings through broader/narrower term relations from the  ActivityType terms to terms of other method taxonomies, including TaDiRAH, Oxford ICT and DH Commons, allow using those taxonomies transparently within NeMO.   The development of NeMO contributes to the work of the Digital Methods and Practices Observatory Working Group of DARIAH (DiMPO), as well as of  Europeana Research within the Europeana Cloud project, providing an intellectual foundation for the analysis of evidence on arts and humanities scholarly activities and needs with regard to digital resource access across Europe. The relevance of the ontology to the DH community was validated through interviews and web surveys, to elicit information needs and patterns in working practices among humanities researchers, as well as two workshops in which these patterns were explored through use cases contributed by researchers. The evidence collected demonstrates that NeMO addresses adequately the knowledge representation needs manifested there. A variety of complex associative queries articulated by researchers in these workshops were also collected, demonstrating the potential of NeMO as an effective mechanism for information extraction and reasoning with regard to the use of digital resources in scholarly work; queries were encoded in SPARQL, a language appropriate for exploiting the serialization of NeMO in RDF Schema (RDFS), thus highlighting the benefits of its potential use as a knowledge base schema.   A prototype implementation of the above functionalities provides an easy to use demonstration of NeMO's potential. Users can articulate queries in structured English, without prior knowledge of any specific query language, using an intuitive user interface offering dynamic feedback of suggestions based on the conceptual schema. Input to the knowledge base is also supported by the same mechanism, guiding the user according to relations and classes provided by the model. A use case will be presented by way of example.  In sum, NeMO offers a well-founded conceptualization of scholarly work, which can function as schema for a knowledge base containing information on scholarly research activity, including goals, actors, methods, tools and resources involved. NeMO can thus be useful to researchers by (a) helping them find information on earlier work relevant for  their own research; (b) supporting goal-oriented organization of research work; (c) facilitating the discovery of yet uncharted paths with regard to resources, tools and methods suitable for particular contexts; and, (d) promoting networking among researchers with common interests. Additional benefits for research groups include support for better project planning by explicitly representing links between goals, actors, activities, methods, resources and tools, as well as assistance for discovering methodological trends, future directions and promising research ideas. Furthermore, for funding organisations, research councils, etc., NeMO can (a) provide a bird’s eye view of funded scholarly activities; (b) enable the systematic documentation of research projects; (c) support evaluation of proposals, monitoring and control of project work, and validation of project outcomes.   Planned improvements include the development of mechanisms for providing recommendations based on semantically related instances and for the semi-automatic population of the knowledge base, as well as specialization of core classes and addition of new terms in Type taxonomies to reflect developments in DH scholarship. ",
       "article_title":"Contextualized Integration of Digital Humanities Research: Using the NeMO Ontology of Digital Humanities Methods",
       "authors":[
          {
             "given":"Panos",
             "family":"Constantopoulos",
             "affiliation":[
                {
                   "original_name":"Athens University Of Economics and Business; Digital Curation Unit, Athena Research Center",
                   "normalized_name":"Athens University of Economics and Business",
                   "country":"Greece",
                   "identifiers":{
                      "ror":"https://ror.org/03s262162",
                      "GRID":"grid.16299.35"
                   }
                }
             ]
          },
          {
             "given":"Lorna M.",
             "family":"Hughes",
             "affiliation":[
                {
                   "original_name":"University of Glasgow",
                   "normalized_name":"University of Glasgow",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00vtgdb53",
                      "GRID":"grid.8756.c"
                   }
                }
             ]
          },
          {
             "given":"Costis",
             "family":"Dallas",
             "affiliation":[
                {
                   "original_name":"Panteion University; University of Toronto; Digital Curation Unit, Athena Research Center",
                   "normalized_name":"Panteion University",
                   "country":"Greece",
                   "identifiers":{
                      "ror":"https://ror.org/056ddyv20",
                      "GRID":"grid.14906.3a"
                   }
                }
             ]
          },
          {
             "given":"Vayianos",
             "family":"Pertsas",
             "affiliation":[
                {
                   "original_name":"Athens University Of Economics and Business; Digital Curation Unit, Athena Research Center",
                   "normalized_name":"Athens University of Economics and Business",
                   "country":"Greece",
                   "identifiers":{
                      "ror":"https://ror.org/03s262162",
                      "GRID":"grid.16299.35"
                   }
                }
             ]
          },
          {
             "given":"Leonidas",
             "family":"Papachristopoulos",
             "affiliation":[
                {
                   "original_name":"Digital Curation Unit, Athena Research Center",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Timoleon",
             "family":"Christodoulou",
             "affiliation":[
                {
                   "original_name":"Athens University Of Economics and Business",
                   "normalized_name":"Athens University of Economics and Business",
                   "country":"Greece",
                   "identifiers":{
                      "ror":"https://ror.org/03s262162",
                      "GRID":"grid.16299.35"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-02",
       "keywords":[
          "digital humanities - diversity",
          "digital humanities - nature and significance",
          "ontologies",
          "knowledge representation",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction If, as Ta-Nehisi Coates has suggested, the most lasting traumas of United States history can be traced back to Americans' \"need ... to think that they are white\" (2015), what might digital literary studies be able to tell us about this self-inflicted delusion? Specifically, how might quantitative textual analysis help us reconstruct the process by which ethnicities, nationalities, religious groups, and other identity categories became dominated by the all-encompassing category of race? One might expect literature to present a complex field of racial discourse, one in which what sociologist Matthew Snipp calls \"administrative definitions of race” (2010) coexist with self-identifications and ethnic stereotypes, colonialist fantasies and half-forgotten family trees. Literary scholars specializing in race and ethnicity have established a rich tradition of close readings that attempt to disentangle this discourse within particular texts, generating information about how individual writers navigate race in the United States. How would this understanding change, however, if we expanded our scope to hundreds of writers and two centuries of American history? What discontinuities or consistencies might we find in the language associated with different ethnicities? Would historical changes in civil rights, immigration, and territorial expansion be visible on the level of fictional discourse?   Corpus and Methods To approach these questions, we have assembled a corpus of 193 works of American fiction across a range of genres. Our selection begins in 1789: the year of Washington’s election and the establishment of the Constitution, it also saw the publication of William Hill Brown’s  The Power of Sympathy, considered the first American novel. 1964, on the other hand, marked the beginning of President Lyndon Johnson’s systematic immigration policy reforms, culminating in the Immigration and Nationality Act of 1965, which finally erased the restrictive quotas that had limited entry into the U.S. for non-white, non-western European populations. Within this extended period, major changes in government policy toward various racial and ethnic groups -- the Indian Removal Act, Emancipation, the Chinese Exclusion Act, the Immigration Act of 1917, the internment of Japanese citizens -- provide cardinal points that guide our analysis and raise fundamental issues about the relationship between fiction and the world it represents. What kinds of socio-political shifts make a difference in literary characterization? Can literature change the direction or accelerate the pace of social change, as in Abraham Lincoln’s oft-quoted but probably apocryphal claim that Harriet Beecher Stowe’s  Uncle Tom’s Cabin incited the Civil War?   To probe our assumptions about the language of identification in the novel, we combine methods that both investigate the formal features of the novels as a whole and extract racialized discourse as it attaches to particular characters. We suggest that the semantics of identity, whether racial, cultural, ethnic or national, operate at two discrete levels: 1) embedded within discourse such that it acts as a background to the particular worldview of the text; 2) at the level of character, where the lexemes of identity become a self-aware system of description, whether leveraged by the narrator, in the reactions of other characters or internally as part of a character’s self-articulation. We argue through this project that characters embody a set of racialized identifiers that operate against a set background understanding of the meaning of identity within the text -- a dialectic between intratextual characterization and intertextual stereotyping that has its origins in, but expands significantly upon, the model of marginalized characters articulated in Alex Woloch’s  The One vs. the Many (2003). The goal of our project is to tease apart these two levels of discursive identity in order to reassemble a new history of the discourse of race in American fiction as it evolves against the backdrop of history and a changing set of aesthetic principles in novel writing.    Phase One: The Racial Unconscious The first stage of this investigation examines the discourse of identity categories as they propagate throughout our corpus. We begin with a set list of various racial determinants that include national origins (German, Italian), ethnic identifiers (Jew, Arab) and racialized categories (Negro, Indian) and identity the pattern of language that attaches to these descriptors over time. In our first pass, we extract the collocates of each of our terms and identify which, if any, are significantly distinctive of that term.   Significance is determined using a Fisher’s Exact test to measure the observed values against the expected frequency of the term as a collocate, using an alpha of 0.05.   For example, the term “foreign” appears significantly often as a collocate of the names of European countries, but never within the vicinity of racial descriptors, or ethnic identifiers. We then extend the process and trace a new set of second order distinctive collocates from the terms we have identified, to see which trace back to our initial set of identifiers and which introduce new discourses into the semantics of race. By visualizing our results as a dynamic network of interconnected language we trace the connections between our primary identifiers, as well as how these relationships change over time. To extend the above example, in the nineteenth century, a language of “foreignness” is connected with European nationalities, while “America/n” is distinctively used as a descriptor of African Americans. By the mid-twentieth century, the terms describing African Americans shift away from the emphasis on their “Americanness” and instead incorporate a set of terms, such as “descent” and “blood” borrowed from the foreign discourse of immigration. Such analyses can help us to identify in precise historical detail both the moment at which particular national or ethnic groups became American and the related but not the identical moment at which they, in Noel Ignatiev’s phrase, “became white” (1995) -- that is, when the language surrounding those groups became unmarked. At the same time, finding consistencies in the language applied to different racial or ethnic groups at different historical moments grants support for Theodore Allen’s claim that the concept of race names “a pattern of oppression (subordination, subjugation, exploitation) of one set of human beings by another,” where the “phenotypical” identity of those sets is less important than the structure of their relationship (2012). This work builds upon the previous work of the members of the Literary Lab on the language of human identification in Anthropology, presented at the 2015 Digital Humanities conference and forthcoming in  Current Anthropology, although it represents a substantial methodological extension over this early project, as well as shifting the emphasis from scholarly writing to literary representations of identity.     Phase 2: Identity as Cultural Construction The second phase of our project examines the construction of individual characters against this backdrop. This not only allows us to observe which characters actively resist the discourse of the period in which they were created, but also how the evolution of this terminology as applied to individuals differs from the cultural construction of identity as a historically contingent socio-cultural phenomenon. That is, does a character described as “black” inherit the descriptors of identity from the language of the period or is discourse radically more individuated on a character by character basis? To test this assumption, we adopt a similar approach to the “BookNLP” developed by Bamman, Underwood and Smith (2014), using Named Entity Recognition to extract characters coupled with a set of scripts to perform co-reference resolution. We then tag the corpus for part-of-speech and extract the distinctive adjectives that appear in dependent positions within 50 words of each mention of the character. This allows us to identify, with reasonable precision, a descriptive terminology for each character. We then tag each character for the racial, ethnic or national identity that is given in the novels and compare our descriptive discourse for characters who embody similar identities across texts, using a set of semantic network diagrams that allow us to trace the contiguities of identity both across genre and across time.   Conclusion By combining these methods, we are able to see, for the first time, not only how the distinctive language of identity alters over the history of the American novel, but how the discourse of characterization functions as both a vehicle for the standard tropes and stereotypes for identity, as well as a point of resistance to the dominant representational language of a given period. It also provides new insights into the process of characterization, especially in regard to the representation of immigrant or minority identities.   ",
       "article_title":"Representations Of Race: Mining Identity In American Fiction, 1789-1964",
       "authors":[
          {
             "given":"Mark Andrew",
             "family":"Algee-Hewitt",
             "affiliation":[
                {
                   "original_name":"Stanford University, United States of America",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          },
          {
             "given":"J.D.",
             "family":"Porter",
             "affiliation":[
                {
                   "original_name":"Stanford University, United States of America",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          },
          {
             "given":"Hannah",
             "family":"Walser",
             "affiliation":[
                {
                   "original_name":"Stanford University, United States of America",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "semantic analysis",
          "genre-specific studies: prose, poetry, drama",
          "cultural studies",
          "literary studies",
          "english studies",
          "corpora and corpus activities",
          "data mining / text mining",
          "content analysis",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" While there have been numerous efforts at framing the history of the Digital Humanities, no study has concretely characterized the extent to which Digital Humanities research is data driven (Gold and Klein, 2012; Schreibman, Siemens, and Unsworth, 2004; Nyhan, Flinn and Welsh, 2015; Terras, Nyhanand Vanhoutte, 2013). Debates related to this topic periodically crop up along the Hack/Yack divide, as recurrent waves of scholars reflect on the varied histories, projects, and positions that comprise the Digital Humanities (Nowiskie, n.d.; Ramsay, n.d.; Cecire, n.d.; Alvarado, 2012). While these debates will likely continue, it is clear that current theoretical and historical contextualization stand to benefit from a more granular evaluation. The benefits of this evaluation hold potential to shed light on data driven research practices across disciplines and academic ranks, distribution of this output by institution type and geographical location, relative research data accessibility, as well as illumination of the scope of data resources utilized to further Digital Humanities research, which in turn holds the potential to inform library efforts to augment Digital Humanities support with more nuanced focus on acquisition, preparation, and provision of data that is more readily usable to Digital Humanists (Bryson et al., 2011; Sustaining the Digital Humanities, n.d.; Rockenbach, 2013). In order to realize these benefits the present study focuses on Digital Humanities praxis that is expressly data driven and computationally contingent. The study of this praxis is achieved through analysis of nearly 500 articles drawn from seven years of Oxford University Press' Digital Scholarship in the Humanities (formerly Literary and Linguistic computing), seven years of Digital Humanities Quarterly (the full run of the journal), as well as the full run of the Journal of Digital Humanities. In order to evaluate data praxis, it was necessary to come to a working definition of “data” scoped to the level of concrete usage patterns in the Digital Humanities. The conclusion that a particular article utilized source \"data\" was based on whether or not the material under analysis played a role in supporting research claims predicated on the affordances of the digital object itself. A close reading of a digital version of Jane Eyre therefore would not meet the criterion of data driven, but topic modeling Jane Eyre would, as this is a form of analysis that is uniquely possible given digital instantiation of the object under study. Articles which were understood more as reports on data-oriented research, rather than active analysis were typically excluded. Assessments, historiographies, and other meta-analysis of computational research represented elsewhere are not treated as data driven for the purposes of this study as the work in question can move forward without leveraging the affordances of a digital object. Even where these types of articles are held to not contain source data under a process of direct computational analysis or representation, they are still considered against a rubric of research data production. Research data is understood to encompass any non-rhetorical, primarily structural data generated as an output that is used to validate research findings (Federal Register Notice Re OMB Circular A-110, n.d.). This might include tabular data, computer code, or survey responses. Research data production is evaluated in order to come to an understanding of how Digital Humanists provide or do not provide access to generated data they use to support their arguments. The authors sought to focus on this aspect of research given a growing movement by scholars, operating primarily outside of the Humanities, to make their data and code accessible to support reproducibility and transparency (Stodden, Leisch, and Peng, 2014). If research data was produced in a given article, the authors proceeded to evaluate whether or not it was accessible. Research data is only considered to be accessible if the data in question is made available in a format that is machine processable. Therefore, a table of research data or an image of a line graph included in an article as a JPG is not accessible because the format renders the data intractable. Furthermore, a subset of a larger set of data, mainly used to illustrate an aspect of an argument rather than providing access to the unmediated source dataset is held to be inaccessible. Collectively, researchers, librarians, and publishers can use this portion of the study to inform assessment of the extent to which current research and publication practice are in line with how the field aims to articulate the integrity of its research claims. On the whole, article level analysis is supported by capturing up to 48 descriptive elements for each article in the target corpus. In aggregate this dataset captures the number of data sources used in a given article, the provider of the data, type of provider, data collection name, content type, format, extent, size, publication pre or post 1923, whether research data is produced, whether research data is accessible, the method of access provided if it is accessible, research data URL, research data type, and a range of demographic data that allows disciplinary characterization of data driven practices by scholars and students, the type of institutions they work in, and where in the world they work. Collectively this data will enable the Digital Humanities community to gain a concrete sense of what proportion of Digital Humanities Scholarship as represented in a core set of journals is data driven. This study indicates that current research and publication practice provide insufficient access to research data. Because the evaluation of a data driven article’s argument requires access to research data, this scarcity seems especially troubling. Additional pragmatic gains to be had from this study include ready access to all data sources utilized over the past 7 years in core Digital Humanities journals. Ready access to this data holds potential to increase awareness of data for Digital Humanities research and pedagogy in addition to informing library acquisition, preparation, and provision of data that can used to support the Digital Humanities. Through its concrete focus on data praxis, this study provides newly comprehensive insight into data driven practices across the Digital Humanities. ",
       "article_title":"Data Praxis in the Digital Humanities: Use, Production, Access",
       "authors":[
          {
             "given":"Thomas George",
             "family":"Padilla",
             "affiliation":[
                {
                   "original_name":"Michigan State University, United States of America",
                   "normalized_name":"Michigan State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05hs6h993",
                      "GRID":"grid.17088.36"
                   }
                }
             ]
          },
          {
             "given":"Devin",
             "family":"Higgins",
             "affiliation":[
                {
                   "original_name":"Michigan State University, United States of America",
                   "normalized_name":"Michigan State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05hs6h993",
                      "GRID":"grid.17088.36"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "digital humanities - pedagogy and curriculum",
          "history of Humanities Computing/Digital Humanities",
          "digital humanities - institutional support",
          "digital humanities - nature and significance",
          "corpora and corpus activities",
          "digitisation, resource creation, and discovery",
          "archives, repositories, sustainability and preservation",
          "English",
          "publishing and delivery systems",
          "copyright, licensing, and Open Access"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Text analysis methods based on word co-occurrence have yielded useful results in humanities and social sciences research. For instance, Venturini et al., (2012) describe the use of concept co-occurrence networks in social sciences. Grimmer and Stewart (2013) survey clustering and topic modeling applied to political science corpora. Whereas these methods provide a useful overview of a corpus, they cannot determine the predicates   Predicate in the sense of an expression relating a set of arguments.   relating co-occurring elements with each other. For instance, if  France and the phrase  binding commitments co-occur within a sentence, how are both elements related? Is France in favour of, or against  binding commitments?  Different natural language processing (NLP) technologies can identify related elements in text, and the predicates relating them. A recent approach is  open relation extraction (Mausam et al., 2012, among others), where relations are derived from the corpus in a data-driven manner, without having to pre-specify a vocabulary of predicates or actors. We are developing a workflow to analyze the Earth Negotiations Bulletin (vol. 12)    http://www.iisd.ca/vol12 , which summarizes international climate negotiations. A sentence in this corpus can contain several verbal or nominal predicates indicating support and opposition (see Table 1). Results were uneven when applying open relation extraction tools to this corpus. To address these challenges, we developed a workflow with a domain model, and analysis rules that exploit annotations for semantic roles and pronominal anaphora, provided by an NLP pipeline.  Our system identifies points supported and opposed by negotiating actors and extracts keyphrases and DBpedia   wiki.dbpedia.org (Auer et al., 2007)  concepts from those points. The results are displayed on an interface, allowing for a comparison of different actors’ positions. The system helps address a current need in digital humanities: tools for the quantitative analysis of textual structures beyond word co-occurrence.  The abstract is structured as follows. First, related work and the corpus are presented. Then, our system is described. Finally, evaluation is discussed.  Material supplementing the paper and access information to the system will be available on the project’s website.    https://sites.google.com/site/nlp4climate     Table :  Typical corpus sentences. Sentence 1 has predicates  supported and  opposed, with several actors each. Example 2 shows a nominal predicate ( proposal). For Sentence 1, five  ‹actor, predicate, negotiation point› propositions are extracted by the system, and the opposing actors ( China, Malaysia, Bhutan) are assigned a proposition which is a negated version (with  ~supported as the predicate) of the proposition for the main verb  supported.     Related work Venturini et al., (2014) created concept co-occurrence networks for the ENB corpus, using Cortext Manager   http://docs.cortext.net , a corpus cartography tool. This analysis does not cover which predicates relate concepts and actors. Salway et al., (2014) used  grammar induction on ENB to identify recurrent actor/predicate patterns; it could be tested whether results with that approach complement ours.   Some studies have used syntactic and semantic parsing for text-analysis of social sciences and humanities corpora. Diesner (2012, 2014) examines the contribution of NLP to the construction of text-based networks. Van Atteveldt (2015) used dependency parsing to apply co-occurrence based methods within sentence elements related to an actor or a predicate. These studies rely mostly on syntactic dependencies and verbal predicates. We are using semantic role labeling as the basis for relation extraction, and treating nominal predicates besides verbal ones. We also developed an interface to navigate the results. Finally, a relevant resource for text-mining on climate corpora is  climatetagger API   API: http://api.climatetagger.net ; Thesaurus: http://www.climatetagger.net/glossary/  , which links concepts against a domain-specific thesaurus (Bauer et al., 2011). This thesaurus could complement our concept-linking results (based on DBpedia, a general ontology).    Corpus Each ENB issue is a 2000 word summary for one day of negotiations. The issues are written by domain experts, who strive for an objective tone and, to avoid biases, use similar expressions when reporting about all participants’ interventions (Venturini et al., 2014). The COP meetings are covered in 255 ENB issues, with ca. 35,000 sentences. The original corpus format is HTML, which we preprocessed into clean text. We dated each issue based on ENB’s table of contents.   System description The system helps analyze patterns of support and opposition between negotiating parties, and the issues about which parties agree or disagree. To achieve this, the system extracts propositions of shape  ‹actor, predicate, negotiation point›,   Terminology adopted:  ‹Norway, preferred, legally-binding commitments› is a proposition, with actor  Norway, predicate  preferred and  legally-binding commitments as the negotiation point.   based on a domain model containing actors and predicates, and applying analysis rules on the outputs of an NLP toolkit. Keyphrases and DBpedia concepts are also extracted from the negotiation points. All extractions, and the corpus itself, are made navigable on a user interface (UI).   NLP toolkit We used the IXA Pipes library   http://ixa2.si.ehu.es/ixa-pipes/  (Agerri et al., 2014), with default models for  tokenization and  part-of-speech tagging. We resolved some types of  pronominal anaphora based on  CorefGraph    https://bitbucket.org/Josu/corefgraph  coreference chains.   Semantic Role Labeling (SRL) (Surdeanu et al., 2008) identifies a predicate’s arguments and their semantic functions or roles (e.g.  agent). SRL was performed with ixa-pipe-srl   https://github.com/newsreader/ixa-pipe-srl ; it provides a wrapper to  mate-tools (Björkelund et al., 2010)  , which tags against the PropBank database (Palmer et al., 2005) for verbal predicates and against NomBank (Meyers et al., 2004) for nominal ones.    Keyphrase Extraction: YaTeA   http://search.cpan.org/~thhamon/Lingua-YaTeA/lib/Lingua/YaTeA.pm  was used (Aubin and Hamon, 2006). This library performs unsupervised term extraction using syntactic and statistical criteria.    Entity Linking (EL): The tool from (Ruiz and Poibeau, 2015) was used. It combines outputs from several public EL services, selecting the best outputs with a weighted vote.    Domain-specific components The  domain model contains actors (negotiating countries and groups) and verbal or nominal predicates. Verbal predicates (from PropBank) can be neutral reporting verbs (e.g.  stated), or verbs related to support and opposition ( recommended,  criticized). The nominal predicates (from NomBank) express similar notions to the verbs (e.g.  proposal,  objection). The model also specifies a predicate type:  report,  support, or  oppose.    Analysis rules were implemented to identify propositions based on the semantic roles of predicates’ arguments, previously obtained with SRL. Most domain predicates involve an agent and a message expressed by the agent (who agrees with the message, objects to it, or just reports it). Thus, actor mentions in a predicate’s A0 argument    In SRL,  A0 corresponds to a predicate’s agent.  A1 is the patient or theme.  AM roles represent adjuncts (time, location etc.) or negation. See Palmer et al., 2005.   represent the actor who expresses the message, and the predicate’s A1 argument 12 often represents the negotiation point addressed by the actor. The generic rule to identify propositions is in Figure 1.     Figure :  General rule to create a proposition   Sentences with  opposed by constructions require a different analysis (e.g.  China, opposed by the EU, recommended…) In such sentences, a different rule creates, for the opposing actors, propositions where the predicate contradicts the main clause’s predicate (see Table 1 for an example). Proposition-creation rules for more specific cases have also been implemented.  The treatment of  negation relies on finding  AM-NEG roles (see footnote 12) attached to a predicate, or negative items ( not,  lack) in a window of two tokens preceding a predicate.   Pronominal anaphora was treated via custom rules operating on the output of a coreference resolver (see footnote 9). We created custom rules since, in the corpus,  he and  she (besides  it) can refer back to a country (pronoun gender depends on the country’s delegate).  To facilitate searches by date-range, propositions are assigned their documents’ date.   Figure :  Main view of the interface. The left panel gives access to the search workflows (Actors, Actions, Points). It also shows propositions for a query (e.g. the actor  Canada), and gives access to the  AgreeDisagree view. The right panel shows the documents in the Docs tab, as well as aggregated keyphrases and DBpedia concepts for the query or for selected propositions, in the other tabs.     User interface The UI (Figure 2) helps analyze actors’ negotiation positions. It allows searching for documents matching a text query ( Text search box), and for propositions matching a given actor ( Actors box) or a given predicate ( Actions box). Propositions matching a query are displayed on the left panel, documents for a query on the right. Aggregated  keyphrases and  DBpedia concepts for the content matching a query (documents or propositions) are displayed in tabs on the right panel. The  AgreeDisagree view provides an overview of keyphrases and concepts from propositions where selected actors agree or disagree. Simultaneous access on the UI to the corpus and the annotations helps researchers validate results.  The implementation framework is Django   https://www.djangoproject.com/ , with Solr search.    https://lucene.apache.org/solr/  We’re working on allowing the user export results and edit the model’s actors and predicates.    Figure :  AgreeDisagree View displays keyphrases and DBpedia concepts from propositions where actors (here the EU and China) agree or (as here) disagree.      Evaluation It is important to assess whether the system can help domain-experts gain insights they would not have otherwise obtained, e.g. detect previously unnoticed generalizations (see e.g. Berry, 2012). This type of evaluation is ongoing; we are collaborating with political scientists, whose initial feedback on the tool has been positive. User validation of the interface is also ongoing. The system’s NLP components were evaluated in literature cited above. Results are state-of-the-art or competitive, and available on our project’s website (sites.google.com/site/nlp4climate). To evaluate the model and analysis rules that create domain-relevant propositions, we have manually annotated a set of corpus sentences with propositions. Details about the test-set, evaluation metrics and results are on the website. We consider the results satisfactory.    Outlook A useful feature would be an annotation confidence score, that users could employ to establish priorities in manual result revision. A useful application of the propositions extracted would be creating network graphs with different types of edges representing support and opposition among parties, and between parties and issues.   Acknowledgements We thank Tommaso Venturini, Audrey Baneyx, Kari de Pryck and Diégo Antolinos-Basso from the Sciences Po médialab in Paris for domain-expert feedback on the system. Pablo Ruiz is supported by a PhD grant from Région Ile-de-France.    ",
       "article_title":"Climate Negotiation Analysis",
       "authors":[
          {
             "given":"Pablo",
             "family":"Ruiz Fabo",
             "affiliation":[
                {
                   "original_name":"LATTICE Lab, École Normale Supérieure, France",
                   "normalized_name":null,
                   "country":"France",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Clément",
             "family":"Plancq",
             "affiliation":[
                {
                   "original_name":"LATTICE Lab, École Normale Supérieure, France",
                   "normalized_name":null,
                   "country":"France",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Thierry",
             "family":"Poibeau",
             "affiliation":[
                {
                   "original_name":"LATTICE Lab, École Normale Supérieure, France",
                   "normalized_name":null,
                   "country":"France",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016",
       "keywords":[
          "information retrieval",
          "semantic analysis",
          "natural language processing",
          "linking and annotation",
          "data mining / text mining",
          "content analysis",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Background  In opera and music theatre, the realisation of a work in performance differs significantly from the abstract concept that is captured in the score. The scenic interpretation, with its own characteristics and specific perspective on the work, is created afresh in every new staging – thus a performance and its experience cannot be determined from the score alone (Cook, 2013). Comparing a mere three stagings of Richard Wagner's  Der Ring des Nibelungen illustrates this point: the Bayreuth premiere in 1876, Boulez'/Cherau's \"centennial Ring\"  in Bayreuth 1976, and the  Ring Cycle in Birmingham in 2014 (which is the subject of our digital annotation capture in this paper). Not only are costumes and decorations entirely different, but also the setting and staging of the action, interpretation and presentation of the characters, visual and scenic aesthetics, and the perspective on and attitude towards the work.  For studies of the reception and perception of an operatic work, keeping a record of a particular performance – the characteristics of individual stagings – is an important requirement. This poses the question of how best to document the ephemeral phenomenon of an opera. Audio-visual recordings appear to provide an answer, but they are neither neither objective nor exhaustive; live annotation by a musicologist in the audience can thereby provide an additional or alternative resource.   Musical Score Annotation Kit We developed the Musical Score Annotation Kit (MuSAK) to capture these musicological annotations, designing it to meet the requirements – and technical compromises – of operating in the environment and timescales of a live production in a working theatre, and providing an interface of sufficient responsiveness and adaptability to meet the needs of a working musicologist. The kit comprises:  A touchscreen tablet device, running a bespoke web-based client through which each individual page of a score can be annotated and pages can be turned; A server based on the Union platform which gathers annotation keystrokes from the tablet client; A second \"score following\" page-turn annotation interface, capturing timings of the realisation (performance) of the material on a particular score page; A Livescribe Echo digital smart pen, used to take notes beyond those described in the annotation key (see below); Capture of audio and video, potentially recording both the staged performance and the musicologist using MuSAK.  Technical details of the MuSAK infrastructure are fully described elsewhere (Page et al., 2015). In this paper we report upon the utility of the toolkit: both in relation to the needs of the musicologist during the annotation event; and in relating the quantitative temporal data captured to the qualitative consideration of its potential musicological interpretation. Particularly with regard to the latter data-derived investigations, a key functionality of the kit for post-performance analysis is the temporal reconciling of the constituent digital media and annotations into a coherent metadata hyperstructure, enabling navigation of the information-dense digital captures (Nurmikko-Fuller et al., 2015).   Annotation workflow and enactment The first full deployment of MuSAK captured annotations during a complete staging of Richard Wagner's  Ring, performed on four nights over five days by the Mariinsky Opera at the Birmingham Hippodrome in November 2014 (Figure 1).      Figure 1: The musicologist and MuSAK in position from her viewpoint at the back of the stalls before curtains up    This multi-stage process is summarised in Figure 2. Digital images were generated from short piano scores of the operas obtained from IMSLP   The International Music Score Library Project (IMSLP)/Petrucci Music Library, http://imslp.org/ , then formatted and cleaned for viewing on the MuSAK tablet. The musicologist spent considerable time before the performance annotating printed copies of these scores which were subsequently re-digitised. These pre-performance notes highlighted musical elements determined directly from the score; these also served as \"signposts\" for potential points of interest which would be revisited during the live performance.      Figure 2: Annotation workflowscore_workflow.png    This necessitated the development of a symbolic key (Figure 3) which allowed annotations to be made at sufficient speed during the live performance, and which was trialled before deployment during co-development of the technical system. During the Birmingham staging the musicologist annotated the tablet-based score image directly using a stylus to draw symbols from the key (Figure 4), allowing for adaptation of the key between the nightly performances, based on experience during fieldwork.     Figure 3: Examples of symbols from the musicologist's keykey_image.jpg        Figure 4: An annotated score pagescore_page.jpg    A significant volume of annotation and related data was gathered: 15 hours of video footage; over 100,000 tablet strokes encoding 8,216 annotations; 1,300 performance based page turns; 1,316 digital score images; and 104 pages of writing producing 13 hours of digital pen replay.   Toolkit Assessment The usability attributes of MuSAK were assessed in interviews with the musicologist after the Birmingham performances according to learnability, efficiency, memorability, and satisfaction, as defined by Ferre (Ferre et al., 2001). Learnability was evaluated through the musicologist’s experience of acquiring the skills necessary to complete the annotation process. She found the system comparable to existing musicological annotation pragmatics, minimising the need for training: Using MuSAK [is] very similar to the process that I as a musicologist used to do regularly...I think it worked very well because [it repeated existing] processes [...and] fitted in with actions I was very well adapted to...the tools were very non-invasive. On satisfactory functionality and performance, the musicologist felt “quite” able to  keep up with the pace of the opera, although believed that the time necessary to make additional freehand annotations and cognitively process observations made the page turn annotations inaccurate:  I was quite well able to keep up with the pace... I have been able to capture...a pretty good picture of...the profile of the performance... an important realisation is that making these scenic [and musical] annotations … which are particular about this performance... requires a lot of time to think and... process even if it is only [around] 10 seconds or 5 seconds...if we want to also capture a detailed and accurate time profile of the performance it would be necessary to have two people… However, an analysis of the page turn data (below) indicates an ability to at least turn pages at a pace highly correlated with the performance. (MuSAK can also be configured to support multiple annotation devices and annotators simultaneously, although this was not done during the  Ring capture.)  The musicologist reported an ability to  capture the idiosyncratic profile of each specific performance, including deviations from the score or expectations based on it, as well as staging, lighting, and the behaviour of the actors:  ...the performance details: are the musical details particular in any places, is something particular loud ...[or]... fast, are any mistakes made ... scenic specifics of the performance, what happens on stage... are people moving a lot, are they using a lot of gestures, are there significant lighting changes, are there changes of scenes... MuSAK was described as  supportive of traditional annotation paradigms, because new skills were not necessary for effective and efficient annotation using the touchpad screen and stylus:   ...intuitive to use because it was mainly very similar to just using pen and paper which everyone who is concerned with analysing music is very used to... so it's very similar to the process that I as a musicologist used to do regularly anyway and ... it worked very well because it took up processes that were there anyway and tried to fit in with actions I was very well adapted to [...] very similar to [...] if you eventually get used to the touch screen which didn’t take such a long time... The additional affordances of a digital system were noted, including the automatic  capture of the temporal profile of the performance and the benefit of being able to easily make corrections:  ...they actually help because with using a pen and paper you wouldn’t be able to undo things if you make mistakes, ...[nor]... be able to afterwards see the timings [...] with pen and paper doesn’t give such an accurate profile of this particular performance [...] you’re capturing the timings [of] not only the music but also the timings of the annotations because afterwards you can look at how dense are the annotations.   Data Interpretation In addition to recording the musicologist’s (subjective) annotations, MuSAK can be used to produce statistics about the acts of annotation to supplement our understanding of the technical system and our interpretation of the musicological context. For example, Figure 5 shows comparative plots of what is, effectively, the rate of performance of each page (from the score-following annotations) compared to the musicologist, indicating an overall ability to \"keep up\" with the pace of the staging.     Figure 5: Comparative plots of performance and annotator    Figure 6 shows the rate of annotations over the course of the four operas, indicating what might be considered as a crude measure of \"music information density\".     Figure 6: Rates of annotation over time during the Ring Cycle performance    Moments which pass a higher threshold of annotation rate have been identified to relate to significant music-dramatic instances in several places, which involve highly concentrated activity on stage (e.g. Hagen killing Gunther in Götterdämmerung).  ",
       "article_title":" Digital Annotation Tooling for Opera Performance Studies  ",
       "authors":[
          {
             "given":"Kevin",
             "family":"Page",
             "affiliation":[
                {
                   "original_name":"University of Oxford, United Kingdom",
                   "normalized_name":"University of Oxford",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/052gg0110",
                      "GRID":"grid.4991.5"
                   }
                }
             ]
          },
          {
             "given":"Terhi",
             "family":"Nurmikko-Fuller",
             "affiliation":[
                {
                   "original_name":"University of Oxford, United Kingdom",
                   "normalized_name":"University of Oxford",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/052gg0110",
                      "GRID":"grid.4991.5"
                   }
                }
             ]
          },
          {
             "given":"Carolin",
             "family":"Rindfleisch",
             "affiliation":[
                {
                   "original_name":"University of Oxford, United Kingdom",
                   "normalized_name":"University of Oxford",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/052gg0110",
                      "GRID":"grid.4991.5"
                   }
                }
             ]
          },
          {
             "given":"David",
             "family":"Weigl",
             "affiliation":[
                {
                   "original_name":"University of Oxford, United Kingdom",
                   "normalized_name":"University of Oxford",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/052gg0110",
                      "GRID":"grid.4991.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-05",
       "keywords":[
          "hypertext",
          "creative and performing arts, including writing",
          "software design and development",
          "digitisation, resource creation, and discovery",
          "content analysis",
          "English",
          "audio, video, multimedia",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"    Detailed accounts have been written of mainframes and cloud computing, social media and online commerce, but there are few books about the more humble aspects of technological culture. Screensavers, bubble-jet printers, computer desks, and other physical technologies are thrown in the trash or overwritten with new versions; the way we talk about computers, the “Web,” and the ways technology shapes culture has changed considerably since the birth of the PC. This paper examines how we can find anthropological details about our relationship with technology through popular media, specifically the television program  Law and Order.  In 2012, I was commissioned by the new media arts organization Rhizome to create a project recording every computer on  Law and Order. After watching all 319 hours of the show (or the equivalent of about two straight months watching 40-hours a week) and extracting approximately 11,000 screenshots of computers and related technologies, it is clear that  Law and Order forms a unique database of images and speech, and one that reflects the fascinations, fears, and biases of its time.  Law and Order’s long run and its “ripped from the headlines” content makes it a useful lens with which to look at a period of great political and economic change in the United States. In particular, the show coincides with a major cultural shift: the rise and eventual ubiquity of computers and networked technologies over a crucial 20-year period in technological history.  Using my  Computers on Law and Order project as a case study, this paper focuses on how these kind of details that can be unearthed from popular media, and that in fact such sources may be the only way to recover the most mundane details. Through a series of categorized screenshots and quotations, I examine several pathways through the archive of the show: the physical infrastructure of computers from shared desktop terminals to smartphones, the development of software interfaces from often-faked text-only input to interactive graphical user interfaces, and peripherals such as mice and printers.  The project can be viewed at:  http://www.computersonlawandorder.tumblr.com  ",
       "article_title":"Computers on Law and Order",
       "authors":[
          {
             "given":"Jeff",
             "family":"Thompson",
             "affiliation":[
                {
                   "original_name":"Stevens Institute of Technology, United States of America",
                   "normalized_name":"Stevens Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02z43xh36",
                      "GRID":"grid.217309.e"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016",
       "keywords":[
          "creative and performing arts, including writing",
          "media studies",
          "internet / world wide web",
          "film and cinema studies",
          "other",
          "programming",
          "cultural infrastructure",
          "English",
          "publishing and delivery systems",
          "audio, video, multimedia",
          "interface and user experience design",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction In 2004, Willard McCarty characterized the Digital Humanities as “an experimental practice”, with modeling at its core: “the way to a computing that is  of as well as  in the humanities: a continual process of coming to know by manipulating representations” (McCarty, 2004). McCarty’s proposition embraced the open-endedness of models, yet this quality tends to be forgotten in the face of 3D’s sensory immediacy, with the result that such models tend to be perceived as static. Diane Favro, a longtime proponent of 3D modeling for architectural historical research, has acknowledged that “while observers intellectually acknowledge that the virtual re-creation is an approximation, not a  doppelgänger for a past reality, this concept is almost immediately subsumed by the experiential power of the presentation” (Favro, 2006). This seems to hold true regardless of the model’s realism. Recent attempts to mitigate this effect emphasize the metadata and decision making processes behind 3D models, whose claim to scholarship is substantiated by virtue of their being supported by a database of textual, graphic, and quantitative sources (Saldaña, 2015). Anyone who has worked on 3D historical models, however, can attest that the act of compiling and layering of source data only reinforces the ultimate inability of such multiplicitous archives to present a coherent picture of the past on their own terms, let alone generate a truly representational or mimetic 3D model. Despite this, it is difficult to prevent even the database-driven 3D model from slipping back to the mode of uncritical simulation.  Simulation intentionally obscures the digital means of production in order to present a more convincing version of reality, in the service of a predetermined purpose. The origins of virtual reality simulation are coincident with the post-WWII military-industrial complex, with military training schemes being some of the primary instigators and funders of computer graphics simulation research (Penny, 2004). The anti-discursive nature of simulations is embedded in their interfaces, which invite the observer to experience the space in as natural a manner as possible, whether through embodied movement or the abstracted kinesis of mouse clicks. The pedagogical implications for academia are clear. VR training programs used in medicine, aviation, and the military rely on the repetitive, unthinking response of users to train them in a particular behavior (Penny, 2004). But what of the open-endedness of the humanistic modeling project? The automatic assumption that 3D space maps real space obscures the relationship of 3D modeling with critical theories of modeling practice in the humanities. Can 3D models be recouped from the totalizing logic of simulation? This paper attempts to access this underexplored theoretical potential of 3D by proposing a reconceptualization of 3D modeling in the Foucauldian sense of  discourse, rather than representation. Foucault’s idea of discourse was not meant to uncover an underlying definitive meaning of a given text or object, that elusive “knowledge” to which so many scholarly 3D models aspire. Rather, discourse is meant to uncover the “rules that are revealed when an object of discourse is modified and transformed”, which often involve implicit power structures (Philp, 2013). 3D digital models, which embed and incorporate many types of information, have the capacity to bring these layers of knowledge into dialogue with each other so that the underlying assumptions inscribed in them are revealed.     Case Studies This paper will examine two of the author’s projects as ongoing attempts to mine the discursive power of 3D modeling in different ways. The first case study is an exploration of the vocabularies of ancient Roman architecture at Magnesia on the Maeander. The project directly engages with the concept of ‘rules’ that allow the formation of discourse as a “system of possibility for knowledge” (Philp, 2013). In the context of procedural modeling, the methodology used here,  rule is a technical term denoting the scripts that generate 3D visualizations, and it also accurately describes the heuristic process of modeling. In writing the procedural rules, the modeler seeks to match a proleptic model with a mimetic one, continually negotiating the gaps between conjecture and representation. This process becomes discourse when the rules that result begin to delineate the shape of knowledge formation itself. Procedural modeling goes part of the way towards formulating a discursive approach to the process of modeling. As a finished product, however, 3D procedural models are indistinguishable from their non-discursive counterparts (Fig.1).      Fig.1 3D procedural model of the Hellenistic city of Magnesia on the Maeander Another possible answer involves dissolving the link between 3D space and representational space. The second case study is an application built in the Processing development environment that takes as its object of study the  Manhattan Transcripts by Bernard Tschumi (Tschumi, 1981). Like the archaeological data behind the 3D city model of Magnesia, the  Manhattan Transcripts comprises different representational modes to form a narrative that leaves many gaps open to interpretation. Originally displayed as an exhibition and later published as a book, Tschumi’s work contains 24 sets of 3 images for the same event, expressed as plan, diagram, and rendering. The digital application attempts to bring these images into dialogue with each other through the medium of 3D space by separating them and recombining them in various ways (Fig.2). 3D space here serves the discursive function of exposing the images’ areas of overlap and discordance in combinations and sequences not possible on the printed page or gallery wall. The sets of photographs, drawings, and movement diagrams can be viewed side-by-side or as semi-transparent layers thus making it easier to investigate the correspondences and differences between each representational mode. In addition, the images can be viewed all together or sorted by medium. Using the keyboard, the images can be cycled like frames in a film, highlighting Tschumi’s own references to the medium of cinema.      Fig. 1 Screenshots from the ‘Manhattan Transcripts’ 3D environment    Conclusions The use of 3D media in the humanities has come of age in the last 15 years and its function as a critical apparatus is still in the process of development. Projects that treat 3D models as more than representational tools have the capacity to expand and illuminate this increasingly widespread form of digital culture. Particularly important is the need for theoretical discourse that resists the intentionality of 3D simulations and provides alternative modes of generative, non-determinant modeling. The case studies mentioned in this paper are but two partial answers to this challenge, part of a growing effort by a contingent of artists and researchers interested in the critical potential of 3D models.  ",
       "article_title":"Modeling as Discourse: The case for 3D",
       "authors":[
          {
             "given":"Marie Giltner",
             "family":"Saldaña",
             "affiliation":[
                {
                   "original_name":"University of North Carolina, Chapel Hill, United States of America",
                   "normalized_name":null,
                   "country":"United States",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "spatio-temporal modeling, analysis and visualisation",
          "visualisation",
          "creative and performing arts, including writing",
          "media studies",
          "maps and mapping",
          "classical studies",
          "archaeology",
          "art history",
          "virtual and augmented reality",
          "other",
          "historical studies",
          "agent modeling and simulation",
          "3D Printing",
          "knowledge representation",
          "English",
          "interface and user experience design"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Mastering a complete workflow for creating editions when the editor works alone, or has no access to technologists, has proven challenging, leading to the creation and adoption of tools that make the final product conform to pre-determined models (Burnard et al., 2006; Pierazzo, 2015). Although having access to such tools can be very salutary in many cases, the need remains for independent scholars to be able to create their own editions based on unique visions, without recourse to grants or large teams. One of the main challenges we face to achieve these goals is our limited ability to train humanities students in the span of a semester the full stack of skills they would need in order to create digital editions according to standards (MLA, 2011). We believe that by adopting a minimal computing ( /) approach, we can achieve such a goal.  This paper reports on a classroom experiment done during a course entitled  Creating a Minimal Edition: From the Manuscript to the Web, that tries to address these issues in context. The main goal of the course is to introduce students to textual scholarship in general, and to digital scholarly editing in particular (Fraistat & Flanders, 2013). Our core goal is to build a TEI-to-Jekyll workflow and a simple customizable web template to be used for small digital editions, and release it openly to scholars. Underneath our prototype, we adopt the principles of minimal computing. By minimal we understand computing done under a set of significant constraints, where we reduce the stack to the minimum needed technology to accomplish a scholarly need: in this case to produce a critical digital edition that meets standards and teach the necessary skills to students within the course of one semester.  We use our created template to publish a small-scale digital scholarly edition online of one of the most remarkable Spanish literary works, the  Lazarillo de Tormes (16th century). The course is conceived as a combination between collaborative research and digital humanities design. At all steps of the process, instructors and students work together toward the completion of the digital edition. The course is divided into lectures and recitation sessions. The first is an opportunity for students to become familiar with central ideas in textual scholarship, while focusing on the textual situation of the Lazarillo; the second allow us to put those ideas into practice in the creation of the digital edition.  Underneath our proposal, there are three main theoretical axes of different nature: ethical, technological, and scholarly.  First, we would like to underscore some issues related to execution and interfaces. Well funded research projects may afford IT services or the creation of standalone systems or \"workstations\" to build and publish their edition, but this is the best case scenario which sometimes is made publicly available; but not always these interfaces are flexible enough and not usable by people with few IT knowledge. Digital scholarly edition, and specially workflow and interfaces, need to be more diffused, more used, easier to build and less expensive, inspired in what has been called the \"bricks approach\" (Pierazzo, 2015: 116-117). Hence, the need to offer a simple framework that begins on an XML-TEI dataset and ends in a minimal interface, customizable by the user.  Second, and tightly connected with the ethical component, is the relevance of technological choices, which deals with two main issues: the use of available standards and the open source ethos. We believe that digital scholarly edition, and DH in general, must rely on free and standards web services and technologies. Furthermore, we align our project on what it has been called “open source critical edition” (Bodard and Garcés, 2009) underscoring the need to make available all the datasets and scripts involved in the project.  Third, we are aware that for our show-case scenario the term “scholarly edition” is somewhat simplified. For our goal we understand “scholarly” in a large sense where the scholarly paradigm and a critical approach to primary sources are applied. Our work consists on the critical representation of one historical document, taking into consideration the full textual tradition (4 printed editions from the 16th century) of the literary work. The scholarly paradigm is obviously rethought from the digital perspective, giving special attention to execution and workflow, that is to say, giving room to understand concepts such as modeling and presentation. Furthermore, we wish to insist in two other main issues connected with DH discussions. On one hand, the fact that scholars need to take full control of their digital tools, as well as understand digital methods from their hermeneutical point of view. On the other one, we explore complex issues such as collaboration and authorship, in our case, wagering for a GitHub proposal.    Description of the Course The course is divided into six main units and is conceived as a collaborative project, where each student is in charge of a main chapter of the literary work.  First, we offer a general introduction to textual scholarship and text editing, paying attention to scholarly editing trends from the 19th century to the present, through a selection of core readings in the field. We also offer a theoretical framework for digital editions, specifically to help students understand how digital editions differ from their traditional counterparts. Afterwards, students are introduced to Github and acquire the methodology needed to work collaboratively. The goal of these two sessions is to create a collaborative and robust work environment, and to ensure that all students gather the basic skills to become fully involved. In Unit 2, we present the primary source: the historical context, the argument, the literary relevance and the text of the  The Life of Lazarillo de Tormes and of His Fortunes and Adversities. This work, published in 1554, is one of the first novellas in Iberian literature and a classic in the picaresque Spanish tradition. We then start planning the digital scholarly edition and its workflow. Students gather the basic approaches to the data modeling and conceptualize the text as a document object, starting from the analysis of the primary source. The next Unit is devoted to the eXtensible Markup Language and the Guidelines of the Text Encoding Initiative. Students gather the basic principles of the Extensible Markup Language, following the Guidelines of the Text Encoding Initiative. We also offer a general overview of the concepts of schemas (RelaxNG) and ODD documents. Each student is in charge of pursuing a textual encoding, marking up the main features: structural parts, typographic features, dates, place, and person names. They also become aware of the process of quality assurance of the text encoding of their peers.  The next steps of the process consists, on the one hand, in introducing students to the basics of Markdown, HTML and CSS, giving students the opportunity to think about data transformation, and to participate in the design and the final presentation format of the edition. On the other, we focus on inputs and outputs and textual migrations. The central node is the eXtensible Stylesheet Language Transformation, and the conversions from text encoding (XML-TEI) to the web (Markdown/HTML).  The last part of the course is devoted to web infrastructure and web publication. Students learn how to build a static website with Jekyll, dealing with the different technologies needed (HTML, CSS, Liquid, Markdown), and how to transfer their work from GitHub to GitHub Pages. We conclude with a minimal introduction to JavaScript, meant to introduce students to simple document interface: in this case the manipulation of the dates, places and person names marked up in the TEI.  As learning goals, we want our students to be able to participate in an authentic research and editing project, engaging in all steps of the process; to become aware of the challenges and opportunities of the digital medium for scholarly research and editing. We aim to teach how to apply different methods and technologies, to grasp the value of standards, and to understand data modeling and transformation from a theoretical as well as a practical perspective (Rehbein & Fritze, 2012). As a “technical” outcome, we seek to offer the basic skills to work independently in several languages (XML – TEI, HTML and CSS, XSLT, Markdown, Liquid, JavaScript), and a basic understanding of infrastructure (Jekyll, GitHub, Github Pages). Because this course is meant for students of Spanish as well, our program allows students to improve their Spanish language skills while engaging in public-facing, task-driven scholarship. Our presentation will give us the opportunity to present the online version of the course, the results of the collaborative edition created along the 27 lectures of the semester, and, finally, to release the first prototype for minimal editions as a Jekyll template, in the hope that can be useful to other DH courses and projects.   ",
       "article_title":" Minimal Editions in the Classroom: A Pedagogical Proposal  ",
       "authors":[
          {
             "given":"Susanna",
             "family":"Allés Torrent",
             "affiliation":[
                {
                   "original_name":"University of Miami, United States of America",
                   "normalized_name":"University of Miami",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02dgjyy92",
                      "GRID":"grid.26790.3a"
                   }
                }
             ]
          },
          {
             "given":"Alex",
             "family":"Gil",
             "affiliation":[
                {
                   "original_name":"Columbia University, United States of America",
                   "normalized_name":"Columbia University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00hj8s172",
                      "GRID":"grid.21729.3f"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "digital humanities - pedagogy and curriculum",
          "teaching and pedagogy",
          "encoding - theory and practice",
          "spanish and spanish american studies",
          "scholarly editing",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" For more than 20 years, researchers at Matrix, the Center for Digital Humanites and Social Science have been working on digital projects in several countries in Africa. While the technologies are critical parts of the digital humanities, ethical considerations also need to be part of any project that involves multiple projects. This is particularly true of Digital African Studies Projects because of the long and bloody history of colonialism, exploitation, and cultural theft. This long paper will explore through the context of two ongoing projects -- “Archive of Malian Photography” and the “Gorée Island Archaeological Digital Repository” — strategies to be deployed to develop sustainable, equitable, and ethical projects. While neither project is a perfect model, the strategies deployed set against the everyday frustrations of multiple partner projects, long distance project management, and problematic working conditions does help to expose what works and what still needs to be changed or augmented. Mali has remained the international nexus of African photography for nearly twenty years. Since 1994, its capital, Bamako, has been home to the only biennial festival of photography from Africa and has produced the continent’s most globally renowned professional photographers. Matrix in collaboration with the Maison Africaine de la Photographie in Mali, is digitizing and rendering globally accessible the archives of the nation’s most important photographers, dating from the 1940s to the present. The “Archive of Malian Photography” addresses the following significant needs for scholarship and teaching in the humanities as well as the preservation of Mali’s cultural heritage: - Although commercial, popular, and scholarly interest in African photography has flourished over the past twenty years, access to photographers’ studio archives is extremely limited and published materials are minimal; - Retained in private archives, these materials are not catalogued, appropriately preserved, or internationally accessible for research and education; - Due to the high commercial value of these archives in global markets, these materials are vulnerable to mistreatment, theft, and exploitation; - Moreover, stored in harsh climactic conditions, the physical integrity of these collections is in serious jeopardy. Photography was introduced to present-day Mali during the 1880s by French military officers and, later, colonial administrators, missionaries, and French expatriates. By the 1940s, an African market for photography developed in the French Sudan, as Mali was then known, and its professional photographers maintained a monopoly over the medium. As a result, their archives contain rare visual documentation of social, cultural, and political life as well as processes of urban development in the country, and in French West Africa more broadly. Spanning the eras of French colonialism, political independence, socialism, and democracy, their archives record important socio-political transformations of present-day Mali, its capital, and smaller towns along the Niger River such as Mopti and Ségu during the twentieth century. Employed by colonial and national governments, while operating private studio enterprises, each photographer’s collection houses unique African perspectives on local histories and practices, including personal and family portraiture, military activities, visits of foreign dignitaries, and images of the coup d’états that toppled the socialist regime of the nation’s first president, Modibo Keïta (1968), and the dictatorial rule of his successor, Moussa Traoré (1991). They also feature the construction of national monuments, governmental structures, bridges, dams, roadways, as well as prominent religious leaders, political figures, cultural ceremonies, and fluctuating trends in personal adornment, popular culture, and photographic practices from the 1940s to the present. In addition to promoting the global accessibility of these materials, the project is designed to protect the physical integrity of the original archives, safeguarding the negatives from further damage and theft. After the popularization of Malian photographers’ images in exhibitions and publications around the world since the 1990s, their archives have become increasingly vulnerable to pilfering and misuse due to their high commercial value in the international art market. Underscoring the severity of this issue, today, some images from the archives of El Hadj Tijani Sitou and Abdourahmane Sakaly are illegally featured for sale online (http://www.african-collection.dk/english/sakaly-1.htm) by a dealer in Denmark who obtained the photographers’ original negatives, along with those from Mamadou Cissé’s archives, from an administrator at the National Museum in Bamako who had been charged with their protection. To date, the dealer refuses to repatriate the negatives. In another case, prints from twelve of Sitou’s negatives appeared on the cover of African Arts in 2008, without his family’s permission. Such pervasive pilfering of negative archives by foreign collectors, dealers, curators, and scholars, as well as by the leaders of local cultural institutions (museums, galleries, libraries), has prevented these rare collections from entering formal, public archives in Mali. The Gorée Island Archaeological Digital Repository seeks is creating virtual 3D representations of cultural heritage materials excavated from archaeological sites in and around Gorée Island, Senegal and share those representations in an open-access, online digital repository. This work answers at least three needs in the scholarly community. First, it enables cultural heritage institutions in Africa to digital preserve archaeological materials in a way that is unobtrusive, relatively inexpensive. The project makes use of photogrammetry, a process wherein a series of images are taken of an object and run through point-recognition software to create a virtual 3D image. This process inexpensive, portable, and relatively easy to complete, making it the jointly sustainable choice for African cultural heritage institutions. Second, the repository allows scholars both within Africa and around the world to have access to these cultural heritage materials without the restrictions and inconveniences of analog-based preservation. Finally, the Gorée Island Digital Repository builds capacity amongst Senegalese students, scholars, and organizations to continue documenting and preserving cultural heritage materials using industry best standards and practices. The project is being developed through the collaboration of the Université Cheikh Anta Diop de Dakar – Sénégal (UCAD), Institut Fondamental d'Afrique Noire, the Smithsonian National Museum of African American History and Culture, Michigan State University, Matrix with the support of the Centre de Researche Ouest Africain. Documenting, safeguarding, preserving, interpreting/reinterpreting, and making accessible the myriad expressions of Africa’s many cultures is vitally important for Africa’s diverse constituent communities as well as for the rest of the world. Museums, libraries, and archives in Africa and around the world face an enormous challenge as Africa’s diverse and rich cultural heritage has been scattered by history and put at risk by wars, illicit trafficking, overwhelming economic challenges, and destruction or erosion due to human and environmental impacts. As pioneering efforts have demonstrated, the digital revolution opens up significant possibilities for long-term preservation and meaningful access that have proven unattainable in an analog world. However, to do so requires thoughtful, ethical, equitable, and sustainable strategies. These considerations need to be built into the very structure of digital humanities projects. Significant barriers remain, however, that severely limit efforts to move beyond these individual projects and fully utilize digital technologies within Africa and with African cultural materials around the world. While we are at a moment of opportunity with digital technologies, this is also a period of crisis where invaluable cultural resources are at risk to be lost forever. This paper also calls for the need for major international leadership initiatives to tackle these barriers in coordinated fashion and chart a path forward for museums, libraries, archives, universities and other heritage preservation institutions to construct equitable international partnerships that can harness this powerful opportunity. ",
       "article_title":"The Lifecycle of a Digital African Studies Projects: Creating Sustainable, Equitable, and Ethical Projects",
       "authors":[
          {
             "given":"Dean",
             "family":"Rehberger",
             "affiliation":[
                {
                   "original_name":"Michigan State University, United States of America",
                   "normalized_name":"Michigan State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05hs6h993",
                      "GRID":"grid.17088.36"
                   }
                }
             ]
          },
          {
             "given":"Ibrahima",
             "family":"Thiaw",
             "affiliation":[
                {
                   "original_name":"Institut Fondamental d’Afrique Noire (IFAN), University Cheikh Anta Diop of Dakar, Senegal",
                   "normalized_name":null,
                   "country":"Senegal",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Deborah",
             "family":"Mack",
             "affiliation":[
                {
                   "original_name":"Smithsonian National Museum of African American History and Culture",
                   "normalized_name":"National Museum of African American History and Culture",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03y5vsd17",
                      "GRID":"grid.446819.6"
                   }
                }
             ]
          },
          {
             "given":"Candace",
             "family":"Keller",
             "affiliation":[
                {
                   "original_name":"Michigan State University, United States of America",
                   "normalized_name":"Michigan State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05hs6h993",
                      "GRID":"grid.17088.36"
                   }
                }
             ]
          },
          {
             "given":"Catherine",
             "family":"Foley",
             "affiliation":[
                {
                   "original_name":"Michigan State University, United States of America",
                   "normalized_name":"Michigan State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05hs6h993",
                      "GRID":"grid.17088.36"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "french studies",
          "cultural studies",
          "anthropology",
          "digital humanities - diversity",
          "art history",
          "archaeology",
          "digital humanities - nature and significance",
          "digital humanities - institutional support",
          "digital humanities - facilities",
          "digitisation, resource creation, and discovery",
          "cultural infrastructure",
          "3D Printing",
          "archives, repositories, sustainability and preservation",
          "English",
          "image processing",
          "copyright, licensing, and Open Access",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction This paper introduces the TEI P5 XML – EpiDoc corpus of inscriptions on stone for ancient Sicily, I.Sicily. The project is one of the first attempts to generate a substantial regional corpus in EpiDoc. The project is confronting a number of challenges that may be of wider interest to the digital epigraphy community, including those of unique identifiers, linked data, museum collections, mapping, and data conversion and integration, and these will be briefly outlined in the paper which will concentrate on the conversion and technical development of the project.   Technical Background of I.Sicily I.Sicily is an online, open access, digital corpus of the inscriptions on stone from ancient Sicily.  The corpus will be mounted at   by the time of DH2016, but is currently on a development server.  The corpus aims to include all texts inscribed on stone, in any language, between approximately the seventh century BC and the seventh century AD. The corpus currently contains records for over 2,500 texts, and when complete is likely to contain c. 4,000. The corpus is built upon a conversion from a legacy dataset of metadata in MS Access to EpiDoc TEI XML. The XML records are held in an eXist database for xQuery access, and additionally indexed for full-text search using SOLR/Lucene. The corpus and related information (museum list, bibliography) are published as Linked Data, and are manipulated through a RESTful API. The records are queried and viewed through a web interface built with AngularJS and jQuery javascript components. Mapping is provided in the browser by the Google Maps API, and ZPR (Zoom, Pan, Rotate) image- viewing is provided by the IIIP image server.   At the time of writing, the main conversion routine is being refined, and the epigraphic texts are being collated for incorporation into the records. An ancillary database of museum collections in Sicily has been constructed and bibliography is held in a Zotero library. Extensive search facilities will be provided, including map-based and bibliographic searching. Individual inscriptions and individual museums will both be provided with URIs, as will personal names and individuals; places will be referenced using Pleiades, epigraphic types, materials, and supports using the EAGLE vocabularies.   The motivations for I.Sicily  The existing epigraphic landscape in Sicily is extremely diverse in two primary regards: on the one hand, the island has a very mixed cultural and linguistic make-up, meaning that the epigraphic material is itself extremely varied, with extensive use throughout antiquity of both Greek and Latin, as well as Oscan, Punic, Sikel, and Hebrew  Recent overview of much of the linguistic tradition in Tribulato 2012; and of the epigraphic material in Gulletta 1999.; on the other hand, the publication of this material has a very uneven record and despite an excellent pre-twentieth century tradition, the existing corpora are far from complete and the ability of key journals such as SEG or AE to keep pace with local publication has been limited. A limited number of museum-based corpora have been published in recent decades (for Catania, Palermo, Messina, and Termini Imerese, as well as the material from Lipari), but this has not greatly improved the overall situation. The combination of these two factors already means that locating, identifying, or working with a Sicilian inscription, or its publication record, is extremely challenging for anyone without extensive experience of the material. I.Sicily has been conceived in the hope of improving the situation in all these areas.    Multilingualism  Sicily is traditionally described as a ‘melting pot’, the ‘crossroads of the Mediterranean’. The situation created by basic technologies such as Unicode and TEI P5 EpiDoc XML mean that there is now no reason not to be language agnostic in the inclusion of material. The opportunities and possibilities offered by these technologies are considerable, since, for example, searching can be made language specific or language neutral. One obvious area where Sicilian studies are currently hampered by this disciplinary partitioning is in the study of onomastics. The Lexicon of Greek Personal Names records most instances of Greek names for the island, but Sicily is no less rich in non-Greek names (Latin and others), and at present there is no onomasticon for the island.  See    Simply by the marking-up and indexing of all names in the island’s inscriptions, I.Sicily will have generated a powerful tool for future study.    Identification and Bibliography  The PHI database of Greek inscriptions has a rich record of Greek texts, but again is text only and limited in outputs.  See    SEG references are available for 733 inscriptions on stone and AE references for 328 (data taken from the I.Sicily database and based upon comprehensive manual trawls of SEG and AE). One major aim of I.Sicily, therefore, is to generate unique identifiers for each inscription - the I.Sicily number, in the form ISic 1234 maintained as URIs, of the form:  . I.Sicily is well placed to do this since its initial dataset is primarily a bibliographic concordance of the lapidary inscriptions of Sicily. One of the associated outputs of the project will therefore be an online bibliography for Sicilian epigraphy, and an online Zotero library has already been created with over 700 records which are referenced in the EpiDoc. A locally cached version of the bibliography will be presented at the I.Sicily site to facilitate detailed bibliographic searching (including the identification of inscriptions by publication) and to allow the generation of customised concordances.    Location, location, location I.Sicily is actively generating rich geo-data for the individual inscriptions, both for the original findspot/provenance and the current location (whether museum-based, on-site, or elsewhere), and we aim to provide map-based searching for inscriptions, as well as text-based searching by ancient and modern place-names. In addition to full listing wherever possible of both ancient and modern place names for epigraphic provenance, we are working to provide detailed location information for each find-spot and current location, through a combination of library and map-based research and the use of autopsy and GIS recording. At present geo-data is being recorded in two forms, both through the use of explicit geographical locations in the form of longitude and latitude records in decimal degree form, and through the use of Pleiades URI references wherever possible.  See    We are committed to the long-term use of Pleiades as our primary reference for ancient places, and to that end we aim to update and improve the Pleiades data for Sicilian locations, in particular name data and sub-locations, in conjunction with the editing of the I.Sicily records.    Translations  The creation and availability of translations is a major goal of the EAGLE project and its collaborators, and I.Sicily is no less committed to that ambition.  See Orlandi et al. 2014: Part II. Translations are very rarely available for any of the published Sicilian inscriptions. It is obvious that the inclusion of translations will make the material much more accessible to a wider audience both of students and the general public. Equally, provision of translations will add to the value of the database as a resource for museums and others curating the inscriptions recorded in the database. To that end, a long-term ambition of I.Sicily is to include translations wherever possible in both English and Italian. We see this as one obvious area where public contribution (‘crowd-sourcing’) will be invaluable.    Limitations and future ambitions  The scale of the enterprise, and the available resources, mean that in its current form the project has limited itself to inscriptions engraved on stone (the coverage of rupestral inscriptions/graffiti and of inscriptions painted on stone/plaster is regrettably uneven). However, there is no reason in principle not to extend coverage in future to include inscriptions on other materials. Similarly, although the current project does not include a programme to mark up linguistic features of the texts, the commitment to the long-term maintenance of the corpus and the open availability of the underlying XML records means that such a project would be entirely possible in the future.  It is our long-term ambition that I.Sicily might become the default location for the publication and dissemination of Sicilian inscriptions; in the shorter term, we hope that it will serve as valuable portal in the world of Sicilian epigraphy and of ancient world open linked data, greatly improving the accessibility of Sicilian epigraphy and so enriching the study of the ‘crossroads of the Mediterranean’.  ",
       "article_title":" Creating An EpiDoc Corpus for Ancient Sicily  ",
       "authors":[
          {
             "given":"James",
             "family":"Cummings",
             "affiliation":[
                {
                   "original_name":"University of Oxford, United Kingdom",
                   "normalized_name":"University of Oxford",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/052gg0110",
                      "GRID":"grid.4991.5"
                   }
                }
             ]
          },
          {
             "given":"Jonathan",
             "family":"Prag",
             "affiliation":[
                {
                   "original_name":"University of Oxford, United Kingdom",
                   "normalized_name":"University of Oxford",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/052gg0110",
                      "GRID":"grid.4991.5"
                   }
                }
             ]
          },
          {
             "given":"James",
             "family":"Chartrand",
             "affiliation":[
                {
                   "original_name":"Open Sky Solutions",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-05",
       "keywords":[
          "xml",
          "English",
          "classical studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" In the effort to capture cultural dynamics, scholars have considered social networks, that is, a graph with people as nodes and their relationships as edges. These social networks are useful; however, to capture dynamics they must be considered over time. In the literature, Time-Varying Graphs (TVGs) have been defined (Aggarwal and Subbian, 2014; Casteigts et al., 2012; Casteigts et al., 2013). In our investigations, we have found benefit in defining TVGs with nodes as societal structures and people as the edges and then considering the dynamics of the societal structures evidenced in the TVGs (Hott et al., 2014; Hott et al., 2015). Here we consider two motivating applications for our extensions to TVGs: early Mormon marital structures and an arXiv.org citation network. The societal structures represented in the marital and church structures of early Mormons in mid-1800s Nauvoo, Illinois, include binary, polygynous, and polyandrous marriages, as well as child and adult adoptions, and membership of individuals in the church organization hierarchy. In this time period the concept of “marriage” is in flux and part of our research is to consider various conceptualizations of “marriage” to better understand the relationship to the formation of the church structure. Each conceptualization we consider as a different “identity lens,” a term we create to describe these different views. We therefore define the  identity-lens function that maps one evolving network to another evolving network. More specifically, given a TVG,   as defined in (Casteigts et al., 2012), the  identity-lens function  maps the nodes and edges in   with a given identity definition to a new Time/Identity-Varying Graph (TIVG)  .   is therefore the view of   under identity lens  i.  In our marital network research (Hott et al., 2014; Hott et al., 2015), we represent marriages as the nodes, with the individuals connecting the marriages of their parents to their own marriages as adults. Every piece of this network is considered to be evolving, since marital relations change, new children are born, family members are adopted, and individuals change membership in the church organizational structure. Initially, this network   may be described as a binary-marriage network, in which each node depicts a marriage between two individuals and their biological children. This is one specific definition of node identity. However, we may examine this network in different levels of granularity: with different definitions of node identity. By extending the marriage definition to all individuals married to the same husband, we may redefine node identity to define polygynous marriages, creating  . A related identity function that maps binary marriages to those with the same wife creates the TIVG  .  Our second motivating example is the arXiv.org   http://www.arxiv.org   citation network. ArXiv.org provides online open access to over one million cross-disciplinary papers, including papers in Physics, Mathematics, and Computer Science. We build a citation TVG from this dataset, linking authors as nodes based on the co-authorship of their papers. Similar to the Nauvoo application, we define multiple identity functions to map this TVG to multiple TIVGs. Under a node identity function combining authors within the same institution, we produce   . Other node identity mappings include departmental affiliation   , and  , in which authors are mapped to their subject areas.  Each of these TIVGs have characteristics that change over time. As we increase the complexity of the nodes through the use of identity lenses, we increase the dynamics of the characteristics, specifically those captured within the nodes. In the Nauvoo dataset, these characteristics include familial relationships among marriage members and church leadership positions held by the members of each marriage. Similarly, in the arXiv dataset, the characteristics include departmental and institutional collaboration. We want and need metrics that are sensitive to these changes within the evolving nodes as well as the overall evolving structure of the network. To capture and analyze these dynamics, we first define sampling methods to produce static graphs depicting the state of the TIVG during a fixed-size interval around each time point, then compute centrality measures over the graph across time for each identity lens. This process creates a distribution of the metric across time, which may then be compared between identity lenses. We conjecture that utilizing different-sized sampling intervals and comparing distributions across identity lenses will provide insights to understanding the TVG and the motivating application it describes. We therefore define two methods to sample TIVG  , in a  -sized time interval around any given time-point  t in  T, to a static graph  . They are given by the following node and edge set definitions:   The union of all nodes and edges extant at any time during the interval.   and   are the “presence” functions for nodes and edges, respectively, as defined in (Casteigts et al., 2012).    Only nodes and edges that exist throughout the entire interval.     As a simple example of these sampling methods, consider a correspondence network, where   is a set of individuals and   defines their correspondence; an edge connecting two individuals is present when a letter is in the mail between them. For an interval length,  , of 1 year, the first sampling method would produce a graph containing connections between any individuals who corresponded by letter at any point that year. In comparison, the second sampling method would only leave connected those individuals with constant communication for the entire year.  Using the sampling methods above, we measure characteristics at time points throughout the lifetime of   and thereby evidence the dynamics. The harmonic centrality,  , is an indication of how close the nodes are to each other, while the betweenness centrality,  , is indicative of how interconnected the nodes are within the graph. They are defined as    where   and   are the harmonic and betweenness point-centrality measures (Wasserman and Faust, 1994) for a given node  , respectively. For brevity, we will define here only  , using Boldi and Vigna’s harmonic centrality definition (Boldi and Vigna, 2014), as     where   is the distance between  . As a concrete example of this measure, consider the graphs in Figures 1 and 2. In the graph of Figure 1, node A acts as the central connection point, or hub. The hub has the shortest distance to any node and therefore high harmonic point-centrality,   = 6. Other nodes must traverse at most two edges to reach any other node, giving them   = 3.5. The overall measure for this graph is   = 4.58. In comparison, the graph in Figure 2 has nodes that are distant from most of the other nodes, e.g., node A has   = 2.45 leading to harmonic centrality,   = 1.02. These two graphs demonstrate that the harmonic centrality of the graph is inversely related to the overall “closeness” of the nodes.     Figure 1. A star graph, with nodes shaded based on relative point-centrality, which has harmonic centrality   = 4.58     Figure 2. A linear graph, with nodes shaded based on relative point-centrality, which has harmonic centrality   = 1.02   Allowing  t to range over the entire lifespan of   and considering multiple sizes for our   interval, we generate distributions of the metric across time and with differing levels of temporal granularity. These distributions give a picture of the dynamics occurring within each TIVG. By comparing the metrics across TIVGs under different identity functions for the same TVG, we hope to more fully capture the dynamics of and understand the original evolving network, and provide insights into the motivating application at hand.  We have therefore defined a new conceptualization of Time-Varying Graphs, specifically the identity-lens function and resulting Time/Identity-Varying Graphs under each identity mapping. We also defined methods for sampling the TIVGs into series of measurable static graphs and provided metrics over those representations. At the conference, we will present visualizations that represent each of our applications from the various perspectives, as well as the findings of these measures: to better understand the definition of marriage in Nauvoo and its relation to church formation, and to illuminate patterns in author and departmental co-citations. ",
       "article_title":"Identity Lenses in Analyzing Evolving Social Structures",
       "authors":[
          {
             "given":"John Robert",
             "family":"Hott",
             "affiliation":[
                {
                   "original_name":"University of Virginia, United States of America",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          },
          {
             "given":"Worthy N",
             "family":"Martin",
             "affiliation":[
                {
                   "original_name":"University of Virginia, United States of America",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          },
          {
             "given":"Kathleen",
             "family":"Flake",
             "affiliation":[
                {
                   "original_name":"University of Virginia, United States of America",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-05",
       "keywords":[
          "cultural studies",
          "data mining / text mining",
          "historical studies",
          "English",
          "networks, relationships, graphs",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction Language use performs as a screen or filter to reality, reflecting speakers’ perception and organization of the world around them (Wardhaugh, 2002). In language, modality deploys various kinds of meaning filter (types of modal expression) which variously color and modify our conceptualizations of the world and enable us to represent it with such purposeful diversity (Hoye, 2005). Modality acts as stance-taking or attitudinal qualifications, e.g. necessity (must, should) and possibility (can, may), expressing the speaker’s opinion of a proposition or a predicate and its subject. Simon-Vandenbergen (1997) shows that modal certainty is an important feature of the discourse of political speakers and give a functional explanation of modal selections in political interviews. Garzone (2013) conducts a corpus-based study to show the decline in the use of “shall” in U.K. legislative texts and the use of other modal/non-modal substitutes for the somewhat offensive “shall.” In this paper, we examine modality use in a corpus of historical news to observe the rhetorical stance of government propaganda at a time of governance crisis. The results indicate that a strong sense of moral persuasion and demand was manifested by significant modal use for social responsibility.   2. Theoretical framework and methodology Theoretical studies to modality include generative, cognitive-pragmatic, and typological approaches (Hoye, 2005). The central notions to linguistic modality in typological sense are possibility and necessity (Lyons, 1995). We adopt Li’s (2004) Chinese modal system which was derived from an English modality framework (van der Auwera and Plungian, 1998). The semantic categories of the modal types and the primary Chinese modal verbs are listed below. Chinese modal verbs are poly-functional, each may indicate more than one modal senses.  Epistemic uncertainty: estimates whether something will become a fact or not and suggests objective possibility, which corresponds to epistemic possibility. The modal verbs in Mandarin Chinese that express epistemic possibility are 能 neng2 (can), 能夠 neng2 gou4 (can), 會 hui4 (may), 可 ke3 (may), 可以 ke3 yi3 (may), 得 de2 (can).  Epistemic probability: predicts the necessity about a finite event or state or concludes about the necessity of a current event. The modal verbs that are used to express a note of conjecture include 該 gai1 (should), 應該 ying1 gai1 (should), 要 yao4 (will), 得 dei3 (must).  Ability: expresses subjective possibility of participant related ability, function, property, or quality. Chinese modal verbs indicating participant-internal possibility include能 neng2 (can), 能夠 neng2 gou4 (can), 會 hui4 (can), 可 ke3 (can), 可以 ke3 yi3 (can), 得 de2 (can). Need: concerns with subjective necessity that corresponds to need internal to the participant involved in the state of affairs. It relates to hope, intention, and interest which come to cause the ultimate action or event. The Chinese modal verbs identified for expressing need are 要 yao4 (need), 需要 xu1 yao4 (need), 須 xu1 (need), 必須 bi4 xu1 (must), and 得 dei3 (must). Permission and circumstantial possibility: deals with possibility out of deontic sources like rules, regulations, authority, or non-deontic objective circumstances. Chinese modal verbs that express the notional categories of permission and circumstantial possibility are: 能 neng2 (can), 能夠 neng2 gou4 (can), 可 ke3 (may), 可以 ke3 yi3 (may), 得 de2 (can). Obligation and circumstantial necessity: involves deontic necessity out of morality or social conventions, and non-deontic necessity out of objective situations and reasons. Modal verbs that express obligation and circumstantial necessity include 要 yao4 (must), 該 gai1 (should), 應該 ying1 gai1 (should), 應 ying1 (should), 當 dang1 (should), 應當 ying1 dang1 (should), 須 xu1 (must), 必須 bi4 xu1 (must), 得 dei3 (must).  The corpus for our investigative purpose is the 228 event Taiwanese news archive, published by the 228 Event Memorial Foundation to compile local news articles during a short period, dated from 2/28/1947 to 5/15/1947, of widespread riot after Chinese takeover of Taiwan at the end of World War Two. The current study focuses on news articles from Taiwan Shin Sheng Daily News (TSSDN), controlled by the government at that time, while all private news publishers were shut down one week after the incident erupted. As a baseline benchmark, we use the Academia Sinica Balanced Corpus of Modern Chinese (Sinica Corpus), designed to be a balanced collection from different areas of genre, style, mode, topic, and source (Huang and Chen, 1992). The two corpora are both Mandarin (modern Chinese) without notable language variants and are considered as comparable.  For both TSSDN and Sinica corpora, we use CKIP segmentation and part-of-speech (POS) tagging for first-line processing (Chang & Chen, 1995). The size of the TSSDN corpus is a total of 0.237 million word tokens. The Sinica Corpus (version 3.1), dated approximately from 1981 to 1997, contains a total of 5.738 million word tokens, which is about 24 times the size of the TSSDN corpus. For the purpose of comparing modal use in the TSSDN corpus and the Sinica corpus, the following procedure was used to extract and observe respective modal use distribution. Step 1. For each word token in the list of common modal verbs, extract sentences that contain the specified word token with the POS tag of auxiliary verb.  Step 2. Rank the modal verbs by its occurrence frequency, i.e., the number of the extracted sentences for each modal verb. Step 3. Exclude the bottom half of the list of modal verbs that show insignificant use and compare the use of significant modal verbs with both absolute frequency and normalized frequency (per million word tokens). Step 4. For the TSSDN corpus, assess each sentence in use of a modal verb and determine its actual modal type. Step 5. Compile the actual use frequency of the modal types in the TSSDN corpus. Step 6. Compile the top five verb semantics following the use of each modal verb for observing the association of modal sense and semantic notion.    3. Results and discussion An initial frequency observation on the list of common modal verbs in the TSSDN corpus is shown in Table 1. This led us to focus on the top half of the modal verbs that are clearly of more significant use in the investigated corpus.  Table 1. Rank list of common modal verbs by frequency in TSSDN corpus   Rank 1 ~ 4 Rank 5 ~ 8 Rank 9 ~ 12 Rank 13 ~ 16   應 ying1 (should) 535 可以 ke2 yi3 (can, may) 206 會 hui4 (may, can) 77 應當 ying1 dang1 (should) 21   要 yao4 (will, must) 476 須 xu1 (must) 170 得 de2, dei3 (can, must) 69 能夠 neng2 gou4 (can) 13   可 ke3 (can, may) 416 應該 ying1 gai1 (should) 138 需要 xu1 yao4 (need) 68 該 gai1 (should) 11   能 neng2 (can) 403 必須 bi4 xu1 (must) 123 當 dang1 (should) 66 需 xu1 (need) 6   Table 2 compares the use of major modal verbs in TSSDN corpus, Sinica corpus, and the newspaper subset of Sinica corpus (29.4% of the whole corpus). It is observed that TSSDN corpus shows a considerable frequency variation of modal verb use when compared with Sinica corpus and its newspaper subset. Among the eight modal verbs, three pairs of modal verbs are actually variants of each other and may be aggregated to better represent the semantic notions of the modal verbs. Table 2. Benchmark comparison of modal verbs use   Modal Verbs TSSDN corpus Sinica corpus Increase Ratio Sinica corpus Newspaper subset Increase Ratio    Absolute Frequency Normalized Frequency Absolute Frequency Normalized Frequency  Absolute Frequency Normalized Frequency    應 ying1 (should) 535 2257.4 3250 566.4 398.5% 1228 728.6 309.8%   要 yao4 (will, must) 476 2008.4 15783 2750.7 73.0% 3135 1860.2 108.0%   可 ke3 (can, may) 416 1755.3 8318 1449.7 121.1% 2439 1447.2 121.3%   能 neng2 (can) 403 1700.4 10867 1893.9 89.8% 3038 1802.6 94.3%   可以 ke2 yi3 (can, may) 206 869.2 9546 1663.7 52.2% 1981 1175.4 73.9%   須 xu1 (must) 170 717.3 786 137.0 523.6% 237 140.6 510.1%   應該 ying1 gai1 (should) 138 582.3 2787 485.7 119.9% 522 309.7 188.0%   必須 bi4 xu1 (must) 123 519.0 3181 554.4 93.6% 788 467.6 111.0%   In Table 3, we aggregate three pairs of modal verb variants of the same semantic notion and re-calibrate the relative amount of modal use in the two corpora and one sub-corpus. The comparison shows that the use of應 ying1, 應該 ying1 gai1 (should) and須 xu1, 必須 bi4 xu1 (must) in TSSDN corpus are significantly frequent than in Sinica corpus and its newspaper sub-corpus, while the use of the other modal verbs are somewhat comparable. This indicates that TSSDN corpus contains a strong attitude and stance through the unusual emphasis of should and must.  Table 3. Benchmark comparison of modal verbs use in semantic notions   Modal Verbs TSSDN corpus Sinica corpus Increase Ratio Sinica corpus Newspaper subset Increase Ratio    Absolute Frequency Normalized Frequency Absolute Frequency Normalized Frequency  Absolute Frequency Normalized Frequency    應 ying1, 應該 ying1 gai1 (should) 673 2839.7 6037 1052.1 269.9% 1750 1038.4 273.5%   可 ke3, 可以 ke2 yi3 (can, may) 622 2624.5 17864 3113.3 84.3% 4420 2622.6 100.1%    要 yao4  (will, must)  476 2008.4 15783 2750.7 73.0% 3135 1860.2 108.0%   能 neng2 (can) 403 1700.4 10867 1893.9 89.8% 3038 1802.6 94.3%   須 xu1, 必須 bi4 xu1 (must) 293 1236.3 3967 691.4 178.8% 1025 608.2 203.3%   Next, we observe how the use of modal verbs is distributed in the modal system to depict various aspects of attitude and stance. Each occurrence of a modal verb in a sentence is categorized in modal type by independent coders. Disputed codes are discussed to reach consensus decision. Table 4 breaks down the occurrence of modal type expression by the poly-functional modal verbs. The results reveal an extremely high concentration on the modality type of obligation, signaling a heavy dose of demand and persuasion of social responsibility from government propaganda.  Table 4. Frequency distribution of modal type expression by modal verbs   Modal Type by Modal Verb Epistemic uncertainty Epistemic probability Ability Need Circumstantial possibility Circumstantial need Permission Obligation   應 ying1, 應該 ying1 gai1 (should)  11    15  647   可 ke3, 可以 ke2 yi3 (can, may) 112  97  248  165    要 yao4 (will, must)  27  74  33  342   能 neng2 (can) 8  168  138  89    須 xu1, 必須 bi4 xu1 (must)    3  77  213   Absolute Frequency 120 38 265 77 386 125 254 1202   Normalized Frequency 506.3 160.3 1118.1 324.9 1628.7 527.4 1071.7 5071.7     Figure 1. Chronological occurrence of modal types on a daily timeline    Figure 2. Chronological occurrence of modal types on a weekly timeline  We also observe the normalized occurrence with respect to word count of news reports on a daily timeline in Figure 1 and weekly timeline in Figure 2. The temporal variation depicts a process of employing the rhetoric of obligation that immediately peaks in the second week, followed by a lower peak in the seventh week before gradually reduced in the third month, over the period in which social order was lost and regained, social activities was disrupted and restored. Table 5 compiles the top five verb semantics, ranked by the occurrence frequency and ratio, associated with the use of each modal verb in a sentence. This helps provide a better rhetorical picture of what is being said, appealed, urged, or even warned. Overall, we observe a rhetoric sense of strict attitude and firm stance on exercising and restoring control of social order.  In conclusion, our study seems to indicate that modality is an effective linguistic feature for extracting narrative stance and provides a convenient contextual view of a corpus. Our future work includes examining more comprehensive modal expression and evaluating against corpora of various historical context.  Table 5. Primary verb semantics following modal verbs   Modal Verb Top Five Verb Semantics with Occurrence Frequency and Ratio Semantic Meaning   應 ying1, 應該 ying1 gai1  (should)  注意 zhu4 yi4 (33) (4.9%) 遵守 zun1 shou3, 遵照 zun1 zhao4 (27) (4.0%) 負責 fu4 ze2 (27) (4.0%) 處分 chu3 fen4, 嚴懲 yan2 cheng3 (27) (4.0%) 檢查 jian3 cha2, 調查 diao4 cha2 (25) (3.7%)   heed comply responsible punish inspect     可 ke3,  可以 ke2 yi3  (can, may)   說 shuo1, 知 zhi 1(63) (10.1%) 恢復 hui1 fu4, 穩定 wen3 ding4 (45) (7.2%) 實現 shi2 xian4, 達成 da2 cheng2 (36) (5.8%) 運輸 yun4 shu1, 通行 tong1 xing2 (32) (5.1%) 報告 bao4 gao4, 提請 ti2 qing3 (26) (4.2%)   speak, know restore, stabilize achieve transport, pass report, submit     要 yao4 (will, must)   知道(zhi1 dao4), 認識(ren4 shi4) (64) (13.4%) 努力 nu3 li4, 加強 jia1 qiang2 (34) (7.1%) 說明 shuo1 ming2 (21) (4.4%) 檢舉 jian3 ju3, 糾正 jiu1 zheng4 (17) (3.6%) 負責 fu4 ze2 (14) (2.9%)   know, perceive strive, strengthen explain report fault, correct responsible     能 neng2  (can)   了解 le3 jie3, 明瞭 ming2 liao3 (39) (9.7%) 恢復 hui1 fu4 (24) (6.0%) 實現 shi2 xian4, 達成 da2 cheng2 (22) (5.4%) 解決 jie3 jue2, 克服 ke4 fu2 (19) (4.7%) 看懂 kan4 dong3, 讀寫 du2 xie3 (16) (4.0%)   understand restore achieve solve, overcome read/write     須 xu1,  必須 bi4 xu1 (must)   登記deng1 ji4, 註冊zhu4 ce4 (12) (4.1%) 持有 chi2 you3 (12) (4.1%) 注意 zhu4 yi1 (11) (3.8%) 懲辦 cheng2 ban4, 處分chu3 fen4 (8) (2.7%) 肅清 su4 qing1, 鎮壓zhen4 ya1 (7) (2.4%)   register carry (valid permit) heed punish exterminate, suppress     ",
       "article_title":"Evaluating Modal Use in News Corpus for Constructing Rhetorical Context of Historical Event",
       "authors":[
          {
             "given":"Jyi-Shane",
             "family":"Liu",
             "affiliation":[
                {
                   "original_name":"National Chengchi University, Taiwan, Republic of China",
                   "normalized_name":"National Chengchi University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/03rqk8h36",
                      "GRID":"grid.412042.1"
                   }
                }
             ]
          },
          {
             "given":"Ching-Ying",
             "family":"Lee",
             "affiliation":[
                {
                   "original_name":"University of Kang Ning, Taiwan, Republic of China",
                   "normalized_name":"University of Kang Ning",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/05hewaf81",
                      "GRID":"grid.459668.0"
                   }
                }
             ]
          },
          {
             "given":"Ke-Chih",
             "family":"Ning",
             "affiliation":[
                {
                   "original_name":"National Chengchi University, Taiwan, Republic of China",
                   "normalized_name":"National Chengchi University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/03rqk8h36",
                      "GRID":"grid.412042.1"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016",
       "keywords":[
          "rhetorical studies",
          "corpora and corpus activities",
          "historical studies",
          "linguistics",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" This paper explores the relationships between countries in the exchange of movies and measures the reciprocal nature of these relationships. This investigation represents an innovative way to explore international exchanges of digital cultural content based on global cinema screenings analysed at the national level. Rather than focus on the market dominance of particular cinemas (e.g. the US or Indian cinemas) we examine the relative strength of two-way relationships in order to understand cultural reciprocity in the film industry. The dynamics of shared cultural exchange are explored in terms of the volume of transactions between ‘cinema nations’ expressed in the form of dyadic networks.  The paper is based on the premise that films can be understood as cultural goods that are distributed both between ‘territories’ or markets and across the globe according to industrially unique spatial patterns and temporal flows. Seeing film diffusion in this way invites us to explore the industrial aspects of movement and location but it also invites reflection on our use of these large datasets. For example, understanding the dynamics of global film exhibition and distribution demands an appreciation of scale and velocity in both the film industry and in a data-driven approach to its study. Data-driven approaches to Cinema Studies are at best an emergent aspect of the discipline (Verhoeven). This paper makes a significant contribution to the development of Cinema Studies by extending a trans-disciplinary, digital humanities approach to critically understanding the dimensions of a global creative industry at scale.  Digitisation and globalisation are full of contradictions in terms of how they impact diversity of screen culture represented by film. On one hand digitisation has facilitated an explosion in the number of films being made and that can be distributed and viewed online. This has had the effect of increasing the diversity of films available to audiences with digital access over the web. On the other hand however, only a relatively small proportion of films produced are released widely into cinemas. This paper seeks to provide insight into diversity at cinema locations that extends beyond the obvious dominance of Hollywood blockbusters. We are interested in drawing attention to equitable reciprocal exchange relationships that exist between nations, even where these may be small in scale, as evidence of alternative practices in the promotion of diversity at the cinema. This enables us to explore relational geographies using dyads in an approach similar to that of Taylor, Hoyler, Pain and Vincigurrra (2013) in their investigation of the connectivity between cities in the services sector. We use dyadic analysis to explore an equitable exchange in film that extends beyond the unilateral to ensure cultural exchange between two nations is assessed as a two-way flow where cultural content from both sides to the dyadic relationships are valued and accounted for.  The data used in this paper is drawn from the Kinomatics Global Showtime dataset (Kinomatics, 2015) which comprises over 330 million individual records of film screenings from across 31,500 venues covering 47 countries, including the US, India, most of Western Europe, Japan, Brazil and Australia for the years 2013 – mid-2015 (see Arrowsmith et al). For this analysis, the kinomatics data is analysed using a variety of methods that draw from a range of disciplinary perspectives including the digital humanities and economics as well as geospatial and computational sciences. Our key tool is Principal Components Analysis that explores dyadic relationships as part of Social Network Analysis (see: Wasserman and Faust, 1999). We apply this approach to ‘nations’ as they are defined by the kinomatics (cinema screening data aggregated at the national level) and imdb (film production aggregated at the national level) datasources. Further to this, in a selection of countries, we employ a Herfindahl-Hirschman Index (HHI) to consider case study analysis of cultural diversity based on cinema screen count data. We use spatio-temporal visualisations as a way to represent the results and propose insights into the relational geographies of film flow and exchange that are found to exist.  Using dyads to explore international exchanges between nations we are able to consider diversity in relation to films screened at the cinema in terms of the two key dimensions stemming from globalised relations between nations, namely in terms of intensive and extensive international exchanges. The intensive dimension aids understanding of the most important national dyads that dominate cinema screenings that can be seen as the core centres of the globalised market for film, while the extensive dimension is focused upon the multitude of links between nations in a broader globalised market for film screened at the cinema. The analysis of dyadic relationships enables us to move beyond the assumption that the flow of cinema is simply unilateral. Instead we are concerned with the relative strength of exchanges in which a strong reciprocal dyadic relationship is one that has an equal exchange between two nodes, in this case, countries. In considering diversity using the HHI we focus on the screening of new release features within the case studies of Australia, France and the Republic of Korea in order to analyse the relationship between cinema venue location (capital cities versus regional), venue type (in terms of the number of screens) and film programming allocations between domestic, US and other imported feature films. We find that as an increasing number of films are being released, non-US derived films are struggling in a tight contest for screen time.  ",
       "article_title":"Using Big Cultural Data To Understand Diversity And Reciprocity In The Global Flow Of Contemporary Cinema",
       "authors":[
          {
             "given":"Deb",
             "family":"Verhoeven",
             "affiliation":[
                {
                   "original_name":"Deakin University, Australia",
                   "normalized_name":"Deakin University",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/02czsnj07",
                      "GRID":"grid.1021.2"
                   }
                }
             ]
          },
          {
             "given":"Bronwyn",
             "family":"Coate",
             "affiliation":[
                {
                   "original_name":"RMIT University, Australia",
                   "normalized_name":"RMIT University",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/04ttjf776",
                      "GRID":"grid.1017.7"
                   }
                }
             ]
          },
          {
             "given":"Colin",
             "family":"Arrowsmith",
             "affiliation":[
                {
                   "original_name":"RMIT University, Australia",
                   "normalized_name":"RMIT University",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/04ttjf776",
                      "GRID":"grid.1017.7"
                   }
                }
             ]
          },
          {
             "given":"Stuart",
             "family":"Palmer",
             "affiliation":[
                {
                   "original_name":"Deakin University, Australia",
                   "normalized_name":"Deakin University",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/02czsnj07",
                      "GRID":"grid.1021.2"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "media studies",
          "film and cinema studies",
          "English",
          "networks, relationships, graphs"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Digital scholarship depends on the availability of data in forms which are tractable to computational techniques, implying storage of data in sustainable archives. This is perhaps even more true of research in the humanities than in science. As the ESF observed in their 2011 report on research infrastructure: “in the hard sciences, datasets tend to be generated rather than collected and tend to be homogeneous in nature …. In Humanities, data tends to be collected and to be heterogeneous in content and format” (European Science Foundation, 2011:5). It is both the possibility of storing the data and the stored data itself which makes up critical infrastructure in many disciplines and innovative scholarly practices may not develop in the absence of such infrastructure. Here, we discuss the development of digital infrastructure in the field of language documentation (within the discipline of linguistics) and try to assess the extent to which the provision of well-funded (by the standards of the discipline) infrastructure is changing scholarly practice. Language documentation as a field in linguistics dates from the publication of the seminal paper ‘Documentary and descriptive linguistics’ (Himmelmann, 1998). It is in large part a reaction by linguists to the challenge of language endangerment (see Hale et al., 1992; and Musgrave, 2015 for a recent survey) and it emphasises the collection of large bodies of data of languages in use. Language documentation began at the same time that it was becoming feasible to make high quality digital recordings, both audio and video, on reasonably priced equipment. Archiving of documentary material was a core component of the program as conceived by Himmelmann, and this was immediately seen to mean digital archiving. Indeed, it can be argued that the whole enterprise of documentary linguistics falls comfortably within the digital humanities (Thieberger, 2014). Two projects began in the first decade of the century to fund researchers to make collections of documentary materials. One was based at the Max Planck Institute for Psycholinguistics (Nijmegen, The Netherlands) and funded by the Volkswagen Stiftung;  Dokumentation Bedrohte Sprache (DoBeS): http://dobes.mpi.nl/ the other was based at the School of Oriental and African Studies (London, UK), funded by Arcadia.  Hans Rausing Endangered Languages Project: http://www.hrelp.org MPI Nijmegen had a well-resourced technical department which took responsibility for developing the archiving stream of the DoBeS project. HRELP had to build their archive from scratch; no existing resources at SOAS were available to support their work. For both projects, it was a requirement of funded research that data were deposited in the relevant archive. Both archives, however, are open to deposits from non-funded researchers.  The DoBeS archive is subsumed under a larger archive called The Language Archive https://tla.mpi.nl which also holds language data from other MPI activities and from other institutions and projects. Figure 1 shows the cumulative deposits in TLA for DoBeS material and for material classified as donated. The figures here represent the number of files retrieved from the catalogue based on their ’Last modified’ field. This does not reflect the accession date of the file in all cases, but it is a reasonable proxy.  We are deeply grateful to Paul Trilsbeek of The Language Archive for assistance in refining this data.    Figure 1. Cumulative deposits in TLA  The archive at SOAS is known as ELAR. Figure 2 shows cumulative deposits in that archive; figures are drawn from information provided by ELAR in the annual reports of HRELP and represent bundles of data deposited rather than individual data files.   Figure 2. Cumulative deposits in ELAR  Although both archives have grown over the periods shown, the patterns are different, with a flattening out in the DoBeS deposits starting around 2011. There was no funding round in 2010. Projects were funded in 2011 and 2012 but 2012 was the final round and while the archive still expects to receive deposits from the last two funding rounds, such deposits will continue to decrease. Figure 3 shows the percentage of donated deposits based on the cumulative figures for each archive.   Figure 3. Comparison of the percentage of donated material in the two archives  For both archives, once the infrastructure was established, non-funded deposits make up an increasing proportion of the archive. TLA has added large amounts of donated material in recent years, in part a conscious effort by the archive to expand its activities beyond the DoBeS project by taking responsibility for existing data sets. One barrier to archiving data is meeting an archive’s requirements particularly in the area of metadata. ELAR uses a more flexible metadata system than TLA (which uses the IMDI scheme,  https://tla.mpi.nl/imdi-metadata/). We might therefore expect more voluntary deposits in ELAR than in TLA, but this is not the case in these data. The different proxies we are using to assess these trends here make it difficult to compare the two exactly, but we can see a clear trend over the last decade which suggests that archiving data is increasingly a part of scholarly practice in this area of linguistics and that there has been progress since the rather gloomy summary provided by Thieberger (2011).  One question raised by this data is the extent to which the donated data is coming from researchers who have also been funded by the relevant program. Both projects provide training to funded researchers and it is possible that the practices learned there are continued when researchers collect data in other projects. In the case of TLA, 36 data sets have been deposited which were not the result of DoBeS funding and in six cases the researcher(s) had been funded by DoBeS for other work. ELAR has material not associated with funded projects deposited by 37 researchers of whom eight had been funded by ELDP for other work. These figures suggest that acquiring experience of the archiving process is a factor in future work practices, and that this factor has had very similar levels of effect in both archives.  The data which we have used in this paper are limited and important questions remain unanswered. The most obvious is how and to what extent are these resources being used. ELAR provide some usage statistics; based on logs for the month of November 2013, estimated traffic on the catalogue and the archive portal is around 680 users per day with an estimated 1.66 million pages served per year. These numbers suggest that the archive is being used a lot, but it is not possible to tell who the users are or what they are doing on the site. More fine-grained data are needed to tell us whether the availability of resources such as ELAR and TLA is changing scholarly practice in accessing language data. The data we present here indicates that, in the field of documentary linguistics, the availability of good infrastructure for digital archiving has had an impact on scholarship. Data is being deposited in the archives beyond the requirements of the relevant funding bodies, and in the case of the archive for which usage statistics are available, it seems that the level of activity is substantial. These changes may in part be due to the level of training provided to funded researchers; but such training should, we suggest, be considered an essential part of the research infrastructure of digital scholarship. Another reason for the changes may be that linguistic scholarship is moving towards recognising primary data as scholarly output (Thieberger et al., 2016). This process is only possible when primary data can be cited using persistent identifiers provided by a repository (cf. NSF (Task Force on Data Policies), 2011:9 (Recommendation 2)). There is therefore the possibility of a virtuous circle here: adoption of best practice in data management will lead to more robust research methods in the field as well as career benefits for researchers. ",
       "article_title":"If You Build It Will They Come? Digital Infrastructure And Disciplinary Practice In Language Documentation",
       "authors":[
          {
             "given":"Simon",
             "family":"Musgrave",
             "affiliation":[
                {
                   "original_name":"Monash University, Australia",
                   "normalized_name":"Monash University",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/02bfwt286",
                      "GRID":"grid.1002.3"
                   }
                }
             ]
          },
          {
             "given":"Nick",
             "family":"Thieberger",
             "affiliation":[
                {
                   "original_name":"University of Melbourne",
                   "normalized_name":"University of Melbourne",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/01ej9dk98",
                      "GRID":"grid.1008.9"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "digital humanities - facilities",
          "corpora and corpus activities",
          "archives, repositories, sustainability and preservation",
          "linguistics",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" What is diplomatic culture or “rhetoric” and can we measure it? This project is an attempt to understand quantitatively the language and structure of U.S. diplomacy as a bureaucratic institution through analysis of a large corpus of diplomatic papers. Since the “cultural turn” of History in the late twentieth century historians have produced cultural interpretations of American diplomacy that highlight gender and racial influences and justifications in diplomatic decision making but there have not been attempts to measure longue duree changes or to quantify them by a standardized measure. 1 This work hopes to fill this gap by introducing a new method of analyzing time-dependent bodies of text. I then apply these methods to a corpus of diplomatic papers to systematically chart changes in concepts and ideology detectable in diplomatic language. The project has two aims. First, I am designing a method of measuring how a concept, as a fixed variable, evolves through a temporal corpus. Tools such as GloVe and Dynamic Topic Modeling are two existing approaches that can be used to understanding the linguistic shift and topic distributions over time. 2 I argue, however, that these tools are not adequate yet for time-sensitive tasks such as tracing concepts over time and attempt to design a new method optimized for this task. Second, mindful of the particular characteristics of this corpus as a set of diplomatic papers I want to apply the most appropriate methods of analysis and engage with the existing historiography on the Cold War to engage with claims historians have made about Cold War ideology. I am trying to answer: what is \"Cold War rhetoric\" in high level diplomacy? how did it evolve over the decades? and how did it become propagated within the diplomatic institution? I approach these questions from two methodological angles - networks and NLP.   Dataset The dataset for this analysis is the entire text corpus of the Foreign Relations of the United States (FRUS, 1861-1980), a collection of published declassified diplomatic papers. The FRUS collection is hand-curated by librarians at the Office of the Historian to be a representative and comprehensive sample of American diplomatic history and has been a dependable primary source base for historians and social scientists for decades. The document types include telegrams, airgrams, notes, and memoranda among others. Most documents have accompanying meta-data such as the name of the sender, name of the recipient, location of the sender, and date when applicable. So far, I have focused on a subset of this corpus, consisting of all papers from 1948 to 1980, to analyze the high Cold War era. This subset is also what is available in xml format online with hand-tagged meta-data and reliably edited text. About 92,000 documents were available for analysis. 3 A notable caveat of this work is that the corpus, while presumably a representative sample, is still a sample of non-random selection and could carry both deliberate and unintended biases of the librarians. The FRUS is a corpus that becomes publically accessible upon publication and has a fixed audience of social scholars. There is an inevitable feedback loop where the curators respond to the requests of the prime users of the collection such as adjusting the proportion of the most „useful“ types of documents. For instance, as the Vietnam War is a highly contested field of scholarship curators may include a higher ratio of papers surrounding the Vietnam War thus distorting the overall representation of topics by exaggerating the war’s significance. This possible limitation is something to keep in mind throughout the analysis of this corpus.   Image A: Example of FRUS document; Stuart as the Ambassador in China to the Secretary of State sent from Nanking on April 25, 1949    Analysis  Descriptive summaries of the meta-data from the corpus shows several clear distributive patterns. From the total set of all documents 42,000 were correspondences (have to and from agents) from which I parsed all available meta-data (name of sender, name of recipient, location of sender, date) and made inferences on missing data based on historical knowledge. The results show that the Department of State (DOS) as a bureaucratic institution is highly centralized around key actors. Not surprisingly, the prime location of correspondence origin is Washington and its overwhelming predominance indicates that the DOS correspondence system was used for sharing information from the center to the peripheries. Similarly, the top senders of the correspondents were concentrated in high administrative positions – the Secretary of the State (SS), Department of State (DOS) administrative center, and the National Security Advisor (NSA). The individuals that have the highest correspondence authorship are therefore those who have held SS or NSA positions such as Dean Acheson (SS), John Foster Dulles (SS), Henry Kissinger (SS, NSA), and Walt Rostow (NSA). The top recipients of correspondences are also SS and DOS confirming a much bilateral relation between central and peripheral offices.    Graph 1A: Location of Correspondence Origin    Graph 1B: Top Correspondences sent from    Graph 1C: Persons that sent the most correspondences    Graph 1D: Top recipients of correspondences  I then mapped the network structure of correspondences to make the problem of bureacratic “culture” more concrete and visual. In this abstract, I have included images of correspondence networks from two discrete periods – when Kissinger served as Secretary of State from 1973-1977 and when Rostow served as National Security Adviser from 1966-1969. From the maps we can see the overall design of flow of information and transfer of knowledge based on the counts of correspondences and their directions. The maps confirm that indeed the bulk of the correspondence happens bilaterally between top administrative posts and peripheral agents. For instance, the DOS acts as the center point of correspondence for all embassies placed abroad and NSA as that for Washington based lower ranking administrative posts, such as the Under Secretaries of State. Further, two distinct communities emerge within the U.S. diplomatic institution. In both images, one can discern that the DOS and NSA act as distinct and separate focal points while the SS connects the two camps of correspondence and acts as a bridging agent of the two communities.    Graph 2A: Map of correspondence networks during the years Kissinger served as Secretary of State (1973-1977)    Graph 2B: Map of correspondence networks during the years Rostow served as National Security Advisor (1966-1969)    Chart A: Sample of select words’ change of top 10 GloVe neighbors from 1860s and 1950s (‘economy’, ‘empire’, ‘freedom’, ‘european’)  With this structural framework, I am using a combination of NLP methods to trace given “concepts” to see how they have changed in meaning over time. I identify concepts as individual terms, such as „liberty,“ or as a collection of related terms (topics). Word vectors are the most intuitive method of tracing changes in word meanings. Global vectors (GloVe) and other word vector models suppose a time stagnant corpus so I divided my corpus into decade-long chunks and worked with the assumption that language would not change in usage and meaning significantly enough to matter within ten-year time spans. My results comparing the nearing neighbors by Euclidean Distance of GloVe outputs of select concepts show there is a qualitative difference in conceptual meaning in the 1860s and 1950s. For instance, the concept „freedom“ in the nineteenth century was associated with more poetic and romanticized terms such as „triumph“ and „humanity“ whereas a century later it came to be linked with legal and defensive terms such as „right“ and „safeguard.“ Historians would contextualize this phenomenon in the American Civil War and the Civil Rights respectively. Then a question arises: Were diplomatic agents using terms that reflect the popularized lingo of their time or were their propagating it themselves? Who, in the DOS, initiated the usage of these concepts in such ways? Can we use the networks mapped above for to interpret this?    Discussion This is a work in progress and there is still much work to be done in finding and developing new tools appropriate for time sensitive text data. Given history is a study of change and its significance, it is imperative that we do not assume a static distribution of words across time, eliminating many otherwise useful NLP tools. While Dynamic Topic Modeling considers time as a variable, it constructs a fixed number of topics based on a collection of words making it less favorable for corpora with minimal predictability and pattern as diplomatic papers. Unlike academic journals such as  Science as Blei and Lafferty have applied their modelling on, diplomatic papers are much less consistent in topics. I have also discovered based on my experience of implementing these tools on the FRUS corpus that because in diplomatic papers certain topics predominate discussions at certain dates, I need to be aware of isolating these topics from purely semantic changes. For instance, in the 1940s, “communism” is closest neighbor based on GloVe results to “chinese” or “ccp” because of the Chinese Communist Revolution of 1949, which does not yield any surprising result about the meaning of “communism” in diplomacy.    ",
       "article_title":" Diplomatic History by Data Understanding Cold War Foreign Policy Ideology using Networks and NLP ",
       "authors":[
          {
             "given":"Eun Seo",
             "family":"Jo",
             "affiliation":[
                {
                   "original_name":"Stanford, United States of America",
                   "normalized_name":null,
                   "country":"United States",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "natural language processing",
          "xml",
          "historical studies",
          "linguistics",
          "English",
          "networks, relationships, graphs",
          "text analysis",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction How can one virtually ‘circle’ some music notation as one would on a printed score? How can a machine interpret this ‘circling’ to select and retrieve the relevant music notation in digital format? This paper will introduce the concept of addressability for music notation, on the basis of a comparison with textual addressability as defined by Michael Witmore (2010). Additionally, the paper will report on the work of  Enhancing Music notation Addressability (EMA), a NEH-funded one-year project that has developed methods for addressing arbitrary portions of encoded music notation on the web.  Many Digital Humanities projects are concerned with the digitization of cultural objects for varied purposes of study and dissemination. Theorists such as Willard McCarty (2005) and Julia Flanders (2009) have highlighted the fact that digitization involves the creation of a data model of a cultural object, whereby scholarly interpretation and analysis is inevitably included in the model. Editorial projects in literary studies, for example, often model sources by encoding transcription and editorial intervention with the Text Encoding Initiative (TEI) format. The ability to identify and name textual structures is a fundamental operation in the creation of such models. Michael Witmore has called text a “massively addressable object” (2010); that is, given certain abstractions and conventions, it is possible to identify areas of a text such as characters, words, as well as chapters or proper names. Reading practices influence and contribute to the development of such conventions and abstractions, but, Witmore argues, addressability is a textual condition regardless of technology. With digital texts, modes of address become more abstract, so that arbitrary taxonomies can be identified as well as more established ones. To exemplify a more abstract mode of address, Witmore suggests items “identified as a ‘History’ by Heminges and Condell in the First Folio”. This enhanced addressability available in a digital context is the engine for textual analysis and scholarly discourse about digital text. This idea of addressability is arguably applicable to many more kinds of “text”, including music notation; indeed, addressing units of music notation (such as measures, notes, and phrases) has long been a powerful instrument in musicology for both analysis and historical narrative.  When talking about music in general, it is important to say that addressing written music notation is not the only instrument of the musicologist. Music exists on several domains besides the written or \"graphemic\" one, each addressable in its own way (see Babbitt 1965). For the purpose of this paper, we focus on written Western music notation, because it shares features with written language and for its prominent role in musicological discourse. Music notation, however, is more complicated to represent digitally than text. Human-computer interaction has since its early days been built around the concept of character and line, which makes dealing with “plain” text a fairly straightforward matter for many basic operations; counting the number of characters in a given plain text document is trivial in any digital environment.  Modern computing systems are able to support complex ancient and modern writing systems, including those requiring right-to-left strings and compound symbols. The Unicode Consortium has been at the forefront of the internationalization of computing systems. Nonetheless, computationally speaking, a “string” of text remains a sequence of characters even in more complex representations. Indeed, many compound Unicode characters still retain sequentiality, i.e. one component comes after the other and the compound symbol only makes sense if they are in the correct order. Music notation is not a string of text; therefore this is not possible. Music notation, on the other hand, requires substantial computational modelling even for the simplest musical text before any further operation is possible. This is particularly evident when music notation is represented with markup, which implies a system based on characters and lines. There are many different ways of representing a single note; some aspects are common to all representation systems, such as information about pitch and duration, but some systems will prioritize certain aspects over others. To give a simple example, one system may represent beams (ligatures between flagged notes, usually shorter in duration), while others may ignore them altogether.  By grouping notes together, beams provide important—but somewhat secondary to pitch and duration—information to the reader of a music score, such as a performer, a musicologist, or an algorithm.  Nonetheless, there are simple units that are typically represented by all music notation systems for common western music notation, such as measure, staff (or instrument), and beat. The EMA project, therefore, developed a URI scheme and an Application Programming Interface (API) to make it possible to target music notation resources on the web regardless of their format. Such a scheme may facilitate (and in some cases enable) a number of activities around music notation documents published on the web. The following table gives a few basic examples of how an implementation of the URI scheme could be useful to musicological research:   Scholarly Visual Procedural    Analysis: being able to address components of music notation for analytical purposes. Example: precisely identify start and end of a pedal tone in Bach’s  Prelude no. 6 in D Minor, BWV 851.   Rendering: rendering music notation in an interactive environment such as a browser or a tablet requires the ability to cut up a large music document. For example to show only the number of measures that fit in a given space.   Processing: extracted portions of music notation can be passed on to another process. For example, given the MEI encoding of the Overture to Mozart’s  Don  Giovanni, extract the string instrument parts and send them to another program that will return an harmonic analysis.     Citation: quote a passage from an encoded music notation file. For example the timpani in the opening bars of the Overture to Mozart’s Don Giovanni.   Highlighting: address a segment of music notation to highlight it in a visual context (e.g. with color).     The EMA project has particularly focused on facilitating citation and attribution of credit, as is discussed in the “Evaluation” section below.   A brief overview of the specification The specification was created to provide a web-friendly mechanism for addressing specific portions of music notation in digital format. This is not unlike the APIs often provided by image servers for retrieving specific portions of an image. Such servers typically operate on a given large image ﬁle and are able to return different zoom levels and coordinate spaces. The International Image Interoperability Framework (IIIF) has recently created an API to generalize interaction with image providers, so that it can be implemented across multiple servers and digital libraries. IIIF was used as a model for the Music Addressability API created for EMA and briefly described here. Consider the following example,  Taken from  Du Chemin:  Lost Voices project, at  http://digitalduchemin.org.  and the notation highlighted in the boxes:       The highlighted notation occurs between measure 38 and 39, on the first and third staves (labelled  Superius and  Tenor — this is a renaissance choral piece). Measure 38, however, is not considered in full, but only starting from the third beat. This selection can be expressed according to a URI syntax:  /{identifier}/{measures}/{staves}/{beats}/ /dc0519.mei/38-39/1,3/@3-3 The measure is expressed as a range (38-39), staves can be selected through a range or separately with a comma (1,3), and the beats are always relative to their measure, so @3-3 means the third beat of the starting measure to the third beat of the ending measure. A complete description of the URI scheme and the API is available at: . In this specification the beat is the primary driver of the selection: it allows for precise addressability of contiguous as well as non-contiguous areas.  Music notation, however, occasionally breaks rules in favor of flexibility. Cadenzas, for example, are ornamental passages of an improvisational nature that can be written out with notation that disregards a measure’s beat, making it impossible to address subsets of the cadenza wit the syntax discussed above. While EMA’s URI scheme offers the granularity sufficient to address the vast majority of western music notation, a necessary future improvement on the API is, indeed, an extension that would make it possible to address music notation with more flexible beat.   Evaluation In order to evaluate the specification, EMA has created an implementation of the API as a web service. While the URI specification can be absolute from a specific representation, the implementation must know how to operate on specific formats. The web service that we coded operates on the The Music Encoding Initiative format and is called Open MEI Addressability Service (Omas).  A demo is available at  .  Omas interprets a conformant URI, retrieves the specified MEI resource, applies the selection, and returns it. An additional parameter on the URI can be used to determine how “complete” the retrieved selection should be (whether it should, for example, include time and key signatures, etc.).     Similarly to an image server, Omas assumes that the information specified by the URL can be retrieved in the target MEI file. If requested, the web service can return metadata information about an MEI file, such as number of measures, staves, beats and their changes throughout the document. This can be used to facilitate the creation of URL requests able to return the selection required.  Finally, EMA partnered with the  Du Chemin:  Lost Voices project to model a number of micro-analyses addressing music notation from their existing collection of MEI documents. In a second phase of the project, the analyses have been re-modeled as Linked Open Data according to the Nanopublication guidelines.  Nanopublication is an ontology for publishing scientific data:  http://nanopub.org. The Nanopublication server for  Du Chemin: Lost Voices is available at:  .  Each EMA nanopublication addresses an arbitrary portion of music notation using the URL specification described here. Omas operates as a web service to connect the nanopublications with the collection of MEI files in  Du Chemin.   ",
       "article_title":" Music notation addressability  ",
       "authors":[
          {
             "given":"Raffaele",
             "family":"Viglianti",
             "affiliation":[
                {
                   "original_name":"University of Maryland",
                   "normalized_name":null,
                   "country":"United States",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "information architecture",
          "information retrieval",
          "music",
          "linking and annotation",
          "software design and development",
          "internet / world wide web",
          "standards and interoperability",
          "digitisation - theory and practice",
          "English",
          "semantic web",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The digital scholarly edition  Stefan George Digital (StGD) is, as its name implies, dedicated to the oeuvre of the German author Stefan George (1868-1933). The digital edition Stefan George Digital is developed in the context of a DiXiT (Digital Scholarly Editions Initial Training Network) fellowship, funded under Marie Curie Actions within the European Commission’s 7th Framework Program. As part of my PhD thesis, StGD is concerned with the role and application of typography within the printed collections of George’s poetry and the formal canon, and development of the so-called  Stefan-George-typeface (St-G-typeface). To capture typographical information within the digital edition, I have tested two approaches that this short paper will discuss: the application of a TEI-based model and the development and the integration of an ontology.    Subject matter  More than any other poet in modern German literature, Stefan George (1868-1933) engaged with writing – particularly typography Although the term typography includes both the micro- and macro-design of a print (and both are equally important when engaging with George’s work), this paper will focus solely on the level of micro-typography, by which I mean the level of the choice, design, and arrangement of types.  – in exceptional ways. From 1897 on, he almost completely abandoned his cursive handwriting, using instead highly stylized block letters. From 1904 on, this individual book hand was transferred into metal: It is still unclear who designed the St-G typeface, though it is presumed that both the book designer Melchior Lechter and the printer Otto von Holten were involved. the third edition of  Das Jahr der Seele (The Year of the Soul) was the first volume printed in the so called  St-G-typeface, a Sans-Serif typeface which emerged when the German dispute between Serif and Black Letter typefaces was in full swing. Accordingly, St-G forms a third, alternative typeface, strongly inspired by modern Sans-Serif typefaces such as the Akzidenz-Grotesk of the Berthold foundry. Furthermore, St-G includes letter shapes of Roman and Carolingian scripts as well as of the Greek alphabet (Kurz, 2007). Between its inception and 1927, the  St-G typeface was changed several times, so that it now exists in various versions.    Problem statement and project goals George linked his poetry and his understanding of aesthetics to the design of his books by introducing an individualized typeface. He broke with typographical conventions at the time by applying a Sans-Serif typeface, by basing the design on his own handwriting, and by referencing historical script models in its formal features. The extraordinary design of St-G and the fact that the author himself was involved in its creation calls for special attention in a scholarly edition. However, previous editions neither include a detailed recording of the printed publications nor their typographical analysis. StGD aims to close this gap by providing a digital scholarly edition that allows for exploring typography in George’s poetical work. In the first phase of research, I will create a digital edition of printed poetry collections by George. I will develop a model to identify and describe typographical forms as well as to allow for citing them. In a second phase of research I will enhance the corpus of StGD with handwritten drafts, thereby allowing further investigations of the relationship between George’s book hand and the typeface. The second research phase is planned in the context of a five-months visiting fellowship within the DigiPal project at King's College London in Spring 2016. Presumably the initial results of this collaboration will also be presented shortly in this paper.    The digital edition The corpus of StGD consists of 29 printed editions of Stefan George’s poetical works published between 1890 and 1933. Individual works are represented in the corpus variable numbers of times according to their textual and typographical variation.  i.e.  The Tapestry of Life (Der Teppich des Lebens) is represented four times (1900, 1901, 1904, 1932),  The Star of the Convenant (Der Stern des Bundes) two times (1914, 1928).  Currently all volumes are encoded according to a customized XML/TEI schema and enriched by bibliographical metadata through FRBR (Functional Requirements for Bibliographic Records) and METS (Metadata Encoding and Transmission Standards). The full texts will be enhanced by corresponding facsimiles, provided via a IIIF (International Image Interoperability Framework) compliant image server using the OpenSeadragon viewer. At the end of the project (April 2017), all contents of the digital edition will be openly available through a Creative Commons (CC-BY-NC-SA) License via the FEDORA-based asset management system GAMS (Geisteswissenschaftliches Asset Management System) < > [all quoted URLs accessed 6.3.2016] .    Focus of the paper Due to the significant role of typography in George’s work, the creation of a digital scholarly edition calls for special attention to graphical features within the documents. This means that typography needs to be classified (i.e. typeface family, font) and its features need to be modelled. The latter includes the description of typographical forms, the identification of stylistic models, and the definition of the semantic function of types in the text. Such typographical enrichment is particularly challenging since neither a commonly shared vocabulary to describe typographical features nor a widely accepted type classification system exists. With regard to this lack of a common standard, StGD has mainly tested out two methods of typographical enrichment which will be discussed in this paper: the application of a TEI based model and the development of an ontology to describe typography.  Typography and the TEI  A distinction can be drawn between two different purposes of encoding features of writing: representation and information enrichment. The first purpose is covered to a great extent by the TEI gaiji module < >  and the application of Unicode. Recent editing projects like  Hugo von Montfort: the poetical work  Website of the digital edition  Hugo von Montfort: the poetical work: < >; example of a XML/TEI encoding: < >.  demonstrate the potentials of these methods, even if they also show that performing them throughout a complete edition is work-intensive and impractical. Concerning the capture of information about writing, especially about typography, the possibilities offered by the TEI are more restricted. Although the element <typeDesc> < >  as part of <msDesc> allows for a description of types in prose, there is no TEI vocabulary to describe types and their features in a formalized way. The paper will report on the benefits and drawbacks of the already implemented elements and attributes and demonstrate their application to material at hand.    Typography and Ontologies There are barely any digital projects dedicated to the modelling of typography. Those that exist include the  Type Repository of Incunabula  Website of the  Type Repository of Incunabula: < >; example of XML encoding: < >  at the Berlin State Library, a database which identifies and catalogues incunabula types. It applies a relatively flat project-customized XML schema that describes types in prose. However, the digital modelling of handwriting has made significant progress over the last decade, since Arianna Ciula coined the term “Digital Palaeography” (Ciula, 2005), and the modelling of typography can benefit from this research. Recent research projects like  DigiPal  < >  and  ORIFLAMMS  < > , though not created in the context of digital scholarly editions, emphasize the strong tendency towards the application of semantic web technologies for the modelling of handwriting (Stokes, 2011 and 2012; Stutzmann, 2013). This paper will discuss their advantages for the identification, formal description and citation of typographical forms. Moreover it will give an overview of the technology or modelling language (i.e. RDFs, UML, SKOS) that might be the most suitable for the purpose of StGD. In this context, the paper will strongly take into account aspects of practicability and re-usability of the model.     Research questions By modelling and analysing typographical information, the digital edition opens up Stefan George’s poetical work for the following research questions: (a) Which formal features does the St-G-typeface contain and what are they referring to?; (b) How has the formal canon of the typeface developed between 1904 and 1927?; (c) Is the development of the book design across George’s work linear or is it marked by any significant breaks?; and (d) How are text and typography interrelated in George’s work, and how is typography applied as a stylistic device?    Context in Digital Humanities StGD is contributing to the field of digital scholarly editing, the focus of which is shifting increasingly to the materiality of the edited documents. This tendency has been encouraged by movements such as New Philology as well as by textual concepts such as the “material text: (Shillingsburg 1997) and the idea of text as interaction of “bibliographical” and “linguistic code” (McGann 1991). In this context writing – as the interface between the text’s message and its documentary carrier – plays a significant role and is particularly difficult to capture (Schubert, 2010). The goal of StGD is to aid in the development and promotion of a future best practice method for the modelling of typography in digital scholarly editions. Furthermore, the project’s thematic focus contributes to the field of digital book history. By putting typography in its focus, StGD represents a first step towards the burgeoning and as-yet-unexplored field of Digital Studies of Typography.  ",
       "article_title":" Stefan George Digital: Exploring Typography In A Digital Scholarly Edition  ",
       "authors":[
          {
             "given":"Frederike",
             "family":"Neuber",
             "affiliation":[
                {
                   "original_name":"Graz University, Austria",
                   "normalized_name":"University of Graz",
                   "country":"Austria",
                   "identifiers":{
                      "ror":"https://ror.org/01faaaf77",
                      "GRID":"grid.5110.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2015-10-16",
       "keywords":[
          "literary studies",
          "german studies",
          "encoding - theory and practice",
          "scholarly editing",
          "English",
          "bibliographic methods / textual studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" RICardo ( Research on International Commerce) is a database gathering bilateral flows of international commerce extracted from a large number of historical statistical sources during the 1787-1938 period. The foundational principles of this database are described in a working paper (Dedinger and Girard, 2015) submitted to the Historical Methods journal. This database begun to be developed by researchers in Economy and History in 2004 with the goal of renewing research on the history of trade globalization. In 2013, an entire new direction was given to the RICardo project. Economists and historians worked together with data scientists and designers in order to build a completely innovative digital tool susceptible to be used for both teaching and research  A French version of this abstract is available at:   .    Exploring international  trade  during the 19 th  century  To this day, there is no equivalent digital resource to the RICardo database that focuses on such an ancient and lengthy time period. The only comparable tools are web applications from the IMF ( data . imf . org), the WTO ( stat . wto . org) and the United Nations ( comtrade . un . org) that offer data and visualizations on commerce during the post-50’s period. As we will show, the methodological problems posed by historical trade statistics (19 th-20 th centuries) are more complex. The first result of our work is a web application   the source code is available at     available online:    http://ricardo.medialab.sciences-po.fr . The database will be publicly released in 2017 upon the start of an international conference marking the two-hundredth anniversary of the publication of Ricardo’s main work (Ricardo, 1817).  We will discuss the methodological choices made during the creation of this data exploration tool – what we call  datascape (Latour et al., 2012) – and will quickly present the research and educational perspectives that the tool allows for, before concluding on the method of transdisciplinary work that was used.    Representing data in their uncertainty In RICardo, the basic informational unit is a trade flow (exportation or importation) between a  reporting unit and a  partner unit. In its present version, the database contains 267000 flows. The  partners are the commercial partners recorded in the annual trade statistics reports of the  reporting countries. The large spatial and temporal coverage of the database raises certain problems. First off, the entities cover very heterogeneous realities: partners can be countries (“United Kingdom”), groups of countries (“United Kingdom & Ireland”) or geographic areas (“British colonies”). Further, the availability of trade statistics before the end of the 19 th century is very problematic (Dedinger and Girard, 2015), which is translated into an absence, in the database, of a non-negligible (17 000) amount of flows throughout the observed period. The challenge we then had to take up was to aggregate heterogeneous data without crushing the corpus’ complexity and accounting for missing data in the series. We attempt to resolve this thorny question by the exploratory analysis of the data (Tukey, 1977).    Three levels of exploration The exploration interface developed offers three levels of entry from global to local:  world view, country view and bilateral view. Each representation offers a specific point of view on the data within the database. The complexity increases as we move from the world view ( World) to the national point of view ( bilateral). In order to lead the exploration throughout these different levels, we have decided to construct a common structure to all three viewpoints.     Figure 1: Temporal filtering   Each one opens unto a main curve representing total flows per year (representation of countries’ total trade for the  world view, of one country’s total trade for the  country view, or tradebetween two specific countries for the  bilateral view) in addition to temporal limits. It is represented as a discontinuous line that is interrupted when there is no known value. In addition, these discontinuities appear also in an underlying temporal axis, indicating the absence of data in the form of a projection on the X-axis. This graphic object is also useful as an interface for temporal filtering: one can graphically select a sub-period to study more specifically (Becker and Cleveland, 1978). As footer for each view’s page, a spreadsheet allows to navigate within the visualized source data and export it as spreadsheet format (CSV). This way, the user can continue the analysis in an external statistical tool. At the center of this common structure, each view offers more detailed exploration.    Studying a country through the prism of its partners The country view allows users to center their analysis on a  reporting country by representing annual trade balances with each  partner under the form of histograms.     Figure 2: Trade balances of partners   This use of  small multiples (Tufte, 1990) facilitates comparisons between partners. The choice of detailing trade balances per year holds the fact that an aggregation on the entirety of the period would have introduced a bias by masking the year in which some data is missing. Furthermore, the ordering of the partners corresponds to the average annual share of each partner on the available years. This metric, represented as a circle at the start of the line, allows on the one hand to display the partners by decreasing importance, and on the other hand to let appear similar  partner entities. Figure 2 therefore shows that “Ireland & United-Kingdom” and “United Kingdom” were the 2 nd and 3 rd partners of the United States during the 19 th century. They are two different entities but a similar partner in reality: the United Kingdom (the overlapping of years 1864-1876 is a consequence of the inclusion of two different sources).    The distorting mirror of bilateral trade flows In the bilateral view, the tool offers to interrogate a pair of  reporting countries by using a representation of their mirror flows. It’s one of the great strength of the RICardo database: a same bilateral flow is declared as an export by one of the countries and as an import by the other, however these two recordings are rarely the same (Dedinger and Girard, 2015). Thanks to the calculation of an indicator (Dedinger, 2012), the bilateral view offers an immediate view of the deviation between mirror flows and its fluctuations over the selected period.     Figure 3: The bilateral view     A tool for researchers and students in social sciences The exploratory formatting of data brings powerful tools to help research on the history of economics. It’s an excellent way to detect possible inconsistencies in data (Leclercq et al., 2013) in order to determine its cause. To achieve such a result, our method consisted in considering data visualizations as both results and media of research (Stefaner, 2010), in fostering collaboration between researchers, engineers and designers through workshops where each data visualization was discussed by articulating the methodological constraints of Economics and History, the ideas and principles of Design and technical workability. Researchers have at their disposal an incomparable tool to deepen an analysis of trade statistics reliability (bilateral view), to compose monographs on the trade history of the world’s countries (country view), to study the history of trade globalization since the beginning of the 19 th century (world view) and to experiment data-driven teaching methods.   ",
       "article_title":" RICardo Project : Exploring 19th Century International Trade  ",
       "authors":[
          {
             "given":"Paul",
             "family":"Girard",
             "affiliation":[
                {
                   "original_name":"Sciences Po médialab, Paris, France",
                   "normalized_name":null,
                   "country":"France",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Béatrice",
             "family":"Dedinger",
             "affiliation":[
                {
                   "original_name":"Sciences Po Centre d'histoire, Paris, France",
                   "normalized_name":"Center for History",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/00mpxgs49",
                      "GRID":"grid.469402.d"
                   }
                }
             ]
          },
          {
             "given":"Donato",
             "family":"Ricci",
             "affiliation":[
                {
                   "original_name":"Sciences Po médialab, Paris, France",
                   "normalized_name":null,
                   "country":"France",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Benjamin",
             "family":"Ooghe-Tabanou",
             "affiliation":[
                {
                   "original_name":"Sciences Po médialab, Paris, France",
                   "normalized_name":null,
                   "country":"France",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Mathieu",
             "family":"Jacomy",
             "affiliation":[
                {
                   "original_name":"Sciences Po médialab, Paris, France",
                   "normalized_name":null,
                   "country":"France",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Guillaume",
             "family":"Plique",
             "affiliation":[
                {
                   "original_name":"Sciences Po médialab, Paris, France",
                   "normalized_name":null,
                   "country":"France",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Grégory",
             "family":"Tible",
             "affiliation":[
                {
                   "original_name":"Sciences Po médialab, Paris, France",
                   "normalized_name":null,
                   "country":"France",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-05",
       "keywords":[
          "information architecture",
          "spatio-temporal modeling, analysis and visualisation",
          "visualisation",
          "project design, organization, management",
          "French",
          "historical studies",
          "knowledge representation",
          "interface and user experience design"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" This short paper describes the initial phases of a Marie Curie Research Project,  Word Formation Latin (WFL), developed at the Centro Interdisciplinare di Ricerche per la Computerizzazione dei Segni dell’Espressione (CIRCSE), at the Università Cattolica del Sacro Cuore, Milan, Italy. The project consists in the compilation of a derivational morphological dictionary of the Latin language, which connects lexical elements on the basis of wordformation rules, through the use of computational linguistic methods.   In the past two decades there has been a considerable increase in the creation of computational language resources for the investigation of classical languages, which have updated the state of the art almost to the same level as that of the resources currently available for modern languages.  However, among the existing language resources, we currently lack, for Latin, a morphological derivational dictionary that connects lexical elements on the basis of Word Formation Rules   Word formation is the creation of a new word from either the combination of two other words ( dish-washer, compounding) or of adding one of more affixes to an existing word ( wash-er, derivation), or from a part of speech change ( clean, verb vs.  clean, adjective).    (WFRs).  A first attempt at constructing a lexicon based on wordformation for Latin was made by Marco Passarotti and Francesco Mambrini in 2012 (Passarotti & Mambrini, 2012). The WFL project has been awarded funding to expand on these efforts. The project has three main aims:   the enrichment of an existing morphological analyser for the Latin language, LEMLAT (Passarotti, 2004), with wordformation information, and the integration of data within a interface similar to Word Manager (Domenig & ten Hacken, 1992), which has been already applied to other modern languages (English, German, Italian); the integration of the information extracted from the resulting derivational morphological dictionary into the morphological layer of annotation the  Index Thomisticus Treebank (IT-TB);    The  Index Thomisticus (IT) is considered a pathfinder in digital humanities; started by Padre Roberto Busa in 1949. It is a database retaining the  opera omnia by Thomas Aquinas (118 texts), plus works by other 61 authors related to Thomas (61 texts). The size of the corpus is around 11 million tokens (150.000 types; 20.000 lemmas). The corpus is fully lemmatised and morphologically tagged. The IT-TB, based at CIRCSE, is the syntactically annotated portion of the IT, and it contains around 300.000 tokens for 15.000 syntactically parsed sentences.    offering the results of the project work via a user-friendly project website which will display the derivational morphological dictionary through a web based search interface. This will allow the lexicon to be accessed:   by single lexical entry, which will show both the ancestors and their derived words; by morphological family;    By “morphological family” we mean the set of lemmas morphologically derived from one common ancestor-lemma   by WFR.    The project relies on the automatic realisation of the linguistic resource both at the level of WFRs creation and to their application on the lexical items included in the morphological analyser LEMLAT. The LEMLAT lexical basis contains around 40.000 lemmas from three major Latin dictionaries ( Georges, 1913-1918; Gradenwitz, 1904; Glare, 1982). We conceived WFRs according to the so-called Item-and-Arrangement model (IA), which follows a morpheme-based approach to morphology. In IA, word forms are analysed as arrangement of morphemes according to the following three axioms:  Roots and affixes have the same status of morphemes (Baudoin’s single morpheme hypothesis); They are dualistic, as they have both a form and a meaning (Bloomfield’s sign base morpheme hypothesis); They are stored in the lexicon (Bloomfield’s lexical morpheme hypothesis).  The aim is to assign a WFR to each morphologically complex lemma (i.e. one morphologically derived from another lemma) and to link each complex lemma to its ancestor. The data are organised and presented according to a system similar to that for morphological dictionaries devised by Word Manager, in which relations between the members of the same morphological family are represented in a tree-graph.  WFRs are grouped in two classes: 1. compounding; 2. derivational. Derivational rules are divided in two categories: a. affixal (in its turn split into prefixal and suffixal), and b. conversive, a derivation process that does not imply any affix; these are manually defined. This happens in two steps: 1. Phase A: Semi-automatic data-driven finding of WFRs:   lemmas are divided into two classes, according to their part of speech and inflectional category;  two lists are produced: an  incipitarium and an  explicitarium, where lemmas are ordered according to a traditional alphabetical order, or to a right-to-left alphabetical order respectively;   prefixal and suffixal rules are created from the two lists respectively, part of speech and inflectional category of the lemma(s) are manually assigned to each candidate rule.  2. Phase B: Application (and evaluation) of the WFRs resulting from Phase A, and creation of the “morphological families”. New rules are added in this phase by confrontation with data. Phase B is divided into two subtasks:  each complex lemma is assigned a WFR. This task is performed by assigning in semi-automatic fashion to each (possibly) complex lemma its most likely WFR according to the PoS of the lemma and the string of its initial (prefixal rules) and final (suffixal rules) characters; morphological families are built.  All those (morphologically simple, or complex   WFRs do not take in input morphologically simple lemmas only, but also complex ones. For example, the noun  excubatio derives by suffixation from the verb  excubo, which is morphologically complex, as it is derived (by prefixation) from the verb  cubo.  ) lemmas that share the same invariable part are automatically assigned to the same morphological family.  Finally, the members of each family are automatically linked to each other according to their PoS, inflectional category and affixes by means of the WFR assignment (2.a). The morphologically simple (i.e. not derived) lemma member is assigned the role of ancestor of the family. Phase A finds the WFR, Phase B applies the WFR to data, obtaining input and output lemmas for each WFR.  Phase A is not to be considered exhaustive, but exploratory: the recall of WFR identified in Phase A is not 100%. The aim in the first phase of the project is to refine the data by tagging the highest number of lexemes using data driven WFRs, which will be increasingly complex, covering most well known wordformation issues.   i.e. stem change featuring internal vowel alternation ( fac.io,  per-fic-io), assimilation of prefix ( fer-o >  *ob-fer-o >  of-fer-o), unclear segmentation ( cre-a-tor or  cre-at-or?), etc.   Given the high number of homographs in Latin, this automatic procedure is regarded as non-ultimate for building the morphological families. However, it is helpful as it provides filtered data that must be checked manually.  This is why we need Phase B during which, by comparison with the evidence given by data, we can identify the rules that were missed in phase A. Manual hard-coding will be necessary for those lemmas produced by poorly productive WFRs, or morphotactically obscure wordformation processes. Evaluation of the language resource is performed by manual checking data organised into homogeneous groups based on WFRs (coverage of rules) and stemming (coverage of morphological families). Precision and recall are used as evaluation metrics in order to calculate the rate of positive and negative cases. To date, 118 WFRs have been found automatically. Around 50 of these rules, those showing a certain degree of morphological transparency, hence easier to obtain through the automatic finding in the input-output relation (e.g. derivational, verb-to-verb, prefixal, etc.), have been added to a SQL database, and resulted in the tagging of some 9000 morphologically complex lexemes. The final resource will be both a standalone dictionary accessible through its own website, and interconnected with the  Index Thomisticus.   The integration with the IT-TB will be operated through the embedding of the dictionary data within the morphological layer of annotation of the treebank, using TEI (Text Encoding Initiative) P5 conformant XML encoding to favour data exchange and linking to other lexical resources. The data resulting from the dictionary, once encoded in XML, will be applied to the IT-TB data. ",
       "article_title":"Morphology beyond inflection. Building a wordformation based dictionary for Latin",
       "authors":[
          {
             "given":"Eleonora",
             "family":"Litta",
             "affiliation":[
                {
                   "original_name":"Universita' Cattolica del Sacro Cuore, Italy",
                   "normalized_name":"Catholic University of the Sacred Heart",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/03h7r5v07",
                      "GRID":"grid.8142.f"
                   }
                }
             ]
          },
          {
             "given":"Marco Carlo",
             "family":"Passarotti",
             "affiliation":[
                {
                   "original_name":"Universita' Cattolica del Sacro Cuore, Italy",
                   "normalized_name":"Catholic University of the Sacred Heart",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/03h7r5v07",
                      "GRID":"grid.8142.f"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-22",
       "keywords":[
          "natural language processing",
          "morphology",
          "classical studies",
          "linguistics",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Audio can be united with video using a number of different techniques. Among the most common are “score to picture” and procedural generation. “Score to picture” is a feature of most modern DAWs (Digital Audio Workstations), such as  Pro Tools, Logic, Cubase, and  REAPER. A composer plays forward the video—usually in the very advanced stages of post production—and sets cues within the software around which a musical soundtrack can be structured. Thus a composer might set a cue to indicate suspense leading up to a particular moment, or the beginning and end of a romantic scene that should be accompanied with incidental music.  Procedural generation goes the other way. Here, a composer creates music—often in a sophisticated audio synthesis environment like  Max/MSP, Pd, Impromptu, or  SuperCollider—and uses properties of the audio signal or of the overall program flow to cue events in a video presentation. Since these are full-fledged (if visual) programming languages, driving video with them often means combining the complexity of software engineering with the complexity of handling audio and video signals.  In this short presentation, we describe our experiments with a method of uniting audio and video that lies somewhere between these two approaches. Unlike the practice associated with contemporary filmmaking, our method begins with a musical score and uses events indicated within it as the set of cues for an animation. Rather than use procedural programming or digital signal processing to inform the creation of cues, we use the ordinary conventions of Western musical notation. To accomplish this, we first represent the score in MusicXML. This might seem an odd choice, given that the MIDI (Musical Instrument Digital Interface) standard was designed precisely to indicate performance events over time. MusicXML, by contrast, was primarily conceived as a way of providing interoperability among software for rendering musical scores as printable objects. Yet MusicXML contains, as one explanation of the standard puts it, a “MIDI-compatible part” concerned with how the music should sound (as opposed to how it should look) (MakeMusic, 2016).  Our system exploits these MIDI-compatible elements—along with several other features of the markup—in order to indicate where a change might occur in an animation. In this way, we are able to use such things as rehearsal marks (sectional markings intended to make it easier for conductors to refer to particular passages), tempo markings, indications of changes in volume (amplitude), emphases, articulations, orchestration, and other aspects of musical notation as cues. And since everything about the duration of a piece and the relationship of the cues within the piece are discernable from the MusicXML file alone, we are able to produce SVG animations that are perfectly in sync with the music from which they are “generated”. In the simplest case, this might involve simple changes in color or the movement of shapes, but the system is fully capable of quite advanced 2D animation. From an artistic standpoint, our way of doing things hearkens back to the earliest days of animation when popular short films were synced to the music of Wagner, Rossini, and Dukas. In this sense, ours is perhaps a new way of doing an old-fashioned thing. But unlike earlier eras, artists today have access to very sophisticated tools for producing digital art. Digital artists regularly use vector graphics programs (like Adobe  Illustrator and the free  Inkscape) that can generate SVG, and scoring programs (like  Finale, Sibelius, and the free  MuseScore) that can generate MusicXML. What is missing, we think, is a robust way to bridge these two technologies.  Our system provides a very sophisticated bridge in the form of  Indigo—an SVG animation system developed at CDRH that we have recently re-engineered along the lines we illustrate above. In this presentation, we briefly explain how Indigo works and demonstrate how it can facilitate interoperability between SVG and MusicXML (perhaps with the world premier of an original animated score in honor of this year's conference theme).  Our presentation requires only the most rudimentary knowledge of musical notation and SVG. ",
       "article_title":"Picture to Score: Driving Vector Animations with Music in the XML Ecosystem",
       "authors":[
          {
             "given":"Stephen",
             "family":"Ramsay",
             "affiliation":[
                {
                   "original_name":"University of Nebraska-Lincoln, United States of America",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          },
          {
             "given":"Brian",
             "family":"Pytlik-Zillig",
             "affiliation":[
                {
                   "original_name":"University of Nebraska-Lincoln, United States of America",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "xml",
          "standards and interoperability",
          "English",
          "audio, video, multimedia"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Launched in 2013, the project  The School of Salamanca. A Digital Collection of Sources and a Dictionary of its Juridical-political Language is establishing a collection of more than a hundred sources from Iberian theologians and jurists of the 16th and 17th century. These texts deal with political and juridical topics and the collection of sources is supplemented by a dictionary that comprises, next to biographical information on the authors of the sources, the development of central terms and ideas of the Western history of political and legal ideas, as it is reflected in the source texts. Both parts, the sources and the dictionary, will be published under Open Access conditions. In the beginning of 2016, the project’s web application will be launched online with the first batch of sources and dictionary entries as TEI-XML texts plus corresponding facsimile images. At the moment, we are running a proof-of-concept mechanism and some experiments, which at the time of release will be constituting a Linked Open Data interface to our data along with a SPARQL-Endpoint. At the beginning of the presentation the project’s rationale and web application will be introduced shortly. But the focus of the presentation will be on giving insights in the workflow, the decision making process and the implementation of the LOD mechanisms, that have been realized: 1. The first aspect accentuated in the presentation is the modelling of the information contained in our TEI-XML data within a Linked-Data-environment: Which TEI elements or attributes are assigned to which objects and predicates of which ontology? How are these assignments processed in order to offer the data as semantic data? Problems that we are dealing with are the questions of how to record the temporal dimension of much biographical information (see Ramos, 2009; Mynarz, 2013), and how to cope with alternative values like e.g. conflicting data about the date of birth of a person? Do considerations such as these affect our main objectives, e.g. the TEI-scheme or the collection of data? Here is what has been settled up to now: We will offer semantic data about the sources and about the authors of the collection. The ontologies we use are mainly the foaf-, bio-, relationship-, and SPAR-ontologies (see, among others, Peroni, 2014). The TEI-data will be transformed into RDF by the xtriples webservice (Schrade, 2015). 2. The second highlighted aspect concerns the very networking of the data itself and its utilization in the project’s infrastructure. This concerns technical issues, such as the questions of which services and resources should be – directly or indirectly – provided in order to offer our data for external reuse? It concerns issues of academic strategies such as negotiations with favored partner projects and data providers over the data they expose and the interfaces they provide. But it also concerns scholarly questions, such as eventual opportunities to handle new research questions, or to handle questions in new ways, opened up by the integration of our data with the data of other LOD-providers. In which form could or should such expanded possibilities be provided on the publicly accessible website? What stance are we taking on rights management and quality insurance of federated queries/data? Again, here is the current status: We have mechanisms for the resolution of resource URIs, for content negotiation and for dumping the complete triple store as well as a SPARQL endpoint in place. We are elaborating federated queries responding to specific, concrete research questions. And we are still in the process of evaluating the Linked Data Fragments (Verborgh, 2014) mechanism. We are in contact with several projects the data of which would nicely complement our own (Schmutz, 2008; Sytsma, 2010; Bullón, 2012; Mrozik, 2016). And we are probing possibilities of offering a configurable interface to (federated) querying to our users and of rendering network information visually in our web application. Except most likely for the last point (due to time constraints), the talk will present the state of affairs we will have achieved in summer 2016. The presentation will thus point out conceptions and their implementations of linking TEI resources into the semantic web, difficulties encountered and needs still left open by scholarly research questions. ",
       "article_title":" The School of Salamanca on the Semantic Web  ",
       "authors":[
          {
             "given":"Andreas",
             "family":"Wagner",
             "affiliation":[
                {
                   "original_name":"Akademie der Wissenschaften und der Literatur | Mainz, Germany",
                   "normalized_name":"Academy of Sciences and Literature",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01kdxra28",
                      "GRID":"grid.461597.8"
                   }
                }
             ]
          },
          {
             "given":"Ingo",
             "family":"Caesar",
             "affiliation":[
                {
                   "original_name":"Akademie der Wissenschaften und der Literatur | Mainz, Germany",
                   "normalized_name":"Academy of Sciences and Literature",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01kdxra28",
                      "GRID":"grid.461597.8"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-08",
       "keywords":[
          "information architecture",
          "renaissance studies",
          "law",
          "linking and annotation",
          "ontologies",
          "historical studies",
          "spanish and spanish american studies",
          "networks, relationships, graphs",
          "theology",
          "philosophy",
          "English",
          "semantic web",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction In the ever-expanding world of digital libraries and cultural heritage collections, bibliographic metadata standards provide a structured approach to managing resources. The capture of additional contextual information further supports the identification, selection, and use of the resources described. As the problem spaces and areas of study of Humanities scholars are increasingly diversified (Henry and Smith, 2010) and supported by digital material and methods, existent approaches and systems are falling short of the needs of users (Fenlon et al., 2014; Varvel and Thomer, 2011). In short, research agendas and investigations have begun to evolve beyond searches based on traditional metadata parameters (author, date, publication place, genre).  Semantic Web technologies have been identified as a potential solution (Bair and Carlson, 2008). They augment keyword-based, full-text approaches to discovery, with methods that rely on named entity identification, relationships between entities, and the potential to leverage interlinked data from a variety of repositories and corpora. A number of different, well-defined ontologies (structural frameworks capturing domain information) created by various bodies within the wider context of the Digital Humanities (DH) have emerged (Isaac, 2013; Stead, 2006). A critical evaluation and comparison between these different structures has, however, been lacking. In this paper, we provide a summary of four bibliographical metadata ontologies, and expand on an earlier initial comparative analysis between them. What follows is a more in-depth discussion of complementary differences and parallels in terms of expressiveness, rather than domain, focus, or perspective. The strengths and weaknesses of these vocabularies are of interest and importance to anyone working with any type of Humanities dataset or research output, whether it be interacting with metadata that describes the resource, features that have been extracted from it, or the resource itself.   Bibliographic Ontologies To date digital libraries have relied heavily on traditional library bibliographic standards, such as MARC   http://www.loc.gov/marc/bibliographic/  . As new research questions have arisen in DH, the limitations of earlier standards have become more pronounced (see Ramesh et al., 2015; Sfakakis and Kapidakis, 2009; Park, 2006; Cantara, 2006; Shreeves et al., 2005), and a number of ontologies designed to map the entities and relationships inherent in bibliographical metadata have emerged.   Rather than aiming to provide a comprehensive analysis of all known examples, we extended a preliminary evaluation of a small number of bibliographic ontologies. Earlier research   http://www.oerc.ox.ac.uk/projects/elephant   bridging the large general corpus of the HathiTrust Digital Library   https://www.hathitrust.org/htrc   with the specialist Early English Books Online - Text Creation Partnership   http://www.textcreationpartnership.org/tcp-eebo/   assessed the different needs of three distinct case study examples and analysed the suitability of existing ontologies to adequate capture associated information, including publication facts and object biographies (Nurmikko-Fuller et al., 2015a). This preliminary analysis examined four ontologies: MODS RDF   http://www.loc.gov/standards/mods/modsrdf/v1/  / MADS RDF   http://www.loc.gov/standards/mads/rdf/v1.html  , BIBFRAME (Miller, et al., 2012), Schema.org   http://schema.org/docs/full.html  , and FRBRoo (Bekiari, et al., 2013). BiBo   http://bibliontology.com/   was originally considered, but excluded from the final comparison as it operates on a finer level of granularity.  In this paper, we elaborate on that initial analysis, and provide access to the entirety of the comparative table (Nurmikko-Fuller et al., 2015b)   http://hdl.handle.net/2142/88356   of which only a representative sample has previously been made available. We summarise the main characteristics of each structure in order to provide context for the detailed discussion outlining the parallels and differences between the models.     Methodology Based on available documentation and extant examples, we conducted an extensive review of the expressiveness of each ontology. Comparing each property and class against possible alignments in the other three led to the identification of parallels and differences between these models. One revelation was the differing extent to which documentation had been left incomplete, highlighting the lack of workflow standardisation in ontology-development even within a shared domain. At times the absence of extensive documentation complicated our ability to confidently assert parallels between the models. The comparative analysis led to the insertion of all classes and properties of each ontology into one cohesive table, aligned wherever the same data could be represented regardless of how the mapping was achieved, and resulting in a table of exactly 500 rows. Five types of alignment were identified:   equivalent, where the same data can be mapped in each ontology using a single class. An example of this is location information (such as publication place), captured via madsrdf:Geographic, bf:Place, sc:Place, and frbroo:F9_Place (equivalent to cidoc:E53) in MODS/MADS, BIBFRAME, Schema.org and FRBRoo respectively.     alternative, which encompasses situations where properties were used in one ontology, but classes in another to talk about the same attribute. An example of this is Schema.org’s sc:birthDate. It takes an entire grouping of entities and relationships to express this same information in FRBRoo (frbroo:P98B_wasBorn frbroo:E67_Birth frbroo:P4_hasTime-Span frbroo:E52_Time-Span frbroo:P78_isIdentifiedBy frbroo:E50_Date).     parallel, where the same data can be mapped using a combination of classes and properties. In the case of the date of creation for a work, MODS/MADS, BIBFRAME and Schema.org all have a single property (mods:dateCreated, bf:creationDate, CreativeWork:dateCreated), whilst FRBRoo necessitates a cluster of classes and properties: F1_Work R19b_wasRealisedThrough F28_ExpressionCreation P2_hasType E55_Type{“Earliest known externalisation”}. We consider these approaches as aligned because the same data can be mapped against them, but parallel rather than exactly equivalent due to different approaches.     partial, where the same data could be mapped using different ontologies to a greater or lesser extent. As an example of this, we cite the assignment of a unique identifiers, captured using a single property in MODS/MADS (modsrdf:identifier) and FRBRoo (which uses a CIDOC property cidoc:P48_hasPreferredIdentifier), but through several possible options in Schema.org  (Thing:sameAs, Book:isbn, Periodical:issn), and via a total of 13 possible properties in BIBFRAME (such as bf:doi, bf:isbn, bf:uri).     granular, which captures differences in levels of granularity. This is illustrated by entity types such sc:CreativeWork and sc:Book, and showcases how categorical alignment across all four ontologies is not always possible. In the case of frbr:F1_Work (a conceptual version of a work, of which digital and physical manifestation are carriers), an equivalent alignment can be drawn to bf:Work, but MODSRDF/MADSRDF has no entity type that fulfills the role of representing that notion.     Comparative Analysis The bibliographic metadata ontologies discussed here differ in their approach and expressiveness. Of the four, MODS RDF/MADS RDF was found to be most descriptive, with FRBRoo an event-based model, and BIBFRAME bridging the two by virtue of containing characteristics typical of either. Schema.org stands out as an ontology that promulgates a model at the crossroads between the other four; however, its focus on instrumenting marketplace transactions also detracts from much of its descriptive power and leaves it orthogonal to the purposes of the others. It has some generic properties that are useful in each of the other ontologies, but also possesses properties and classes that end users do not require outside of point of service systems. From the perspective of a DH user of these ontologies, each is (to some degree) a victim of its provenance and the motivations of its designers. The benefits and failings of each are different, and they all incorporate a number of idiosyncrasies:  MODS/MADS has a data structure that replicates the XML serialization format and is frequently realized in RDF as empty nodes.   Schema.org affords humanists with a vocabulary that combines events and descriptions but differs from the other models in its focus.   FRBRoo spans beyond the remit of bibliographical metadata by mapping relationships via temporal entities; this results in greater complexity for the representation of the same data, often necessitating a cluster of classes and properties.   BIBFRAME adopts a different method, recreating MARC in RDF using methods and approaches more in line with graphical thinking, as well as extending beyond that format.    Conclusion Our review examines the structure and scope of four ontologies designed for the representation of bibliographic metadata as applied to cataloguing digital source material in the Humanities. From this analysis direct equivalences, parallels, and complementary differences have emerged: there are many similarities in aim, scope, and expressiveness, but none of the considered ontologies completely satisfy scholarly needs on their own. Moving between them is feasible, but not achievable without some lossiness, as illustrated by the examples for  granular alignment (see  Methodology). For the comprehensive mapping of all the aspects of a given dataset, these models need to be supplemented with less bibliographically-focused ontologies. Our analysis has highlighted the need to formalize the mappings, best practices, and transformations, as these are key to the correct (re)use of ontologies across projects and domains.   We have provided DH researchers with a window into the digital corpora design process. Knowing the requirements of domain scholars to have interactions with finer-grained research objects, we will be looking at standards like BiBo, Web Annotation, and others during the next round of research.   Acknowledgement The authors gratefully acknowledge their colleagues Pip Willcox, Bodleian Libraries, University of Oxford; Colleen Fallaw, and Megan Senseney, Graduate School of Library and Information Science University of Illinois at Urbana-Champaign, for their invaluable contributions to the creation of the Bibliographic Ontologies Comparative Features Dataset, available at  https://www.ideals.illinois.edu/handle/2142/88356.   ",
       "article_title":"A Comparative Analysis of Bibliographic Ontologies: Implications for Digital Humanities",
       "authors":[
          {
             "given":"Terhi",
             "family":"Nurmikko-Fuller",
             "affiliation":[
                {
                   "original_name":"University of Oxford, United Kingdom",
                   "normalized_name":"University of Oxford",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/052gg0110",
                      "GRID":"grid.4991.5"
                   }
                }
             ]
          },
          {
             "given":"Jacob",
             "family":"Jett",
             "affiliation":[
                {
                   "original_name":"Graduate School of Library and Information Science University of Illinois at Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Timothy",
             "family":"Cole",
             "affiliation":[
                {
                   "original_name":"Graduate School of Library and Information Science University of Illinois at Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Chris",
             "family":"Maden",
             "affiliation":[
                {
                   "original_name":"Graduate School of Library and Information Science University of Illinois at Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Kevin R.",
             "family":"Page",
             "affiliation":[
                {
                   "original_name":"University of Oxford, United Kingdom",
                   "normalized_name":"University of Oxford",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/052gg0110",
                      "GRID":"grid.4991.5"
                   }
                }
             ]
          },
          {
             "given":"J. Stephen",
             "family":"Downie",
             "affiliation":[
                {
                   "original_name":"Graduate School of Library and Information Science University of Illinois at Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "internet / world wide web",
          "GLAM: galleries, libraries, archives, museums",
          "ontologies",
          "English",
          "semantic web",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction This paper presents first empirical analyses and visualizations of a large corpus of graphic novels – an increasingly popular form of book-length comics aimed at adults – that is currently in the process of being assembled and digitized. We introduce an XML vocabulary and visual editor that we have developed for the annotation of our corpus, and reflect on the challenges presented by a cultural form that is characterized by the complex interaction of text and images. Analyzing the specific narrativity of this and other multimodal cultural forms (including illustrated books and magazines, theater, film, television and computer games), we argue, calls for a combination of quantitative and qualitative methods drawn from such diverse fields as narratology, digital art history, and cognitive science. In contrast to corpus analyses of literary texts, which have made great strides in recent years, comparable work on visual narrative still remains in its infancy and at the periphery of the digital humanities. While this can be traced, in part, to copyright issues, such scholarship also faces a number of crucial technical and methodological hurdles – from image description, classification, and object recognition to the operationalization of narratological concepts. Given the dominance of visual storytelling in modern and contemporary culture, overcoming these hurdles will represent an important contribution to the further development of DH research. The introduction presents our corpus and the wider research questions of our interdisciplinary group. This is followed by a brief overview over the “Graphic Narrative Markup Language” (GNML), which builds on TEI, and the visual editor developed for the annotation of graphic novels, but which is also applicable to other multimodal forms. A version of this editor will be available as open-acess software by the time of the conference. Part two introduces a number of analyses and visualizations combining the study of text and images that make up graphic novels. The final part moves to the quantitative and qualitative analysis of graphic novels with the help of eye-tracking. This approach allows us to study the construction of storyworlds by empirical readers, and thus opens up an aspect of narrative that remains severely underrepresented in DH, and the humanities at large.   1. GNML-Editor: Tools for (Semi-)Automatic Annotation Whereas the automatic analysis of text corpora has become feasible in many instances, such automation currently remains a pipe dream for multimodal narratives. In the case of comics and similarly hand-drawn, or otherwise non-perspectival, images, object identification depends on lengthy training efforts and registers a relatively high error rate. Similarly, standard OCR programs fail at recognizing the (quasi) hand writing that dominates comics. As a consequence, our corpus study presently depends on manual and semi-automatic annotation. For this purpose, we have developed the XML-language GNML, which builds on TEI and previous efforts by John Walsh (2012), to describe all textual and visual properties of graphic novels. To minimize errors during the annotation process, our visual GNML-editor supports annotators with integrated spell checking and auto-completion mechanisms. An automatic recognition of panels is complemented by a function that recognizes the borders of individual captions, speech bubbles, and characters to accelerate annotation. Further automations, such as an in-built OCR for narrative text that conforms to standard fonts, are currently under development. As the conceptual basis of the editor (visual objects with graphic and textual characteristics) is not limited to comics but can be applied to other text-image combinations in visual culture (from illustrated manuscripts to film and TV), the editor will be generalized for the annotation of such formats.   2. Quantitative Analyses and Visualizations of Graphic Novels Part two presents approaches that combine image and text analysis for a number of structural features of graphic novels. Methods developed for digital literary studies, such as topic modeling, are of limited value for the analysis of visual culture given the dominance of images. In contrast, studies of large-scale image sets have so far shown little interest in narrative analysis. To complicate matters further, most narratological concepts are drawn from the study of literary texts, and it remains questionable to what extent they can be successfully applied to visual narrative. In a first analysis of the corpus, which is still in the process of being digitized and annotated, we look at the historical development of the visual and textual elements of about 150 book covers of graphic novels. This includes a grammatical and semantic analysis of their titles with the help of a statistical language parser, as well as the stylistic and visual attributes of their design and cover images. In a second step, we move to more detailed studies of a first sub-corpus that consists of the ten most-cited titles within our larger set of graphic novels. Such a small sub-set does not allow for genre comparisons or for studying historical developments within the form. However, we can consider the narrative features of representative texts within our corpus. In order to do so, we compare a network analysis of characters with their visual prominence and respective share of text, and complement this with a stylistic analysis of the latter.    3. Eyetracking Analysis of Multimodal Narrative The experimental observation of eye movements has proven a reliable measure of the human processing of text and images, and allows us to form hypotheses about the construction of storyworlds by empirical readers. The final part of the paper aims to show the value of this method by considering excerpts from a first, explorative corpus of canonical graphic novels. In contrast to theoretical scholarship on comics, which has emphasized the primacy of images (Groensteen, 2009), our experiments demonstrate that readers focus most of their attention on the text. Not only is it usually read first, but many images are either not focused on at all, or analyzed purely in peripheral vision. Whether images are viewed depends, among other variables, on their informational content: if either visual aspects or the storyline continue from one panel to the next, it is much more likely that a panel will be skipped by the reader than if they are distinguished more clearly from its immediate predecessor. We also look at the interaction between visual and textual levels: do reading habits differ if text and images refer to distinct storylines? Finally, we report on experiments that focus on comic reading expertise, for which we propose a new empirical measure. In sharp contrast to the reading of text alone, where experience and reading speed are positively correlated, experienced comics readers focus on the visual aspects of the panels for an extended amount of time. This time appears to be invested wisely, since they are able to better understand the story, as shown by an empirical content test. Taken together, these results suggest that the text and image work together to transmit the narrativity of graphic novels, and that a specific type of expertise is required to understand multimodal narratives. This maps well onto the hypothesis that comics and other forms of sequential art use a particular kind of visual language (McCloud, 1993), which has been analyzed in psycholinguistic terms by Cohn (2013).   4. Summary and Conclusions We present a new DH project aiming at collecting and analyzing a corpus of graphic literature, enriched by human annotations as well as by a corpus of eye-movement recordings to measure the momentary distribution of readers’ attention. First example analyses on both global and local levels demonstrate the potential of this approach. A toolchain for description, annotation, and analysis of these data is being developed, and is of potential use for a wider field of studies in cultural analytics of image-related and multimodal material. In perspective, the corpus will be further enhanced by automated description, using features developed in the field of computer vision (Farabet et al., 2013; Krizhevsky et al., 2012; Rigaud et al., 2015; Serre et al., 2007).  ",
       "article_title":"Corpus Analyses of Multimodal Narrative: The Example of Graphic Novels",
       "authors":[
          {
             "given":"Alexander",
             "family":"Dunst",
             "affiliation":[
                {
                   "original_name":"University of Paderborn, Germany",
                   "normalized_name":"University of Paderborn",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/058kzsd48",
                      "GRID":"grid.5659.f"
                   }
                }
             ]
          },
          {
             "given":"Rita",
             "family":"Hartel",
             "affiliation":[
                {
                   "original_name":"University of Paderborn, Germany",
                   "normalized_name":"University of Paderborn",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/058kzsd48",
                      "GRID":"grid.5659.f"
                   }
                }
             ]
          },
          {
             "given":"Sven",
             "family":"Hohenstein",
             "affiliation":[
                {
                   "original_name":"University of Potsdam, Germany",
                   "normalized_name":"University of Potsdam",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/03bnmw459",
                      "GRID":"grid.11348.3f"
                   }
                }
             ]
          },
          {
             "given":"Jochen",
             "family":"Laubrock",
             "affiliation":[
                {
                   "original_name":"University of Potsdam, Germany",
                   "normalized_name":"University of Potsdam",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/03bnmw459",
                      "GRID":"grid.11348.3f"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-15",
       "keywords":[
          "visualisation",
          "english studies",
          "xml",
          "corpora and corpus activities",
          "digitisation, resource creation, and discovery",
          "audio, video, multimedia",
          "English",
          "image processing",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Understanding users’ online behaviour is of growing interest to academic researchers in a variety of fields. Traditionally, in the marketing domain commercial research companies map consumer behaviour to understand when and where customers decide to buy products. For this purpose, web metrics of individual websites serve as detailed source of information on when, how and at which section a user enters a website. Recently this type of data is also being used by cultural heritage institutes to understand the interest of their visitors (De Haan and Adolfsen, 2008), to track where their digital content is being reused (Navarrete Hernández, 2014) or to understand the query’s users perform in search systems by analysing the log files (Batista and Silva, 2002; Huurnink, 2010). In this type of research, the website is the central research object providing traces that Menchen-Trevino (2013) calls ‘Horizontal Data sets’. These contain data that are ‘organized around a specific type of trace, for example search terms, web browsing log files, tweets, hashtags, likes or friend and follower ties’ (Menchen-Trevino, 2013: 331). An advantage of using this type of data is that they are not obtrusive to the respondents since they are created automatically as users are surfing the web. However, this also leads to an ethical disadvantage since users are not aware that their online behaviour is being examined, nor could they give their consent to have their data being analysed. While Horizontal Data Sets are organized around one type of trace, Vertical Data Sets are organized around research participants that deliberately ‘give permission for researchers to collect their digital traces’ (Menchen-Trevino, 2013: 331).   Since mid-1990s, commercial research agencies have started to collect these types of vertical data by building tools and panels of respondents whose online behaviour is monitored 24/7 to provide data on usage across media and purchase behaviour (Coffey, 2001; Napoli, 2010; Taneja and Mamoria, 2012). Similar to television viewing rates, these lists are mainly created to gain more insight in the background of website visitors in order to provide potential advertisers with information on how to reach their online target audience in the best possible manner. Obviously these commercial research data contain very rich information, also for academics who are interested in collecting real-world Web use data. However, apart from lists of the most popular domains that are published as open data by companies such as Alexa and Similarweb   http://www.alexa.com/topsites ,  http://www.similarweb.com/global  , data containing information about visits to each individual page and information about the background of the panel is not available. Main arguments of commercial agencies to not collaborate with scholars is to ensure the confidentiality of their respondents’ identity and to prevent scholars to gain insight into the techniques applied by the companies.   Nevertheless, researchers in a variety of disciplines are interested in tracking online behaviour in a real-world situation. Especially in the communications science realm, scholars experimented with several techniques of tracking people’s online behaviour (Ebersole, 2000; Tewksbury, 2003; Findahl et al., 2013; Findahl, 2009; Munson et al., 2013; Damme et al., 2015; Menchen-Trevino and Karr, 2012). Striking about these pioneering monitoring studies is its multi-method approach. By default each does not only monitor web use but also compares its outcome with either survey, diary or interview data. By triangulating the results, these researchers try to overcome the critique on classic studies on media consumption that often deploy either surveys or diaries registering self-reported media behaviour (e.g. Van Cauwenberge, d’Haenens and Beentjes, 2010; Schrøder and Kobbernagel, 2010; Taneja et al., 2012; Reuters Institute for the Study of Journalism, 2015). These methods strongly rely on the memory of the participants while several scholars found respondents often overestimate their media use (Ebersole, 2000; Prior, 2009; Robinson, 1985). Furthermore, since filling in diaries and surveys on news consumption is very labour-intensive, its outcomes mainly focus on  when media have been consumed or on  which devices, while it remains unclear  what has been consumed. One way to gain insight in the consumed news content is focus on metrics of individual news organisations (Batista and Silva, 2002; Boczkowski et al., 2011; Lee et al., 2012; Usher, 2013) or the most clicked items (e.g. Boczkowski et al., 2011; Karlsson and Clerwall, 2013; Lee et al., 2012; Nederlandse Nieuwsmonitor, 2013). However, given the focus of these studies on individual websites or most-clicked articles it remains unknown which genres of news websites constitute users’ 24/7 news menu. Do they e.g. visit news about sports mainly in the morning and news on politics mainly during the evening? Taneja at el. (2012) tried to overcome this problem by literally following 495 users throughout an entire day. However, even with this labour-intensive fieldwork it proved not to be possible to incorporate the genres of consumed news items.   Web monitoring tools such as the above mentioned examples, now offer a less labour-intensive and more precise way of registering digitally consumed news items. By deploying these techniques, we could overcome the knowledge gap of the 24/7 news consumption menu. Therefore, we created  the Newstracker, a custom built system that collects web activities of specified and authenticated users, cleans the data by removing non-relevant data, extracts the associated content and stores this as a new dataset to be used for analysis. While most existing online tracking studies mainly report the visited websites, our set-up goes two steps further. We did not only monitor the website titles but also the actual visited URLs and crawled all textual and visual contents of the visited websites. Since one of the problems when monitoring a person’s online behaviour is the magnitude of the data that is being collected (Batista and Silva, 2002; Manovich, 2012; Vicente-Marino, 2013: 43), we deployed automated content analyses techniques (Atteveldt, 2008; et al., 2012) to detect the topics that are being discussed in the news items. This enabled us to calculate the topical online news consumption during the day.  In this paper we will describe the set-up of ‘The Newstracker’ in a study on the online news consumption of a group of young Dutch news users and its applicability for other types of Digital Humanities research such as user studies focussing on formulating requirements based on existing user behaviour. We will demonstrate the workflow of the Newstracker and how we designed the data collection and pre-processing phase (see figure 1).   Figure 1. Workflow of the Newstracker application, illustrating the two main phases: Data Collection and Pre-processing. The latter consists of three stages: cleaning, content extraction and merging  By reflecting on the technical, methodological and analytical challenges we encountered, we will illustrate the potential of online monitoring tools such as the Newstracker. We will end our paper with discussing its limitations by stressing the need for a multimethod study design when aiming not only to register but also to understand online user behaviour. ",
       "article_title":"Tracking Online User Behaviour With A Multimethod Research Design",
       "authors":[
          {
             "given":"Martijn",
             "family":"Kleppe",
             "affiliation":[
                {
                   "original_name":"Vrije Universiteit Amsterdam, Netherlands, The",
                   "normalized_name":"VU Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/008xxew50",
                      "GRID":"grid.12380.38"
                   }
                }
             ]
          },
          {
             "given":"Otte",
             "family":"Marco",
             "affiliation":[
                {
                   "original_name":"Vrije Universiteit Amsterdam, Netherlands, The",
                   "normalized_name":"VU Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/008xxew50",
                      "GRID":"grid.12380.38"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-08",
       "keywords":[
          "user studies / user needs",
          "English",
          "media studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The Exhibitium ProjectGeneration of knowledge about temporary art exhibitions for a multivalent reuse was the topic of the proposal presented to the 2014 competition organized by the BBVA Foundation for projects in the field of Digital Humanities, resulting selected from over 250 submissions. The project website is available at: http://exhibitium.com. The Exhibitium Project began in January 2015 and will end in December 2016, so currently we are completing the first phase., awarded by the BBVA Foundation, is a data-driven project developed by an international consortium of research groups.They are: iArtHis_Lab (http://www.iarthislab.es) and Khaos (http://khaos.uma.es) at the University of Málaga; Techne, ingeniería del conocimiento y del producto (http://www.ugr.es/~tep028/quienes_somos_es.php) at the University of Granada; and CulturePlex at the University of Western Ontario (http://www.cultureplex.ca). One of its main objectives is to build a prototype that will serve as a base to produce a platform for the recording and exploitation of data about art-exhibitions available on the Internet.Specifically, the ultimate Exhibition’s purpose is to extract unprecedented and strategic knowledge about temporary art exhibitions through the use of a variety of data mining techniques. Therefore, our proposal aims to expose the methods, procedures and decision-making processes that have governed the technological implementation of this prototype, especially with regard to the reuse of WordPress (WP) as a development framework. According to the project's purpose, it was necessary to create a device that, to the extent possible, could capture in automated way information on art exhibitions from any Internet source. Consequently, the inquiry into the possibilities of web mining strategies emerged as a priority from the early stages. Taking into account the high expressiveness and flexibility of linguistic structures usually used in the description of art exhibitions, our project opted for a mixed platform which combines the potential modeling system based on textual indicators, the heuristic means that characterize some methods -such as the Bayesian classification- and the human supervision provided by a well trained team of editors.  1. General overview. WordPress as a framework of the Expofinder system  As Baumgartner et al. (2009: 1) established, the web data extraction task is usually divided into five different functions: (1) the web interaction, which mainly comprises the navigation through predetermined web pages containing the information sought; (2) the extraction of the searched data by means of a software that identifies, extracts and transforms them into a structured format; (3) the setting of a specific calendar that enable to perform automatically the extraction tasks in regular sequence; (4) the processing of the captured data, which includes filtering, transformation -if applicable-, refining and integration; and (5) the delivery of the resulting structured data to a variety of data analysis-based systems. Assuming this distribution as the most convenient for our purposes, we decided to include them in the Exhibitium’s architecture grouped into two large blocks. A. A block consisting of an automated capture system of information robust enough to ensure the reliability of the collected data. B. A second block made up by the set of elements necessary to store the data, including functions for filtering, cleaning, management, structuring and description. This block also incorporates a system to export the collected data to those platforms that will process and analyze them during the second phase of the project. Block A was called Beagle, and block B became known as ExpoFinder. Both blocks work in a coordinated manner, so that what is extracted by Beagle is put at the disposal of ExpoFinder. The two blocks are part of an unified system configured by a cyclic algorithm: Beagle captures, ExpoFinder analyzes and approves the captured information, the team of editors validates or discards what ExpoFinder has previously approved, and Beagle recaptures again (see figure 1).   Figure1. Beagle + ExpoFinder. Operating plan (simplified)  Regarding the software, after preliminary versions based on own developments, it was decided that the most interesting option between the free software and open source solutions currently available (as the openness philosophy is a sine qua non requirement of this project) would be to use WP as framework of the system.Although, in reality, according to the Tom McFarlin’s statement in his popular page «tuts +» (http://tutsplus.com/), it is more a foundation that a framework. And maybe he is right: a framework consists of a set of conventions as well as libraries and tools that allow us to easily start working on an application. In short, it provides the means by which an application can be built from scratch, from the database schema to the front end. However, a foundation allows to «extend» an existing application. WP has its own well-defined internal mechanisms, and the foundation simply expands its operation or takes advantage of it for their own benefit. The main benefits that the use of WP as framework offers for our project can be synthesized in the following items: a database with a flexible and very solid organizational structure; a layer of a core application with numerous hooks which allow to maximize its functionality; and a high easy management system to carry out tasks on the two sides (server and client), assuming both administrator and user roles.The advantages that a robust mechanism as such provided by WP offers for the maintenance of a security system (essential in any development accessible through Internet), or the substantial savings in time and resources involved in a CRUD structure records management -which is both sufficiently malleable to suit any need and rigid enough to follow canonical deployment patterns (such as the «nonce» safeguards in the capture forms), are weighty arguments when opting for the use of one framework or another. Thus, for the implementation of the Beagle-ExpoFinder system we took advantages of the predefined data base, the available APIs and the set of data visualization templates to build solutions using an application that is already fully functional. We used WP without adaptations, that is, as it can be downloaded from the Internet. All the functionality of our application lies, then, on the code itself that constitutes WP, so it is not supported on variant versions (forks) of the original program. Hence, any improvement provided by the computing community will be directly usable by our project without further adaptations. As part of the requirements of the development of the Beagle-ExpoFinder system (B + E), from the beginning it was considered that the programming work did not constitute a «tailored suit» for the Exhibitium project. On the contrary, we expect that this work can be useful in other projects with a small number of modifications or by using configuration files or other similar systems. For that reason, our choice was to implement B + E by means of a «WP theme», solution that easily allows us to readapt the software to different purposes.   2. Beagle and Expofinder development and technical features  Beagle captures – as it has been said- by automated means web data concerning temporary art exhibitions from any source of information, and includes a filtering mechanism. The automated capture process uses the tools of WP API, particularly WPCron. Likewise, the frequency of the process is configured according with the options offered by ExpoFinder to the system administrator. Beagle employs two statistical complementary functions to «predict» the degree of the adequacy of the captured information to the ExpoFinder preconditions:Even though this document is not largest enough to expose in detail the list of selected preconditions of significant terms used by Beagle in order to filter the captured information, we want to emphasize that this is a «weighted» relationship of lexemes in which each root term is assigned a total «weight» in the set (1 to 3). When the entire process is complete, the absolute amount of the sum corresponding to the found terms is weighted with the relative values (relating to the length of the text where they have been detected) to assign a positive or negative evaluation to the whole information. 1. One of this is based on the intersection of a set of «positive» and «negative» keywords with a proportional weight assigned to each one, which is also based on the shortest path algorithm of Bellman-Ford;The Bellman-Ford algorithm (or Bellman-Ford-Moore) calculates the shortest paths from a single source vertex to all other vertices in a weighted digraph. It is slower than Dijkstra's algorithm for the same problem, but more versatile, since it is suitable to deal with graphs using negative numbers for edge weights. ExpoFinder takes advantage of it in its weighting mechanism, useful for us because we work with lists of lexemes for words used as «positive» or «negative» markers. 2. The other is defined by its heuristic nature; it employs a naive Bayesian classifierIn machine learning terminology, the «naive Bayesian» constitutes a family of simple probabilistic classifiers based on the application of Bayes’ theorem about the hypothesis of independence between variables. Widely studied since the 1950s, it began to be used since the beginning of the next decade as a taxonomy method capable of self-optimization in the recovery community text. We use the frequency of occurrence of a given lexeme as a trigger, so that ExpoFinder can contribute to the semi-automated selection of relevant information from the experience gained. It is not a pure discriminative mechanism, but an auxiliary tool that has proven to be useful for application operators. to guide the «human» editor during the task of discriminating whether or not an information captured by Beagle is valid. The latter is able to improve their efficiency through continuous learning processes (each discarding or acceptance made by the «human» editor refines the system «perceptiveness» (see figures 2 and 3).   Figure 2. Screenshot. Exhibitions list (fragment). See the Bayesian classifier indicator  ExpoFinder also includes a control system (QC) that identifies the mistakes and failures, which are also associated with the human editor who made them, so that he/she can perform the appropriate corrections (see figures 3 and 4).   Figure 3. Screenshot. Automated evaluation of efficiency     Figure 4. Screenshot. Quality control (QC). Resume  In its current state of development, the Beagle-ExpoFinder system captures and selects daily about 100 references from more than 12,000 web sources of information. Its error rate during the recording and validation processes is about 3.9%, below the 5% initially considered as permissible.  ",
       "article_title":"WordPress as a Framework for Automated Data Capture, Filtering and Structuring Processes. The New Order of the Authors",
       "authors":[
          {
             "given":"Antonio",
             "family":"Cruces Rodríguez",
             "affiliation":[
                {
                   "original_name":"University of Málaga, Spain",
                   "normalized_name":"University of Malaga",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/036b2ww28",
                      "GRID":"grid.10215.37"
                   }
                }
             ]
          },
          {
             "given":"Nuria",
             "family":"Rodríguez Ortega",
             "affiliation":[
                {
                   "original_name":"University of Málaga, Spain",
                   "normalized_name":"University of Malaga",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/036b2ww28",
                      "GRID":"grid.10215.37"
                   }
                }
             ]
          },
          {
             "given":"Carmen",
             "family":"Tenor Polo",
             "affiliation":[
                {
                   "original_name":"University of Málaga, Spain",
                   "normalized_name":"University of Malaga",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/036b2ww28",
                      "GRID":"grid.10215.37"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "Spanish",
          "databases & dbms",
          "art history"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction  The established pattern for a scholarly digital edition today is the website, which in many cases has a unique and well thought out user interface. It concentrates all information pertaining to the topic, but allows little interaction of the reader with the texts beyond what has been designed into the user interface by the developers of the site.  Although there are also many efforts to go beyond this and experiment with new forms of reading in the digital age, for example protocols for sharing annotations or reading communities, they have not yet reached a stage where they would be available to mainstream researchers.  In contrast to the polished and well advertised flagship editions of digital projects at prestigious institutions, there is also a continuing trend of making the source texts that feed into digital editions available in a way that not only allows, but actively encourages tinkering with the texts. The distribution for this latter type of texts is frequently on the site github.com, which is the world largest repository for software development  The site claims: \"GitHub is where people build software. More than 11 million people use GitHub to discover, fork, and contribute to over 28 million projects.\" ( http://www.github.com, accessed 2016-03-02). , but as a free site for data sharing with collaborative editing it is becoming more popular for other purposes as well.  Some of the large scale text projects using Github in this way include the  Chinese Buddhist Electronic Text Association (CBETA)    (accessed 2016-03-02), more than 4000 Chinese Buddhist texts in TEI P5 markup. ,  gitenberg   (accessed 2016-03-02); a project that uploaded more than 40000 electronic texts from the Gutenberg.org project to make it possible to improve the texts that are otherwise distributed without the possibility for the readers to provide corrections.  and  EEBO-TCP   (accessed 2016-03-02), which contains 25000 texts released to the public on Jan. 1, 2015.  to name just a few.  The presentation proposed here reports on the  Kanseki Repository, a project that tries to establish a link between these two different types of text dissemination and through this combination to achieve the best of the two worlds: The texts can still be presented through a sophisticated web interface, which uses Github as a backend storage and reads the texts from there for presentation to the user. These texts can be forked (that is, cloned and copied to the user's account) which makes it possible for the user to revise or annotate them. The system is set up in a way that it will show this private copy of the text if configured so by the user.    Methodological considerations  The goal here is not simply to upload as many texts at possible to public archives, but rather to develop a platform that allows the basic tenets of scholarly editing to co-exist and thrieve with the possibilities and affordances of the digital medium. It is therefore of paramount importance, to consider the requirements as voiced from practioners of scholarly editing and implement them as transparently as possible.   Record and interpretation  In a seminal article, the Swiss scholar Hans Zeller (1995) emphasised the fact that all scholarly editing should make a clear distinction between the record of what is transmitted and the scholarly interpretation thereof. While this distinction is blurry at times, it has informed the design of the  Kanseki Repository, which arranges the editions of a text it represents into those that strive to faithfully reproduce a text according to some textual witness ('record') and those that critically consider the content and make alteration to the text by adding punctuation, normalizing characters, collating from other evidence etc. ('interpretation').    Additional requirements  Peter Shillingsburg (2015) outlines the following requirements of a digital edition (slightly edited for clarity):  a. Digitize images of all the documents. That will make it possible, from anywhere in the world, to see any document side-by-side with any other document without traveling from Tokyo to Marburg and New York.  b. Prepare a table of variants to show how all the documentary texts differ from one another.  c. Write a textual history that explains the relationships among the variant documents and explains why we should care–why it is important to know.  d. Transcribe at least one of the documents so that the variants list can be more easily used. Or transcribe all the documents so that readers can select and read any one. Transcribing all the documents will also make machine collation possible.  e. Edit one of the transcriptions to correct obvious errors. This will preserve the text as a historical documentary text but will help readers avoid the distractions caused by scribal or compositorial errors.  The architecture developed in this project strives to implement as much of this as seems feasible within the limits of the current funding and other constraints. The components will be further described below.     Main components of the project architecture The architecture of the project consists conceptually of two parts: (1) the text repository and (2) clients accessing this repository. Of these clients there are currently two, both developed within the project.   Backend: Repository of texts: github.com/kanripo  Since every text has its own unique history and witnesses, every text is allocated to its own repository (in the technical sense). These repositories, currently more than 9000, are are organized according to the traditional Chinese cataloging principles and kept under the Github account @kanripo. Since the texts are publicly accessible, they can be freely downloaded and cloned even without ever touching the client interfaces. Due to the large number of texts and the Github interface, which seems foreign and intimidating to most readers of classical Chinese texts, special clients cover most needs in interfacing with the texts.    Client interfaces   Web interface at www.kanripo.org  This website provides access to the texts, including full-text search, display of transcribed text and facsimile(s) of different editions. Users can log in using their Github credentials and get access to more advanced functions such as selecting lists of text of special interest, advanced sorting functions by text category or date as well as cloning of texts to the Github user account and editing on site. The site went into testing mode in October 2015 and a first public release has taken place in March 2016.   Mandoku, a stand alone tool for further immersive reading and study  In addition to the website, an Emacs module called Mandoku (see Wittern, 2012, 2013, 2014a, 2014b) has been developed (as an extension to Orgmode, see Domnik et al., 2015), that uses the API of the website to provide the same search functions, but clones the texts of interest to the user to her local machine, thus providing advanced editing possibilities and offline access.     Towards a platform for text-based Chinese studies  All modes of interaction described above are based on the distributed version control system git, using the Github site as a 'cloud storage'. However, in addition to providing storage, Github also provides a feedback mechanism through \"pull-requests\", where users can flag corrections to a text for the @kanripo editors to consider for inclusion in the canonical version, thus making it available to all users. The model outlined here is extensible and allows other developers of websites related to Chinese studies to access the same texts, and provide specialized services to the user, for example by enhancing the text through NLP processing. These enhanced versions can be saved (\"committed\" in git language) in the same way to the users account and are then also visible to the client programs described here. This will open the door to a open platform of texts for Chinese studies, where the texts of interest to the users form the center of a digital archive, with different services and analytical tools interacting and enhancing it. The user, who makes a considerable investment in time and effort when close reading, researching, translating and annotating the text, never loses control of the text and does not need to worry about losing access to it when one of the websites goes offline. By providing versioned access to the texts in question, it is also possible to make any analytical results reported in research publications reproducible (Rawal, 2015) by indicating the additional tools and processes needed, ideally also in a Github repository in the same ecosystem.  The aim is not just to provide a static, completed, definitive edition of a text, but as fertile a ground as possible for the interaction between the text and its readers, hopefully improving both through this process.   ",
       "article_title":" Public and Private Views of Texts in Digital Editions – The Case of the Kanseki Repository  ",
       "authors":[
          {
             "given":"Christian",
             "family":"Wittern",
             "affiliation":[
                {
                   "original_name":"Kyoto University, Japan",
                   "normalized_name":"Kyoto University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/02kpeqv85",
                      "GRID":"grid.258799.8"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "philology",
          "digitisation, resource creation, and discovery",
          "scholarly editing",
          "digitisation - theory and practice",
          "English",
          "asian studies",
          "bibliographic methods / textual studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction A well chosen file format is an important aspect in the process of archiving digital information. A typical example are image files for archival purposes. Unfortunately it can be shown, that even a well-defined file format like TIFF can have partially proprietary, vendor-based features. Today it is inevitable to digitize analogue photographs or images because the originals are endangered by unstoppable physical decay. Because of the continuous process of scanning, digital images are an important part of our cultural heritage already and they account for a constitutive part of our contemporary multimedia output in social, scientific and economic ambits (Kuny et. al., 1998). Unfortunately any digital object must be migrated in periodic cycles, because of technological changes. Hardware migration is only one of the steps needed to ensure that a digital file can be rendered in future. The file format definition is of the same importance. If such a definition becomes obsolete, the existing file must be converted into another one that is not in danger to be outdated (Heath et. al., 2011). Therefore it is necessary to judge and optimize all relevant factors that define the sustainability of a file format definition. Even more important is the persistence of a file format if a storage solution with very long cycles of migration is used, as presented in the Monolith-Project (Fornaro et. al., 2014) or the Rosetta-Project. In those cases a lifecycle of several decades can be expected. We discuss in this document the long-term stability of existing image file formats and derive possible new approaches. We will show in detail what weaknesses exist, that endanger the future rendering of the content. In addition an image file format definition for archival needs is proposed, based on the already existing widespread standard TIFF. The proposed approach follows the concept of the Portable Document Format (Oettler et. al.,2013), PDF and its archival derivative PDF/A. The recommended specification is called TI/A, Tagged Image for Archives.   Problem If digital file formats are not well chosen, the content won’t be accessible in future because it can no longer be decoded as a consequence of one or multiple technological issues (Rothenberg, 1995). A “file format” basically defines the logical structure and meaning of the bits within the bit stream and thus it is essential for correct interpretation and proper rendering of the coded data. Unfortunately a file format or parts of its logical structure and definition can become obsolete, like hardware does. As a result the information renders useless, even-though the bit stream is still properly readable. To prevent such obsolescence the file format must be migration. In most cases this is more complex than creating a simple duplicate of a bit stream; the file has to be restructured. In addition every migration can reduce image quality or introduce artefacts. Therefore it is necessary to use a file format for long-term preservation of digital data that is stable, simple, well-documented and reliable. Unfortunately most image file formats do not fulfill the needed requirements. Even open standards like the Tagged Image File Format (TIFF) can have partially vendor specific, proprietary content that decreases long-term stability. TIFF is one of the most widespread formats used to represent high quality image data in archives. TIFF is a well known, established, flexible, adaptable file format for handling images and data within a single file, by including various header tags. TIFF offers some features that are rarely used and not supported by most applications. The TIF format is quite complex and parts of the original definition have become obsolete, while new, not formally standardized additions have been made. As a consequence it would be easily possible to create a TIFF file that is fully conformant to the TIFF Revision 6.0 specifications but would be virtually useless because no existing software is be able to open and render it, migration is the needed.  Migration is an expensive task. Therefor numerous approaches for archival storage solutions exist that do not or only rarely need to be migrated. Most of those solutions make use of a very stable carrier and a simple interface to access data. Monolith is such an example for an “eternal” storage. Monolith (Gubler et. al., 2006) is based on chromogenic optical film, that has a life expectancy of up to 500 years. The data is stored on Monolith as 2D-barcode, enriched with human readable metadata. For such a solution the data format is of very high importance because any format obsolescence reduces migration cycles drastically.    Approach Since a digital archive has the goal that the file can be rendered in a indefinite but possibly far future, a simplistic approach is necessary. Therefor a TIFF suitable for long term archiving should require only a minimal set of features (tags) that are necessary to allow a correct future rendering of the data and the essential descriptive metadata. We therefore propose a subset of the full functionality of TIFF that is fully compatible with the de-facto TIFF standard itself but marks some tags as mandatory, some as optional and some as forbidden in order to guarantee the correct rendering in the future. In addition to the core functionalities, it is crucial to define a minimal set of metadata for archival applications, following standards like Dublin Core or METS (Loeffler et. al., 2007). In analogy to PDF/A format we propose to call this specification TI/A or Tagged Image for Archives. In cooperation with the University of Girona in Spain and EASY INNOVA, a technology and innovation centre of Girona, we have started the process of specifying TI/A in co-operation with multiple memory institutions of Switzerland and Europe. Of course the concept of using a subset of the functionality of TIFF can be applied to any other format common for archiving digital image data like JPEG2000/A (Buckley, 2013) or even video or motion picture like DCP/A (Fornaro et. al., 2014: Goethels, 2009).   Results It can be shown that a smart chosen file format is very important for successful archiving [9]. With the help of numerous institutions and experts we have drafted a recommendation, based on the existing TIFF standard (http://ti-a.org). The exchange of needs, requirements, dos and don’ts will lead us to a final draft specification of an ideal archival file format for high quality image data that is well supported by an international network of experts. Following the original standard definition of TIFF allows us to define a format that is fully compatible with existing decoders. This approach makes it not necessary to have “out on the market” software modified or enhanced by any means. Based on that preliminary work we will try to have the document standardized by the International Standard Organisation, ISO. Such a precise definition of the functionalities and their implementation in a Tagged Image File for Archives will help to increase the sustainability of the original image format drastically.   ",
       "article_title":"File Formats for Archiving: Stability and Persistence Issues",
       "authors":[
          {
             "given":"Peter R.",
             "family":"Fornaro",
             "affiliation":[
                {
                   "original_name":"University of Basel, Switzerland",
                   "normalized_name":"University of Basel",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s6k3f65",
                      "GRID":"grid.6612.3"
                   }
                }
             ]
          },
          {
             "given":"Lukas",
             "family":"Rosenthaler",
             "affiliation":[
                {
                   "original_name":"University of Basel, Switzerland",
                   "normalized_name":"University of Basel",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s6k3f65",
                      "GRID":"grid.6612.3"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016",
       "keywords":[
          "English",
          "audio, video, multimedia",
          "standards and interoperability"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Preamble The goal of the study is to show links between lexical and social diachronic change. The study is conducted in the culturomics framework (Michel et al., 2011). In contrast to the Big data approach the study promotes the idea of medium data, i.e. amount of data which allows both to make quantitative and qualitative analysis (Bonch-Osmolovskaya, 2015). The main characteristics of the medium data are:  The reliability of sources, which metadata can be filtered manually The sufficiency of the data amount for reliable statistical measures The possibility of additional semantic mark-up  The research is based on the data from Russian National Corpus (ruscorpora.ru) (see Plungian, Sitchinava, 2003). The study pursues changes of context frequencies for the lexeme road in the period from 1800 till 2000, and correlates the observations with social and economic progress as well as change in conceptual language space   Choice of concept Russia is a big country, so transportation has been traditionally a critical problem. The choice of the word  road for culturomics study is based on our expectations of the concept’s centrality for the economy, society and culture in Russia of the 19 th–20 th centuries.  Road appears to be a productive sign in terms of semiotics of art (Tchepanskaya, 2003,), that’s why I expected to collect numerous relevant contexts both in fiction and nonfiction. At the same time  road in Russian has several meanings, the nature of its polysemy has been treated a lot in previous works (Arutiunova, 1999). We can distinguish three basic meanings which are contrasted by the position of Observer (Paducheva, 2006) – the one that percepts the  road. The first meaning is  road as a physical object, a line on the ground the observer sees while standing on it. It can be characterized by the quality of its surface or surrounding landscapes (i.e. dirty road). The second meaning is  road as a vector, a line on a map, that connects two points (i.e central road). The observer operates in this case with the abstract idea of the road’s topology. The third meaning is metonymical and it stands for the travel-event the Observer experiences while moving along the road (i.e. tedious road). Finally due to semiotic abundance  road is frequently used in metaphorical sense (i.e. life path = “road of life”). At the same time, the first three meanings present the most important parameters that determine mobility of population: quality of roads, connectedness between localities and time and quality of journey. Therefore, it seems insufficient to track frequency change of  road occurrences in the corpus in general, but it is important to distinguish how the frequency of different meanings has been changing.     Method Different meanings of  road can be captured by attributive constructions as adjectives usually refer to only one sense. The corpus has been divided into 7 time periods from 1800 to 2000. To make the sub-corpora comparable the 19 th century has been divided into two periods of 50 years and the 20 th century into five periods of 20 years. The contexts, containing constructions of adjective plus  road has been extracted from every sub-corpus. The noisy entries has been removed, the data has been lemmatized and normalized as ipm. As a result, I obtained a database with 15000 constructions, containing more than 1500 unique adjectives.  On the next step, all the adjectives have been categorized by 20 semantic domains. The domains correlate with four basic meanings of  road defined below but render more specific characteristics of different  road parameters. The most frequent construction “zheleznaya doroga” (literary  metal road, meaning railroad) has been selected in a separate category.   Then I applied hierarchical clusterization to the data of 20 categories, see Figure 1   Figure 1: Clusterization of semantic categories for adjectives describing road  The data of the categories in one cluster has been summarized and then plotted on the graph (see Figure 2)       Analysis The data allows plenty of research scenarios, comparing different domains, such as, for example:  sources of domain contents and diachronic change of the distribution of sources (for example, fiction or nonfiction)  widening or narrowing of the category through time (how many adjectives are in the category), as a well as persistence of the content to time and economical or social changes substitution of one adjective to another (for example, all the changes connected with replacement of horses to cars) and its time frame migration of an adjective from one adjective to another  In this abstract, I will focus on the most prominent changes of cluster graphs. As Fig 2 shows the railroad (RR) cluster and the direction and centrality(D and C) cluster are the most distinctive in their behavior. In the beginning of the 19 th century, the existence of big central roads from one town to another completely determined mobility opportunities of Russian population. We see that more than 50% of all the occurrences of  road are associated with D and C attributes ( Warsaw road, big road – as a specific term of central road). In the 1851, the railroad between Moscow and St.Peterburg has been open and this fact nicely correlates with the crossing of the RR and D and C graph in the period of the 1850s. The intensive growth of the RR cluster in the second half of the 19 th century reflects not only the growth of railroad communications in Russia but also great conceptual influence of the railroad innovation, which can be also traced in numerous literary pieces of this period such as Tolstoy’s Anna Karenina or Dostoevsky’s Idiot. The intriguing fact about RR cluster is its consistent fall in the 20 th century that may of course correlate with developing automobile transportation. The sharp fall of RR cluster in the 1960s corresponds to the growth of civil airlines; see Figure 3 that demonstrates quite the opposite trend for air transportations starting from the 60s    Figure 3: Graph of air transportation, line with triangles marking passenger traffic  The most important generalization that arises from the observations above is that in the 20 th century the topological (vector) meaning of  road is consistently fading while the reference to a road as a physical object on the contrary increases in frequency. In other words, while economic and industrial growth results in diversity of mobility means,  road as a concept in lexical space has changed the balance of its meanings reducing the  connection idea. At the same time the metonymic usage of  road as a journey has been increased in the 20 th century as a well as the metaphorical usage, the both categories are very similar in their data values so they have formed one common cluster. In 1960s, the  connection idea is transferred from direct usages of D and C cluster to figurative usage of Journey and Metaphor cluster. This means that we can document the moment when the idea of  connection is separated from the physical movement along the road. The tedious road now is sitting in the airport for many hours waiting for your flight.    ",
       "article_title":"Two Centuries of Russian Roads – Diachronic Study of Polysemy in the Context of Cultural Change",
       "authors":[
          {
             "given":"Anastasia",
             "family":"Bonch-Osmolovskaya",
             "affiliation":[
                {
                   "original_name":"National Research Unversity Higher School of Economics Moscow, Russian Federation",
                   "normalized_name":"National Research University Higher School of Economics",
                   "country":"Russia",
                   "identifiers":{
                      "ror":"https://ror.org/055f7t516",
                      "GRID":"grid.410682.9"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-13",
       "keywords":[
          "semantic analysis",
          "cultural studies",
          "corpora and corpus activities",
          "content analysis",
          "linguistics",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" In August 1902 the British newspaper The Times inserted an article announcing “The passing of the grocer” as a result of a crisis in the small retail trade that apparently had bankrupted more than 900 stores (Winstanley, 1983). Across the Atlantic, Canada’s shopkeepers faced a crisis equally significant, with overabundance of shops and a lot of bankruptcies (Monod, 1996). The same happened in continental European cities like Paris or Milan, in the final decades of the nineteenth century (Nord, 1986; Morris, 1993). Much of what happened this time was the result of an demographic growth in the cities in the nineteenth century that almost everywhere would have important consequences in the reorganization of urban space, leading to changes in their economic and social geography. Lisbon was no exception and the city’s retail trade, after a phase of growth in the 1880s, faced a crisis in the last ten years of the century (Alves 2012). This occurred while the city was transformed, witnessing an expansion into new urban areas to accommodate an increasing volume of inhabitants. Through the analysis of the geographical distribution of Lisbon’s retail trade, between 1890 and 1910, it is possible to verify the impact of this crisis on the way small businessmen apprehended the urban space, as well as the opportunities and risks that those changes could represent to them. The study is based on a very detailed information about the localization and characteristics of every single shop in the city streets, gathered from fiscal sources in the municipal archives (around sixteenth thousand records for each of the three years, 1890, 1900 and 1910) and analyzed through the use of spatial statistical analysis tools. This volume of information and the potential introduced by a Digital Humanities / Spatial Humanities approach, brings in methodological challenges regarding the digitization and geocoding process of several thousands of records, as well as opens new research possibilities and new research questions, namely: about the role of women in the retail trade business, since previous work based on smaller sets of data analyzed with traditional qualitative methods had misrepresented or even ignored gender issues in the retail business (Alves, 2012); or about the influence of the dwelling rents in determining changes in the social space of a city in the Belle Époque, only possible by the digital analysis of a very large, temporal and geographical encompassing amount of data for all the city. In terms of the methodology two were the challenges we faced in two distinct stages of research, data collection and its georeferencing. The first challenge relates to the difficulty in scanning a volume of data of this nature, only available on archive, with no available funding to carry out a project of mass digitization of the original tax records. This was associated with the fact that all sources are handwritten, drawn up by several employees, with equally different handwritings, which would make it impossible, given the current state of handwritten character recognition technology (Beatty, 2010; Brumfield, 2014), for an automatic or even semiautomatic data treatment. The solution followed a close approach to crowdsourcing projects (Causer et al., 2012), using shared databases and collaborative work, a method already developed with excelent results in previous studies (Alves and Queiroz, 2013; Alves and Queiroz, 2015). The second challenge went through georeference about fifty thousand addresses obtained in this documentation, using essentially the computer's processing power. It is recognized that there are advantages and disadvantages in using several geocoding methods (Zandbergen, 2008). Its application to Lisbon is made difficult due to the fact that the city have gone through a deep urban morphology transformation and experienced significant changes in its street names throughout the twentieth century (Alves, 2005; Oliveira and Pinho, 2006). Nevertheless, it remains one of the best methods for automatic assignment of geographic coordinates. The challenge was to think of a process that could overcome the difficulties listed, without having to go through the full reconstitution of the urban network. Not least because the available sources do not made possible the recreation of all the buildings and their functional classification in a GIS environment, as was achieved in other projects (Dunae et al., 2013). The option went through reconstruction of the existing streets network of the time, based on geo-referenced digital cartography. Thus an addresses’ database was created, with slight adaptations on the geocoding algorithm of the GIS platform used, allowed for a success rate of around 90%. As for georeferencing, the first and greatest of all the problems is related to the urban changes that the city has undergone over the last 120 years. On the one hand, strong population growth led to the expansion or changes in the corresponding urban area, even If we take into account just the three years analyzed. The city of 1890 is very different from the one in 1910, in the layout of the streets, in the expansion into new areas, construction or renovation of its buildings. On the other hand, Lisbon had the particularity to overcome, between 1890 and the present, four very different political regimes that have left a peculiar and deeply transformative mark in the city's toponomy. Even if the urban area was stable, only the change of street names over more than a century of profound political changes, would pose great challenges to an automated geocoding process. For this there was first the need to rebuild the streets network of the time, trying to recreate the map of Lisbon of the Belle Epoque. This map was fixed for 1890 and then it was possible to apply the normal geocoding techniques, adapting either the collected data source, or the software algorithm used, in order to overcome difficulties so small, but so significant in the final results, such as the fact that the software was incapable to deal with some Portuguese names and characters or to recognize certain types of streets that were used at the time but ignored or underused today. Or the fact that the names of some streets are identical or very similar in different areas of the city. This is an iterative process of trial and error so as to refine the best model data that maximizes the results obtained. However, there was still the issue of street names changing over time. For the period in question this problem is not very complicated, since only at the end of 1910 took place the first regime change, the Republican Revolution, with the consequent wave of place names changed. But there was almost annually, specific changes in street names. These changes were incorporated in the database, maintaining the address structure already georeferenced for 1890, allowing to incorporate the data for the same streets that appeared in the sources with different names. These two ways to overcome the problems with georeferencing were only possible due to the existence of sources, either cartography or streets itineraries, which allowed to go adapting the original map so as to incorporate, with a similar success rate, the 1900 and 1910 data. As for the quality or accuracy of georeferencing, it is obvious that the ideal would be to have the possibility to rebuild not only the network of city streets, but also the actual location of the various housing/trade blocs. Unfortunately no sources for this level of detail are available. In the original shapefile map, which represented the actual city streets, in 2012, the streets were targeted with the actually existing building numbers in each block. Since we have no way of saying, at this stage of the research, how many buildings existed on every block in 1890, it was decided to distribute all points along the entire length of the street, according to the geocoding algorithm. Obviously, this option can cause some significant deviation in less consolidated urban areas of the city, but it is also true that the areas where the overwhelming majority of small businesses traditionally where implemented account for already established streets. So that problem here is much lower, and the geocoding accuracy is much higher. I did not use crowdsourcing primarily due to the available time for the collection of data and to poor accessibility to the original sources. Seems odd to state this because apparently these are precisely justifications for using crowdsourcing. However, it is known, also through the literature already cited in the abstract, that the use of crowdsourcing can be a lengthy process and not always easy to check on the quality of final results. Furthermore, it requires easier access to sources through digital copies because not all potential participants have availability or even competence for archive work. In our case, the sources correspond to thick volumes of bound sheets, deposited in a municipal archive without major conditions for local consultation and without the financial ability to carry out its full or partial scan. There was also the problem of the quality of the data, given that we are talking about handwritten information sometimes difficult to read. In this sense, the use of a collaborative work, through volunteer history students with availability and sensitivity to archival work was decisive. This data collection was made via a PostgreSQL database, shared with all students through ODBC, so that all data collected by one of the students become available for subsequent use by others. This process, already tested in other project with very good results (Alves and Queiroz, 2013; Alves and Queiroz, 2015) , saved up a lot of time in data collection and redundancies were avoided, because a new address o r a new commercial activity detected by one of the students in the sources once registered in the database, was automatically available and could be used/selected by all the others. Even if the original record contained an error, since it was registered only once in the database, because the data model prevented duplicates, the validation task was also facilitated. The reconstruction of the 1890 street map, base of all subsequent work, was made through a current map in shapefile format, provided by the Municipality. This map was superimposed on a digital copy of an old city map properly georeferenced. The mere overlap of the two maps identified a broad set of streets that did not exist at the time (eliminated from the shapefile) and others that changed their layout (fixed in the shapefile). Using streets itineraries, namely one for 1890, very complete, mentioning the name, location and building numbers of all city’s streets, it was possible to correct the shapefile street names that had gone through changes over these 120 years. The link between this corrected shapefile and the shops’ addresses collected in the shared database allowed for geocoding, with minor adjustments to the software algorithm. Overcoming these issues, it was possible to get a far more dynamic source, more accessible and manageable, able to respond quickly to old research questions or to introduce new perspectives about Lisbon and its retail trade in the Belle Époque. Just to mention one possibility, in the Lisbon municipal archive there are many available data that could be crossed with the information collected on shops and shopkeepers. In particular, it has data on electoral census and elections, with voters lists organized by parishes and addresses, for several years of the nineteenth and twentieth century and this data can also be mapped and analyzed using the methodology now developed. The same can happen with data on primary education, for example, because this fund has information on the addresses of the students also. Given that the addresses of the shops are being collected in a separate table and that it allows to be connected to either the GIS or to other tables with different data, whether on small businesses, on elections or on education, the use of this methodology will allow for results in other projects or to add new information relevant to the history of the city without the need to start everything from scratch. In this sense, while recognizing the limitations of GIS (Boonstra, 2009; Bodenhamer et al., 2010; Silveira, 2014), it must be highlighted the capabilities of its spatial analysis, processing and data visualization tools, that has been particular useful in the context of urban history (Hillier, 2010; DeBats and Gregory, 2011). In recent years, the use of GIS for the study of cities and their retail trade (Beascoechea Gangoiti, 2003; Mirás Araujo, 2008; Bassols and Oyón Bañales, 2010; Novak and Gilliland, 2011; Ünlü, 2012) has enhanced the theoretical framework and the possibility of international comparative studies, which represented a definitive stimulus to the application of these methodologies to this case study. To this matter it is useful to mention that a session comparing four case studies for Iberian port cities will occur in the ESSHC in Valencia, this year, putting together comparative analysis about Barcelona, Bilbao, Coruña and Lisbon, in the first decades of the twentieth century (https://esshc.socialhistory.org/esshc-user/programme?day=55&time=145&session=3003). ",
       "article_title":"Geocoding Thousands of Fiscal Records: Methodological Approach for a Study on Urban Retail Trade in the Belle Époque",
       "authors":[
          {
             "given":"Daniel",
             "family":"Alves",
             "affiliation":[
                {
                   "original_name":"IHC, FCSH, New University of Lisbon, Portugal",
                   "normalized_name":"University of Lisbon",
                   "country":"Portugal",
                   "identifiers":{
                      "ror":"https://ror.org/01c27hj86",
                      "GRID":"grid.9983.b"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-09",
       "keywords":[
          "maps and mapping",
          "historical studies",
          "English",
          "geospatial analysis, interfaces and technology"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" As Virtual Reality emerges as an accessible technology, researchers have begun to experiment with its potential for data visualisation. In this paper I will use two particular VR visualisations of linguistic data as a springboard for discussing the affordances of different immersive technologies for research. My development of the linguistic visualisations discussed here was funded by a transdisciplinary grant from the Centre of Excellence for the Dynamics of Language (COEDL). The first visualisation is a geographically-anchored walkthrough of data held in the PARADISEC linguistic archive (Thieberger and Barwick, 2012). A user moves across a representation of a geographic region and sees markers on the landscape representing the metadata of the relevant PARADISEC materials for that location: number of speakers, amount and diversity of material held. Audio and text appear when the user gazes at a marker. Looking up, the user can also see the historical relationships between the languages as a network. Such a visualisation could be adapted to represent the holdings of other kinds of archives, so the discussion in this paper has implications for digital humanities more generally. The second visualisation is a more abstract three-dimensional cloud of coloured points laid out on three axes. The user is located initially in the midst of this cloud, and can move through it in any direction. The colours, sizes and shapes of the points, as well as their location on the x, y, and z axes, can be mapped to variables of any kind of data, and therefore this too has implications beyond linguistic research. For my purposes, they represent linguistic features derived from the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013). To demonstrate one use case, different measures of linguistic complexity can be graphed on the x, y, and z axes (e.g. WALS features 1A+2A, 22A, 49A, etc). Colouring the points according to the geographical location of the languages, with shape determined by genetic affiliation makes it easy to explore clusters of types of complexity. Both of these visualisations can be viewed in a pseudo-3D environment using WebGL in a desktop browser. For a more immersive experience, however, they can also be viewed inside a virtual reality headset such as the Google Cardboard or Oculus Rift. Alternatively, the visualisations can be displayed on the inside of a fulldome such as those used in a planetarium. Through a recent successful LIEF grant, Western Sydney University and a number of partners have been able to construct an ultra-high resolution experimental fulldome (‘DomeLab’ http://www.niea.unsw.edu.au/research/projects/domelab).  Display via virtual reality headset is the most straightforward sort of immersion. The viewer is embodied amidst the data, and simple movements from everyday life (e.g. looking around) translate directly into the virtual space. Display of the visualisation in a browser is at the other extreme of (non)immersion. However, many users are familiar with the translation of three-dimensional space onto a flat screen from modern computer games, so it can still give a sense of movement, rotation, and interactivity.  A hybrid of these two experiences, perhaps less familiar, is the dome. The user lies on the floor inside the dome, and the visualisation is displayed on the concave walls of the dome above. Because the user is surrounded by the screen, the experience is far more immersive than the display of a visualisation in a computer browser. Metaphors of movement and space have to be adapted for such a display, however, as most of the display surface is above the user.  This paper will discuss the reasons why we might want to explore linguistic data visually, and what we gain or lose by doing so in the various environments described above. Lev Manovich notes that information visualisation is the representation of datasets in such a way as to reveal structure (Manovich, 2010). Linguists are obsessed with structure. Since at least Pāṇini (4th century BC, see Vasu, 1891), linguists have conceived individual languages as highly structured; since the Neogrammarians if not before, ‘language’ as an abstraction that changes and exists outside of its speakers has also been considered a highly structured object of study; sociolinguists are interested in the structure of speech communities and speaker networks; psycholinguists and neurolinguists in the linguistic structures of the mind and the brain. Linguistics is therefore a field that is primed to search for and find structure in its data, and is constantly searching for new ways to do so. Manovich also describes data visualisation as a matter of reduction and spatialisation, often involving a remapping of the non-visual to the visual. For linguists, reducing linguistic data to features and remapping it to the visual (audio to text, for example) are business as usual. What I would like to discuss in this paper is the spatialisation of the data, and the embodiment of the researcher(s) within that data space. A very active branch of linguistics involved in data visualisation is linguistic typology. Typology concerns itself with how languages are distributed within the possibility space of all conceivable languages: i.e. “what’s where and why” (Bickel, 2007). Because linguistic data has an inherent geographical dimension, the predominant linguistic visualisation typologists use is a map. I have followed this lead for my PARADISEC visualisation. However, Manovich also points out that both designers and their audience tend to treat spatial dimensions as primary (Manovich, 2010:8). Therefore it makes sense also to experiment with mapping these to other, perhaps more significant features of the linguistic data. This is the impetus behind my data graph visualisation. By translating the WALS data to a more abstract 3D graph, different relationships, clusters and structures become apparent. In the full paper I demonstrate examples of this. Data visualisations can have a variety of purposes (see e.g. Dransch, 2000; Purchase et al., 2008). They can be a way to explore data to generate ideas or observations that then inspire future research. They can be an attempt at scientific modelling, instantiating more fully formed pre-existing ideas. Or they can be a way of communicating information to others. An early study of user experience in 2D, 3D and immersive data visualisation (Modjeska, 2000) showed that users generally find 2D representations of the data more efficient, but enjoyment and motivation increases with the degree of immersiveness. This suggests that VR visualisations might be best suited for the playful exploration of data described above than for serious scientific modelling. They may also be well suited for communication of ideas when it is not essential that the audience grasp complex details.  These are therefore the motivations behind the two linguistic data visualisations I describe. With the PARADISEC map visualisation it is not essential that a user come away with perfect recall of what exactly the archive holds, but rather with a sense of its richness, and increased motivation for using the archive in the future. The data graph visualisation is aimed at researchers who want to explore language data creatively in order to generate new ideas. Finally, I discuss how the affordances of the VR headset versus the Dome depend on one’s beliefs about the locus of knowledge production. In the ‘lone wolf’ researcher model, an individual generates ideas in a relative vacuum, or at least, the important connections for the researcher are among and with the data itself, not with other people. For this model, a VR headset is ideal. In the world of the headset, the researcher is alone with the data: the mundane is quite literally blacked out. The dome, on the other hand, is designed to be experienced communally. The floor is covered with large pillows, and people lie head-to-head or side-by-side. Curtains delimit the boundaries, but this threhold is permeable: people come and go. It is natural to discuss the visualisations above with those who lie beside you in the half-dark. In this way, research conversations become a kind of academic pillow-talk, naturally imbued with the playfulness, intimacy, and gentleness of such. In the VR headset, you become part of the data; in the dome you become part of a community. ",
       "article_title":"Academic Pillow-Talk and Two Immersive Explorations of Linguistic Space",
       "authors":[
          {
             "given":"Rachel Marion",
             "family":"Hendery",
             "affiliation":[
                {
                   "original_name":"Western Sydney University, Australia",
                   "normalized_name":"Western Sydney University",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/03t52dk35",
                      "GRID":"grid.1029.a"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-07",
       "keywords":[
          "visualisation",
          "games and meaningful play",
          "maps and mapping",
          "virtual and augmented reality",
          "archives, repositories, sustainability and preservation",
          "linguistics",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" How best can humanities researchers access and analyse large-scale digital datasets available from institutions in the cultural and heritage sector? What barriers remain in place for those from the humanities wishing to use high performance computing to provide insights into historical datasets? This paper describes a pilot project that worked in collaboration with non-computationally trained humanities researchers to identify and overcome barriers to complex analysis of large-scale digital collections using institutional university frameworks that routinely support the processing of large-scale data sets for research purposes in the sciences. The project brought together humanities researchers, research software engineers, and information professionals from the British Library Digital Scholarship Department http://britishlibrary.typepad.co.uk/digital-scholarship/, UCL Centre for Digital Humanities (UCLDH)  http://www.ucl.ac.uk/dh, UCL Centre for Advanced Spatial Analysis (UCL CASA) http://www.bartlett.ucl.ac.uk/casa, and UCL Research IT Services (UCL RITS) http://www.ucl.ac.uk/isd/services/research-it to analyse an open-licensed, large-scale dataset from the British Library. While useful research results were generated, undertaking this project clarified the technical and procedural barriers that exist when humanities researchers attempt to utilize computational research infrastructures in the pursuit of their own research questions.    Overview The drive in the Gallery, Library, Archive, and Museum (GLAM) sector towards opening up collections data, http://openglam.org/, an initiative to promote free and open access to digital cultural heritage datasets.  as well as the growth in data published by publicly-funded research projects, means humanities researchers have a wealth of large-scale digital collections available to them (Lui, 2015, Terras 2015). Many of these datasets are released under open licences that permit uninhibited use by anyone with an internet collection and modest storage capacity. A few humanities researchers have exploited these resources, and their interpretations make claims that change our understanding of cultural phenomena (for example, see Schmidt, 2014; Smith et al., 2015; Cordell et al., 2013; Huber, 2007; Leetaru, 2015). Nevertheless, there remain major barriers to the widespread uptake of these data sets, and related computational approaches, by humanities researchers, which risks diminishing the relevance of the humanities in “big data” analysis (Wynne, 2015). These barriers include:   fragmentation of communities, resources, and tools; lack of interoperability; lack of technical skills: “mainstream researchers in the humanities and social sciences often don’t know what the new possibilities are” (ibid) and seldom have the technical experience to experiment (Hughes, 2009; Mahony and Pierazzo, 2012).  A common response to this lack of awareness and computational skills is to build web-based interfaces to data For example,  Mining the History of Medicine (http://nactem.ac.uk/hom/) or Language of the State of the Union (http://www.theatlantic.com/politics/archive/2015/01/the-language-of-the-state-of-the-union/384575/).   or federated services and infrastructures For example CLARIN (http://clarin.eu/), Common Language Resources and Technology Infrastructure, and DARIAH (https://www.dariah.eu/) Digital Research Infrastructure for the Arts and Humanities.. Whilst these interfaces play a positive role in introducing humanities researchers to large-scale digital collections, they rarely fulfil the complex needs of humanities research which constantly questions received approaches and results, or allow researchers to tailor analysis without being limited by shared assumptions and methods (Wynne, 2013).     Method We explored the challenges associated with deploying and working with large-scale digital collections suitable for humanities research, using a public domain digital collection provided by the British Library The British Library has various digital datasets, including (but not limited to) 7m pages of historic newspapers, 1m out of copyright book illustrations, 100,000s of scientific articles, text from over 60,000 books, 1000s of UK theses, and various digitized medieval manuscripts. We chose here just one of its large scale datasets to work with in this pilot phase. For the terms under which the British Library makes collections available, see http://www.bl.uk/aboutus/terms/copyright/.. This 60,000 book dataset covers publication from the 17th, 18th, and 19th centuries, or – seen as data – 224GB of compressed ALTO XML that includes both content (captured using an OCR process) and the location of that content on a page. Using UCL's centrally funded computing facilities https://wiki.rc.ucl.ac.uk/wiki/Legion, just one of the High Performance Computing facilities available at UCL for researchers, see http://www.ucl.ac.uk/isd/services/research-it/research-computing.  we worked from March-July 2015 with RITS and a cohort of four humanities researchers (from doctoral candidates to mid-career scholars) to ask queries that could not be satisfied by search and discovery orientated graphical user interfaces. Working in collaboration we turned their research questions into computational queries, explored ways in which the returned data could be visualised, and captured their thoughts on the process through semi-structured interviews.    Results We successfully ran queries across the dataset tracking linguistic change, identifying core phrases, plotting and understanding the placing of illustrations, and mapping locations mentioned within core texts. We found that building queries that generate derived datasets from large-scale digital collections (small enough to be worked on locally with familiar tools) is an effective means of empowering non-computationally trained humanities researchers to develop the skill-sets required to undertake complex analysis of humanities data. All code, data, visualisations and other outputs from this pilot project are freely available at https://github.com/UCL-dataspring     Figure 1: A search for mentions of various infectious diseases (Cholera, Whooping Cough, Consumption, and Measles) across the 60,000 book dataset. We compared the profound spikes for Cholera in the dataset with known data regarding epidemics in the UK (Chadwick, 1842; Wall, 1893) which appear as the bars on the graph, showing a relationship between the first major UK outbreak of Cholera and its appearance within the written record of the time. There are further pronounced spikes 1870s and 1880s: these are not associated with UK epidemics, but there were outbreaks in the US and elsewhere. Identifying the texts that refer to these outbreaks allows us to look more closely at these clusters and to understand the relationship between public health, epidemiology, and the published historical record.   From a technical perspective, this pilot highlighted various sticking points when using infrastructure developed predominantly for scientific research. 224GB is only moderately large by comparison to the scientific datasets UCL RITS usually encounters, but although there are shared assumptions between research infrastructures (adoption of technical standards, and the sharing of tools, approaches and research outputs (Wynne, 2015)) most of the UK’s university eScience For more on the UK’s eScience infrastructure, see the work of the eScience Institute, http://www.esi.ac.uk/. Plan-Europe is the Platform of National eScience Centers in Europe (http://plan-europe.eu/). In the United States, the equivalent of eScience is known as Cyberinfrastructure, see the National Science Foundation’s guides: http://www.nsf.gov/div/index.jsp?div=ACI.  infrastructure has been constructed specifically to run scientific and engineering simulations, not for search and analysis of heterogeneous datasets. Our task here had a large textual input, a simple calculation, and a small output summary. By comparison, the typical engineering simulation addresses moderately sized numerical input data, runs a long, complicated calculation, and produces a large output. Poor uptake in the arts and humanities (Atkins et al., 2010; Voss, 2010) has meant that these resources have not been optimised for these workloads. The file system and network configuration of Legion – UCL RITS's centrally funded resource for running complex and large computational scientific queries across a large number of cores – did not match the way that the dataset in question was structured (a large number of small zipped XML files).   The complexities associated with redeploying architectures designed to work with scientific data (massive yet very structured) to the processing of humanities data (not massive but more unstructured) should not be understated, and are a major finding of this project. Relevant libraries (such as an efficient XML processor) needed to be installed and optimised for the hardware. Also, the data needed to be transformed to a structure that the parallel file system (Lustre) could address efficiently (that is, fewer, larger files).  Best practice recommendations for comparable projects emerged from this work: the need to build multiple derived datasets (counts of books and words per year, words and pages per book, etc) to normalise results and maintain statistical validity; the necessity of documenting decisions taken when processing data and metadata; and the value of having fixed, definable data for researchers to explain results in relation to (and in turn, the risks associated with iterating datasets). Pointers to how to process the derived datasets were welcomed, but it was at this stage that the researchers were confident to “go it alone” without our support. We also discovered that a core set of four or five queries gave most of the humanities researchers the type of information they required to take a subset of data away to process effectively themselves: for example, keywords in context traced over time; NOT searches for a word or phrase that ignored another word or phrase, etc. As Higher Education Institution (HEI)-based subject librarians regularly handle routine research queries, we contend that training librarians to aid humanities researchers in carrying out defined computational queries via adjustable recipes would improve access to infrastructure, and cut down on the human-resource intensive nature of this approach. In turn, research computing programmers could be invoked as collaborators for their expertise, such as for developing more complex searches beyond the basic recipes.    Conclusion We successfully mounted large-scale humanities data on high performance computing University infrastructure in an interdisciplinary project that required input from many professionals to aid the humanities scholars in their research tasks. The collaborative approach we undertook in this project is labour intensive and does not scale. Nevertheless, we found many research questions can be expressed with similar computational queries, albeit with parameters adjusted to suit. We recommend, therefore, that HEIs or HEI clusters looking to build capacity for enabling complex analysis of large-scale digital collections by their non-computationally trained humanities research should consider the following activities:  Invest in research software engineer capacity to deploy and maintain openly licensed large-scale digital collections from across the GLAM sector in order to facilitate research in the arts, humanities and social and historical sciences, Invest in training library staff to run these initial queries in collaboration with humanities faculty, to support work with subsets of data that are produced, and to document and manage resulting code and derived data.  Our pilot project demonstrates that there are at present too many technical hurdles for most individuals in the arts and humanities to consider analysing large-scale open data sets. Those hurdles can be removed with initial help in ingest and deployment of the data, and the provision of specific, structured, training and support which will allow humanities researchers to get to a subset of useful data they can comfortably and more simply process themselves, without the need for extensive support.   ",
       "article_title":" Enabling Complex Analysis of Large-Scale Digital Collections: Humanities Research, High Performance Computing, and transforming access to British Library Digital Collections  ",
       "authors":[
          {
             "given":"Melissa",
             "family":"Terras",
             "affiliation":[
                {
                   "original_name":"University College London",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"James",
             "family":"Baker",
             "affiliation":[
                {
                   "original_name":"University of Sussex",
                   "normalized_name":"University of Sussex",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00ayhx656",
                      "GRID":"grid.12082.39"
                   }
                }
             ]
          },
          {
             "given":"James",
             "family":"Hetherington",
             "affiliation":[
                {
                   "original_name":"University College London",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"David",
             "family":"Beavan",
             "affiliation":[
                {
                   "original_name":"University College London",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"Anne",
             "family":"Welsh",
             "affiliation":[
                {
                   "original_name":"University College London",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"Helen",
             "family":"O'Neill",
             "affiliation":[
                {
                   "original_name":"University College London",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"Will",
             "family":"Finley",
             "affiliation":[
                {
                   "original_name":"University of Sheffield",
                   "normalized_name":"University of Sheffield",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/05krs5044",
                      "GRID":"grid.11835.3e"
                   }
                }
             ]
          },
          {
             "given":"Oliver",
             "family":"Duke-Williams",
             "affiliation":[
                {
                   "original_name":"University College London",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"Adam",
             "family":"Farquhar",
             "affiliation":[
                {
                   "original_name":"British Library",
                   "normalized_name":"British Library",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/05dhe8b71",
                      "GRID":"grid.36212.34"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-24",
       "keywords":[
          "information architecture",
          "corpora and corpus activities",
          "GLAM: galleries, libraries, archives, museums",
          "programming",
          "data mining / text mining",
          "content analysis",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" In the AIUCD ( Associazione per l'Informatica Umanistica e la Cultura Digitale) conference held in Bologna in September 2014 Manfred Thaller wondered \"Are the Humanities an endangered or dominant species in the digital ecosystem?\" (Thaller, 2014, also Thaller, 2012). The answer was not simple nor linear and directly involved the Digital Humanities (DH from now on) as a disciplinary and research field that was still poorly defined, DH hold the promise to bring out the Humanities from the Indian reserve where they are now confined, provided certain conditions will be met. In particular DH specialists should:  1) conceive of themselves as researchers and not as conversationalists; 2) strive for a vision; 3) change the epistemology of the Humanities; 4) drive technology and not be driven by it. A few months after the conference Serge Noiret wrote on Digital History (one of my fields of investigation) trying to clarify what actually characterizes this subject within the wider field of DH, and - within the Digital History itself - what is the specific task of the Digital Public History (Noiret, 2015; see also Robertson, 2014). We could place Noiret’s article completely under the first point of Thaller’s list: Digital History and Digital Public History are clearly seen as areas of research and not merely as new forms of communication of old disciplines. Moreover he answered to items 2 and 3 too, essentially proposing a more accurate taxonomy of DH. In a way, he seems to answer Thaller’s question with an accurate definition of some components of the meta-discipline itself. I do not want to linger in this paper on the definition of DH as a whole nor of its components: so many authors in recent years have-debated to define what someone thinks to be (or could be) a discipline and others a research or work field (McCarty, 2005; Svensson, 2010). Every exercise of definition of a “new” area of research is, of course, useful, but at the same time it is potentially frustrating and risky. Frustrating because, as many authors and research centers have declared, despite its now long history, the DH is still an emerging field, and as well as an open, multifaceted, ever-changing one; risky, because each taxonomy of knowledge unavoidably builds walls and fences that encase the knowledge itself in a series of sterile boxes. This could be, in my modest opinion, the risk in Noiret’s essay. It's more important to go beyond a possible but also difficult definition of DH and their several sub-disciplines, focusing our attention on items 2 and 3 of the Thaller list instead, namely on the need to have our own vision and on the importance to characterize DH in terms of the emerging changes of method in our daily research. In particular I will try to connect the concepts expressed by both scholars, looking on the one hand to the recent history of DH in Italy (i.e. degree courses, associations, meetings) and on the other hand to my own research projects at the University of Pisa, especially inside the DH course degree (https://www.unipi.it/index.php/ects/ects?ects_id=IFU-L) and within the Digital Culture Laboratory (http://labcd.humnet.unipi.it/). Starting from some specific cases I wish to reason on the possible vision of the DH.  I will focus very briefly on some projects.   For what concerns  Epigraphical Studies, Public History and Education:   Epigrapisa: A re-reading partly driven and partly spontaneous of the epigraphic messages left over time in a city. Competence/knowledge at work: history, public history, epigraphy, paleography, writing, dramatize, processing images, audio and video, web design.   Teaching (Digital) Epigraphy: a novel education experience in teaching students to transcribe and interpret Roman inscribed lead tags, using a Digital Autoptic Process (DAP) in a Web environment. Competence/knowledge at work: history, education, epigraphy, paleography, writing, manipulating images, collaborative tools.   Pisa e l’Islam and Pisan Romanesque meets Contemporary America: two examples of historical web dissemination with a reasoning about both the potential and the limits of the medium to involve the audience. Competence/knowledge at work: history, arts, archaeology, public history & archaeology, epigraphy, GIS, writing, dramatize, processing images audio and video, web design.      In the area of  Digital Public History:   Tramonti. Itinerari tra generazioni lungo i crinali della Val di Vara a complex project aimed at enhancing the cultural heritage of an Italian rural valley through the active participation of residents. Competence/knowledge at work: history & archaeology, public history & archaeology, invented archives, education, writing, dramatize, GIS, manipulating images and videos, libraries, collaborative tools, web design, project management.      In the field of Digital Editions:   Codice Pelavicino Digitale: the digital edition of a medieval manuscript built to provide all services of the digital world and to invite the readers to actively participate. Competence/knowledge at work: history & public history, text encoding, philology, paleography, codicology, writing, manipulating images, collaborative tools, web design, project management.     By shortly describing these project I will not try to figure out what distinguishes them from each other, but, on the contrary, what characterizes all of them as Digital Culture projects and what they tell us about a possible vision of DH:  they are digital; they are inevitably and necessarily interdisciplinary; they are open; they were built in a kind of new Renaissance workshop, a digital craft (DIGICRAFT).    They are digital. This may seem trivial but it is not. These are projects “born digital” not because the digital world offers the most useful tools to achieve the same purpose in relation with the “real” world, but because they could not exist outside the incredible interaction between real and digital world that it is now our life. They are digital because they might not otherwise exist.   Interdisciplinarity is compulsory. DH is a field unavoidably and profoundly interdisciplinary and we have to deal with each project as a complex set of activities and skills that crosses, by its true nature, several fields; this change of practice and approach implies by itself a methodological revolution, because it requires an organization of work similar to a Renaissance workshop (a DIGICRAFT), with an articulated division of labor in relation to several levels of skills, where education and training could be provided by the same learners, coordinated by a strong and mature central idea.    Openness is a result and a choice. Working in a multidisciplinary team built upon research and with different tools, sustainability requires using open source tools, sharing data between individuals and giving everything to the public. Then Openness is a natural result, even it is also an ethical, political and philosophical choice as the Digital Manifesto 2.0 says: “the digital is the realm of the open, open source, open resources\".   A DIGICRAFT. In a Renaissance workshop it was possible to produce different objects: statues, paintings, goldsmith or less valuable coroplastic objects. Each handwork was a “project” that included on the one hand a strong artistic and cultural vision (meaning, style, function, purpose, style) and on the other hand a complex set of different techniques made by different workers with different levels of capacity. The owner of the shop (or the head-artist) had not necessarily to know each technique as an expert, but his employees could in many ways be superior and the various members of the workshop could learn from each other. The owner had to keep the team together with a well clear idea of the work itself. Likewise a DIGICRAFT could work (and actually does in our LabCD in Pisa) in the same way: each project is taken over as an interdisciplinary complex object that requires specific skills and different but profoundly related competences. The basis (first phases) of the work are often composed by students of the Bachelor and Master's degree in DH, who work, in the labCD, as interns or undergraduates. The work is directed by one “manager” but followed at various stages by experts, who assign specific tasks to the students, always ensuring an active connection among everyone in the team through the usual or more useful collaborative tools. While the work goes on, often happens that some student acquire, in a particular technique or phase, a greater capacity and knowledge than the others and then he/she becomes able to propose substantial changes in the work chain. The manager is not required - nor humanly could - to know every aspect in depth, nor to be fully aware of all the problems related to it, or to master each technique: however, he/she must be able:  *) to see always clearly the aim and the nature of the work;  *) to communicate effectively with everyone in the team.  A “digicraft” is anywhere on a DH project teachers and students exchange knowledge and leverage this interaction to offer innovative and effective solutions, combining the theoretical reasoning with practices and skills. This is possible only if the manager and the team share a common strong vision of what a DH project is, embracing a \"systemic\" or “organic” or “holistic” thinking of DH itself.   The core of DH is unitary and lies in the conviction that the digital turn has permeated every aspect of our lives as people and scholars modifying them deeply.  In the 70s of XXth century has increasingly gained ground a vision of Humanities Computing that kept almost unchanged the traditional disciplines within their rigid internal divisions and distinguished the humanist from the expert in information technology, hoping and promoting a dialogue between the two main areas (still in Fusi 2011, I, p. 1-2). Today this position is no longer sustainable. The web in first place and the web 2.0 in the second (but also the Big Data emerging field as well as the Data Visualization tools) have slowly but surely changed the research landscape especially demolishing the barrier between tools, methods and ways of sharing. We are obviously still in a transitional phase. Highly specialized sub-areas remain (and also in the future will exist) and obviously several scholars strive to better define the old / new digital disciplines (digital history, digital philology and so on), but there is also a complementary phenomenon pointing to an inclusive and unitary vision of DH.  From the perhaps limited but interesting Italian observatory I believe this change has affected both the terminology used in the establishment of centers, associations and degree programs (AIUCD, Digital Humanities degree, Digital Cultural Heritage, Arts and Humanities School), both the organisation of courses and meetings.  It 's more and more widespread the awareness that we are a new type of scholar (and graduate, and PhD), the digital humanist, someone who has a mixed formation, an open mind, is able to master both languages and the main methodological issues of the two areas without considering one serving the other. In doing so, we need to maintain the epistemological strictness that each discipline involved in the DH has developed over time: commingling does not mean carelessness or inaccuracy; but in the same time we have to claim the change or the changes in each methodology in order to build a new global epistemology. In order to do this the digital humanist has to embrace a \"systemic” or “organic” or “holistic” thinking of the humanities, leave the enclosure of the academic fields and get away from the temptation to create an old-new rigid taxonomy. Speaking of “systemic” or “organic” or “holistic” thinking / view (I’m sorry for the repetition but the adjectives organic and holistic mean different things not only in different languages but mainly in different subjects), I refer to the epistemological approach that has emerged in some areas of the research over the past thirty years and which tends to oppose the reductionist approach flourished since the seventeenth century onwards and imposed in almost all sectors of the so-called \"hard sciences\" (Capra-Luisi 2014). As we know reductionism believes that studying in depth a peculiarity of a phenomenon and understanding it completely it will be possible, by progressive addition of discoveries, illuminate the entire system. The reductionist approach has been, as we know, the basis for the scientific revolution of the modern age, but it also led in the eighteenth and nineteenth century to an exasperate fragmentation of the fields of scientific research. This phenomenon has also heavily influenced the Humanities,often creating absurd barriers and hyper-specialized languages, that have closed researches in several walled gardens. I believe that this long wave has exhausted its strength and that precisely the DH can reverse the trend. Now a new methodological approach have arisen alongside the reductionist thinking, considering the “system”, the “whole”, something more and different than the sum of its components. The “systemic thinking” reasons in terms of relationships, networks, patterns of organizations and processes; it proposes a change of paradigms: from the vision of the world as a machine to the world as a network; it takes account of the fundamental interdependence of all phenomena.  This change of paradigms could and should affect the DH as well for the reasons listed above, promoting a systemic view of this meta-discipline and therefore pushing Digital Humanists to deeply transform the old practice of work. ",
       "article_title":"Digicraft and 'Systemic' Thinking in Digital Humanities",
       "authors":[
          {
             "given":"Enrica",
             "family":"Salvatori",
             "affiliation":[
                {
                   "original_name":"University of Pisa, Italy",
                   "normalized_name":"University of Pisa",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/03ad39j10",
                      "GRID":"grid.5395.a"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "digital humanities - pedagogy and curriculum",
          "digital humanities - diversity",
          "history of Humanities Computing/Digital Humanities",
          "digital humanities - nature and significance",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Technological advances in digitization, automatic image analysis and information management are enabling the possibility to analyze, organize and visualize large cultural datasets. As one of the key visual cues, shape feature has been used in various image analysis tasks such as handwritten character recognition (Fischer, 2012; Franken, 2013), sketch analysis (Eitz, 2012), etc. We assess a shape descriptor, within the application domain of Maya hieroglyphic analysis. Our aim is to introduce this descriptor to the wider Digital Humanities (DH) community, as a shape analysis tool for DH-related applications. The Maya civilization is one of the major cultural developments in ancient Mesoamerica. The ancient Maya language infused art with uniquely pictorial forms of hieroglyphic writing, which represents an exceptionally rich legacy (Stone, 2011). Most Maya texts were written during the Classic period (AD 250-900) of the Maya civilization on various media types, including stone monuments. A special class of Maya texts was written on bark cloths as folding books from the Post-Classic period (AD 1000-1519). Only three such books (namely the Dresden, Madrid and Paris codices) are known to have survived the Spanish Conquest. A typical Maya codex page contains icons, main sign glyph blocks, captions, calendric signs, etc. Fig. 1 illustrates an example page segmented into main elements (Gatica-Perez, 2014). In this paper, we are interested in the main signs.    Figure 1. Page 6b of the Dresden Codex, showing individual constituting elements framed by blue rectangles (Hu, 2015), Green arrows indicate reading order of the main sign blocks, generated by Carlos Pallan based on SLUB (  ) online open source image.    Maya hieroglyphic analysis requires epigraphers to spend a significant amount of time browsing existing catalogs to identify individual glyphs. Automatic Maya glyph analysis has been addressed as a shape matching problem, and a robust shape descriptor called Histogram of Orientation Shape Context (HOOSC) was developed in (Roman-Rangel, 2011). Since then, HOOSC has been successfully applied for automatic analysis of other cultural heritage data, such as Oracle-Bones Inscriptions of ancient Chinese characters (Roman-Rangel, 2012), and ancient Egyptian hieroglyphs (Franken, 2013). It has also been applied for generic sketch and shape image retrieval (Roman-Rangel, 2012). Our recent work extracted statistic Maya language model and incorporated it for glyph retrieval (Hu, 2015). The goal of this paper is two-fold:  Introduce the HOOSC descriptor to be used in DH-related shape analysis tasks (code available at:  );  Discuss key issues for practitioners, namely the effect that certain parameters have on the performance of the descriptor. We describe the impact of such choices on different data types, specially for 'noisy' data as it is often the case with DH image sources.    Automatic Maya Hieroglyph Recognition We conduct glyph recognition with a retrieval system proposed in (Hu, 2015). Unknown glyphs are considered as queries to match with a database of known glyphs (retrieval database). Shape and context information are considered. Fig.2 illustrates a schema of our approach. We study the effect of different HOOSC parameter choices on the retrieval results.    Figure 2. Retrieval system pipeline.    Datasets We use three datasets, namely the 'Codex', 'Monument' and 'Thompson'. The first two are used as queries to search within the retrieval database ('Thompson').    Figure 3. Digitization quality: (left) raw glyph blocks cropped from Dresden codex; (middle) clean raster images produced by removing the background noise; (right) reconstructed high-quality vectorial images.   The 'Codex' dataset contains glyph blocks from the three surviving Maya codices. See Fig.3 for examples. Glyph blocks are typically composed of combinations of individual signs. Fig.4 shows individual glyphs segmented from blocks in Fig.3. Note the different degradation levels across samples. We use two sub-datasets: 'codex-small', composed of 156 glyphs segmented from 66 blocks, for which we have both clean raster and high-quality reconstructed vectorial representations (see Fig.4) to study the impact of the different data qualities on the descriptor; and a 'codex-large' dataset, which is more extensive, comprising only the raster representation of 600 glyphs from 229 blocks.     Figure 4. Example glyph strings generated from blocks shown in Figure 3.   The 'Monument' dataset is an adapted version of the Syllabic Maya dataset used in (Roman-Rangel, 2011), which contains 127 glyphs of 40 blocks extracted from stone monuments. It is a quite different data source to the codex data, in terms of historical period, media type, and data generation process. Samples are shown in Fig.5.     Figure 5. Example blocks and segmented glyph strings form the 'Monument' dataset.   To form the retrieval database ('Thompson'), we scanned and segmented all the glyphs from the Thompson catalog (Thompson, 1962). The database contains 1487 glyph examples of 892 different sign categories. Each category is usually represented by a single example image. Sometimes multiple examples are included; each illustrates a different visual instance or a rotation variant. Fig.6 shows glyph examples.     Figure 6. Thompson numbers, visual examples, and the syllabic values of glyph pairs. Each pair contains two different signs with similar visual features (Hu, 2015). All examples are taken from (Thompson, 1962).     Shape-based retrieval Feature extraction and similarity matching are the two main steps for our shape-based glyph retrieval framework.     Figure 7. Extracting HOOSC descriptor: (a) input clean raster image; (b) binary image; (c) thinned edge of (b); (d) reconstructed vector representation of (a); (e) thinned edge of (d); (f) corresponding groundtruth image in the catalog; (g)-(k) spatial partition of a same pivot point with five different ring sizes (1, ½, ¼, 1/8, 1/16, all defined as a proportion to the mean of the pairwise distance between pivot points) on the local orientation field of the thinned edge image (c). Note that we zoomed in to show the spatial context of 1/16 in (k).   Glyphs are first pre-processed into thin lines. To do so, an input glyph (Fig.7(a)) is first converted into a binary shape (Fig.7 (b)). Thin lines (Fig.7(c)) are then extracted through mathematical morphology operations. Fig.7(c)-(d) show the high-quality reconstructed binary image, and the extracted thin lines.  HOOSC descriptors are then computed at a subset of uniformly sampled pivot points along the thin lines. HOOSC combines the strength of Histogram of Orientation Gradient (Dalal, 2005) with circular split binning from the shape context descriptor (Belongie, 2002). Given a pivot point, the HOOSC is computed on a local circular space centred at the pivot's location, partitioned into rings and evenly distributed angles. Fig.7 (g)-(k) show different sizes of the circular space (referred to as spatial context) partitioned into 2 rings and 8 orientations. A Histogram-of-orientation-gradient is calculated within each region. The HOOSC descriptor for a given pivot is the concatenation of histograms of all partitioned regions. We then follow the Bag-of-Words (BoW) approach, where descriptors are quantized as visual words based on the vocabulary obtained through K-means clustering on the set of descriptors extracted from the retrieval database. A histogram representing the count of each visual word is then computed as a global descriptor for each glyph. In all experiments, we use vocabulary size k=5000. Each query is matched with glyphs in the retrieval database, by computing shape feature similarity using the L1 norm distance.    Figure 8. Six pairs of glyph signs (Hu, 2015). Each pair contains a query glyph from the 'Codex' dataset (right), and their corresponding groundtruth in the catalog (left).     Incorporating context information Shape alone is often ambiguous to represent and distinguish between images. In the case of our data, different signs often share similar visual features (see Fig.6); glyphs of the same sign category vary with time, location, and the different artists who produced them (see Fig.8); additionally, surviving historical scripts often lose visual quality over time. Context information can be used to complement the visual features. Glyph co-occurrence within single blocks encodes valuable context information. To utilize this information, we arrange glyphs within a single block into a linear string according to the reading order (see Fig.4 and Fig.5), and consider the co-occurrence of neighbouring glyphs using an analogy to a statistical language model. For each unknown glyph in the string, we compute its probability to be labelled as a given category by considering not only the shape similarity, but also the compatibility to the rest of the string. We apply the two language models extracted in (Hu, 2015), namely the ones derived from the Maya Codices Database (Vail, 2013) and the Thompson catalog (Thompson, 1962), which we refer to as the 'Vail' and the 'Thompson' models. We use Vail model with smoothing factor α=0 for the 'Codex' data, and the Thompson model with α=0.2 for the 'Monument' data.    Experiments and Results Our aim is to demonstrate the effect of various HOOSC parameters on retrieval results.  Experimental setting We illustrate the effect of 3 key parameters:  Size of the spatial context region within which HOOSC is computed  A larger region encodes more context information and therefore captures more global structure of the shape. However, in the case of image degradation, a larger region could contain more noise. We evaluate five different spatial contexts as shown in Fig.7(g)-(k). The circular space is distributed over 8 angular intervals.   Number of rings to partition the local circular region  This parameter represents different partition details. We evaluate either 1 or 2 rings, the inner ring covers half the distance to the outer ring. Each region is further characterized by a 8-bin histogram of the local orientations.  Position information  Relative position (i, j) of a pivot in the 2-D image plane can be concatenated to the corresponding HOOSC feature.   Results and discussion Fig.9 shows the average groundtruth ranking in the retrieval results with different parameter settings, on three query sets,  e.g. 'Codex-large', 'Codex-small' and 'Monument'. Each query image usually has only one correct match (groundtruth) in the retrieval database. The smaller the average ranking value, the better the result. From Fig.9 we can see the following:   In most cases, the best results are achieved by using the largest spatial context, with finer partitioning details (2 rings in our case); When the location information is not considered, results show a general trend of improving with increasing ring sizes. However, the results are more stable when the position information is encoded,  e.g. a smaller ring size can also achieve promising results when the location information is incorporated. This is particularly useful when dealing with noisy data, where a smaller ring size is preferred to avoid extra noise been introduced by a larger spatial context;  The results do not benefit from a finer partition, when a small spatial context is considered. However, results improve with finer partitions, when the spatial context becomes larger.  Position information is more helpful when a small spatial context is considered.  Fig.10 shows example query glyphs and their top returned results.    Conclusion We have introduced the HOOSC descriptor to be used in DH-related shape analysis tasks. We discuss the effect of parameters on the performance of the descriptor. Experimental results on ancient Maya hieroglyph data from two different sources (codex and monument) suggest that a larger spatial context with finer partitioning detail usually leads to better results, while a smaller spatial context with location information is a good choice for noisy/damaged data. The code for HOOSC is available so DH researchers can test the descriptor for their own tasks.   Acknowledgement We thank Edgar Roman-Rangel for co-inventing the HOOSC algorithm and providing the original code. We also thank Carlos Pallán Gayol, Guido Krempel, and Jakub Spotak for producing the data used in this paper. Finally, we thank the Swiss National Science Foundation (SNSF) and the German Research Foundation (DFG) for their support, through the MAAYA project.    Figure 9. Retrieval results on each dataset, with various feature representation choices. (left) shape-base results; (right) incorporating glyph co-occurrence information.      Figure 10. Example queries (first column) and their top returned retrieval results, ranked from left to right in each row. Groundtruth images are highlighted in green bounding boxes.   ",
       "article_title":" Assessing a Shape Descriptor for Analysis of Mesoamerican Hieroglyphics: A View Towards Practice in Digital Humanities  ",
       "authors":[
          {
             "given":"Rui",
             "family":"Hu",
             "affiliation":[
                {
                   "original_name":"Idiap Research Institute, Switzerland",
                   "normalized_name":"Idiap Research Institute",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/05932h694",
                      "GRID":"grid.482253.a"
                   }
                }
             ]
          },
          {
             "given":"Jean-Marc",
             "family":"Odobez",
             "affiliation":[
                {
                   "original_name":"Idiap Research Institute, Switzerland; Ecole Polytechnique Federale de Lausanne (EPFL)",
                   "normalized_name":"Idiap Research Institute",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/05932h694",
                      "GRID":"grid.482253.a"
                   }
                }
             ]
          },
          {
             "given":"Daniel",
             "family":"Gatica-Perez",
             "affiliation":[
                {
                   "original_name":"Idiap Research Institute, Switzerland; Ecole Polytechnique Federale de Lausanne (EPFL)",
                   "normalized_name":"Idiap Research Institute",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/05932h694",
                      "GRID":"grid.482253.a"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "information retrieval",
          "cultural studies",
          "archaeology",
          "content analysis",
          "English",
          "image processing",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Standardization has become an increasingly important process in relation to academic research, as it provides a better way for exchanging information. Humanities and cultural studies have followed, however, a heterogeneous path in which creativity and tradition play an essential role. Comparative studies in literature, and especially poetry, are a clear example of this eclectic situation. There is not a uniform academic approach to analyze, classify or study the different poetic manifestations. Things get even worse when comparing poetry schools from different languages and periods. The result of this uncoordinated evolution is a bunch of varied terminologies that means to explain analogous metrical phenomena through the different poetic systems whose correspondences have been hardly studied (González-Blanco and Sélaf, 2014). The existing digital poetic repertoires and databases are a good example of this situation, as they constitute a rich kaleidoscope of multilingual virtual poetry, constituted by lyrical collections in French (Nouveau Naetebus), Italian (BedT), Hungarian (RPHA), Medieval Latin (Corpus Rhythmorum Musicum, Annalecta Hymnica Digitalia, Pedecerto), Gallego-portuguese (Oxford Cantigas de Santa María, MedDB2), Castilian (ReMetCa), Dutch (Dutch Song Database), Occitan (BedT, Poèsie Neotroubadouresque, The last song of the Troubadours), Catalan (Repertori d’obres en vers), Skaldic (Skaldic Project), or German repertoires (Lyrik des Minnesänger), among others. Interoperability among these different poetic corpora would be desirable, as having a common search engine to extract information from all of these databases at the same time would have a deep impact for comparative studies in literature, linguistics and other humanities disciplines. We are, however, far from this reality as interoperability is not possible due to a lack of standardization both in technological and philological fields.   Philological standardization During the Middle Ages and the Renaissance, the powerful influence of Latin made scholars inherit the terminology of Classical poetry treatises and apply it to Romance languages, regardless of their different linguistic traits and verse structures. When vernacular theories started to arise, each literary school set up its own terminology and classification system. This multiplicity led to complex situations, such as the creation of conceptual genres that only exist in some traditions.  Spanish conceptualization models are a good example to illustrate this situation: the classical system of Bello (1955), first published in 1835, divided all the structures into binary and ternary feet (imitating the classical Latin terminology):   Binary  Ternary       Trochaic: óo   Iambic: oó       Dactylic: óoo   Amphibrachic: oóo   Anapestic: ooó      Later, the musical analysis system applied by Navarro Tomás (1956) was followed as a valid system through many years, using concepts like anacrusis. In the last years, there have been many different approaches to explain the Spanish panorama, as it is shown by the semantic comparative model designed by the Czech Oldrich Bělič.  The international context is richer, especially in English, with two prominent schools: 1) A traditional approach based on stress and classical feet; and 2) A generative approach based on the terminology and concepts shown through text grids that take into account word boundaries, with a strong impact on poetic theories (Gerber, 2013: 147). The models described are just an example of the idiosyncrasy that can be observed in each literary tradition. Although the current ICT infrastructures are prepared to harvest different types of collections and models, it is necessary at a first stage to standardize metadata and map vocabularies and terminology at the philological level in order to build a consistent able to be shared between the different traditions.   Technological standardization The lack of unified criteria is translated into many different uncoordinated technologies when research data are transformed to build digital projects and do not even follow a standard, in most cases. The multiplicity of technologies used includes SQL databases, TEI and XML markup languages, semantic web technology standards (RDF, OWL, SKOS), natural language processing systems (NLP) and visualization tools.  Relational databases have been deeply used by the first digital poetic repertoires combining an ER (Entity-Relationship) model, together with the data model based on records for the logical implementation (Elmasri and Navathe, 2011, 27-ss). The problem of representing ER composition model is that the result shown is data centered, but it is not enough to mark textual items that need to be analyzed from a metrical point of view. There are other projects based on XML solutions, as TEI has a specific module for poetry analysis, “Verse”, with a rich set of tags to describe metrical schemas, rhymes, accentual structure and syllabic varieties. However, this model is not widely used by the different projects, and the lack of philological unified criteria makes it difficult to translate literary schemas into XML tags, making researchers create new tags or express nuances with customized attributes for each project.  The key for interoperability both in philological and technological fields is a common reference system, for which semantic web technologies are a powerful solution. Building a linked data model by adding a semantic layer of metadata to the existing databases does not alter their internal structure. This solution requires, however, to assume unified criteria on the philological model that serves as a reference. Although semantic web technologies have had success in archives libraries and museums (group known as LODLAM http://lodlam.net/), its application to poetic corpora is very different, as there are only a couple of studies dealing with some of the above mentioned aspects (Bootz and Szoniecky, 2008 and Zöllner-Weber, 2009), but there is not a standard conceptual model of ontology referred to metrics and poetry. The closest works related to this topic are probably the conceptual model of CIDOC, the controlled vocabularies of English Broadside Ballad Project http://ebba.english.ucsb.edu/ and the linked data relations offered by the Library of Congress (http://id.loc.gov/), which do not offer enough information on metrics vocabulary. There are also interesting computational approaches which use automated linguistic analysis or text mining, based on the morphological and phonetic structure of each language. Results have been impressive, as one of the greatest advantages is the speed of the analysis of big amounts of text (Gervás, 2000). Nevertheless, the integration of these technologies with the previous models described is not easy, and solutions are often customized for the variation of natural language used, most times standard English.    Our approach What we propose in this paper is not a new method for analyzing poetry, but an abstract model based on a working methodology supported by a double standardization system, both at philological and technological levels. In relation to this aspect, it must be highlighted that it needs to be carried out by an interdisciplinary and coordinated team, which requires a careful design of data architecture in different levels. Our proposal aims to set up a procedure to combine philological criteria to map vocabularies and concepts which might have common means and properties in the different traditions and to insert them into an abstract framework in which each of these elements can fit as individuals of an ontology which gathers the main poetics concepts shared by most traditions. We have worked on some first approaches in this sense, building our first ontology prototype, based on our ReMetCa Spanish project: www.purl.org/net/remetca (González-Blanco, and Del Rio, et al., 2014), populated with a controlled vocabulary in SKOS, that can be found in http://vocabularios.caicyt.gov.ar/pmc/. These preliminary results, which served as prototype and a basis to build the current model, have been applied to different poetry projects, such as the edition of Cancionero de Baena: http://sade.textgrid.de/exist/apps/SADE/Dialogo_Medieval/index.html or the description of poetic collections in http://www.poetriae.linhd.es/, but they need to be expanded and improved to analyze other poetic systems. This paper has been developed thanks to the research projects funded by MINECO and led by Elena González-Blanco: Acción Europa Investiga EUIN2013-50630: Repertorio Digital de Poesía Europea (DIREPO) and FFI2014-57961-R. Laboratorio de Innovación en Humanidades Digitales: Edición Digital, Datos Enlazados y Entorno Virtual de Investigación para el trabajo en humanidades, and the Starting Grant ERC-2015-STG-679528 POSTDATA.  ",
       "article_title":"DH Poetry Modelling: a Quest for Philological and Technical Standardization",
       "authors":[
          {
             "given":"Elena",
             "family":"González-Blanco",
             "affiliation":[
                {
                   "original_name":"Universidad Nacional de Educación a Distancia (UNED), Spain",
                   "normalized_name":"National University of Distance Education",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/02msb5n36",
                      "GRID":"grid.10702.34"
                   }
                }
             ]
          },
          {
             "given":"Gimena",
             "family":"Del Rio Riande",
             "affiliation":[
                {
                   "original_name":"Seminario de Edición y Crítica Textual (SECRIT-IIBICRIT CONICET), Argentina",
                   "normalized_name":null,
                   "country":"Argentina",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Clara",
             "family":"Martinez Cantón",
             "affiliation":[
                {
                   "original_name":"Universidad Nacional de Educación a Distancia (UNED), Spain",
                   "normalized_name":"National University of Distance Education",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/02msb5n36",
                      "GRID":"grid.10702.34"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016",
       "keywords":[
          "semantic web",
          "literary studies",
          "data modeling and architecture including hypothesis-driven modeling",
          "standards and interoperability",
          "philology",
          "ontologies",
          "knowledge representation",
          "content analysis",
          "English",
          "multilingual / multicultural approaches",
          "prosodic studies",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" While early English drama has been subject to several DH investigations, because “a quite explicit and rigid system of metadata is part of the genre itself” (Mueller, 2014: par27), French theatre of the 17th and 18th century remains largely underexplored, in spite of demonstrations and case studies presented by authors with large international audiences (Moretti, 2005). This paper presents some results of a project inspired by the explorations of the English corpora, but dedicated to the computer aided identification and analysis of theatrical topoi in the French domain. Topoi can be defined as “recurrent configurations of thematic or formal elements” (adapted from Weill and Rodriguez, SATOR, 1996) and they are highly significant for the French plays, which are largely based on a common stock of characters, scenes and themes, and in which novelty often consists in reuse with a shift of former materials. Considering the vast amount of plays produced over the period going from, roughly, 1630 to 1790 (some 12000 plays, even if the text of many of them is lost), the use of computational tools to track the presence of various topoi over the time appears appropriate. Computer aid can help answer in a (more) rigorous way questions concerning the aesthetics of the French theatre of the period, too often reduced to “classicism”, and historical interrogations about moments and reasons when certain meaning units are more in use or relatively abandoned. While the results of this endeavour are at a very early phase, for reasons to be detailed in the presentation, the very attempt to clarify some of these literary questions shows that the use of the computer is not neutral and modifies the problems to be solved. After a more detailed presentation of the activities undertaken to date, this paper will reflect upon the way in which the methods and tools used to approach the texts modify the initial research questions. Dramacode: an expanding community for French drama Identifying topoi using a computer asks in the first place for the existence of digitised texts in a more elaborate format than the pdfs or plain texts offered by Gallica (see http://gallica.bnf.fr). Various initiatives have been taken in this domain in France, either by scholars supported by HEIs and research institutes (see “Molière project” http://obvil.paris-sorbonne.fr/corpus/moliere/moliere) or by individuals with computer skills and a special interest in literature from the 17th and 18th century (see http://www.theatre-classique.fr). Unfortunately, this diversity of initiatives resulted in a great variety of encoding practices, more or less TEI compliant. To reduce this diversity and move towards common standards, an organisation has been created on GitHub. Under the name of “Dramacode”, it hosts some 882 plays written or staged between 1630 and 1810 (roughly, 7,3% of the dramatic production of the period), to be progressively re-tagged and enriched with further mark-up. “Dramacode” allows also to publish the contents with a harmonised style-sheet, and to share work on experiments of visualisation of data extracted from the plays. Finding theatrical topoi through linguistic analysis and literary mark-up Within this corpus, the plays of Louis de Boissy (1694-1754) have been selected for a pilot study, destined to build strategies for gathering sufficient clues, through various queries, about the existence of a topos in an unknown (i. e., not previously read) play, digitised and encoded in another project.  Louis de Boissy was a French play writer who enjoyed a certain success in the 18th century mainly because of two of his plays, Le Français à Londres and L’Homme du jour. Beyond these achievements, what renders his work interesting is precisely its mediocrity. Boissy’s works are a perfect example of the reuse of topoi, sometimes to follow the fashion, sometimes in an attempt to distinguish himself by revitalising conventional themes and dialogues. He has also the particularity of having worked for the three main theatrical spaces of the period, namely the Comédie-Française, Théâtre Italien and Théâtres de la Foire. Therefore, his texts become a privileged observatory of French comedy in the first half of the 18th century. Two approaches of topoï identification are developed. The first one consists in developing an ontology for characters and scenes description. Roles and description of roles from the existing plays have been extracted, cleaned, and analysed using an excel spread-sheet. Ten categories appear necessary for describing characters from classical French scenes; five of them were known and formalised since Aristotle, or, at least, since the rediscovery of Aristotle by French authors and thinkers of Renaissance, then 17th century: nationality (“climat” in Marmontel’s words, see entry “Moeurs” in Eléments de littérature, 1787), age, sex, occupation (“état”) and temperament. Five others were deemed of interest, as allowing potential automatic analysis aimed to spot changes in aesthetic principles: the ontological category (human/ nonhuman/ semihuman), the dramatic or actantial role, the social status (“king”, “prince”, “marquis”, “Dom”…: it seemed useful to distinguish these from information about “état”), the family position, and the collective nature of some characters. In parallel, types of scenes and other significant recurrences are manually identified and marked-up as “recognition”, “fight”, “complot”, etc. Relevant lines or phrases (labelled with xml:ids) are regrouped as <span>s in the <back> section of <text>; if each span receives, for the moment, a literal description, this will progressively feed into another library and will be later referred to using @ana. The second approach is based on a linguistic analysis of the speeches, using AntConc to generate concordances and to identify key-words. The initial objective was to spot key-words or linguistic idiosyncrasies of the character of “petit-maître”, whose identification was the primary goal of this research. Indeed, “petit-maître” are not always labelled as such, and therefore their presence in French drama has been underestimated and incorrectly related to a specific period in time (mainly, “decadence comedy” after 1680). If, in time, a complete mark-up of the characters according to the above-mentioned categories (i. e., <temper>) will help identifying as “petits-maîtres” those who are not called as such by roleDesc or other characters in the play, speech identification is for the moment the only option for answering the research question. New research vistas When observing the trajectory of this study, the digital approach can be said as having significantly modified the initial perspective and research questions. The TEI tagging of characters and scenes revealed the need to refine methods and categories developed by the narratology and the theatrical analysis in 1970 and 1980, preparing the grounds for more accurate description of character building. In the meantime, the effort to describe a stylistic specificity of the “petits-maîtres” revealed to be rooted in a larger, two folded research question, concerning the potential differences in speech between characters of the same author. Digital humanities have been, to date, more interested in global stylistic analysis on one side, and in authorship studies, on the other side, while this research project intends to apply the methods of the second approach to fictional speakers. While such an endeavour can be justified by the pretences of French dramatists of the classical centuries to act as kind of “sociolinguists”, the question is as well that of the methods to automatize the analysis of specific speeches when conducted on a large scale (more than 9300 distinct characters presents in the above-mentioned set of plays), and of the explanations to be found for the observed recurrences. Indeed, during the pilot study executed on Boissy plays, it appeared that his characters group, by their speech, less in male and female speakers, but rather in two unexpected classes, one formed by those who tend to speak more about themselves (characters saying “je”), the second by those who address themselves to the the others (characters saying “vous”). The importance of “I” and “You” has already been spotted in theatrical studies (see Craig, 2004), but not the observed repartition, which needs, however, to be checked against the larger corpus, a very time consuming operation for the moment.  It is to be expected that further observation of concordancers and n-grams extracted from very large collections of digitised plays will put into light such other unexpected recurrences, triggering a new understanding about what is a classical play, and contributing to the on-going debate about the nature of the theatre. ",
       "article_title":"Playing with French Drama: from Old Research Questions to New Research Tools",
       "authors":[
          {
             "given":"Ioana",
             "family":"Galleron",
             "affiliation":[
                {
                   "original_name":"LIDILEM, U. Grenoble Alpes, France",
                   "normalized_name":"Grenoble Alpes University",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/02rx3b187",
                      "GRID":"grid.450307.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-02",
       "keywords":[
          "french studies",
          "genre-specific studies: prose, poetry, drama",
          "literary studies",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Data visualisation has become one of the most relevant DH topics, due to the advent of Big Data in Humanities research practices, and to the need to make complex statistical analyses accessible to users without a technical background. Although several visualisation libraries, such as d3.js, are now freely available online and are relatively easy to use, it is still a challenging task to provide simple and effective interface design, avoiding both over-complex and over-simplified solutions. When the data to be displayed have undergone complex processing, for instance automated text analysis, it is of paramount importance to preserve all the information conveyed by such analyses, while making it understandable to the users.  In this work, we present a collaboration between communication design and natural language processing (NLP) researchers, devising effective strategies to display different aspects of the semantic content of texts. The outcome of the collaboration is the ORATIO platform, specifically developed to  compare different points of view automatically extracted from text. The most challenging tasks, indeed, concerned the visualisation and the exploration of differences and overlaps detected through automated text processing.    Use case Our use case concerns the comparison between Nixon’s and Kennedy’s speeches uttered during the U.S. presidential campaign in 1960. The corpus consists of 282 documents by Nixon (830,000 tokens) and 598 documents by Kennedy (815,000 tokens) From http://www.presidency.ucsb.edu/1960_election.php. The overall goal of the project was to track the difference in language and content between the two opponents, and make it available through a platform which makes use of a “generous interface”: first providing all the information to the user, and then enabling him to handle the visual model through a number of options and filters (Whitelaw, 2012). Infact, in our setting, researchers are supposed to reshape and reduce the visualizations in order to prove theories or discover new interesting aspects related to the processed text. The proposed navigation pattern complies with the paradigm “Overview first, zoom and filter, details on demand” (Heer and Shneiderman, 2012).  Other existing approaches do not start from an overview, but from an empty window, where the user can build up a personal view, while investigating the relationships inside the data. We rely on such approaches in order to design the last visual model of the platform (Fig. 6), while the others take from the first one, starting from an overview.   ORATIO Description To cope with corpora richness, a multiple view approach has been adopted (Mauri, Pini, Ciminieri and Ciuccarelli, 2013): rather than providing a single view, with all the information, five different perspectives have been identified, each exploring a different piece of information in a comparative way. The first view is the  Summary, whose goal is to provide the user with a general overview of the two corpora, including geographical, temporal and size information. Each corpus is associated with an imagine and a color (blue for Kennedy, red for Nixon), which remain consistent across all the platform views. Under  Summary, users can see how speeches are distributed on a map (according to the place where the talk was given, included in the metadata), on a timeline (based on day of the speech in the metadata), and what linguistic features characterise each corpus (i.e. number of speeches, average words in a document and total number of words). For instance, in Figure 1 a compact representation of three corpus dimensions is given: the x-axis represents the timeline, the y-axis includes the list of cities where the speeches were given, and the dimension of the bubbles corresponds to the number of speeches uttered in a certain place at a certain time point.  The visualisation shows, for instance, that Nixon pledged to visit all the 50 States, while Kennedy did not held any speech in some States that were less critical to the victory of the elections (e.g. Hawaii or Vermont). Another interesting aspect of the electoral campaign emerging from this view is that, despite having visited less States, Kennedy was more active than Nixon: he stopped in a higher number of cities (239 cities overall, against the 172 cities visited by Nixon), and had about twice as many speeches, press releases, statements and remarks as his opponent (about 550 for Kennedy and 260 for Nixon). This is highlighted by the prominence of blue over red bubbles.    Fig.1: Summary view of the two speech corpora   The second view, called Affinity, targets the need to understand the relevance of topics in the political debate and the presence of important differences between the two candidates. In this view, specific word classes such as verbs, keywords or persons’ names are displayed as circles, whose size is proportional to the number of occurrences in text. The more the terms occur in both corpora, the more they are displayed towards the center of the window. If they occur prevalently (or only) in Kennedy’s or Nixon’s speeches, they are displayed towards the left or the right side of the window, respectively (Fig. 2).    Fig. 2: Affinity view showing the most relevant personal entities discovered in the corpora.   The third view, displaying People, gives a network-based representation of the people automatically recognized in the corpora by a Named Entity Recogniser (Finkel et al., 2005). If two or more people are mentioned within the same sentence, they are linked in a spatialized graph. As with the other views, users are then able to filter out elements from the visualization, in order to discover new patterns (Fig. 3a). In our specific use case, filters and other selection strategies are really useful, since the complete network is very large and difficult to read at a glance (Fig. 3b).    Fig. 3a: People view after filtering Fig. 3b: The default network in People view   The Places view provides a comprehensive visualisation of the geographical information contained in the two corpora. It displays the metadata about the place where the speeches were uttered together with the GPEs mentioned in the speeches, automatically extracted with the same Named Entity Recogniser used for Persons. These two pieces of information are usually displayed separately, since the most widely used visualisation strategies based on heatmaps would not allow to distinguish them. However, we devised a solution where both can appear on the same map, while being easily distinguishable: the locations where a speech was uttered are marked with a cursor, while the mentioned places are highlighted on the map as colored areas. The comparison shows that Kennedy devoted more attention to specific areas outside US, while Nixon was more concerned with domestic policy. For instance, Kennedy mentioned several times places in Latin America, since one of the key themes of his campaign was the “Good Neighbor” policy, a topic not covered by Nixon.    Fig. 4: Places view with visited places (marked with cursor) and mentioned places (colored)   The last view, named Concordances, is inspired by linguistic research and recalls the family of concordancer tools (see for instance Kehoe and Renouf, 2002). In contrast with the previous models, this functionality takes a different approach, since there is no overview and the user is supposed to create a representation in order to answer questions and prove hypotheses. Specifically, a user can look for a particular keyword or concept and see all the sentences where it appears, typographically aligned to ease readability. In a second step, other important terms close by the given concept can be displayed as well (Fig. 5).    Fig 5: the Concordances view, displaying the use of “today”, compared with the presence of the term “begin”.     Conclusions We presented the ORATIO platform, specifically developed to compare the content of two different corpora in the political domain. The work is the outcome of a collaboration between researchers in Communication Design and Natural Language Processing applied to Digital Humanities. Although NLP allows to process and extract information from large corpora with minimal efforts, it has drawbacks, which are then inherited by the presented platform. For instance, persons’ nodes (Fig. 3) need to be disambiguated in order to merge nodes representing co-referring mentions (e.g. “J. F. Kennedy” and “Jack Kennedy”). Also geo-political entities (Fig. 4) require disambiguation and geo-referencing. This was performed completely automatically, but errors are possible, and this kind of visualisation makes it even more straightforward to spot them.  In order to address these issues, possible solutions could be to 1) give users the possibility to inspect the content of the documents containing displayed information (from distant to close reading), and then 2) give them the possibility to manually correct the displayed information (e.g. drag and drop some elements in the space, delete nodes, etc.). The development of new interfaces enabling such human intervention would be very important and represents the future direction of our research.  ",
       "article_title":" Visualisation Strategies for Comparing Political Ideas with the ORATIO Platform  ",
       "authors":[
          {
             "given":"Tommaso",
             "family":"Elli",
             "affiliation":[
                {
                   "original_name":"Politecnico di Milano",
                   "normalized_name":"Politecnico di Milano",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/01nffqt88",
                      "GRID":"grid.4643.5"
                   }
                }
             ]
          },
          {
             "given":"Giovanni",
             "family":"Moretti",
             "affiliation":[
                {
                   "original_name":"Fondazione Bruno Kessler, Italy",
                   "normalized_name":"Fondazione Bruno Kessler",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/01j33xk10",
                      "GRID":"grid.11469.3b"
                   }
                }
             ]
          },
          {
             "given":"Rachele",
             "family":"Sprugnoli",
             "affiliation":[
                {
                   "original_name":"Fondazione Bruno Kessler, Italy; University of Trento",
                   "normalized_name":"University of Trento",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/05trd4x28",
                      "GRID":"grid.11696.39"
                   }
                }
             ]
          },
          {
             "given":"Michele",
             "family":"Mauri",
             "affiliation":[
                {
                   "original_name":"Politecnico di Milano",
                   "normalized_name":"Politecnico di Milano",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/01nffqt88",
                      "GRID":"grid.4643.5"
                   }
                }
             ]
          },
          {
             "given":"Giorgio",
             "family":"Uboldi",
             "affiliation":[
                {
                   "original_name":"Politecnico di Milano",
                   "normalized_name":"Politecnico di Milano",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/01nffqt88",
                      "GRID":"grid.4643.5"
                   }
                }
             ]
          },
          {
             "given":"Sara",
             "family":"Tonelli",
             "affiliation":[
                {
                   "original_name":"Fondazione Bruno Kessler, Italy",
                   "normalized_name":"Fondazione Bruno Kessler",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/01j33xk10",
                      "GRID":"grid.11469.3b"
                   }
                }
             ]
          },
          {
             "given":"Paolo",
             "family":"Ciuccarelli",
             "affiliation":[
                {
                   "original_name":"Politecnico di Milano",
                   "normalized_name":"Politecnico di Milano",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/01nffqt88",
                      "GRID":"grid.4643.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "visualisation",
          "natural language processing",
          "English",
          "interface and user experience design"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Building on the success of Annotation Studio—MIT’s collaborative, open source annotation tool—a new application called the Idea Space expands the functionality of the platform to cover the point in the cycle of close reading where annotation turns to writing composition. This new interface allows students to arrange annotations into an outline, which, when exported to a word processor, retains citations of the original text along with full metadata and links to their annotations. This paper discusses the basis for the Idea Space in assessment of student annotation and in conceptions of digitally aided student scholarship. It also describes the development of the Idea Space as a modular application, complementary to Annotation Studio but easily adaptable to other environments, and presents a working prototype of the Idea Space.  The Annotation Studio online application has been used successfully over the past three years as a tool to support student textual annotation and collaborative reading, and its user base has expanded exponentially over the past two years in educational institutions around the world. In addition to facilitating textual annotation and reading-group formation, Annotation Studio has the potential to provide a writing space in which student annotations may be organized and used as the basis for the development of arguments by the individual or by collaborating groups. We are working, in effect, to develop a pedagogy and a tool that will support a seamless integration of the processes of close reading, annotation and writing. The aim of this new tool, the Idea Space, is to help students learn to collect sources, analyze content, form arguments, properly cite sources and confidently manage the writing process.  Recent assessment has shown that deep engagement with texts through forms of close reading and annotation can have a significant impact on students’ ability to become better academic writers. A few studies have pointed specifically to the role of collaborative annotation in strengthening and expanding students’ annotation practices, their engagement with textual analysis, and their ability to incorporate and analyze sources in their written work. Yet, collaborative annotation, supported by an online social annotation tool, such as HyperStudio’s Annotation Studio, is a relatively new pedagogy, and initial adoption of Annotation Studio has focused on its efficacy as a tool to improve students’ critical reading. Preliminary experimentation has suggested that the tool can help students visualize verbal patterns in texts; deepen their in-class discussions; anchor their claims with more detailed textual evidence; dig deeper into contextual and cultural meaning; and provide more thoughtful peer review of each other’s texts in progress. Other parts of the writing process, such as organizing evidence, analysis, and claims into coherent argument structures, lie just outside the scope of the current implementation of Annotation Studio.  The Annotation Studio team have therefore prototyped a new “Idea Space” extension of Annotation Studio, in which each student’s annotations are displayed and organized around key terms. In this space, the student’s or group’s annotations (including images and other media) may be sorted, arranged and structured to form arguments and provide the basis for developing supporting materials that compose the substance of academic writing. The Idea Space will help students organize citations, comments, and preliminary text to create an outline of an essay which in turn can be fed back into the annotation tool for further revision and for review by writing instructors. The Idea Space will provide both a workshop space for the development of writing and a window into the writing process itself, from which an instructor will be able to engage with the student.  While the Idea Space presents a novel interface, its scholarly and pedagogical basis is as grounded in ancient practices as annotation itself. When Juliet Fleming takes on the question of how we can usefully define “what reading is,” she proposes the metaphor of “cutting.” Fleming’s suggestion—that reading has always been a process of pulling material from a text and adapting it to the reader’s purposes—resonates with the way in which many students currently use Annotation Studio: first flagging evidence in the text through the collaborative annotation process, then later drawing on these same pieces of evidence to compose written arguments. Just as digital interfaces help realize an ideal environment for annotation, they also offer myriad ways in which to support and enrich this process of cutting. The ease of sorting, filtering, duplicating and rearranging material in digital settings affords a fast and intuitive way to turn from annotative reading to composition. The capacity of these virtual materials to retain links to one another, keeping track of where and when annotations or edits have been made, can not only relieve reader-authors of some of the work of managing citations, but also give scholars a greatly augmented record of their own process. This record can, persist throughout the scholarly cycle when works exported from the Idea Space are themselves uploaded into Annotation Studio.  While the Idea Space is conceived as adding to Annotation Studio’s support for student work, its modular design and implementation enable it to work in combination with other tools and sources of data. The application’s first data module retrieves annotations from the Annotation Studio database, whose format conforms to open annotation standards, meaning that the interface could easily extend other annotation environments. A simple design for adding new data modules gives instructors the ability to tailor the application for use with their own combinations of tools and archives. Far more than a visual organizer for annotations, the Idea Space is a tool through which users can combine, juxtapose and adapt any scholarly material in the composition of compelling and richly contextualized writing.  ",
       "article_title":"Writing Composition in the Close Reading Cycle: Developing The Annotation Studio Idea Space",
       "authors":[
          {
             "given":"Kurt",
             "family":"Fendt",
             "affiliation":[
                {
                   "original_name":"Massachusetts Institute of Technology, United States of America",
                   "normalized_name":"Massachusetts Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/042nb2s44",
                      "GRID":"grid.116068.8"
                   }
                }
             ]
          },
          {
             "given":"Suzanne",
             "family":"Lane",
             "affiliation":[
                {
                   "original_name":"Massachusetts Institute of Technology, United States of America",
                   "normalized_name":"Massachusetts Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/042nb2s44",
                      "GRID":"grid.116068.8"
                   }
                }
             ]
          },
          {
             "given":"Andy Kelleher",
             "family":"Stuhl",
             "affiliation":[
                {
                   "original_name":"Massachusetts Institute of Technology, United States of America",
                   "normalized_name":"Massachusetts Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/042nb2s44",
                      "GRID":"grid.116068.8"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "digital humanities - pedagogy and curriculum",
          "teaching and pedagogy",
          "hypertext",
          "creative and performing arts, including writing",
          "linking and annotation",
          "software design and development",
          "scholarly editing",
          "knowledge representation",
          "English",
          "bibliographic methods / textual studies",
          "interface and user experience design"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" First published posthumously in 1690, Furetière’s  Dictionnaire Universel had aroused controversy well before its publication. Nevertheless, it was quickly followed by an enlarged and corrected version edited by the protestant scholar Henri Basnage de Beauval. It is this extended version of 1701, with its broad coverage of terminological language that is our subject.  The Furetière project seeks to render the entire 1701 dictionary available as an open access digital resource in an XML-TEI compliant format. Given the size of the task, and the current total lack of finding with little hope in the short term, the first stage will be an attempt to map and describe terminological coverage by reference to a small number of themes, namely architectural, legal and maritime terminology. This paper will demonstrate the mapping procedure being carried out using the Atlas Ti Computer-Assisted Qualitative Data Analysis Software (CAQDAS) and the building of a model for the TEI encoding.  Historical background Furetière was both a member of the French Academy and participant in the dictionary building team, hence the uproar at his publishing what was seen as a rival dictionary. This explains why a Catholic priest should end up being published by the protestant publisher in the Netherlands, Arnaud Leers. The dictionary was a success, but still needed much revising. This was carried out by the French émigré and scholar, Henri Basnage de Beauval. Despite being the most complete edition, only one attempt at digitising was ever made (Wionet and Tutin, 2001). Our aim is to encode the Basnage dictionary but with cross references to the edition of 1690 and the publication of Corneille (1694) so as to see how the terminological entries evolved and to what extent complementary information can be found in the Corneille dictionary (Williams, Forthcoming).   Using CAQDAS One important task for the Digital Humanities community will be to bring or adapt existing tools to disciplines that are often less digitally aware. Thus, in using a CAQDAS tool to explore the three dictionaries, we aim not only to use a very powerful tool to allow pre-digitising analysis and mapping of data, but also to bring this technology into the sphere of literary analysis. CAQDAS were essentially created to meet the needs of sociologists carrying out rigorous qualitative analyses on data from multiple supports. Literary specialists also carry out qualitative analyses, but often tend to use highlighters to work on printouts of PDFs. By using a CAQDAS to work through a document as big and as complex as the Furetière’s dictionary, we aim to show how an electronic highlighter that allows coding and network analyses can be used in humanities research, and particularly in our own. There are a number of commercial CAQDAS on the market, and only one open-source tool. Although open-source tools are important to the community, we have adopted Atlas Ti as being a very powerful tool that is evolving rapidly with new functions.  The 1701 Basnage edition is over 4000 pages of very tight text. Text quality is poor which precludes use of OCR. Whilst it is structured, we are in a period of great experimentation in dictionary compilation leading to a complex meta and microstructure. The only way forward is to read the text. Using Atlas Ti, we skim the pages so as to locate and code entries designated as terms, to find the different introductory formulae, as well as spotting potential search formulae for unmarked terms, and to list the domains and crafts to which they belong. The coding system allows us to carry out a bottom-up conceptualisation of the dictionary with the quotations allowing us to create a headword list that links directly to the entries in the PDF file. Knowing the terminological domains covered by the dictionary is interesting in itself, but it also gives further keys for reaching unmarked terms. Networks allow us to reorganise domains and crafts into groups, such as legal terminology – represented by several domain names- and maritime terminology, which is often closely linked to language of fortification, architecture and law. Atlas Ti allow to output data in a machine-readable format so that the headword list could be transformed as an organised lexicon for use in the XML encoding process.   XML TEI Use of Atlas Ti as an electronic highlighter with all its coding and management functions does permit a full qualitative analysis of the data. However, it is still not possible to share the data itself. Digitising the entire dictionary is a mammoth task that is only feasible using crowd sourcing over a long period of time. Marking up only thematically designated terminological domains allows us to create and test a model for the data as well as making available machine readable data rapidly. The dictionary is unique in having definitions accompanied by lengthy examples and citations thus providing both technical and phraseological information that will only be retrievable using in-depth mark-up. Whereas Wionet and Tutin (op.cit) marked up one letter, our plan is to attempt to follow the terminology through the entire dictionary. The first stage consists of marking up terms from a small number of highly productive thematic fields isolated using the CAQDAS analysis. The first field to be explored in depth is related to maritime activities as designated by ‘terme de marine’ (maritime term) so as to open a collaboration with French historians so as to compare an élite vision of naval terminology as compared by the situation in a major naval port, that of Lorient which founded in 1666 by the Compagnie des Indes, and then became a major naval base from 1703.  Our aim is to illustrate the decisions taken and how these affect output through visualisation, but also analysis using linguistic analysis tools as TXM and a database system as BaseX. Once more advanced, the data will be put online using a query interface. At the moment, we are sharing code on GitHub under the name Basnage. Mark-up is being carried out using Oxygen so as to mark-up using the TEI guidelines and use XQuery to ensure consistency. Entries can be extremely complex, as will be illustrated by reference to the verb  Abatre. This means that the TEI guidelines do not always adapt easily to what is found in the text. However, we are endeavouring not to customise the guidelines so as to retain full compatibility with other dictionaries and allow easier linking with source texts.   The dictionary tends to group words orthographically so that there is a main headword in large capitals, which also carried the grammatical information, but then a series of subentries that generally have the headword in small capitals. These subentries may include the specialised terminological usage. To complicate affairs, polysemy is illustrated within an entry with short comments and examples. To handle this, we are using <superEntry> to group the whole, and then <entry> for what might be considered as subentries. <Sense> is used to cover polysemy within a given entry. The main entry for ABATRE has three main senses illustrated by a series of synonyms, with each sense accompanied by numerous examples, and occasionally a citation with bibliographic reference. The examples frequently contain collocations that activate a particular synonym, it would be useful to mark-up and illustrate this. Similarly, citations generally only give a link to a person, often via an abbreviated name, as in MEN for Ménage or ABL for M. d’Ablancourt. These are generally listed at the beginning of the dictionary, but it is not always the case and sometimes the abbreviations are inconsistent. Sometimes, even if a text is not named, it is possible to link to a source document. Given that Basnage was a prolific letter writer at the centre of network of European scholars, and the author of the  Histoire des Ouvrages des Savants, and important task will be to cross link to this valuable source of information.  Collocation mark-up means that we can link dictionary analysis to words in a wider context as fund in Frantext or, when a text has already been digitising by using a language analytical tool as TXM. Mark-up also aims to make an onomosiological analysis possible using BaseX rather than simply presenting the data in a linear semasiological format.    Conclusion This is a mammoth mark-up task which we believe is rendered easier by mixing tools so as to permit on-going analysis whilst gradually digitising the whole into XML compliant TEI. This strategy means that other scholars can use data without having to wait for the entire digitalisation process to be completed. In so doing, we seek to explore data whilst collaborating in the dissemination and improvement of digital tools and also a contribution to the art of digital mark-up of early dictionaries.  ",
       "article_title":"Tackling Terms In Furetiere’s 'Dictionnaire Universel'",
       "authors":[
          {
             "given":"Geoffrey Clive",
             "family":"Williams",
             "affiliation":[
                {
                   "original_name":"Université Grenoble Alpes, France",
                   "normalized_name":"Grenoble Alpes University",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/02rx3b187",
                      "GRID":"grid.450307.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-01",
       "keywords":[
          "french studies",
          "xml",
          "encoding - theory and practice",
          "lexicography",
          "digitisation - theory and practice",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Producing and coding bot ‘listeners’ has today become almost as easy as automated music production has been for years. Machines can thus both ‘create’—and ‘listen’ to ‘music’ (whatever we mean by these categories). In fact, such notions are capricious within the contemporary streaming music landscape. The project, “Streaming Heritage: Following Files in Digital Music Distribution” (financed by the Swedish Research Council) studies emerging streaming media cultures in general, and the music service Spotify in particular, with a bearing on the digital challenges posed by direct access to musical heritage. Rediscovering older music is key for Spotify, and the project is hence on the one hand geared towards investigating the institutional challenges of streaming musical heritage, and on the other hand—and foremost—to develop new digital research methods. Situated at HUMlab (Umeå university) part of the project is essentially about Turing testing Spotify. Building on the tradition of ‘breaching experiments’ in ethnomethodology, my research group seeks to break into the hidden infrastructures of digital music distribution in order to study its underlying norms and structures. The key idea is to follow files’ (rather than the people making or using them) on their distributive journey through the streaming ecosystem. The setting include the distribution and aggregation of self-produced music/sounds through Spotify; the set-up of our own record label (for research purposes); the programming of bots to inform, explore, mimic, and ultimately subvert notions of usage and listening; the tracing of Spotify’s history through constantly changing interfaces (web archiving and documenting these). Research questions range from various way how streaming music is commodified? What sounds are perceived as music (or not) according to Spotify and adjacent aggregating services? How is metadata generated, ordered, and valued—and what kind of metadata is actually available? What normative world views are promoted and materialized by streaming architectures? What kind of infrastructures proliferate behind the surfaces of on-demand services? My presentation departs from the fact that one-fifth of Spotify’s catalogue of 30 million songs haven’t once been listened to at all. Under the computational hood of streaming services all streams are equal, and every stream thus means (potentially) increased revenue from advertisers. Spotify is hence likely to include—rather than reject—various forms of (semi-)automated music, sounds and (audio)bots. At HUMlab we therefore set up an experiment—SpotiBot—with the purpose to determine if it was possible to provoke, or even to some extent undermine, the Spotify business model (based on the 30 second royalty rule). Royalties from Spotify are only disbursed once a song is registered as a play, which happens after 30 seconds. The SpotiBot engine was be used to play a single track repeatedly (both self-produced music and Abba’s ”Dancing Queen”), during less and more than 30 seconds, and with a fixed repetition scheme running from 10 to n times, simultaneously with different Spotify account. Based on a set of tools provided by Selenium the SpotiBot engine automated the Spotify web client by simulating user interaction within the web interface. From a computational perspective the Spotify web client appeared as black box; the logics that the Spotify application was governed by was, for example, not known in advance, and the web page structure (in HTML) and client side scripting complex. It was not doable within the short experiment to gain a fuller understanding of the dialogue between the client and the server. As a consequence, the development of the SpotiBot-experiment was (to some extent) based on ‘trial and error’ how the client behaved, and what kind of data was sent from the server for different user actions. Using a single virtual machine—hidden behind only one proxy IP—the results nevertheless indicate that it is possible to automatically play tracks for thousands of repetitions that exceeds the royalty rule. Even if we encountered a number of problems and deviations that interrupted the client execution, the Spotify business model can be tampered with. In other words, one might ask what happens when—not if—streaming bots approximate human listener behavior in such a way that it becomes impossible to distinguish between a human and a machine? Streaming fraud, as it has been labeled, then runs the risk of undermining the economic revenue models of streaming services as Spotify.  ",
       "article_title":"SpotiBot-Turing testing Spotify",
       "authors":[
          {
             "given":"Pelle",
             "family":"Snickars",
             "affiliation":[
                {
                   "original_name":"Umea university, Sweden",
                   "normalized_name":"Umeå University",
                   "country":"Sweden",
                   "identifiers":{
                      "ror":"https://ror.org/05kb8h459",
                      "GRID":"grid.12650.30"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-24",
       "keywords":[
          "English",
          "audio, video, multimedia",
          "media studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Network visualizations are the most complex visualizations possible, but sometimes they are not capable of describing system-complexity. Even if they are the most widely employed visualization techniques, they still have limitations. Indeed a) their relations are not sufficient to analyse complexity and b) networks do not distinguish between qualitative differences of represented entities. Starting from the actual network model, how could one manipulate this visualization to improve complexity comprehension? In this paper, we propose a solution called  trajectory. The trajectory has two major points of difference compared to the network: the trajectory a) represents not only distances, but also durations, and b) it displays kinetic entities according to their evolution with time.  The discourse is articulated around these four points. Considering that networks are tools widely used by digital humanists, we propose a new language to improve the quality of represented data: a new network based on a vertical timeline. Complexification of the network visualization is not just a new language, but also a tool that would give the field of Digital Humanities the most complex of all possible visualizations.   Networks How could one improve the visualization of complexity? To answer this question, we need to investigate the qualities and defects of network visualizations. A network is a wonderful way to visualize complexity: in this model, limitless relations and entities can be mapped. A network neither draws frontiers nor imposes quantitative limits to relations and entities. How does one imagine an even more complex visualization? In other words, how does one draw a “greater infinity” of relations and entities? It seems impossible indeed; the network could possess interminable relations and entities. But a trick is still possible – to enrich entities by adding a temporal dimension. Conventionally, network visualizations are created considering the interdependence between distance and attraction, i.e. the spatial relationship. In fact, network visualizations are stable images, keeping each entity immobile, without possible evolutions. Changes in connections and disconnections of entities happen because of temporal events taking place in reality. So, if we want to achieve an even more detailed representation of complexity, we need to introduce another dimension for relations and entities – the dimension of time. Time and space are intimately linked through movement. To enhance the richness of the visual language, we need to visualize the movement of entities through static simulation, as done in  Spiral Expansion of Muscles in Action by Umberto Boccioni, the Futurist artist who introduced the art of depicting sculpture in movement (Figure 1). We do not need to draw more actors or relations, it is enough to improve their representation with a more elaborated shape. In this way, complexity will be managed not only in terms of infinite spatial entities and relations, but also in terms of infinite time-based entities and relations.    Figure 1. Umberto Boccioni, Spiral Expansion of Muscles in Action, 191    Movement and complexity If network visualization is to represent movement, it needs the fluidity through a new dimension of reality. This is the reason why relations and entities have to be both spatial and time-based. Movement lends a continuity to time and space, and is the key to better understand the representation. Consequently, the aim is to visualize the evolution of entities through a sequence of time-based networks. Formerly, Kandinsky had noticed that nodes are fixed (1947, 32-35). To create dynamic visualizations, these nodes should progress. Now, network visualizations enriched by time could be said to be in motion. Visually speaking, network entities are represented by points; giving them a movement signifies that the points have to be represented through lines (Kandinsky 1947, 57; Ingold 2007). This solution allows us to lend more complexity to the basic point representation. Moreover, Kandinsky claims: “Considered in terms of substance, it [the point] equals zero” (1947, 25). The contrast between the point and the line in network visualizations finds another analogy in the Kandinsky’s thoughts, who considers the line as the antithesis to the point: “The line is, therefore, the greatest antithesis to the pictorial proto-element – the point” (Kandinsky, 1947: 57). Thanks to the line representation, continuity enters the complexity of representation as the fabric stitching together the narrativity of several network sequences. Moreover, continuity implies an inherent relationship between both distance  and duration. If the network visualization tends to be infinite in terms of spatial relations and entities, it is fairly weak to highlight time, another infinite dimension. The introduction of the trajectory brings another dimension to the visualization: the infinity of durations of relations and entities.    Figure 2. The DHLAB network is created by co-authoring: each node is an author and each edge a collaboration for a paper. Nodes represent also external collaborators    Figure 3. The DHLAB network is divided into years. Each trajectory represents an author who published during the years, tracing his continuity    The visualizations in practice Data are extracted from Infoscience, EPFL’s publication repository. This repository is public, and everyone can access it and obtain the data presented in this paper. Technically speaking, we queried all publications associated with the DHLAB. This article has two figures representing the DHLAB in Lausanne. Both visualizations present the same information in two different ways: the first shows collaborations on a plain surface – this figure is a classical network visualization (Figure 2); the second is based on the trajectory idea: points previously arranged on a flat surface are transformed into lines reifying the continuity of people (Figure 3). If we affirm that complexity representation has indeed been “complexified”, how this could help digital humanists?  The first figure shows how the laboratory professor is placed at the center – the network would probably be very different without him and would be divided in parts. So, he is the core of the laboratory with good reason. The networks consists of two big clusters, which means that the professor is not included in all the publications of the laboratory (a rule that is compulsory in some labs). The second figure immediately displays the literary production year by year. For example in 2013, the laboratory was relatively young to widely publish scientific works, collaborations have a greater size in 2014 and 2015. Compared to the network visualization, the trajectories depict the variability of links with time quite well by splitting the network into a sequence of networks. The trajectories also describe the centrality of the professor better. For example in 2014, he basically worked with a group of people (shown at his left) and a colleague (at his right). In the same year, this colleague had a group of authors with whom he collaborated. Following the trajectories of that group, we can easily see that in 2014, it was not directly linked to the professor whereas in 2015, they all worked together. This is a detail revealed by trajectories that was not visible with classical network visualization.   The tool (conclusion) Gyorgys Kepes wrote: “each new visual environment demands [...] new way of measuring” (1995, 13). This phrase introduces two concepts that would be of interest to the communities of Digital Humanities and Data Visualization: context and method. We could suppose that the context is given by humanists and the method by designers, but DH is the result of a blend between the two. Designers can not think of methods out of context, the opposite is true for humanists. As Johanna Drucker says, we are experiencing “a momentary blindness among practitioners” (2011, 5). To overcome this blindness, we put together the efforts of designers and humanists: the visualization here described represents interdisciplinarity – the context and the method at the same time. For this reason, we think that trajectories are a good example. The DH community is close to having the sensibility to solve the interdisciplinarity issue. For this reason, theory has to be combined with practice to produce solid research tools for the DH community. Trajectories are not simply a theoretical concept but also a tool that will be presented at the DH 2016 conference and which will be available to digital humanists for decomposing their networks, or, in other words, to lend representations their complexity.  ",
       "article_title":"The Trajectories Tool: Amplifying Network Visualization Complexity",
       "authors":[
          {
             "given":"Alexandre",
             "family":"Rigal",
             "affiliation":[
                {
                   "original_name":"EPFL, Switzerland",
                   "normalized_name":null,
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Dario",
             "family":"Rodighiero",
             "affiliation":[
                {
                   "original_name":"EPFL, Switzerland",
                   "normalized_name":null,
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Loup",
             "family":"Cellard",
             "affiliation":[
                {
                   "original_name":"EPFL, Switzerland",
                   "normalized_name":null,
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-01",
       "keywords":[
          "English",
          "visualisation"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Hearts and Minds: the Interrogations Project is an interactive virtual reality artwork developed by an interdisciplinary team including humanists, social scientists, artists, and computer scientists from four different universities. The project attempts to extend and make accessible difficult narratives of war and abusive violence based on actual accounts from soldiers involved. The work offers models for engaging with testimony and oral history. It uses visualization to build new discourse around challenging topics and to bridge concepts that enable storytelling. While many uses of visualization technologies are focused on providing accessible representation of “big data,” in this case, the technologies are being used as a narrative platform to represent a complex contemporary issue and to provide a platform for discussion and debate of military interrogation methods and their effects on detainees, soldiers, and society.    Hearts and Minds makes use of the CAVE2 TM environment for a multisensory artwork addressing a complex contemporary problem: as American soldiers are returning from wars in Iraq and Afghanistan, it is becoming increasingly clear that some of them participated in interrogation practices and acts of abusive violence with detainees for which they were not properly trained or psychologically prepared. This has in turn left many soldiers dealing with Post-Traumatic Stress Disorder after their return home. More than 1.8 million US troops have served in Iraq and Afghanistan, with 37% having been deployed at least twice (Litz, 2009) The mental health impact of these wars is still under research as many veterans are at risk for chronic PTSD. American soldiers and citizens are left with many unresolved questions about the moral calculus of using torture as an interrogation strategy in American military operations.    Fig 1. A performer interacts with the CAVE2 version of Hearts and Minds at the EVL in Chicago   Development  Hearts and Minds bridges art, computer science, and social science research. Artist Roderick Coover (Temple University) and writer Scott Rettberg (University of Bergen) worked with the research scholars John Tsukayama and Jeffrey Stevenson Murer (St. Andrews University) to distill central themes and stories from the significant and extensive research project—based on hundreds of hours of original interviews with veterans—carried out by Tsukayama (Tsukayama, 2014). These interviews include revelations of a highly sensitive nature, including narratives of participation in acts of abusive violence that entailed violations of human rights. The interviewees granted Tsukayama the right to use their stories in his dissertation and in subsequent research outcomes derived from it, provided that their identities remained anonymous. The tapes of recorded interviews were destroyed after transcription, except for short samples to prove their authenticity, and Tsukayama did not retain any personal contact information for the soldiers he interviewed. The text was condensed into an accessible and coherent set of stories that would preserve the accuracy of the testimonies while voice actors would perform the roles of veterans, further assuring their anonymity.  Coover and Rettberg worked with artist Daria Tsoupikova and scientist Arthur Nishimoto at the Electronic Visualization Lab (University of Illinois at Chicag, 2014) to transform this controversial and challenging research into an accessible form through visualization and dramatization. Together they developed an interactive virtual environment with imagery, 3D models and panoramic photographic backgrounds to bring story elements together. Working across these environments allowed new kinds of connections to be made between home spaces and battlefields, and between domestic objects and the memories they become attached to. The voice recordings performed by Philadelphia-based actors were integrated with interactive media elements in a peformative interactive environment. Objects, 3D environments, textures, and some animations were developed in Maya (Autodesk Inc., CA). Maya speeds up the production process through its rich selection of tools supporting all stages of modeling, including surface creation and manipulation, texturing, lighting, rigging, and animation. The visual, auditory and narrative elements were brought together in the Unity platform (Unity Technologies Inc., CA), software that is typically used by computer game developers. The getReal3D plugin for Unity developed by Mechdyne Corporation is used to run Unity across the CAVE2™ cluster (Mechdyne Corporation, 2014). User interaction was scripted using the Omicron (Electronic Visualization Lab, 2014) input abstraction library developed by EVL.    CAVE2 Performance In its first iteration,  Hearts and Minds was presented as public performances at the University of Illinois Chicago Electronic Visualization Lab in June and July 2014 (Galatzer-Levy, 2014). Chicago-based performance artist Mark Jeffrey led a performance of the interactive work. As the audience entered the space, they found themselves in a temple environment listening to each of the four soldier character’s stories of enlistment—why they originally chose to become soldiers and what motivated their perspectives on the purpose for military service.   Jeffery then led the audience to the boy’s room, where activating four individual objects launches stories of first encounters with abusive violence in military experience, such as in hazing rituals during basic training, or after first arriving in Iraq. When each trigger object is selected within the 3D visual space, the domestic space falls away and a surreal desert landscape is revealed. This transition serves as a metaphor for the interior state of the individual solider, as it is coherent with accounts of soldiers experiencing Post-Traumatic Stress. It is also intended to bring audience members into a “listening state” where they can focus on the individual voices and the issues they raise. Objects in a living room space and a suburban backyard move us further into the field of battle, and there we encounter harrowing stories of interrogation, torture, and moral conflicts confronted differently by each of the characters.  An important component of the performances of  Hearts and Minds is that the experience of the artwork is followed by audience discussion. The ultimate purpose of this work is to promote dialogue and debate about the contexts and circumstances of the use of torture in battlefield torture in recent history. In this sense the project shares an aim with Augosto Boal’s Theatre of the Oppressed, in that attempts to offer audiences “the aesthetic means to analyze their past, in the context of their present, and subsequently to invent their future, without waiting for it.” (International Theatre of the Oppressed Organization, 2014) During a 2015 presentation of the project at the Oslo Human Rights Film Festival (Coover et al., 2015), discussion participants included both a veteran police interrogator, who shared his experience that more humane methods of interrogation than torture were universally more effective, and a number of prisoners of conscience who had themselves been subjec to abusive violence and felt provoked by the work to share their own experiences as victims of torture.     Accessible Versions and Alternative Platforms One of the challenge for makers of immersive virtual reality artworks, particularly those developed in CAVEs and other custom-built VR environments is that these artworks tend to be more often read about in the literature of the field than experienced first-hand. The CAVE2™ at the EVL for example is an active research lab facility with keycard access at the center of a large engineering building on the UIC campus.  Hearts and Minds has been shown there at several special events and on specially arranged tours, but it is not the ideal accessiblity situation to reach a broader audience. Because  Hearts and Minds was developed in Unity, it is possible to port the application to other environments. In order to make the work more accessible for new audiences, the  Hearts and Minds team is developing new versions of the work: a Mac OS standalone application suitable for cinematic performance or installation (Coover et al., 2014), a Unity web-player version which will be published on the World Wide Web and will be accessible freely to the public, a version suitable for iPads and other tablet computers, and a version for Oculus Rift. The stand-alone application version has been used for performances in cinema enivronments in Paris, Bergen, Oslo, Krakow, Michigan, and elsewhere, and the other application versions will be publically released in 2016.    Fig. 2 Presentation of the interactive cinematic version of Hearts and Minds at the 2015 Oslo Human Rights / Human Wrongs festival    Completing  Hearts and Minds as Critical Digital Media and Digital Humanities Project  The  Hearts and Minds team is currently developing the tablet version of the work, which will include both the tablet (iPad and Android) application, and a package of contextualizing essays, interviews and commentaries both specific to the work and that provide more detailed treatments of the issues the work raises. These essays and interviews will for example contributions by political scientists and social scientists about the situations out of which abusive interrogations practice emerged, contributions by digital humanists considering interactive media artworks as a mode of critically and socially engaged research, and contributions by designers and computer scientists considering the history and technologies involved in virtual reality art. Coover and Rettberg’s long paper for DH 2016 will include a presentation of this new version of the project, and will further situate  Hearts and Minds in an emergent corpus of critically engaged new media art.   ",
       "article_title":"Addressing Torture in Iraq through Critical Digital Media Art—Hearts and Minds: The Interrogations Project",
       "authors":[
          {
             "given":"Scott",
             "family":"Rettberg",
             "affiliation":[
                {
                   "original_name":"University of Bergen, Norway",
                   "normalized_name":"University of Bergen",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/03zga2b32",
                      "GRID":"grid.7914.b"
                   }
                }
             ]
          },
          {
             "given":"Roderick",
             "family":"Coover",
             "affiliation":[
                {
                   "original_name":"Temple University",
                   "normalized_name":"Temple University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00kx1jb78",
                      "GRID":"grid.264727.2"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "visualisation",
          "hypertext",
          "media studies",
          "film and cinema studies",
          "virtual and augmented reality",
          "historical studies",
          "English",
          "audio, video, multimedia"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Figurative language poses a challenge to Natural Language Processing systems, while being a ubiquitous phenomenon that is deeply ingrained in every-day language. As corpus studies suggest, figurative language devices appear on average in every third sentence of general-domain text (Shutova, 2015), thus making the development of automatic recognition and interpretation systems play an important role in many text mining use cases, especially those aiming for a deeper semantic understanding of texts. Furthermore, acknowledging the pervasiveness of such language forms - and of the emblematic device of metaphor in particular - allows for a change in perspective, not conceiving it as a merely rhetorical device but as a genuinely cognitive mechanism that manifests itself in language in the form of surface metaphorical expressions. Such surface expressions usually follow a directionality principle common in figurative language which is to project one domain of experience (the source, e.g. war) onto another (the target, e.g. argument), the first one typically being more concrete and the second one being more abstract. Taken together, such surface expressions (e.g. She shot down all of my arguments) consitute a conceptual metaphor argument is war, a cognitive phenomenon that can be studied through its expression in language. This line of thought has been established as Conceptual Metaphor Theory (Lakoff and Johnson, 1980), by now a widely adopted and empirically grounded approach (e.g. Gibbs, 2008) that has opened up a interdisciplinary field of research, not least with involvement from computational linguistics.  Analysing metaphorical language use from a (cognitive) anthropologic and psycholinguistic point of view has various possible applications: The approach qualifies for research questions from the fields of critical discourse analysis, media studies, and philosophy, as it sheds light on a collective subconscious, encompassing ideological subtexts, and maybe even pre-discursive existential territories (Guattari, 2008) as traced out in late 20th century philosophy. Another area of application is text classification in literary studies: Found metaphorical expressions and conceptual mappings can be used as features to describe the relative similarity of observed texts and thus lend themselves to genre identification and authorship attribution (Lodge, 1988).  The research described here takes up this theoretical framework and builds upon a computational metaphor identification and aggregation approach as proposed by Shutova and Sun (2013). Unsupervised machine learning, namely a hierarchical soft clustering strategy known as Hierarchical Graph Factorization Clustering (HGFC), is employed to build up a graph of concepts that reflects aggregate metaphorical mappings. Using conceptual metaphor as a unit of observation allows for a sensible aggregation and tracing of surface metaphorical expressions in large scale corpora, and in this case is also used to follow diachronic developments in a corpus of historical German literature. Furthermore, as a correlate of cognitive processes it should provide an empirically grounded access to the conceptual systems, e.g. cultural and moral models, of examined texts and their times.  The main idea of the approach is to cluster nouns - which are taken to be concepts - according to their selectional preferences, that is, \"the tendency for a word to semantically select or constrain which other words may appear in a direct syntactic relation with it\" (Roberts and Egg, 2014). In the resulting clustering, figurative language use becomes visible as violation of the most frequent selectional preferences representing the normal, non-figurative case. It is an approach that determines the metaphorical in relation to the normal, which also entails that a sufficient amount of non-metaphorical language use needs to be present in the data. In the case of a diachronic corpus of literature that means to balance the corpus using historical dictionaries and encyclopaedias in order to introduce more prosaic language use.  The dataset is drawn from a large text collection (The Digital Library, 2016) and contains up to 1700 German novels from the early 16th up to the beginning of the 20th century. Preprocessing consists of POS-tagging, lemmatization, and dependency parsing, allowing for an extraction of nouns and their corresponding verbs according to certain grammatical relations - subject, direct object, and indirect object relations. Verbal constructions are only one type of realization, but they do cover a significant part of metaphors usually encountered in the wild. Furthermore, it should be straightforward to generalize the approach in order to include adjectival constructions and similes, which would allow to cover most of the possible metaphorical expressions. Preprocessing is performed using a modular pipeline (Jannidis et al., forthcoming), tailored to the processing of book-length German texts. Subsequently, a number of most frequent nouns (e.g. 2000) and corresponding verbs are extracted. The verbs act as features for the concept clustering and can come from various sources, not necessarily the same corpus as the most frequent nouns. This could be used as a way to introduce balancing text types into the model, without altering the concept graph as derived from the literary corpus. The resulting noun-verb feature matrix is then normalized for each noun vector to sum to 1 and the Jensen-Shannon divergence between pairs of noun vectors is used as a measure to calculate the similarity matrix (the initial concept graph).  With the similarity matrix in place, clustering methods can be applied in order to generate a suitable tree of concepts. Different approaches were tested at this point (using implementations from Python machine learning library scikit-learn, cf. Pedregosa et al., 2011): 1) connectivity-based or agglomerative clustering, which includes average, complete, and - the baseline from Shutova and Sun (2013) - Ward linkage 2) density-based clustering, namely DBSCAN and HDBSCAN, and 3) for subspace-based methods, spectral clustering, as well as spectral bi- and co-clustering. Results where manually reviewed and an internal evaluation measure, the silhouette coefficient, was used to assess the quality of generated clusterings. Results indicate that in this setting, spectral clustering performs very similar to the baseline, while the other methods produce clusterings of inferior quality. This exploration of readily available methods shed some light onto the requirements for unsupervised metaphor identification and aggregation. In addition, tests with balancing and pruning were performed on smaller development corpora: Solely using encyclopedias produces a model that contains mostly synonym and antonym relations but no metaphorical mappings. Similarly, models consisting only of literary texts can lack non-figurative uses for concepts. What can also be observed is that the balancing leads to deeper models, e.g. concepts accumulate more features and aggregate better.  To give an intuition, example clusters from the baseline results on a subset of 383 novels are reproduced here, showing the top ten features for each concept:  IDEAS ARE FOOD education / bildung (10): geben-dobj beanspruchen-dobj taxieren-dobj voraneilen-subj überstrahlt-dobj ausspräch-subj nahestehen-subj heraustreiben-dobj ermangelnd-dobj abschöpfen-dobj  memory / erinnerung (48): geben-dobj wachzurufen-dobj stören-dobj mahnen-dobj aufgrischen-dobj verlöschen-subj wiederzuerwecken-dobj neubeleben-dobj frischen-dobj hervorschießen-subj  hunger / hunger (10): geben-dobj erweren-dobj büssen-dobj schaben-subj überhen-iobj verschmachten-dobj stärkern-dobj bittern-subj hinausgetreiben-subj trainieren-iobj  EMOTIONS ARE PLANTS flower / blume (47): pflücken-dobj liegen-subj lieben-dobj begießen-dobj welken-dobj duften-dobj duftet-subj durchhauten-subj hingesenken-subj erblüht-dobj  emotion / gefühl (90): liegen-subj ersticken-dobj abstumpfen-dobj hervorraufen-dobj halten-iobj entspinnen-dobj hinausdehnen-dobj aufwekken-dobj anhielen-subj arten-subj  In principle, cluster labels are manually assigned using categories from Lakoff’s master metaphor list (Lakoff et al., 1991). Such is the case with the first example, IDEAS ARE FOOD, while the second one, EMOTIONS ARE PLANTS, is not present in the list and was created to appropriately describe the cluster.  Pending work includes testing HGFC and providing means to include metadata for modeling the diachronicity of the data. Considering the time span covered by the corpus, some orthographic and lexical variation will have to be handled, either by use of a specialized spelling normalization system or a more rigoros treatment such as stemming. It can be noted that HGFC combines some of the characteristics exhibited by the surveyed approaches and running it on the full size corpus will significantly improve on the baseline in terms of the amount of metaphorical expressions and conceptual mappings induced. The system will be evaluated using either a small gold standard of annotated sample sentences or manually compiled conceptual mappings in a confined domain (e.g. using Lakoff et al., 1991), which should give some indication of its precision in the domain of historical German literary texts. ",
       "article_title":"Metaphor Mining in Historical German Novels: Using Unsupervised Learning to Uncover Conceptual Systems in Literature.",
       "authors":[
          {
             "given":"Stefan",
             "family":"Pernes",
             "affiliation":[
                {
                   "original_name":"University of Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "semantic analysis",
          "anthropology",
          "literary studies",
          "natural language processing",
          "corpora and corpus activities",
          "german studies",
          "data mining / text mining",
          "philosophy",
          "linguistics",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction A digital edition of Fernando Pessoa, considered one of the most significant Modernist poets and writers, is being established through a collaboration between scholars from the Institute of Literature and Tradition (IELT) of the New University of Lisbon and the Cologne Center for eHumanities (CCeH) of the University of Cologne. The edition focuses on the contrast between the potential character of Pessoa’s numerous lists of editorial projects and his few actual publications in lifetime. Though the digital platform is primarily designed to support the research aims of the project, the procedures can be reused by others (encoding, scripts). From a theoretical standpoint, the question of „work genesis“ (on a macro and micro level) is relevant to a wider audience. In the case of Pessoa, the publication of the editorial lists and plans together with the published works in lifetime contributes to the discussion on the genesis and status of each stage of a work.   State of Research and Aims It has been much debated by literary scholars whether Fernando Pessoa’s work should be characterized as fragmentary or unitary (see Coelho, 1949; Gusmão, 2003; Martins, 2003; Sepúlveda, 2013). Due to the low number of publications in lifetime, the existence of a vast literary archive and the use of many fictional authors, named “heteronyms” by the author, the coherence of his work has been questioned. The present edition, in contrast, is based on the assumption that Fernando Pessoa constantly worked on the organization of his work, as witnessed by the many editorial lists, notes and plans which are part of his estate (Cunha, 1987; Sepúlveda, 2013). These papers are the subject of the digital edition at hand, together with the 60 publications of poems that Pessoa realized in journals and literary magazines. That way, the genesis and evolution of his work as planned by himself can be studied and the published texts be examined in the light of the editorial projects preceding, accompanying and following them during the author’s life. A significant part of the documents witnessing the editorial activity of the author have not been published before. For those which have been published, past editions in bookform define themselves by following either the first or the last version of a text, or by choosing from the existent variants from a hermeneutical standpoint (Duarte, 1988; Galhoz, 1993; Martins, 2011; Castro, 2013). As the textual variants and hesitations are critical to understand shifts in the conception of the work, e. g. when work titles are assigned to heteronyms or the composition of a planned publication is changed, the digital edition aims at offering several coexistent forms of transcription. To facilitate the understanding of the texts and the work they trace, editorial comments are to be added to the documents. In terms of editorial methodology, the digital edition targets at exploring the possibilities to combine different editorial procedures from a documentary, a diplomatic, a genetic and an enriched edition, overcoming previous oppositions between different editorial approaches to the poet’s work. Relations between the editorial projects and the published texts are formalized via the encoding of “work references”. By extending the concept of work to include projections which the author makes on a macro level, i.e. outlining his works in terms of titles, authorship, structure and publication organs, the present edition contributes to the debate about the status of the work and its place in the digital edition (Robinson, 2013; Sahle, 2013).   Methods and Results The realization of the digital edition relies primarily on procedures and technologies coming from the X universe. Transcriptions of the documents and metadata are encoded according to the standard of the Text Encoding Initiative (TEI-P5). The TEI documents are stored in an XML database (eXist) hosted on a web server together with the digital facsimiles. While the work on the documents is going on, a GitHub repository reachable at < > is used to hold successive versions of both the TEI documents and the application which is developed. Scripts written in XQuery and XSLT control the transformation of the documents’ underlying representation into the presentation layer. Some of the components shipped with eXist are used: Bootstrap, JQuery and eXist’s Templating Framework plus a SIMILE timeline and the OpenSeadragon image viewer.     Figure 1: Technical setup and workflow   The TEI encoding is controlled with a RelaxNG schema based on an ODD file. The recorded metadata include bibliographic details, the date or period of the document’s genesis or publication and a genre classification (editorial notes, editorial lists, editorial plans, poetry). Among the encoded textual phenomena are additions, deletions, substitutions, omissions, variants and notes. First and last variants are especially marked to allow for the establishment of a first and last version of the text. Moreover, the following types of entities are labeled: persons, journals, texts and works. Special attention is paid to the four principal authorial figures of Pessoa: Alberto Caeiro, Álvaro de Campos, Fernando Pessoa and Ricardo Reis. Besides registering their mere occurrence, the role they have is determined (they can be mentioned as author, editor, translator or topic of a work). Occurring persons, journals and works are managed in a central knowledge base which serves as the basis for their identification in the documents and the creation of comprehensive indexes. The following figure illustrates how the relationship between an editorial list and a publication is established via the reference to works:    Figure 2: Example of the encoding of work references in TEI   The document MN909 is an editorial list. One item of the list is a mention of the work “Cancioneiro” or “Itinerario”. In the central work list, those titles are assigned to the poems of Fernando Pessoa. One of the poems is “Abdicação” which in turn is represented by a publication carrying that title. Thus, the relationship between the editorial list and the publication is made explicit through the encoding and can be exploited e.g. in the edition’s presentation. Work on the digital edition has been going on since October 2014. It is planned to launch the edition within 2016. Until then, more documents will be uploaded and work on the presentation will be finalized. Results so far can be viewed at < >.  The presentation is multilingual (Portuguese and English). Access to the material is facilitated by different browsing options: by author, documents, publications, works, genre, and chronology. A simple and an advanced search enable the user to make specific and custom requests. As a first analytic approach to the question of the evolution of Pessoa’s work during lifetime, an interactive timeline visualising the chronology of editorial projects and publications has been created.    Figure 3: Timeline of Pessoa’s editorial projects and publications <>           The publications and documents are presented in a synoptic view, juxtaposing the transcription on the left and the facsimile on the right side. The presentation of the documents containing the editorial lists, notes and plans is enhanced by offering the possibility to switch between the diplomatic transcription, the critically established text in the first and last version and a so-called “customized version” where the user can combine transcription features (e. g. if original line breaks should be maintained or omitted). Additionally, the synoptic view can be changed to transcription-index mode, where occurring persons, texts and journals are shown alongside the transcription of the document and links to the central indexes are established. The following figures illustrate the above mentioned features:    Figure 4: Synopsis of editorial list and facsimile <>      Figure 5: Synopsis of editorial list and index references <>   For each of the four principal authorial figures, the relationships between “works”, “publications” and “documents” are presented on an individual page (see figure 6). In the example, the works of Ricardo Reis are listed starting with attested alternative titles of the whole work (“Odes de Ricardo Reis”, “Odes”, “Livro de Odes”), followed by the titles of individual works. Links to the documents show where the works are mentioned in the editorial projects and links to publications indicate where a work has been published by Pessoa.    Figure 6: Work list for Ricardo Reis <>     Conclusion The digital edition of Fernando Pessoa shows that the author was continuously concerned with the organisation of his work and that he persistently planned its publication, even publishing little in lifetime. This supports the hypothesis that his work does have a certain unity, revealing a consistent development and not a fragmentary purpose, as suggested by a large number of critics. In the edition, this understanding is underpinned by the encoding of micro-genetic textual phenomena in a basic way to elucidate shifts in the conception of work titles, authorship and publication plans in single documents. Developments of the editorial projects on the macro level are traced by encoding mentioned entities in the documents and thereby establishing links between documents, publications and abstract works. Different editorial procedures are combined to explore the potential of a digital edition which proves fruitful to bring the understanding and editorial presentation of Pessoa’s work further than existing print editions, as well as contributing to clarify a major critical topic concerning the author’s envisioned unity of his work.  ",
       "article_title":" Tracing the Genesis of Pessoa's Envisioned Work: a Digital Edition of his Editorial Projects and Publications  ",
       "authors":[
          {
             "given":"Ulrike",
             "family":"Henny",
             "affiliation":[
                {
                   "original_name":"Universität Würzburg, Germany",
                   "normalized_name":null,
                   "country":"Germany",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Pedro",
             "family":"Sepúlveda",
             "affiliation":[
                {
                   "original_name":"Universidade Nova de Lisboa, Portugal",
                   "normalized_name":"Universidade Nova de Lisboa",
                   "country":"Portugal",
                   "identifiers":{
                      "ror":"https://ror.org/02xankh89",
                      "GRID":"grid.10772.33"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "literary studies",
          "linking and annotation",
          "xml",
          "encoding - theory and practice",
          "scholarly editing",
          "English",
          "bibliographic methods / textual studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Today, cultural heritage sites, museums and other places of historical or societal value can often be visited on the Internet. Panoramic images and virtual tours allow the user to access distant sites from home via handheld devices as well as conventional desktop devices. In this way, these applications strongly reduce the threshold for getting acquainted with various cultures, their respective artefacts and unique heritage. But can this popular and usually touristic way of presentation be used to introduce valid scientific information to a broad public? This question has been posed at the Academy of Sciences and Literature | Mainz regarding its project \"Die Deutschen Inschriften\".   The research project “Die Deutschen Inschriften” The long term research project “Die Deutschen Inschriften” is a joint undertaking of six German Academies of Sciences and the Austrian Academy of Sciences. The research focuses on collecting, editing and interpreting medieval and early modern Latin and German inscriptions. They often occur in conjunction with figurative elements or spatial as well as architectural features. The inscriptions themselves are mostly in medieval Latin or in historical or regional varieties of the German language. The geographical area of research consists of Germany, Austria and South Tyrol. The inscription records range from approximately 500 AD to 1650 AD (Brandi, 1937; Kloos, 1973; Nikitsch, 2008). The project’s scholars carry out their research within a wide scope of interests ranging from art history, philology and linguistics to the history of ideas. The research results are published in 90 volumes. More than 43 of these volumes, including over 17.000 records, are currently accessible through the online database “Deutsche Inschriften Online” (German Inscriptions Online, www.inschriften.net).   Virtual cultural heritage Observing an item within a cultural heritage site in isolation frequently limits the understanding of it. This is due to its removal from the big picture of the entire ensemble in its historical, cultural and spatial context. Two different approaches of representing historical sources in their spatial context are being explored by the projects “Inschriften im Bezugssystem des Raumes” (Inscriptions in their Spatial Context, IBR) and the virtual tours through St. Stephan in Mainz and St. Michael in Hildesheim. Project IBR utilised methods of laser scanning and semantic web technologies, in this regard aiming at a more specialised target audience. The virtual tours of St. Stephan and St. Michael on the other hand were developed as a means to visualise the spatial cultural sphere for an audience with a lower degree of specialized knowledge. In doing so the applications were generally aiming at a broader audience (Lange/Unold 2015; www.spatialhumanities.de/ibr/startseite.html; www.inschriften.net/hildesheim/ rundgang.html) The virtual tour’s objective was to arrange the scientific edition’s epigraphical items in their spatial context and to put the scientific sources on display to a diverse audience in an easy accessible and comprehensible manner. In Hildesheim, the interrelation between the church and bishop at the time of construction, Bernward of Hildesheim, is shown and emphasised through the inscriptions and their placement. (Kruse, 2012). So, the visualisation of this connection and the importance of inscriptions as biographical evidence as well as general historic sources are a further aim.   Generic virtual tours Several software applications for creating virtual tours already exist that enable everyone to build virtual tours. The software offers the modelling of the views, their arrangement within a tour, the navigation, interactive elements as well as the display on a map. Images, short text and links can be added to describe selected objects. The lack of possibilities to import and embed detailed textual information and to load them from external repositories, limits the benefits in a scientific context. Thus, a software for generic virtual tours was developed to integrate and provide a generic approach for adding content from external repositories (databases, text documents, etc.). Taking the tour to St. Michael in Hildesheim as an example for the possibilities: the virtual tour enables the user to navigate within the church accessing several fixed viewpoints, to look around and to zoom in on interesting looking spots. Interactive elements indicate the possibility of switching to a different viewpoint, provide information about specific points of interest, most of them inscriptions on stone tablets or tomb slabs or other (art-)historic artefacts. Pop-up windows can contain general information about the inscriptions, a full transcription, as well as a translation of the inscribed texts. Furthermore, images of the inscription, historical sketches, or old photographs as well as audio information can be provided. All this information is displayed in the context of the tour, without exiting the panoramic visualisation. Links to the critical edition of the inscriptions in “Deutsche Inschriften Online” provide easy access to the full scholarly content including scientific apparatus and further bibliographic introductory material. The software allows for subdividing the information about the epigraphical and pictorial agenda utilized throughout the church into multiple sub-tours. Each tour is therefore enabled to concentrate on a specific topic and by this means to weave a unique narrative of the site and its cultural sphere. Furthermore, various preservation stages and relocations of objects and decorations can be pointed out. The conception of such thematic tours prevents the user from \"getting lost\" in the virtual environment and from overlooking the content's contextual conclusions and messages. Throughout the tour the viewer receives information in structured order, e.g. the events are sorted chronologically as is the rule in the context of historical information (Rizvic, 2014; Tan/Rahman, 2009).   Cultural coding = generic coding Creating digital code in the public sector is by necessity an open source process. In contrast to coding in the realm of the competitive private sector, in the cultural realm it is not—and must not be—imperative to shield digital products, project data and know how. Open source software enables people from outside the project to reuse and adapt the outcome as well as to contribute. A generic approach increases the reusability of the code and the application. Hence the virtual tour was designed as browser-based and implemented as a generic JavaScript application using HTML5, WebGL and the Three.js framework. (Neovesky/Peinelt, 2015). An encapsulated package was released that enables others to build their own virtual tour without programming knowledge. A simple JSON file is used for configuration and data exchange. Adaption of this core JSON file makes it possible to create a custom virtual tour. All the cultural institution has to add are suitable images, positions and texts which can for example be received via data interfaces and web APIs. The generic virtual tour and the user manual are available on GitHub (https://github.com/digicademy/virtualTour).   Conclusion The virtual tour to the inscriptions of St. Michael’s Church in Hildesheim was born from the idea of combining a popular visual representation with unrestricted access to scientific research and publications. By means of using a generic approach to the technical implementation, the software is not limited to any specific set of circumstances and does not represent a single-use scenario. Although the application was developed with a specific object in mind, it can easily be adapted to fit other projects within the fields of research, education and visualisation. Even though the application focuses upon a broad audience, the visualisation can also be of interest to researchers from diverse academic background and disciplines. In this manner scholars are able to receive an impression of a distant site’s or compound’s spatial context. They are as well enabled to present their research in an easy to use, appealing and low-threshold way. The distribution under a free license (GNU GPL V3) enables users to use, adapt and extend the software.  ",
       "article_title":" Visualising Cultural Spheres – Virtual Tours and Epigraphical Data ",
       "authors":[
          {
             "given":"Anna",
             "family":"Neovesky",
             "affiliation":[
                {
                   "original_name":"Academy of Sciences and Literature | Mainz, Germany",
                   "normalized_name":"Academy of Sciences and Literature",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01kdxra28",
                      "GRID":"grid.461597.8"
                   }
                }
             ]
          },
          {
             "given":"Max",
             "family":"Grüntgens",
             "affiliation":[
                {
                   "original_name":"Academy of Sciences and Literature | Mainz, Germany",
                   "normalized_name":"Academy of Sciences and Literature",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01kdxra28",
                      "GRID":"grid.461597.8"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "spatio-temporal modeling, analysis and visualisation",
          "mobile applications and mobile design",
          "software design and development",
          "virtual and augmented reality",
          "programming",
          "English",
          "audio, video, multimedia",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Many pre-print texts emerge from an authorial context that was collective rather than individual. Pre-print texts may be the result of an accretive process, where portions of the work were added or subtracted to conform to the needs of copyist or audience, and where the surviving versions can reflect the contribution of many different minds (Fisher, 2012). A truly sensitive translator of a text from the past must take into account the historical setting of the written product, what has been called the social logic of the text (Spiegel, 1990) to interpret the words for the reader and convey the meaning for those who wrote them and for their contemporary audiences (Paul, 2008). Just so, the job of the translator has become increasingly complex as our notion of text also expands. Ideally, the process of translating pre-modern texts would mirror their progressive creation, incorporating multiple voices and layers of interpretation. With the integration of digital modes of thinking and acting into our daily lives, what some digital theorists have called “eversion” (Jones, 2014), new textual communities in some way mimic those that participated in the collective process of pre-print text creation. Digital tools have made it possible to assemble voices in a virtual community of textual reception, one in which participants need not be in the same physical space to exchange ideas in real time about the meaning of a text.  Scholars at Fordham University have embarked upon two different translation projects that are sensitive to elements of pre-print textual creation and that aim to forefront the process in ways understood by digitally-inflected communities. The projects emerge from a pragmatic vision of work-sharing, yet simultaneously co-opt our increasing comfort with virtual communities to recreate the collective context of pre-print texts. These projects come from unrelated disciplines and cultural contexts: the first brings together scholars with linguistic, literary, and art historical expertise to produce a new translation of the Codex Aubin, a manuscript written in Nahuatl created between 1576 and 1609 by indigenous scribes in Mexico City; and the second, led by the French of Outremer Legal Texts Working Group, brings together nine scholars to examine and translate three thirteenth-century legal texts written in a type of Old French from the Holy Land. Although digital tool-builders have recognized the power of the collective translation process from the standpoint of a division of labor, most translation platforms are designed to manage work-flow, not to accommodate incertitude, discussion, or to express a plurality of interpretations (Gambier, 2014). Using the norms of crowdsourcing as a point of departure, Fordham scholars have worked with program-developer Ben Brumfield to substantially extend his open-source digital edition tool FromThePage (http://www.fromthepage.org/) to allow groups of scholars to translate selected texts in a communal fashion. For example, since the 1841 editions of the French-language legal texts were digitized by the Internet Archive, the existing FromThePage-Internet Archive integration was extended to ingest the OCR produced by the digitization process as a starting point for page transcripts (Beugnot, 1841). The translation process itself required modifying the data model to support parallel texts for each transcribed page, as well as new user interfaces for editors to toggle between transcript and facsimile while translating.  Since users may be unfamiliar with Old French or Nahuatl, the presentation interface was revised to support a translation-first experience. Translation teams working on both Fordham FromThePage projects include language specialists, historians, and content experts who bring their proficiencies to the discussion, but who will nonetheless disagree along the way. The assembled communities are closed and highly selective; our end product will be a curated conversation that reflects the specialization of each team member. Using FromThePage gives us the option to make the deliberations transparent to our end-users in a way that remains epistemologically true by mapping the process of consensus necessary for a final translation. Reaching an agreement on our final versions, for example, may be the result of one scholar arguing for a reading according to his or her own expertise, or of a collective ignorance and the need to choose the most neutral term possible; our final translations will expose this decision-making process and situate our users in the midst of communal textual production. Achieving this transparency required a number of unanticipated changes. The first concerned the commenting mechanism: while editors transcribing a manuscript often need to discuss unclear handwriting, translators’ discussions of variant readings need to be surfaced for readers.  In addition, parallel texts required a total re-write of the full-text search mechanism to handle verbatim text of the original, editorial emendations, and English translations. Finally, the nature of the sources revealed fundamental challenges in the representation of text within FromThePage; since the page division of the Assises is an artifact of 19th-century typesetting, semantic divisions had to be de-coupled from the facsimile pages for online readers and consumers of the exported TEI-XML documents. All of these enhancements were re-incorporated into the FromThePage source code, allowing other digital edition projects to benefit from the new translation and OCR correction features. The projects underscore the communally-crafted nature of our chosen texts by marking conflicting opinions, and the results will serve as a test-case for other kinds of collaborative textual projects, particularly those that contain non-standard languages or terminologies. ",
       "article_title":"Collaborative Translation Using FromThePage",
       "authors":[
          {
             "given":"Laura",
             "family":"Morreale",
             "affiliation":[
                {
                   "original_name":"Fordham University, United States of America",
                   "normalized_name":"Fordham University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03qnxaf80",
                      "GRID":"grid.256023.0"
                   }
                }
             ]
          },
          {
             "given":"Barbara",
             "family":"Mundy",
             "affiliation":[
                {
                   "original_name":"Fordham University, United States of America",
                   "normalized_name":"Fordham University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03qnxaf80",
                      "GRID":"grid.256023.0"
                   }
                }
             ]
          },
          {
             "given":"Thomas",
             "family":"O'Donnell",
             "affiliation":[
                {
                   "original_name":"Fordham University, United States of America",
                   "normalized_name":"Fordham University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03qnxaf80",
                      "GRID":"grid.256023.0"
                   }
                }
             ]
          },
          {
             "given":"Nicholas",
             "family":"Paul",
             "affiliation":[
                {
                   "original_name":"Fordham University, United States of America",
                   "normalized_name":"Fordham University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03qnxaf80",
                      "GRID":"grid.256023.0"
                   }
                }
             ]
          },
          {
             "given":"Brian",
             "family":"Reilly",
             "affiliation":[
                {
                   "original_name":"Fordham University, United States of America",
                   "normalized_name":"Fordham University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03qnxaf80",
                      "GRID":"grid.256023.0"
                   }
                }
             ]
          },
          {
             "given":"Ben",
             "family":"Brumfield",
             "affiliation":[
                {
                   "original_name":"FromThePage",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "medieval studies",
          "french studies",
          "digital humanities - multilinguality",
          "project design, organization, management",
          "scholarly editing",
          "translation studies",
          "English",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Distributed version control technologies, the most popular protocols of which are git, subversion, and mercurial, have long been popular among computer programmers for their abilities to track changes in a codebase and foster collaboration among coders. When combined with code management platforms such as GitHub, Bitbucket, or GitLab, they become even more powerful, enabling sophisticated bug tracking, project planning, and open-source code publication. Although these technologies have not yet been in widespread use in the humanities, their potential for use with corpus creation and textual editing is far-reaching. This paper describes Git-Lit, an open-source, community-centered initiative to parse, version control, and publish to GitHub roughly 50,000 scanned public-domain books from the British Library, thereby facilitating decentralized, open-access, and democratic scholarly editing. The Git-Lit initiative addresses these problems:   Electronic texts are difficult to edit. Traditional text repositories like Project Gutenberg and the Oxford Text Archive maintain central, canonical versions of their texts that, in most cases, are virtually immutable. If a reader spots an OCR error in an ebook, he or she must rely on contacting the publisher to propose a correction. Even with an infrastructure such as Project Gutenberg’s Distributed Proofreaders, the process of releasing a corrected edition may take months or years. Git-Lit aims to radically streamline the improvement of an electronic text in two ways. First, ease of editing is achieved through GitHub's push-button forking (making a copy of a repository in one's user account) and in-browser editing---a reader may spot a mistake, correct it, and submit a pull request for the change in mere seconds, all without leaving the browser. Second, the decentralized model ensures that no single text may be considered unquestionably canonical, although  de facto canonicity might be democratically achieved through repository voting mechanisms such as GitHub’s stars.   Electronic texts often lack editorial history. Owing, in some cases, to the age of an electronic text, its editorial provenance is often lost. Many Project Gutenberg editions, for instance, are transcribed from unknown print editions, and the history of their revisions is similarly opaque. Version control mitigates these problems by recording every edit, editor, and edition in the history of the text. When two editions diverge, git provides sophisticated tools for analyzing the differences between these editions. Sites like GitHub further provide graphical network charts, showing the genealogy of each version. Since contributions to a given text are logged according to individual contributor, credit for a given edition may be assigned according to the individual’s percentage of total contributions, minimizing the danger, for instance, that a professor may take credit for his or her graduate student's work.   Textual corpora are difficult to assemble. With some exceptions, notably the download function of the NLTK corpus module, downloading a text corpus involves compiling texts from diverse and heterogeneous sources. A would-be text analyst must click through a sequence of web pages to find the corpus he or she wants, and then either download a number of .zip files, or email the corpus creator to request a copy. With multiple texts, this can be a labor-intensive process that is not easily scriptable or automated. Git provides an easy way to solve these problems: by making texts available through the git protocol on GitHub, anyone that wishes to download a text corpus can simply run the command git clone followed by the repository URL. Parent repositories can then be assembled for collections of texts using git submodules. A parent corpus repository might be created for nineteenth-century  Bildungsromane, for instance, and that repository would contain pointers to individual text repositories. These categories would not necessarily be mutually exclusive, and would allow for arbitrary curation of custom corpora. This provides a major advantage over the traditional directory structure model, where the existence of overlapping datasets necessitates the storage and maintenance of redundant data.   ALTO XML is not comfortably human-readable. ALTO XML, the OCR output format used by the British Library texts, as well as texts created by the Library of Congress, is extremely verbose. It encodes the location of each word on the page, and often gives the OCR certainty for each word. While this format is useful for archival purposes, plain text editions are more useful for reading and for most brands of computational text analysis. Git-Lit parses the British Library’s ALTO XML, and creates markdown versions of each text that are easily edited. Since markdown is readily converted to other document formats using tools such as Pandoc, this allows each text to be easily exported to PDF, EPUB, LaTeX, Docbook, and others. Additionally, Git-Lit is currently working on a system that leverages GitHub's built-in Jekyll HTML compilers to convert each text into a web page hosted on github.io, effectively creating 50,000 readable web editions. These new editions will exist as git branches in parallel with the markdown and ALTO XML editions. Since git maintains efficient copies of every historical version of the text, no information about the text is lost in these conversions. Anyone that wishes to improve the conversion script and create newer, better editions of the original files may freely do so by branching the text from an earlier git commit.   Git-Lit software works by first parsing the XML metadata included with each text. This metadata is used to programmatically generate a repository name and a README.md file that describes the text, a document which GitHub will automatically render into a web page at the repository root. This file, along with standard CONTRIBUTING and LICENSE files, is then committed to local git repositories, initiating version control of the texts. The resulting local repository is then uploaded to GitHub via Python bindings to the GitHub API. Parent repositories are then created using git submodules for each collection of texts based on the their associated Library of Congress subjects. This enables a text analyst interested in 19th century poetry, for instance, to download all of the British Library’s released works in this genre simply by running git clone https://github.com/git-lit/19th-century-poetry.git && git submodule update --init --recursive. Since British Library texts are not the only ones being published to git-based platforms like GitHub---notable version-controlled corpora on GitHub include texts from the Text Creation Partnership and the early modern corpus Shakespeare His Contemporaries---git provides a common protocol for sharing, modifying, and distributing texts and textual corpora. Anyone may aggregate these corpora into parent repositories using git submodules. The Git-Lit project will soon launch a web application that will routinely scrape GitHub and other open repository sites for any textual corpus, thereby automating the process of discovering and indexing available corpora. This mechanism will also serve to democratize the curation of corpora, since the corpus index will be sorted by the number of GitHub “stars”, or votes, a repository has engendered from the community. The 50,000 British Library texts processed by Git-Lit, as well as many of the other open corpora described here, are currently being integrated into DHBox, the cloud-based Digital Humanities software suite. Soon, these corpora and many others will be available for download by selecting them from a web-based interface, where they will then be available for analysis using pre-installed versions of the Python NLTK, R, and other textual analytic tools. This paper discusses how Git-Lit’s methods might be used by other digital humanities projects involved in the creation or analysis of large text corpora, and how digital humanists may contribute to the Git-Lit project. (As an open-source project, Git-Lit welcomes contributions in the form of bug reports, feature requests, or code.) The paper also discusses some of the storage and computation limitations of electronically publishing texts via code repositories, and some of the technical problems encountered by the Git-Lit project. Finally, it suggests pedagogical uses of git-based collaborative digital editing, such as classroom compilation of anthologies or digital scholarly editions. The applications of these technologies are wide-ranging, and are neither proprietary to this project nor to services such as GitHub, but remain concepts of openness and collaboration with powerful implications for the digital humanities. ",
       "article_title":" Git-Lit: an Application of Distributed Version Control Technology toward the Creation of 50,000 Digital Scholarly Editions  ",
       "authors":[
          {
             "given":"Jonathan",
             "family":"Reeve",
             "affiliation":[
                {
                   "original_name":"Columbia University, United States of America",
                   "normalized_name":"Columbia University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00hj8s172",
                      "GRID":"grid.21729.3f"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "literary studies",
          "standards and interoperability",
          "corpora and corpus activities",
          "digitisation, resource creation, and discovery",
          "scholarly editing",
          "data mining / text mining",
          "archives, repositories, sustainability and preservation",
          "English",
          "publishing and delivery systems",
          "crowdsourcing",
          "copyright, licensing, and Open Access",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction: Digital musicology Computer-based methods in musicology have been around at least since the 1980s   The popular series „Computing in Musicology“ started around 1985. For an overview of all volumes of the series cf. http://www.ccarh.org/publications/books/cm/; Note: All URLs mentioned in this text were last checked on March 3, 2016. . Besides the creation of digital editions (cf. Kepper et al., 2014; Veit, 2015), scholars in this area of study have also been interested in quantitative approaches for musicological analyses (cf. Müllensiefen and Frieler, 2004; Vigilanti, 2007). Such quantitative analyses rely on music information retrieval (MIR) systems, which can be used to search collections of songs according to different musicological parameters. There are many examples for existing MIR systems, all with specific strengths and weaknesses. Among the main downsides of such systems are:    Usability problems, i.e. tools are cumbersome to use, as they oftentimes only provide a command-line interface and also require some basic programming skills to utilize them; example: Humdrum   http://www.humdrum.org/    Restricted scope of querying, i.e. tools can only be used to search for musical incipits; examples: RISM   https://opac.rism.info/ , HymnQuest   http://hymnquest.com/    Restricted song collection, i.e. tools can only be used for specific collections of music files; various examples of MIR tools for specific collections are described in Typke et al. (2005)   A particularly promising MIR tool can be found in Peachnote   http://www.peachnote.com/  (Viro, 2011), which uses optical music recognition (OMR) software to index more than one million sheets from the Petrucci Music Library   http://imslp.org/ , aiming to provide a search interface for musicology which can be seen as an analog of the Google Books Ngram Viewer   https://books.google.com/ngrams . Despite many existing software solutions, we believe that accurate OMR is still a major challenge in digital musicology. At the same time, there are numerous databases   http://www.musicxml.com/music-in-musicxml/  at hand, that provide machine-readable music documents, fully annotated with MusicXML (Good, 2001) markup.   On this account, we designed MusicXML Analyzer, a generic MIR system that is trying to overcome the weaknesses of existing MIR tools, and that allows for the analysis of arbitrary documents encoded in MusicXML format.    MusicXML Analyzer: Basic functionality and implementation details MusicXML Analyzer can be used to analyze songs in a quantitative manner, and to search for specific melodic patterns in a collection of songs. The results of the analyses are rendered as virtual scores and can be viewed in any recent web browser. In addition, the queries and the results can be played as a synthesized audio file; all analyses can also be exported as PDF or CSV files. The tool comprises three main components: (1) the upload function, (2) the analysis function, and (3) the search function. After one or more files in MusicXML format have been uploaded via an intuitive drag-and-drop dialog, the analysis component parses the data and calculates basic frequencies; the results are stored in an SQL database and can be displayed in a dashboard view (cf. Fig. 1).    Figure 1: Snippet from the dashboard view, showing basic frequencies for a corpus of MusicXML documents.  The dashboard displays the following information, either for an individual song, or for a corpus of multiple songs:  Overall statistics for single notes, rests and measures  Types of instruments used in the song (if described in the MusicXML data) Frequency distribution for single notes, intervals, keys, note durations and meters   Via a dedicated search function, a corpus of MusicXML documents can be queried for melodic patterns on different levels of information:  Search for a sound sequence; example: c’, c’, g’, g’ Search for a rhythmic pattern; example: eighth note, eighth note, quarter note, quarter note Search for melodic patterns, i.e. a combination of sound sequence and rhythm; example: c’ / eighth note, c’ / eighth note, g’ / quarter note, g’ / quarter note  Search queries can be entered via a virtual staff that was realized with the VexFlow library   http://www.vexflow.com/  (cf. Fig. 2). Once a search pattern has been entered, it can also be played as a synthesized Midi sequence, which was realized with the Midi.js library   http://mudcu.be/midi-js .     Figure 2: Interface for entering queries to identify tonal, rhythmic, or melodic patterns in a corpus of MusicXML documents.  After a query has been submitted, all results – i.e. the songs that contain the search pattern – are displayed in a list view. The list contains the name of the song and also the number of total occurrences of the search pattern in that song. By clicking on one of the song items in the list, a virtual score is rendered for the whole song; the search pattern is highlighted whenever it occurs in that virtual score (cf. Fig. 3). The whole song can be played directly in the web browser, or downloaded for further analyses as a PDF (realized with the jsPDF library   https://parall.ax/products/jspdf ).    Figure 3: Virtual score rendering of a document from the results list; the search pattern is highlighted in red color.  MusicXML Analyzer was implemented by means of standard web technologies (HTML, CSS, JavaScript, PHP), in particular by utilizing the following libraries and frameworks: Laravel   http://laravel.com/ , jQuery   https://jquery.com/ , D3.js   http://d3js.org/ , Bootstrap   http://getbootstrap.com/ , Typed.js   http://www.mattboldt.com/demos/typed-js/ , Dropzone.js   http://www.dropzonejs.com/ .   A short demo video that showcases the main functionality of the tool is available at   https://dl.dropboxusercontent.com/u/4194636/MusicXML-Analyzer.mp4   A fully functional online demo   Due to some technical limitations of our server environment, the initial access to the online demo may take a few seconds to wake up the server from  idle mode.   of MusicXML Analyzer is available at    http://music-xml-analyzer.herokuapp.com/   MusicXML Analyzer can also be downloaded and modified (according to the MIT open source license) from GitHub:    https://github.com/freakimkaefig/Music-XML-Analyzer      Future directions In its current implementation, MusicXML Analyzer performs an exact match search, i.e. only documents which have the exact same value in their MusicXML markup will be found by the search function. We are planning to implement a more sophisticated melodic similarity algorithm (cf. Grachten et al., 2002; Miura and Shioya, 2003) that allows for the configuration of different similarity thresholds.  At the same time, we are adapting MusicXML Analyzer for a recent project on a large corpus of German folksongs. Besides monophonic melodies, this collection of folksongs also contains machine-readable metadata (region, date, etc.) and lyrics. Accordingly, we are trying to enhance the search features of MusicXML Analyzer in a way it can not only search songs for melodic patterns, but also for metadata parameters and keywords from the lyrics. Such an enhanced MIR system could be used to analyze the following research questions:  Are there characteristic melodic and linguistic patterns for German folksongs, from a diachronic perspective as well as from a regional perspective? Are there melodic-linguistic collocations, i.e. do certain melodic patterns co-occur with certain keywords or phrases?   ",
       "article_title":"Tool-based Identification of Melodic Patterns in MusicXML Documents",
       "authors":[
          {
             "given":"Manuel",
             "family":"Burghardt",
             "affiliation":[
                {
                   "original_name":"Media Informatics Group, University of Regensburg, Germany",
                   "normalized_name":"University of Regensburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01eezs655",
                      "GRID":"grid.7727.5"
                   }
                }
             ]
          },
          {
             "given":"Lukas",
             "family":"Lamm",
             "affiliation":[
                {
                   "original_name":"Media Informatics Group, University of Regensburg, Germany",
                   "normalized_name":"University of Regensburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01eezs655",
                      "GRID":"grid.7727.5"
                   }
                }
             ]
          },
          {
             "given":"David",
             "family":"Lechler",
             "affiliation":[
                {
                   "original_name":"Media Informatics Group, University of Regensburg, Germany",
                   "normalized_name":"University of Regensburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01eezs655",
                      "GRID":"grid.7727.5"
                   }
                }
             ]
          },
          {
             "given":"Matthias",
             "family":"Schneider",
             "affiliation":[
                {
                   "original_name":"Media Informatics Group, University of Regensburg, Germany",
                   "normalized_name":"University of Regensburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01eezs655",
                      "GRID":"grid.7727.5"
                   }
                }
             ]
          },
          {
             "given":"Tobias",
             "family":"Semmelmann",
             "affiliation":[
                {
                   "original_name":"Media Informatics Group, University of Regensburg, Germany",
                   "normalized_name":"University of Regensburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01eezs655",
                      "GRID":"grid.7727.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016",
       "keywords":[
          "information retrieval",
          "music",
          "software design and development",
          "xml",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The character network of a given narrative (novel, play, film, graphic novel, etc.) models the structure formed by the relations in its character-system (Woloch, 2003). A relation between two characters symbolises their co-presence in parts of the narrative; the entire set of relations between all characters constitutes a formal model of this character-system and lends itself to display and analysis. For example, Moretti (2011) used network modelling to compare the importance of protagonists from Shakespeare’s  Hamlet, while Trilcke  et al. (2015) created character networks for 465 German plays and used them to initiate a wider study of German Theatre.  Most applications of character network analysis have disregarded temporality, possibly because of its representational complexity. Consequently, all relations in the system are considered as happening at the same time: one cannot distinguish if a given edge symbolises a relation at the start, at the end, or in several parts of the work under study. Furthermore, because temporality is not being accounted for, there is usually no way of relating the network visualisation with the unfolding of the source narrative. While prototypes such as those discussed in Roberts-Smith  et al. (2013) offer sophisticate ways of dynamically visualising the text of theatre plays, they do so in a way that is unrelated to character network modelling.  Based on these observations, we set out to develop an open source web application which models the character-system of theatre plays as a sequence of network states synchronised with the actual narrative content ( https://github.com/maladesimaginaires/intnetviz). This paper proposes a high-level overview of our application , successively focusing on the underlying structure extraction process, the conception of the graphical interface, and the range of uses envisioned for it. In the conclusion, we evoke the ways in which we intend to develop it and reflect on the potential significance of this development at a more epistemological level.    Tool overview The underlying workflow has been divided into two parts. First, the text of a play is processed using  Orange Textable (, Xanthos 2014), an open source text analysis add-on for the  Orange Canvas (http://orange.biolab.si/) visual programming environment: in particular, the play is divided into its component parts (acts, scenes, lines) and the characters present in each scene are identified and associated with each line. These data are then imported into a web interface based on the open source  D3 JavaScript library (, Bostock et al., 2011), which allows the user (author, researcher, teacher, etc.) to manipulate the character network without prerequisite installation.    Structure extraction The workflow is designed to facilitate the later inclusion of new data. The play used in the initial development phase was Molière’s  L’Ecole des femmes, as found in raw text format on the Project Gutenberg website ( http://www.gutenberg.org/files/43535/43535-0.txt). Theatre plays constitute a privileged input for the automatic extraction of structural information: such information is in general explicitly encoded in these data, as illustrated by the excerpt of Molière’s play reproduced on Figure 1. It is worth noting that the extraction phase can be readily adapted to take advantage of cases where structural information has been formally encoded using TEI-XML annotation for instance, such as the “Théâtre classique” database used by Karsdorp  et al. (2015). We are currently investigating the possibility of extending our approach to a large body of plays in this way.    Figure 1. Excerpt of L’Ecole des femmes illustrating the explicit structuration of the data   Structure extraction is performed by a mostly linear chaining of segmentation and recoding operations based on regular expressions which gradually transform the raw text of the play into the tables that will be later used for controlling the web interface. Each table has the same number of rows, corresponding to the extracted lines of the play. The first table gives the text of each line (lightly annotated in XML) along with the associated character and stage directions (Table 1). The second table indicates the presence or absence of each character when each line is spoken (see Table 2).   Table 1. First rows of the table containing the extracted text and stage directions    Table 2. First rows of the table indicating the presence or absence of each character when each line is spoken (the first four columns are identical to Table 1 and have been omitted, along with the last few character columns)  Applied to  L’Ecole des femmes, the process was found to give fairly reliable results: about 98% of the lines were correctly extracted. Most errors could be fixed by determining  a priori the set of acceptable character names in a given play. An unexpected source of errors was the assumption that characters could be consistently identified by a single string; even without taking into account situations where a character is explicitly “renamed” as part of a surprise effect, linguistic processes such as the succession of indefinite and definite articles in French discourse lead to formal variations in the designation of characters.    Interface Data files are then imported into a browser and D3 is used to build an interactive visualisation of the character-system (a demo is available at  http://bit.ly/network-demo) where the state of the network is synchronised with the current line (Figure 2). Character nodes are positioned using a force-based layout. Browsing the text simultaneously updates the network, line, and position indicator (act, scene, line). At each step, the weight of edges (expressed by their width) increases with the number of co-presences between the characters up to this point. Similarly, the weight of nodes (their radius) increases with the number of lines spoken by this character.    Figure 2. User interface. [Above] Agnès is speaking in presence of Horace and Arnolphe; all characters have already intervened except Oronte and Enrique, who will appear in the final scenes. [Below] Three moments of the unfolding character network (lines 41, 359 and 536)  A character node can be in one of four states. It is:   “active” when the character is speaking. “activated” when it is present in the scene but not active. “previously activated” if it was present in the play but is not currently active nor activated. “not yet activated” if its first appearance is in a later scene.  The latter state makes it possible to offer a view of the final network state, highlighting absences as well as presences in the earlier stages.   Uses We display each line in front of the matching network state for a reason: the network does not exhaust the richness of the play. Since co-presence analysis does not consider the actual content of lines, it cannot–nor aims to–account for discursive references to characters. However, by extracting an actantial model from speech turns, the tool provides a concise and dynamic representation of the enunciative structure of the play. Paired with background knowledge about theatrical narration, such a visualisation offers new ways of reading. From a philological perspective, relating sociological variables (gender, age, social status) with structural properties of the network (dynamical statistics, centrality measures, word and line counts) helps clarifying which profiles lead the interactions, which ones merely react to them, and which ones are excluded from them, revealing potential social stigma. By expliciting not only the presence but also the absence of links between certain characters, the proposed visualisation may contribute to a visual and interactive deconstruction of the plays (Derrida, 1967). By allowing the user to browse through successive pictures of the interactions, the interface provides a unique opportunity to \"play\" the play and visualise the flow of speech between characters. We believe that this playful dimension is particularly interesting for pedagogic uses, especially when discovering a new narrative work with students. Last but not least, our method can also be applied to any text in the course of the writing process. In this context, disposing of a visualisation of existing interactions at each moment of the text may help the writer distance herself from an impressionistic representation.    Perspectives and conclusion One of the most significant benefits of a digital humanities approach is the dialectic relation created between the development of a prototype and the verification of scientific hypotheses. Discussing seemingly trivial design issues like the number of colors requested to map the network relations has frequently led us to expliciting and fruitfully questioning differences in our epistemological backgrounds. Thus we consider the following perspectives not only as potential improvements to our tool but also, more importantly, as opportunities to challenge our own theoretical preconceptions:  Further enriching the visualisation (e.g. by adding line count histograms or highlighting of the first intervention of a character) would benefit the back-and-forth movement between the content and the structure of a given play. The dynamical nature of the interface would enable us to compare the distinctive features of different plays not only in terms of the final state of character networks, but also and more importantly, of their evolution; this should help bring out \"interactional styles\" and discover  Familienähnlichkeit (Wittgenstein, 1953) by author or period.  Other kinds of texts will benefit from such a dynamical analysis: other fictional texts such as screenplays, but also linguistic transcriptions as used in conversation analysis.   By allowing the user to browse the content of a narrative and manipulate an interactive character network, our motivation is ultimately to contribute to a better integration of distant and close reading practices.  ",
       "article_title":" Visualising the Dynamics of Character Networks  ",
       "authors":[
          {
             "given":"Aris",
             "family":"Xanthos",
             "affiliation":[
                {
                   "original_name":"University of Lausanne, Switzerland",
                   "normalized_name":"University of Lausanne",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/019whta54",
                      "GRID":"grid.9851.5"
                   }
                }
             ]
          },
          {
             "given":"Isaac",
             "family":"Pante",
             "affiliation":[
                {
                   "original_name":"University of Lausanne, Switzerland",
                   "normalized_name":"University of Lausanne",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/019whta54",
                      "GRID":"grid.9851.5"
                   }
                }
             ]
          },
          {
             "given":"Yannick",
             "family":"Rochat",
             "affiliation":[
                {
                   "original_name":"Swiss Federal Institute of Technology in Lausanne, Switzerland",
                   "normalized_name":"École Polytechnique Fédérale de Lausanne",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s376052",
                      "GRID":"grid.5333.6"
                   }
                }
             ]
          },
          {
             "given":"Martin",
             "family":"Grandjean",
             "affiliation":[
                {
                   "original_name":"University of Lausanne, Switzerland",
                   "normalized_name":"University of Lausanne",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/019whta54",
                      "GRID":"grid.9851.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-28",
       "keywords":[
          "visualisation",
          "teaching and pedagogy",
          "English",
          "networks, relationships, graphs",
          "interface and user experience design",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Motivation Digital literary studies have embraced social network analysis as a powerful tool to formalize and analyze social networks in literary texts (Elson et al., 2010b, Hettinger et al., 2015). Extracting networks automatically from texts is still a challenging task with the following steps: identification of all character references (which is not identical to named entity recognition), coreference resolution (CR) and a final step defining the amount of interaction between the characters, for example by the amount of verbal exchanges or the co-occurrence in a text segment. In the following we will discuss different ways to solve this task using an annotated corpus of German novels. One of the related problems is the definition of an evaluation metric which connects the computational problem to literary concepts like “main characters” and “character constellation”. Our goal is to find the best way to capture the intuition behind these literary concepts in a formalized procedure. For this purpose we introduce a new way of evaluating automatically extracted networks. We make use of carefully created and revised summaries of German novels, provided by Kindler Literary Lexicon Online   http://kll-online.de . Besides, this work is to the best of our knowledge the first to compare different methods of creating and evaluating automatically extracted character networks.    Related Work Social Network Analysis (SNA) is a well-established discipline, e.g. in the social sciences, which literary studies can apply for the analysis of character networks (Trilcke, 2013). Approaches to automatic extraction of SNs from literary text using NLP techniques have been manifold. Most works start by identifying entities in the text and connect them via CR. Park et al. (Park et al., 2013) extract SNs based on proximity of names in the text and define a kernel function to distinguish protagonists from less important characters. Celikyilmaz et al. (Celikyilmaz et al., 2010) use an unsupervised actor-topic-model to create SNs from narratives. Elson et al. associate speakers with direct speech passages in novels (Elson et al., 2010a) and create SNs from the dialogues to validate literary hypotheses like whether the amount of dialogues is inversely proportional to the amount of characters that appear in the novel (Elson et al., 2010b). Moreover, three end-to-end systems for the extraction and visualization of SNs from English literary texts already exist: PLOS (Waumans et al., 2015) works similarly to the approach by Elson et al. by creating networks from dialogue interactions. He et al. use their own speaker identification system to detect family connections between entities (He et al., 2013). SINNET by Agarwal et al. (Agarwal et al., 2013b) finds different types of directed events in a text and creates a directed SN from these events.   Data This work is based on a corpus of 452 German novels from the TextGrid Digital Library   https://textgrid.de/digitale-bibliothek . Expert plot summaries from Kindlers Literary Lexicon Online are available for 215 of these novels. As the following experiments are partly based on direct speech, we analysed the novels with regard to the direct speech they contain. We selected 58 novels with the highest possible amount of direct speech for which there was also a summary on hand.  Those 58 novels have been split into tokens and sentences with OpenNLP   https://opennlp.apache.org/ , POS-tagged and lemmatized by the TreeTagger (Schmid, 1995), further processed by the RFTagger (Schmid and Laws, 2008) and the morphological tagger from MATE-Tools   https://code.google.com/p/mate-tools/ . Additionally, we use the dependency parser by Bohnet (Bohnet and Kuhn, 2012) to analyze the sentence structure. Named Entity Recognition is done with the tool by Jannidis et al. (Jannidis et al., 2015) and the rule-based component by Krug et al. is used for CR. The detection of the speaker and the addressee for each direct speech passage is also part of the CR (Krug et al., 2015). In the summaries from Kindler, Named Entities and Coreferences have been manually labeled by two annotators.    Methods We use four different methods to identify the most central characters in the novels and evaluate their quality by comparison with the characters occurring in the Kindler summaries. The first method relies only on the frequencies of the characters in the text: the most central characters are those appearing most often in the novel (coreferences resolved). The second methods counts only those entities that have at least once been detected as speaker or addressee of direct speech. The other methods each construct a different type of social network and make use of SNA to find the most central characters. The first network is based on co-occurrences of characters in the same window of text: an edge between two characters exists if they are mentioned in the same paragraph and the weight of the edge is the number of paragraphs in which this is the case. The second network is created using the dialogue structure of the text. For each direct speech for which both speaker and addressee could be detected, an edge is drawn between those two. Longer dialogues consequently lead to higher edge weights between the participants. Thus, both network types are undirected and weighted. Examples for networks that were created with those methods are shown in Figure 1. To identify the most central characters we use the weighted degree of each node (i.e. the sum of the weights of all edges incident to a node) in decreasing order. This metric is most intuitively interpretable with regard to the importance of characters in a fictional world. In the following paragraph, we compare the rankings with the summaries and discuss possible sources of error and their influence on the results.    Figure 1: Automatically extracted SNs for Goethes: “Die Wahlverwandtschaften”. The left picture shows the ten most connected characters when an interaction is created for a common appearance in a paragraph. The right picture shows the corresponding network when only direct speech is used as interactions.   Evaluation Evaluating automatically extracted SNs is not a trivial task and there are no established practices. Elson et al. (Elson et al., 2010b) validate literary hypotheses, (Park et al., 2013) and (Waumans et al., 2015) analyze typical distributions that they expect of literary character networks. Agarwal et al. (Agarwal et al., 2013a) evaluate a machine-generated network of  Alice in Wonderland against a manually conceived version by comparing typical SNA metrics like different centrality measures.  In this work, we want to compare the methods for identifying the most central characters as described in section 4. As a gold standard, we use the manually annotated Kindler summaries. The generated rankings for each novel, as well as the rankings from the summaries are first cleaned up so that only real names remain. Our evaluation is based on the assumption that a summary contains all important characters. Since those summaries are carefully created and even revised by experts we propose that this assumption holds. For each summary, we create a ranking of the mentioned characters by [a] the number of occurrences (gold_count from here) and [b] the order of occurrence (gold_order from here). We relax the ranking assumption and only select the top 5 (top 10) figures from the summary rankings and compare them against the top 5 (top 10) characters in the automatically obtained rankings for the novels without respecting the particular ordering. If the name of a character from the gold standard is exactly found in an automatic ranking, there is a match. Table 1 shows the resulting correspondences with the two gold rankings, averaged over all 58 novels. Table 1: Overview of the successfully matched entities between the two relaxed rankings from the summaries (gold_count, gold_order) and the generated relaxed rankings for the top 5 and the top 10 entities (DSN= Direct Speech Network; PN = Paragraph Network;DSC = Direct Speech Count; Count = simple frequency)   Algorithm DSN_5 DSN_10 PN_5 PN_10 DSC_5 DSC_10 Count_5 Count_10   gold_count 40.5% 50.2% 39.3% 51.6% 38.9% 49.0% 40.1% 52.0%   gold_order 38.6% 45.1% 41.3% 48.6% 37.5% 45.3% 41.2% 48.5%     Results and Discussion Table 1 displays first results for the identification of main characters in novels. Nevertheless, none of the methods yields very high scores for this kind of evaluation. Interestingly, the simpler approaches seem to be suited well for the task. The low values can be explained by a variety of errors which can be grouped in three categories. Firstly, a character might not be among the top 10 of the relaxed ranking from Kindler. If automatic matches to lower positions in the ranking are allowed, the score in Table 2 can be reached. Table 2: Accuracy of the matching, independent of the position in the automatic ranking   Algorithm DSN_Max PN_Max DSC_Max Count_Max   gold_count 55.1% 56.6% 53.8% 57.3%   gold_order 58.0% 64.7% 55.1% 64.7%   We can see that approximately 60% of the characters can now be matched unambiguously. The highest percentage of errors is due to incorrectly resolved coreferences. Clusters of the same character that have not been merged during the CR do not only create redundant elements in the rankings, wrongly merged clusters also mean, that one character can never be matched correctly. If coreference errors are ignored, the results are as shown in table 3. Table 3: Accuracy of the matching, independent of the position in the automatic ranking, CR errors ignored   Algorithm DSN_Maxcr PN_Maxcr DSC_Maxcr Count_Maxcr   gold_count 79.7% 81.2% 78.8% 81.2%   gold_order 58.6% 65.3% 55.6% 65.3%   The third error type of originates from different spellings of the same name which make an unambiguous matching very difficult (e.g. “Amanzéi” vs. “Amanzei”, “Lenore” vs. “Leonore”). Those kinds of errors are caused by different encodings, since the novels and the summaries originate from separate sources. Further reasons which render the matching more difficult or impossible respectively are missing or incorrectly detected Named Entities. The error analysis shows that future improvements are especially needed for the CR or procedures which avoid CR, since those have a better chance to succeed.   Conclusion In this paper we showed work in progress to extract SNs from German novels. We compared four different approaches to the identification of central characters and evaluated against manually annotated summaries. Two presented methods rely on direct speech, the other methods can be applied to any novel. At least for this task, the more challenging approaches of determining speaker and addressee of direct speech and creating networks from the resulting interactions did score slightly lower than the simpler approaches. To improve the results, future work especially needs to be invested into the creation of a less error-prone CR system.   ",
       "article_title":"Comparison of Methods for the Identification of Main Characters in German Novels",
       "authors":[
          {
             "given":"Fotis",
             "family":"Jannidis",
             "affiliation":[
                {
                   "original_name":"University of Wuerzburg, Germany",
                   "normalized_name":null,
                   "country":"Germany",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Isabella",
             "family":"Reger",
             "affiliation":[
                {
                   "original_name":"University of Wuerzburg, Germany",
                   "normalized_name":null,
                   "country":"Germany",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Markus",
             "family":"Krug",
             "affiliation":[
                {
                   "original_name":"University of Wuerzburg, Germany",
                   "normalized_name":null,
                   "country":"Germany",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Lukas",
             "family":"Weimer",
             "affiliation":[
                {
                   "original_name":"University of Wuerzburg, Germany",
                   "normalized_name":null,
                   "country":"Germany",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Luisa",
             "family":"Macharowsky",
             "affiliation":[
                {
                   "original_name":"University of Wuerzburg, Germany",
                   "normalized_name":null,
                   "country":"Germany",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Frank",
             "family":"Puppe",
             "affiliation":[
                {
                   "original_name":"University of Wuerzburg, Germany",
                   "normalized_name":null,
                   "country":"Germany",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016",
       "keywords":[
          "literary studies",
          "german studies",
          "English",
          "networks, relationships, graphs",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Current computational models of intertextuality run the risk of ignoring several desiderata: on the one hand, they mostly rely on single methods of quantifying text similarities. This includes syntagmatic models that look for shared vocabularies (unigram models) or (higher order) ( k-skip-) n-grams (Guthrie et al., 2006). Such approaches disregard the two-level process of sign constitution according to which language-related, paradigmatic relations have to be distinguished from their text-related, syntagmatic counterparts (Hjelmslev, 1969; Miller et al., 1991; Raible, 1981) where the former require language models of the sort of neural networks (Mikolov et al., 2013), topic models (Blei et al., 2007) or related approaches in the area of latent semantic analysis (cf., e.g., (Paaß et al., 2004)). On the other hand, computational models should enable scholars to revise their computations. The reason is the remarkably high error rate produced by statistical models even in cases that are supposed to be as “simple” as automatic  pre-processing. Thus, scholars need efficient means to make numerous corrections and additions to automatic computations. Otherwise, the computations will be hardly acceptable as scientific data in the humanities (Thaller, 2014).     Table 1: Nine scenarios of generating Wikiditions out of corpora of (referring) literary (hyper-)texts and their (referred) hypotexts. Examples: (1) Kafka's \"Bericht für eine Akademie\" (in the role of a hypertext) versus Hauff's \"Der Affe als Mensch\" (in the role of a hypotext); (2) Kafka's \"Bericht für eine Akademie\" versus all \"Affentexte\" (Borgards, 2012) since the end of the 18th Century (including works of, e.g., Hauff, E. T. A. Hoffmann, Flaubert etc.); (3) Kafka's \"Beim Bau der Chinesischen Mauer\" versus the \"Prager Tagblatt\" from August 1914 to March 1917; (4) Kafka's \"Oeuvre\" versus Nietzsche's \"Geburt der Tragödie aus dem Geiste der Musik\"; (5) a selection of Kafka's \"Oeuvre\" versus a selection of Nietzsche's \"Oeuvre\"; (6) Kafka's \"Oeuvre\" versus a newspaper corpus (e.g., sampled from the \"Prager Tagblatt\"); (7) the complete works of several authors versus a single hypotext (e.g., Goethe's \"Faust\"); (8) the complete works of several authors versus a corpus of \"Faust\" texts; (9) the complete works of several German authors versus the complete works of several French authors.   This paper presents  Wikidition as a  Literary Memory Information System (LiMeS) to address these desiderata. It allows for the automatic generation of online editions of text corpora. This includes literary texts in the role of (referring)  hypertexts (Genette, 1993) in relation to candidate (referred) hypotexts by exploring their  intra- and intertextual relations – see Table 1 for nine related research scenarios. In order to explore, annotate and display such relations, Wikidition computes multi-layer networks that account for the multi-resolution of linguistic relations – on the side of the hypo- and the hypertexts. The reason is that hypertextual relations (in the sense of Genette) (that occur in the form of transformations, imitations or any mixture thereof) may be manifested on the lexical, sentential or the textual level (including whole paragraphs or even larger subtexts). As a consequence, Wikidition spans lexical, sentential and textual networks that allow for browsing along the constituency relations of words in relation to sentences, sentences in relation to texts etc. In this multi-layer network model, intrarelational links (of words, sentences or texts) are represented together with interrelational links that combine units of different layers. Figure 1 shows the range of sign relations that are mapped. To this end, Wikidition combines a multitude of text similarity measures (beyond  n-grams) for automatically linking lexical, sentential and textual units regarding their (1) syntagmatic (e.g., syntactic) and (2) paradigmatic use. We call this two-level task  linkification.     Figure 1: Sign relations that are automatically explored and annotated by Wikidition (Mehler et al., 2016): on the level of words (Module (5) – paradigmatic –, (6) and (7) – both syntagmatic), on the level of sentences (Module (3) – paradigmatic – and (4) – syntagmatic) and on the level of texts (Module (1) and (2) – both paradigmatic). Wikidition additionally includes a component for wikification (i.e., for linking occurrences of concepts to articles in Wikipedia (Mihalcea et al., 2007)) and especially for automatically inducing lexica out of input corpora (i.e., for linkification). Arcs denote links explored by Wikidition; reflexive arcs denote intrarelational (i.e., purely lexical, sentential or textual) links. In this way, intra- and interrelational links are maintained by the same information system.   Beyond linkification, Wikidition contains a module for automatic  lexiconisation (see Figure 2). It extracts lexica from input corpora to map author specific vocabularies as subsets of the corresponding reference language. Input corpora (currently in English, German or Latin) are given as plain text that first are automatically preprocessed; the resulting wikiditions are mapped onto separate URLs to be accessible as self-contained wikis. By means of lexiconisation, research questions of the following sort can be addressed:  What kind of German does Franz Kafka write? (E.g., Prager Deutsch.) What terminologies does Franz Kafka use in “In der Strafkolonie”? (E.g., engineering terminology.) How does his German depart from the underlying reference language? Since texts are not necessarily monolingual (because of using citations, translations, loan words, verbal expressions etc.), the same procedure can be applied by simultaneously looking at all foreign languages being manifested in the texts under consideration (right side of Figure 2).     Figure 2: Left side: schematic depiction (red) of the vocabulary of an author (e.g., Franz Kafka) as manifested within Wikidition's input text(s) (e.g., \"In der Strafkolonie\") as mainly overlapping with the vocabulary of the corresponding reference language (e.g., German).   To this end, Wikidition distinguishes three levels of lexical resolution: superlemmas (e.g. German  Tätigkeit), lemmas (e.g.,  Thätigkeit) and syntactic words (e.g.,  Thätigkeiten ( nominative,  plural)) as featured sign-like manifestations of lemmas (lower part of the figure). Note that this model diverges from the majority of computational models to textual data which start from tokens as manifestations of wordforms (referred to as types) and which, therefore, disregard the meaning-side of lexical units. Based on linkification and lexiconisation, Wikidition does not only allow for traversing input corpora on different (lexical, sentential and textual) levels. Rather, the readers of a Wikidition can also study the vocabulary of single authors on several levels of resolution: starting from the level of superlemmas via the level of lemmas down to the level of syntactic words and wordforms (see Figure2).     Table 2: Notions of human, computer-supported and machine-based reading. Wikidition addressesmachine close reading by integrating semantic web (SW) resources and the human mind (HM) (asthe ultima ratio of interpreting its computations). T1, ..., Tm span the input corpus of m (hyper-)texts; Xn denotes the contextualizing corpus of hypotexts of size n that is explicitly consulted by the readingprocess. Machine close reading is similar to human reading in that it focuses on small, rather than bigdata.   While the linkification component of Wikidition relates to principles of WikiSource and Wikipedia, the Wiktionary project is addressed by its lexiconisation module. Wikidition uses numerous computational methods for providing interoperability and extensibility of the resulting editions according to the wiki principle. In this way, the dissemination of computer-based methods is supported even across the borders of digital humanities in that scholars are enabled to make their own exploratory analyses. However, Wikidition does not address a big data scenario in support of distant reading (Moretti, 2013), nor does it aim at emulating human reading in the sense of machine reading (Etzioni, 2007). Rather, Wikidition addresses what we call  machine close reading in that it aims at massively supporting the process of (scientific or literary) reading by means of computational methods (see Table 2).    Evaluation We exemplify Wikidition by example of three pairs of text. Regarding the layers of lemmas and sentences, Table 3 shows that Wikidition generates extremely sparse networks (whose cohesion is below 1%) of high cluster values and short average geodesic distances in conjunction with largest connected components that comprise almost all lexical and sentential nodes. In this example, we compute paradigmatic associations among words by means of word2vec (Mikolov et al., 2013) while sentence similarities are computed by means of the centroids of the embeddings of their lexical constituents. Networks are filtered by focusing on the first three most similar neighbors of each node – obviously, this does not interfere with the small-world topology of the networks. Each pair of texts is additionally described regarding the subnetwork of syntactic words and sentences. This is done to account for the impact of inflection on networking. As a result, the networks are thinned out (cohesion is now at most 0.5%), but neither the sizes of the largest connected components nor the cluster and distances values are affected considerably. Obviously, differentiation leads to sparseness, but in a sense that the general topology is retained. By focusing on a single level of resolution (e.g., paradigmatic relations among words), sub-networks are generated that fit into what is known about universal laws of complex linguistic networks (Mehler, 2008). See (Mehler et al., 2016) for additional evaluations of Wikidition.  Table 3: Wikiditions of three text pairs (Kafka: Beim Bau der Chinesischen Mauer // Nietzsche: Die Zeit der Zyklopen-Bauten; Kafka: Ein Bericht für eine Akademie // Rathenau: Höre, Israel; Kafka: In der Strafkolonie // Rauchberg: Statistische Technik) compared by their cluster value c, the average geodesic distance of their nodes l, the fraction of nodes in their largest connected components lcc and by their cohesion coh (the number of links in relation to all possible links). First line of each text pair: nodes comprise lemmas and sentences; second line of each pair: nodes comprise syntactic words and sentences. Networking is conditioned by the operative preprocessor (Eger et al., 2016).  Edition #nodes #links c lcc l coh   Kafka // Nietzsche 1624 10391 0,27 1 3,31 0,008    2401 13644 0,34 1 3,56 0,005   Kafka // Rathenau 1749 10782 0,28 1 3,36 0,007    2473 13609 0,36 1 3,60 0,004   Kafka // Rauchberg 4034 30951 0,25 0,999 3,35 0,004    6830 44369 0,31 0,999 3,62 0,002     Conclusion We presented Wikidition as a framework for exploring intra- and intertextual relations. Wikidition combines machine learning with principles of several wiki-based projects (Wikipedia, WikiSource and Wiktionary) to generate multi-layer networks from input corpora by integrating syntagmatic and paradigmatic relations on the lexical, sentential and the textual level. Our approach addresses intra- and inter-level networking in a single framework while adhering to laws of networking as being explored by complex network theory. In this way, input corpora get traversable in line with both empirical findings about characteristics of linguistic networks and the multi-resolution of sign relations whose space complexity is preferably reduced. Currently, Wikidition exists as a prototype that is further-developed by means of several edition projects in order to be finally made available as open source software. Wikidition is open for the cooperative development of digital editions.  ",
       "article_title":" Wikidition: Towards A Multi-layer Network Model of Intertextuality  ",
       "authors":[
          {
             "given":"Alexander",
             "family":"Mehler",
             "affiliation":[
                {
                   "original_name":"Goethe University, Germany",
                   "normalized_name":null,
                   "country":"Germany",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Benno",
             "family":"Wagner",
             "affiliation":[
                {
                   "original_name":"Beijing Institute of Technology, China",
                   "normalized_name":"Beijing Institute of Technology",
                   "country":"China",
                   "identifiers":{
                      "ror":"https://ror.org/01skt4w74",
                      "GRID":"grid.43555.32"
                   }
                }
             ]
          },
          {
             "given":"Rüdiger",
             "family":"Gleim",
             "affiliation":[
                {
                   "original_name":"Goethe University, Germany",
                   "normalized_name":null,
                   "country":"Germany",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "hypertext",
          "natural language processing",
          "corpora and corpus activities",
          "data mining / text mining",
          "English",
          "networks, relationships, graphs"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The automatic classification of literary genres, especially of novels, has become a research topic in the last years (Underwood, 2014; Jockers, 2013). In the following we report on the results from a series of experiments using features like most frequent words, character tetragrams and different amounts of topics (LDA) for genre classification on a corpus of German novels. Two problems will be in the main focus of this paper and they are both caused by the same factor: The small number of labeled novels. So how can experiments be designed and evaluated reliably in a setting like this. We are especially interested in testing results for significance to get a better understanding of the reliability of our research. While statistical significance testing is quite established in many disciplines ranging from psychology to medicine, it is unfortunately not yet standard in digital literary studies.  The scarcity of labeled data is also one of the reasons some researchers segment novels. We will show that without a test for significance it would be easy to misunderstand our results and we will also show that using segments of the same novel in the test and the training data leads to an overestimation of the predictive capabilities of the approach.   Setting In the following we will describe our corpus and feature sets. Our corpus consists of 628 German novels mainly from the 19th century obtained from sources like TextGrid Digital Library   textgrid.de/digitale-bibliothek  or Projekt Gutenberg   gutenberg.spiegel.de . The novels have been manually labeled according to their subgenre after research in literary lexica and handbooks. The corpus contains 221 adventure novels, 57 social novels and 55 educational novels; the rest belongs to a different or more than one subgenre.  Features are extracted and normalized to a range of [0,1] based on the whole corpus consisting of 628 novels. We have tested several feature sets beforehand and found stylometric and topic based to be the most promising (c.f. Hettinger et al., 2015). To represent stylometric features we employ 3000 most frequent words (mfw3000) and top 1000 character tetragrams (4gram). Topic based features are created using Latent Dirichlet Allocation (LDA) by Blei et al. (2003). In literary texts topics sometimes represent themes, but more often they represent topoi, often used ways of telling a story or parts of it (see also Underwood, 2012; Rhody, 2012). For each novel we derive a topic distribution, i.e. we calculate how strongly each topic is associated with each novel. We try different topic numbers and build ten models for each setting to reduce the influence of randomness in LDA models. We remove a set of predefined stop words as well as Named Entities from the novels as we have shown before that the removal of Named Entities tends to improve results.    Evaluation Classification is done by means of a linear Support Vector Machine (SVM) as we have already shown in Hettinger et al. (2015) that it works best in this setting (see also Yu, 2008). In each experiment we apply stratified 10-fold cross validation to the 333 labeled novels and report overall accuracy and F1-Score (c.f. Jockers, 2013). The majority vote (MV) baseline for our genre distribution yields an accuracy score of 0.66 and F1 score of 0.27 (see fig. 1).    Fig. 1: Cross table for majority vote baseline  In the cross tables of Figure 1 and 2 each column represents the true class and each row the predicted genre. Correct assignments are shaded in grey, average accuracy in green and average F1 score in red.    Fig. 2: Cross table for mfw 3000 as an example for classification results  Because there are not many labeled novels for genre classification we expanded our corpus by splitting every novel into ten equal segments. Features are then constructed independently for the resulting 3330 novel segments. To test the influence of the LDA topic parameter  t in conjunction with having more LDA documents we evaluate topic features for  t =100, 200, 300, 400, 500 (see figure 3 and 4).     Fig. 3: Accuracy scores for novels and novel segments and different feature sets    Fig. 4: F1 scores for novels and novel segments and different feature sets  Results show that our evaluation metrics tend to drop if novels are segmented. This could mean that genre is indeed a label for the whole literary work and not parts of it. On the other hand many differences are pretty small. Therefore we would like to test if these differences are statistically significant or if they should be attributed to chance.    Tests of statistical significance  When working with literary corpora there are few genre labels available for two reasons. First, the task of labeling the genre of a novel is strenuous; second, literary studies have mostly concentrated on a rather small sample, the canonical novels. Another issue is the creation of a balanced corpus, because for historical reasons the distribution of literary genres is not uniform and also the process of selecting novels for digitization has made the situation even more complicated. This generally results in data sets of less than 1000 items or even less than 100, see for example Jockers (2013) where 106 novels form a corpus or Hettinger et al. (2015) where we evaluate on only 32 novels.  The problem arising from small corpora is that small differences in results may originate from chance. This can be investigated by using statistical tests (c.f. Kenny, 2013; Nazar and Sánchez Pol, 2006). A standard tool to detect if two data sets are significantly different is Student’s t-test which we will use in the following to control the results of our experiments. We use two variations of Student’s t-test with  α = 0.05:    the one-sample t-test to compare the accuracy of a feature set against the baseline  the two-sample t-test to compare accuracy results for two feature sets  In both cases the data set considered consists of ten accuracy results from ten-fold cross validation and accordingly 100 data points for LDA from its ten models. Due to the small sample size we drop the assumption of equal variance for the two-sample t-test. The results for the one-sample t-tests show that every single feature set yields significantly better accuracy than the baseline (66.4%). We can therefore conclude that feature sets classify novels not randomly and that they do incorporate helpful genre clues.    Fig. 5: P-values for two sided t-test with α = 0.05 on accuracy of genre classification using 333 German novels  P-values for the two-sided t-tests are reported in Figure 5. Due to the large number of tests we apply Holm-Bonferroni correction; the resulting statistically significant outcomes are shaded in grey. From Figure 5 it follows that differences between segmented and not-segmented novels are  not statistically significant in most cases except for LDA with  t = 100. Besides results do not differ significantly for different topic numbers  t = 100, 200, 300, 400, 500.   An important assumption of the two-sample t-test is that both samples have to be independent. This is the case here as each time we do a cross validation we split the data independently from any other cross validation run. Thus, even if we repeat our experiments for a number of iterations (see e.g. Hettinger et al., 2015) we still get independent evaluation scenarios. Therefore we can apply the two-sided t-test in our setting to support our claims. In case of dependency of samples we could instead use paired t-tests on accuracy per novel.    Novel segmentation A crucial factor when segmenting novels is how to distribute the segments between test and training data set. We decided that in our case we have to put all of the ten segments a novel was divided into either in the test or in the training data set as we want to derive the genre of a novel not seen before. Another possibility which Jockers (2013) exploited is to distribute segments randomly between training and test set. In his work “Macroanalysis” Jockers investigates how function words can be used to research aspects of literary history like author, genre etc. In the following we want to replicate the part concerning genre prediction using German novels.  When segments of one novel appear in both test and training data we achieve an accuracy of 97.5% and F1 score of 95.9% - that is close to perfect (see fig. 6). Such a partitioning of the novels dramatically overestimates predictive performance on unseen texts. In comparison, Jockers (2013) achieves an average F1 score of 67% on twelve genre classes. His results are worse because we are only using three different genres while he is doing a multiclass classification with 12 classes. But nevertheless 67% probably still overestimates the real predictive power of this approach, because in our setup using the segments in both, test and training data, increased F1 by more than 17%.   Fig. 6: Results for different partitioning strategies    Conclusion In this work we looked at the methodology and evaluation of genre classification of German novels and discussed some of the methodical pitfalls of working with data like this. We discovered that only some of our results turned out to be statistically significant whereas for example the statement, that stylometric perform better than topic-based features, could not be fortified. Therefore our opinion is that research findings on small data sets should be scrutinized especially carefully for example by using statistical tests.   ",
       "article_title":"Significance Testing for the Classification of Literary Subgenres",
       "authors":[
          {
             "given":"Lena",
             "family":"Hettinger",
             "affiliation":[
                {
                   "original_name":"University of Wuerzburg, Germany",
                   "normalized_name":null,
                   "country":"Germany",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Fotis",
             "family":"Jannidis",
             "affiliation":[
                {
                   "original_name":"University of Wuerzburg, Germany",
                   "normalized_name":null,
                   "country":"Germany",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Isabella",
             "family":"Reger",
             "affiliation":[
                {
                   "original_name":"University of Wuerzburg, Germany",
                   "normalized_name":null,
                   "country":"Germany",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Andreas",
             "family":"Hotho",
             "affiliation":[
                {
                   "original_name":"University of Wuerzburg, Germany",
                   "normalized_name":null,
                   "country":"Germany",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016",
       "keywords":[
          "literary studies",
          "natural language processing",
          "german studies",
          "English",
          "text analysis",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Linking Etymological Dictionaries When studying low-resource languages, historical documents or dialectal variation, researchers often face the problem that lexical resources are sparse, dated, or simply unavailable. At the moment, the problem is addressed by different initiatives to either aggregate language resources  e.g. ELRA  http://www.elra.info  in a central repository or to collect metadata about them  e.g. OLAC   . The availability of this huge and diverse amount of material, often in different formats, and with a highly specialized focus on selected language varieties, poses the challenge how to access and search this wealth of information. Our project aims to address both aspects:   uniform access to lexical resources. At the moment, most resources are distributed across different providers. Platforms to query or browse this data are available, but they use different representation formalisms and remain isolated from each other. We employ Linked Data to develop interoperable representations to access distributed resources in a uniform fashion.   search across multilingual resources. We are not only interested in a specific language, but also, in related varieties: Much of the material we have is sparse, and we can address gaps in our lexical knowledge by consulting background information about form and meaning of possible cognates in other languages.   The project will implement search functionalities as web services and provide a prototypical web interface that allows to query Linked Data versions of open lexical resources. As a first step towards this goal, this paper addresses representation formalisms and data modelling, illustrated for an etymological dictionary of the Turkic language family.   Linked Open Data Linked (Open) Data defines rules of best practice for publishing data on the web, and since (Chiarcos et al., 2012), these rules have been increasingly applied to language resources, giving rise to the  Linguistic Linked Open Data (LLOD) cloud (Chiarcos et al., 2013)  e.g.   , http://linguistic-lod.org . A  linguistically relevant resource constitutes Linguistic Linked (Open) Data if (1) its elements are uniquely identifiably by means of  URIs, (2) its URIs  resolve via HTTP, (3) it can be accessed using  web standards such as RDF and SPARQL, and (4) it includes  links to other resources. It is Linguistic Linked Open Data (LLOD) if – in addition to these rules –, it is published under an  open license. For etymological dictionaries, the capability to refer to and to search across distributed data sets (federation, dynamicity, ecosystem) in an interoperable way (representation, interoperability) allows to design novel, integrative approaches on accessing and using etymological databases, but only if common vocabularies and terms already established in the community are being used, re-used and extended. (Moran & Brümmer, 2013) established lemon (McCrae et al., 2011)  http://lemon-model.net  for representing etymological data. Inspired by the pre-lemon inventory (de Melo, 2014), we introduce lemon extensions for etymological relations, illustrated for the linked data edition of the Starling Turkic etymological dictionary. With further dictionaries for Turkic languages becoming available as a result of our project, these are linked with each other and with language resources from contact languages such as Mongolian, Iranian, Caucasian, Arabic, and Russian.    Turkic Etymology in Starling The  Tower of Babel (Starling)    is a web portal on historical and comparative linguistics (Starostin, 2010), widely used in academia to publish etymological datasets over the internet. Starling allows exploring its dictionaries by means of faceted browsing using a coarse-grained phylogenetic tree (Fig. 2.a). We illustrate its data structures for the Turkic Etymological Dictionary (Dybo et al., 2012) with an example result for the query  meaning=\"bird\" (Fig.2.b). Following the  Proto-Turkic root, we find a cross-reference to the Altaic dictionary, and the  meaning (sense) of the proto-form in English and Russian. The following entries pertain to  cognates in different Turkic languages: They provide complex information including one or multiple  forms,  co-indexed with the meaning field, and optionally augmented with additional gloss (e.g., ‘moth’ for Middle Turkic/Chagatai), bibliography (as a hyperlink, Fig. 3) or additional comments (e.g., < Az. for Halaj). We used an XML export of the Starling data (Fig. 1) to create RDF and (by converting cross-references) Linked Data.     Fig. 1: XML snippet      Fig. 2a: Starling phylogenetic tree for faceted browsing    Fig. 2b: First query result for meaning “bird” in the Turkic etymological dictionary    Fig. 3: Bibliographic information for Abush      Data Model for the Turkic Etymology Following LLOD conventions, we employ the Ontolex/Lemon vocabulary (McCrae et al., 2011)    as shown in Fig. 4. Originally developed to add linguistic information to existing ontologies, Lemon evolved into a de-facto standard to represent lexical resources as LLOD. Here, we focus on Lemon extensions to represent etymological cognates: Etymological relations involve a relationship on the level of meaning (sense) and on the level of form, and thus require a novel property between one  LexicalEntry and another. Between etymological cognates, it is not always clear whether one was the source of the other, or a more indirect relation holds. To express a generic etymological link without additional directionality information, we introduce the property  lemonet:cognate. If source and target are known, a subproperty  lemonet:derivedFrom is introduced. Similar to  lemonet:cognate, it is transitive, but it is not symmetric. Distinguishing  lemonet:cognate and  lemonet:derivedFrom follows de Melo’s apparent directionality differentiation. Here, however, we provide a formal definition as a (minimal) extension of Lemon following (Chiarcos & Sukhareva, 2014) which supports inferring general cognate relations by subsumption and transitive/symmetric closure. In the Starling data, the directionality of etymological links is generally known, so we represent etymological relations with  lemonet:derivedFrom between lexical entries from different Lemon lexicons. By subsumption inference, transitivity and symmetry of its superproperty,  lemonet:cognate relations can be inferred automatically between all language-specific forms.     Fig. 4: Lemon-Core (Ontolex) module     Applications The  Comparative-Lexicographical Workbench (Fig. 5) will provide novel search functionalities extending the functionality of existing platforms, form-based search and a gloss-(meaning-) based search, currently applied to the Turkic language family and its contact languages.    gloss-(meaning-)based search. Dictionary lemmas are complemented with a gloss paraphrasing their meaning. Linked Data allows transitive search over sequences of bilingual dictionaries (e.g., Kazakh-Russian-English).   form-based search. Given a lexeme in a particular language, say, Kazakh, and a set of related languages, say, the Turkish languages in general, the system will retrieve phonologically similar lexemes for the respective target languages.   Both search functionalities aim to detect candidate cognates. The data provided by Starling represents a gold standard, but can also be directly integrated into the search process: In Fig. 5, we query for Chalkan  ана and possible cognates from Turkic (as an inherited word) or Mongolic (as a possible source of loan words). The results are organized according to the taxonomic status of the varieties in  www.multitree.org. They include a gloss from a Chalkan dictionary (marked by subscript C), but in addition provide form-based matches (subscript +) from the Starling dictionaries (S), e.g., with Turkish  ana and its etymologically corresponding forms, etc.     Fig. 5: Design study: Form-based search in the Comparative-Lexicographical Workbench     Summary We described preliminary steps towards the development of a Comparative-Lexicographical Workbench that uses Linked Data formalisms to retrieve cognates as given in etymological dictionaries as well as to automatically identify cognate candidates from different languages (which are similar in form and meaning). In our presentation, both will be illustrated for the Turkic language family, and we will show how both aspects complement each other.  ",
       "article_title":" Etymology Meets Linked Data. A Case Study In Turkic  ",
       "authors":[
          {
             "given":"Christian",
             "family":"Chiarcos",
             "affiliation":[
                {
                   "original_name":"University of Frankfurt, Germany",
                   "normalized_name":"European University Viadrina",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/02msan859",
                      "GRID":"grid.33018.39"
                   }
                }
             ]
          },
          {
             "given":"Frank",
             "family":"Abromeit",
             "affiliation":[
                {
                   "original_name":"University of Frankfurt, Germany",
                   "normalized_name":"European University Viadrina",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/02msan859",
                      "GRID":"grid.33018.39"
                   }
                }
             ]
          },
          {
             "given":"Christian",
             "family":"Fäth",
             "affiliation":[
                {
                   "original_name":"University of Frankfurt, Germany",
                   "normalized_name":"European University Viadrina",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/02msan859",
                      "GRID":"grid.33018.39"
                   }
                }
             ]
          },
          {
             "given":"Max",
             "family":"Ionov",
             "affiliation":[
                {
                   "original_name":"University of Frankfurt, Germany",
                   "normalized_name":"European University Viadrina",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/02msan859",
                      "GRID":"grid.33018.39"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-05",
       "keywords":[
          "digital humanities - multilinguality",
          "linking and annotation",
          "standards and interoperability",
          "ontologies",
          "historical studies",
          "lexicography",
          "knowledge representation",
          "linguistics",
          "query languages",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Triggered by an article by Lev Manovich   How to compare one million images?  (Manovich, 2012: 249-78) we were intrigued to find out “how to compare and analyse thousands of architectural floor plans”? The reason for this interest was the availability of the archive of the   Bureau Wederopbouw Boerderijen  (Office for Farm Reconstruction, 1940-1955) with data of 7700 farmhouses that were destroyed during World War II and were subsequently rebuilt – reconstructed or redeveloped in a new form – between 1940 and 1955 (Elpers, 2008).  Since it seemed impossible to analyse almost 8000 drawings by hand we decided to study the possibility of automatic identification and categorization of these floor plans. For this paper, we explored several strategies and digital tools to find out more about their usefulness in art and architectural history in general to answer questions about change, distribution of spaces, typology, genre, etc.   Automation of architectural floor plans By examining the differences between and changes of the floor plans of the reconstructed farms from the period between 1940 and 1955 and by comparing them with reconstruction sketches of the old destroyed farms, we hope to be able to answer questions such as: how was the struggle about tradition and modernization which determined the debate about farm construction in the middle of the twentieth century (Elpers, 2013) resolved in practice? This question focuses on the maintenance or abandoning of the so-called ‘streekeigen bouwen’ (traditional regional building)(Lambert, 2007) and related planning principles. Which traditional elements were retained and which were not? Which new elements were added? Can any general structural patterns be recognized? For this aim we would like to be able to automatically categorize and recognize elements in the architectural drawings. Following the visualization method and software (ImagePlot) by Manovich we would also like to create clusters of images of farms ordered by morphological structure, type, architect, region and timelines as output.  As architectural historians with limited programming knowledge, we started looking for existing software to find answers. Search engines usually use databases with (meta)data about objects. In our case we would rather be able to study the drawings directly on its visual content. We hoped that a combination of image processing software, techniques for pattern recognition, OCR and vectorization would bring us closer to our goals. Following information about conventions of farmhouses e.g. specific rooms, and workspaces like sheds could be labelled and found. This method, that we use in our study, has been, amongst others, described by Hansson (1998).   Preprocessing A test-set of about 1000 images was scanned in 300 dpi TIFF: 400 reconstruction drawings and 600 new floor plans. For each farm we have two sets of scans available: drawings of the old, destroyed farms and plans of the new farms. Further, we have built a repository with the TIFF-images in relation to a simple database.    Illustration 1. Image of original architectural drawing Westkanaaldijk D74, showing plan, views and situation plan  The architectural or engineering drawings show not just one floor plan of a farm but also plans of other floors, sheds and outbuildings, cross sections and several views, a drawing of the lay-out of the premises etc. Often several details are depicted, sometimes added in a separate drawing. Although the human eye can instantly distinguish which element is the main floor plan, one has to teach the computer how to find it. In order to isolate the floor plan from all the other parts of information, the white empty spaces surrounding the different elements in a drawing could be taken as a division space, separating the individual elements. First, we had to isolate the floor plan in the drawing and deconstruct its information. Unfortunately we could not find software that could ingest the drawings and produce the isolated floor plans, without programming. As a consequence, since it appeared too complicated to automatically find the position of the floor plan on the drawing, we skipped this part for the moment and prepared manually a new set of cut-out images of the floor plans.  With a simple image-processing program it appeared possible to manually deconstruct the contents of a drawing. In this way we could find out what steps were needed to automate the process. A drawing of Westkanaaldijk D74 in the village of Heumen (middle east of the Netherlands) was selected as a random example for the first test that shows the following manipulations:  OCR-text Vectorize the floor plan Floor plan with text labels Walls (thick, medium, without openigs) Walls (thick, medium, with openings and numbers)  Although most techniques could be carried out with simple imaging software, such as CorelDraw – and we tried some other free online programs such as Inkscape, VectorMagic, Gimp, etc – the number of actions or manipulations just for one drawing appeared quite high. Although it works reasonably well for one drawing, this is exactly the process that we want to automate and connect with a search system.    Illustration 2. Architectural drawing of Westkanaaldijk with vectorized plan and labelled spaces.  To scale up the process a bit, in a test with 10 floor plans of farms in the city of Groesbeek (middle east of the Netherlands) the following manipulations were carried out:   the plan of ground floor and first floor were manually cut out images were converted from heavy tiff to the lighter jpg reconstruction drawings were converted from 1:200 to 1:100 resolution of new plans were converted from 300 dpi to 150 dpi x/y axes were calibrated a central axis was determined it was noted that the drawings were not always on the right scale, while the inscribed measurements were noted correctly rooms are taken to be rectangles (which can be corrected later, but in most cases will not affect the results) the main spaces of farmhouses are accurate, while the interior spaces should be estimated on sight  It appeared that most walls have static measurements that can be brought into a library: the thickness of the walls depends very much on the type of bricks they are built with. These bricks have common measurements such as the  Waalformaat (210 x 100 x 50 mm). The thickness of a wall therefore varies from 27 cm to 6 cm. The different sizes could then be coded by colour and compared.     Deconstructing drawings, isolating elements and detecting symbols In order to deconstruct each drawing (see e.g.: Henderson, 2014; Dosch, 2000) the following steps were determined and carried out:  Building a repository with images and database (same scale, direction)  Finding the position of the floor plan on the drawing  Determine (outlines of) spaces / rooms OCR all text information Recognizing walls (determining and labelling exterior and interior)  Recognizing specific rooms (determining and labelling livingroom, kitchen, bathroom, hall, bedroom, stables etc.)    Determining difference between reconstruction drawing (original state) and new plan and difference between reconstruction drawings from different years     Illustration 3. Cut-out plans showing ground floor and other floors    Illustration 4. Cut-out floor plans, vectorized showing the volumes of spaces.  To carry out the recognition part such as wall detection, text recognition and labelling, and making comparisons of old and new, the approach of Ahmed and Liwicki (Ahmed et al. (2011; 2014) appears promising for solving some of our problems. Although they focus on using sketches to be recognized and compared with floor plan drawings in a repository, we would like to see how their system works for us with our repository of cut-out floor plans. Timelines can relatively easy be visualized using the metadata - with the tag “year” - in the database example (Borner, 2010; Alkhoven, 1998; 2008). Other tests with visualization software such as ImagePlot produced interesting and beautiful views of the drawings but could not yet provide the required results for analysis. More experimentation and possible adaptation of the software is needed.                Next steps We set out to find answers on how we could compare thousands of drawings of floor plans, what digital tools and best practices are currently available to analyse them and how we could visualize differences in sets of floor plans. In this first stage of the process we have gathered information about the preprocessing stages and we were able to perform some experiments with the scanned architectural drawings. We discovered many – free–tools but it turned out difficult to distinguish among them and to choose the best ones that could solve each part of the problem. We have tested several apps but since it was not our objective to deal with coding, we could not adapt the software and we were therefore limited in doing our research. As a consequence much had to be done manually in a computer-assisted way.  Since our overall problem appeared a bit too ambitious to carry out within the given boundaries of time and budget, we have limited ourselves to producing a list of ingredients and a well-argumented recipe formulating the next sequence of steps to take. These form the basis for a new research proposal for the Digital Humanities’ call in The Netherlands. A long article with more detailed information about the process, experiments, and results, will be ready later this year.  ",
       "article_title":" Comparing Architectural Floor Plans: New Strategies New Digital Tools For Architectural Historians ",
       "authors":[
          {
             "given":"Patricia",
             "family":"Alkhoven",
             "affiliation":[
                {
                   "original_name":"Meertens Institute (KNAW), Netherlands, The",
                   "normalized_name":"Meertens Institute",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/05kaxyq51",
                      "GRID":"grid.450081.8"
                   }
                }
             ]
          },
          {
             "given":"Ronald",
             "family":"Stenvert",
             "affiliation":[
                {
                   "original_name":"BBA (Bureau voor Bouwhistorie en Architectuur), Netherlands, The",
                   "normalized_name":null,
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Sophie",
             "family":"Elpers",
             "affiliation":[
                {
                   "original_name":"Meertens Institute (KNAW), Netherlands, The",
                   "normalized_name":"Meertens Institute",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/05kaxyq51",
                      "GRID":"grid.450081.8"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "visualisation",
          "art history",
          "English",
          "image processing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   The Guidelines of the Text Encoding Initiative Consortium (TEI) have been used throughout numerous disciplines producing huge numbers of TEI collections. These digital texts are most often transformed for display as websites and camera-ready copies. TEI Simple  (Rahtz et al, 2014)  project was the first one to propose more prescriptive approach providing the baseline rules of processing TEI into various publication formats, while offering the possibility of building customized processing models within the same infrastructure. For the first time in history of TEI there exists a sound recommendation for default processing scheme, which significantly lowers the barriers for entry-level TEI users and enables better integration with editing and publication tools. The TEI Simple project was a Mellon-funded collaboration between the TEI Consortium, Northwestern University, the University of Nebraska at Lincoln, and the University of Oxford.   The new (on track for acceptance by early 2016) TEI method for documenting processing models gives editors and TEI customisers a method for high level recording of processing intentions in a machine-processable but implementation agnostic manner. Nevertheless, the processing model is a new proposal and needs to be extensively tested before announcing it a success. As the TEI Technical Council works to integrate the TEI processing model extensions created by the TEI Simple project, we endeavour to employ it on real world projects, both of which have been running for a significant number of years and have already produced vast collections of material: historical documents of the US Department of State, Office of Historian (  ) and the corpus of Ioannes Dantiscus’ correspondence (  ). The Office of the Historian publishes a large collection of archival documents of state, especially those appartaining to foreign relations. The Dantiscus project spans over ten thousand original sources from the early sixteenth century including correspondence, poetry, and diplomatic documents. This makes it a good test case for implementation of the TEI Processing Model because it is far beyond the scope of the original TEI Simple sample collections. Having the material previously published with custom-built XQuery/XSLT packages means that we are in a position to compare the results of using an approach based on the processing model with the previous one in terms of the quality and consistency of final presentation but also in more quantitative ways like the size of necessary code base, development time and ease of the long-term maintenance.  The first challenge is, obviously, rephrasing the transformations previously formulated in XQuery/XSLT using ODD meta-language extensions proposed by TEI Simple project. Preliminary results are very encouraging even though, as expected, it became necessary to extend the behaviours library to accommodate some specific needs. From the developer’s perspective it is immediately clear that using the TEI processing model brings substantial advantages in development time and results in much leaner and cleaner code to maintain. For the Office of Historian project figures suggest code reduction by at least two-thirds in size. Numbers are even more impressive realizing that the resulting ODD file is not only smaller, but much less dense, consisting mostly of formulaic <model> expressions that make it easier to read, understand and maintain, even by less skilled developers.  To a lesser extent, but it is still interesting to see if, thanks to the additional layer of abstraction that processing model brings to the table, the editors can become more independent from developers in tweaking the processing rules. This heavily depends on the personal predilections of the editor, but again, in cases where editors are already deeply involved in the decisions about encoding on the level of XML markup and do have some fluency in XPath and/or CSS our results show that it is perfectly reasonable to expect them to tailor the existing high-level processing models to fit their specific needs in a majority of cases. We will also investigate the effect of incorporating the Processing Model into eXist-db   native database and application framework  (Meier et al, 2016)  environment in terms of easening the learning curve, for the non-technical users in particular.   The processing model at the time of writing this paper proposal is not a mature technology yet, in the sense that it still lacks the critical mass of its practitioners as well as formal acceptance by the TEI Technical Council (although this will have been integrated into the TEI infrastructure by the time of DH2016). This presentation aims to present both challenges and open questions as well as already demonstrated advantages of applying this technology. It will draw on the evidence from early adopters available by the time of DH2016. It is not only the quantitative measures of improvements in technical implementations that will be reported on, but the variation in methodologies employed by the test projects and others. ",
       "article_title":" A lesson in applied minimalism: adopting the TEI processing model  ",
       "authors":[
          {
             "given":"Magdalena",
             "family":"Turska",
             "affiliation":[
                {
                   "original_name":"University of Oxford, United Kingdom",
                   "normalized_name":"University of Oxford",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/052gg0110",
                      "GRID":"grid.4991.5"
                   }
                }
             ]
          },
          {
             "given":"James",
             "family":"Cummings",
             "affiliation":[
                {
                   "original_name":"University of Oxford, United Kingdom",
                   "normalized_name":"University of Oxford",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/052gg0110",
                      "GRID":"grid.4991.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2015-10-16",
       "keywords":[
          "xml",
          "data modeling and architecture including hypothesis-driven modeling",
          "encoding - theory and practice",
          "English",
          "publishing and delivery systems"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Doing comparative researches on a large literary corpus is often lengthy and demanding. With the digitization of contents, scholars have started using computers. Semantic information is essential for an appropriate understanding of texts and is an extremely important factor in cross-textual analysis. This is why we have developed a new semantic engine built especially for Digital Humanities, called DeSeRT   A French version of DeSeRT is freely available online at http://obvil-dev.paris-sorbonne.fr/desert/  . This paper presents some of the results that were obtained from a corpus of the 17 th and 18 th century texts characteristic of the debates about theater in the classical age. The following paper is divided into four parts: after a first section that briefly describes the search engine used and the different investigations it allows, a second section introduces the corpus used. Then, the third section shows some of the results obtained.    The DeSeRT Search Engine DeSeRT has been designed to identify and compare rewriting, paraphrasing or reformulation. It is based on an idea, already developed (Barron-Cedeño et al., 2013; Ferrero and Simac-Lejeune, 2015), according to which, even if the reformulations cannot be reduced to paraphrases, they retain the meaning of original texts by using either the same words or words of similar meaning. As a consequence, the detection of co-occurrences of a few semantically equivalent lemmas in small blocks of texts is sufficient to capture the equivalent meaning and, therefore, to identify the reformulation of the same ideas. This is implemented in four steps:   Dividing texts into small blocks of words that are partially overlapping. Typically each block may contain 300 words and two consecutive blocks overlap by one-half, but both the block size and the proportion of overlap can vary. Extracting the meaningful lemmas using a POS tagger. This step enables the exclusion of some syntactical categories, such as prepositions or articles, and to get the lemma associated to each word. The current implementation makes use of TreeTagger   http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/ , but this could easily be changed.  Indexing each block with the lemmas obtained at step 3. Without going into implementation details, let us remark that the index is stored using the SQLite   http://sqlite.org/  database management system.  Retrieving blocks that contain many identical or semantically equivalent lemmas. This fourth module exploits a dictionary of synonyms to recover blocks that have similar meanings. It is also possible to use a thesaurus to restrict the search to a set of predefined terms, but this is not a necessity. The proximity between two blocks of text is based on an Okapi similarity measurement (Spärk-Jones et al, 2000), of which evaluation is greatly simplified by the fact that all blocks have the same size.   More precisely, the Okapi similarity measure of a block B with a set of terms T =  t 1 , … t n is given by the following formula:   where  f(t i , B) is the frequency of the term  t i in the block  B, | B| is the size of block  B, k and  b are two free parameters chosen in advance (usually chosen as k∈[1.2, 2] and b=0.75),  avdl is the average document description length in the collection and  IDF(t i ) is the  Inverse Document Frequency of term  t i is given by:    It appears that, when the size of each block is the same and when the frequency of terms in blocks is supposed to be at max 1 (this approximation is justified since the blocks are small), this formula can be simplified. It then becomes equivalent to the information theoretic measure of the terms in blocks, i.e.    where  Pr(t i ) is the probability of the term  t i in the overall corpus.  Using this score, it is possible to measure the similarity between blocks or between a block and a set  T of terms  t i. As a consequence, DeSeRT can be used in different ways. The research queries may be done through words or concepts, i.e. words that are expanded using the dictionary of synonyms. It is also possible to compare any text (or file) to a corpus: then, DeSeRT detects corpus blocks where the meaning is similar to blocks of the given text, which allows, for instance, the arguments in a dispute or the anecdotes and the common places that are reused to be followed. Lastly, it is also possible (but not mandatory) to add a thesaurus or ontology to focus the search on a given semantic field.  Note that, based on techniques developed to detect plagiarism, many tools already exist that are designed to identify paraphrases, reuses and borrowings, i.e. sequences of words that are approximately identical, e.g. (Ganascia et al. 2014; Horson et al. 2010). However, these techniques are unable to spot reformulations of the same ideas or allusions to previous texts. DeSeRT has been designed to overcome these limitations.   The Hate of Theater  The project  Haine du Théâtre    http://obvil.paris-sorbonne.fr/projets/la-haine-du-theatre    (“The Hatred of Theater” in French) aims to analyze theater disputes in Europe using scientific approaches and critical editions of polemical texts. The team’s reflections are mainly focused around the discovery of the circumstances and the arguments used in theater controversies all across Europe, not limited to France, but also in England, Spain, Italy, and the Germanic area, from the last decades of the 16 th century up to the beginning of the 19 th century.  The corpus of the project collects many texts written in French during the 17 th and the 18 th centuries. The purpose of this project is to explore the gray areas of theater controversies in order to outline a global overview and to discover where and how the polemics began, their chronological discrepancies and the links between them and their contemporary resurgences. The total collection of the  Haine du Théâtre texts is, by now, made up of 27 texts.     Exploitation of DeSeRT on the Hate of Theatre Corpus  Discovery of reuses Querying the 27 texts of the  Haine du Théâtre corpus, we found much reuse of similar passages and texts. DeSeRT is not only useful for detecting those parts of text that deal with the same concept, but is also a very good tool to find borrowings.   For example, comparing two texts, the  Défense of Voisin (1666) and  Traité de la Comédie of Nicole (1667), we discover immediately that the  Traité has been included in the  Défense by Voisin, which is a very long text, not only once, but twice. The first time, Voisin presents it as a re-publication, then he re-uses phrases similar to those employed by Nicole in different passages and he sprinkles them in his  Défense.  The keywords of this correspondence are very well detected by DeSeRT, as can be seen in the example below.     Furthermore, continuing the analysis of the text we discover that in these two texts the actor is frequently associated with the idea of purity in religion.      Reformulations of Ideas DeSeRT also shows in detail the parts of the corpus that are similar or that develop the same ideas. This may either be done on demand, according user requests, i.e. to given texts, or to the overall corpus, which automates the process. For instance, we have found many topics common not only to the texts by Nicole and Voisin, but also to two others e.g. the theme of the idolatry as the “mother of all spectacles” in (Aubignac, 1666) and (Conti, 1666). Note that, in the following figures, we have greyed the identical passages with the MEDITE system, which only spots strict homologies, without considering many words that appear identical to DeSeRT because they correspond to the same lemma or to two synonymous lemmas. As a consequence, the number of gray zones considerably underestimates the semantic proximity detected by DeSeRT.    Secondly, as shown in the following figures, the topic “renouncing the Devil” ( Renoncer au Diable in French) is regularly present in the texts written by Aubignac, Conti and Voisin,   D’Aubignac, 1666 : 59, 62, 65, 72, 73, 74, 79, 217. Conti, 1666 : 88, 105, 120, 144, 173, 182, 184. Voisin, 1671 : 59, 86, 88, 97, 113, 114, 124, 165, 205, 212, 228, 407, 427, 433, 451, 463, 481.  while it appears only once in (Nicole, 1667: 477).        Further results of DeSeRT lead us to understand that many others expressions are common to the four authors, such as the description of the theatre as a “flesh of pestilence” ( chair de pestilence) or as a “school of the debauchery”.            Conclusion To conclude, DeSeRT allows the discovery of reformulations and similar phrases, as well as related topics and passages in a corpus. It is usable on any kind of corpus that is made up by files in the txt format. As we have briefly shown in this paper, this search engine enables users to identify crucial passages of a specific corpus according to two types of detection: the discovery of reuses, such as plagiarism or hidden rewritings, and the reformulation of ideas, which can be manually given by the user (as words or concepts) or automatically extracted by the DeSeRT search engine.  ",
       "article_title":"Crossed Semantic Analysis of Literary Texts with DeSeRT",
       "authors":[
          {
             "given":"Jean-Gabriel",
             "family":"Ganascia",
             "affiliation":[
                {
                   "original_name":"Sorbonne Universités, Université Pierre and Marie Curie (Paris 6); Laboratoire d'Informatique de Paris 6 (LIP6), CNRS UMR 7606; Labex OBVIL (Observatoire de la vie littéraire)",
                   "normalized_name":"Sorbonne University",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/02en5vm52",
                      "GRID":"grid.462844.8"
                   }
                }
             ]
          },
          {
             "given":"Chiara",
             "family":"Mainardi",
             "affiliation":[
                {
                   "original_name":"Sorbonne Universités, Université Paris-Sorbonne (Paris 4); Labex OBVIL (Observatoire de la vie littéraire)",
                   "normalized_name":"Sorbonne University",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/02en5vm52",
                      "GRID":"grid.462844.8"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-02",
       "keywords":[
          "french studies",
          "semantic analysis",
          "literary studies",
          "natural language processing",
          "corpora and corpus activities",
          "data mining / text mining",
          "content analysis",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The use of Semantic Web Technologies in Digital Humanities projects has increased over the years, but only recently annotation tools and research environments have started to create and use semantic graphs (e.g. Pundit   http://thepund.it  , Grassi et al. 2013, or CWRC   http://www.cwrc.ca  , Rockwell et al., 2012). While the sharing of annotations and data into a formalized Web of Data is a central part of these projects, in this paper we would like to focus on capacities for qualitative research by using semantic graphs and linking these with quantitative data. Qualitative research approaches have thus far not been positioned at the core of Digital Humanities (Drucker, 2012). However, recent discussions about close and distant reading e.g., (Moretti, 2013) point out a manifestation of a strict opposition between qualitative and quantitative research on the level of data. We would like to reconfigure this separation between qualitative and quantitative data by taking into account its epistemological practices and apparatuses. Qualitative and quantitative data can be regarded as “thin descriptions”. Both need an interpretative act to become “thick descriptions” (Love, 2013). This involves a broad range of research capacities, whereby a semantic graph addresses parts of it and creates a link between both paradigms by connecting the different data. This main argument of the paper will be demonstrated through examples from ongoing Digital Humanities research projects based on the semantic research environment Semantic CorA   http://semantic-cora.org  , where we experimented with epistemological apparatuses which were developed through participatory design approaches and collaborative ontology engineering.   Re-Configuring the apparatus of qualitative and quantitative research  The recent focus in Digital Humanities on epistemological apparatuses, including their materiality, performativity and relation to theory, offers the possibility to sharpen the respective analytical concepts for data and research apparatuses. Ramsey and Rockwell (2012) describe tools as a “telescope for the mind” and offer a materialistic epistemology. Concerning the opposition of qualitative and quantitative research, Manovich (2011) describes the potentials of big data by contrasting quantitative methods (i.e., statistical, mathematical, computational) with qualitative methods (i.e., as used in History, Literature Studies, Anthropology, qualitative Social Sciences and Psychology) and the different kinds of underlying data. While quantitative approaches commonly rely on surface data, qualitative data are described as deep data. Manovich posits equivalent epistemological depth of both kinds of data but hints to the different scale of contact points with the object of interest.  Venturini and Latour (2010) point to the micro/macro distinction in Social Sciences, which corresponds to the qualitative/quantitative separation at the methodological level. The new capacity of digital, computerized methods, they point out, are quali-quantitative methods, which do not rely on the opposition of statistical analysis and ethnographic observation. Love (2013) concretizes this link between qualitative and quantitative approaches as well and takes into account the epistemological apparatus by comparing Literature Studies and ethnographic research. Instead of placing thin description in opposition to “thick description” (Geertz, 1973), Love (2013: 403) argues for the significance of thin description, which she considers as an integral part of thick description, and demands for a reflective engagement with the full range of empirical methods. In this sense the epistemological apparatuses are per se boundary-making practices, which “enact what matters and what is excluded from mattering” (Barad, 2007: 148) and demand an accountability of this material-discursive practice of epistemological apparatus design.    Designing Semantic CorA The realization of Sematic CorA was driven by a participatory design and agile development approach of both the software components as well as the ontology. A main goal was to enable classical qualitative researchers to realize their research by using the potentials of a semantic graph. The design followed the research practice though an evolutionary process, which was initiated by an analysis of needs and requirements (i.e., site visit, artefact analysis, interviews) and was followed by nearly weekly meetings with requirement articulations and prototype testings. Semantic MediaWiki   http://semantic-mediawiki.org/   was used as a technological platform, which was extended with newly developed tools (extensions) to address the needed research capacities.    Linking qualitative and quantitative Research with Semantic CorA  Openness and flexibility: In qualitative research, the openness of the research object and the flexibility to adjust the knowledge base play a central role in grasping the complexity of the phenomenon of interest (Bauer, Aarts, 2007). Using a semantic graph as an epistemological tool offers the possibility to create new entities, add properties or relate these to other entities. In this way, a network is created step by step based on the research material, which can be extended and re-arranged in the research process. So an all-encompassing fixed schema is not a precondition when starting a research project. The semantic graph consists of pages (representing entities), which are described by properties or links. Figure 1 demonstrates the schema of the semantic graph for exploring historical educational lexica (ranging from 1774 to 1945). Integrated bibliographic data (e.g., lexicon, article, author) and digital images of the lexica from a digital library   http://bbf.dipf.de/digital-bbf/spo   build the data basis for the graph, whereby the syntax supported by the SMW platform and data edit forms offer to enrich the graph. For example, a researcher establishes the relation `Is About´ between the article and a person and adds further properties (date of birth/death, gender, nationality, domain of mattering). In this respect, a thin description has been created, indicating a network of the phenomenon with more than 6 million values and 100,000 pages.   Additionally, the technological platform itself offers the possibility to link elements of the graph to established Semantic Web vocabularies (i.e., BIBO or DC); this has been performed for a large part of the data.      Figure 1: Schema of the semantic graph for exploring educational lexica   Balancing particularities and formalizations: In qualitative research it is necessary to balance formalization with respect to the particularities of the phenomenon of interest and the research material. Star describes this balance as achieved through facetted classifications in the methodological approach of the grounded theory (Star, 1998: 227). As previously described, a semantic graph offers the possibility to create a network, which calls for formalization and sets the boundaries of the phenomenon. To enable a more `fuzzy´ or qualitative thin description, each node and property of the graph can be described in an unformalized way using the classical text tools of a wiki. Additionally, an open approach for qualitative content analysis is followed to enable annotations of the text (Figure 2). In this way, the annotation is connected to the article on the basis of the semantic graph (Figure 1), providing links between the annotations of the qualitative content analysis and the further network of the semantic graph. This constitutes a thin description based on qualitative data (e.g., through close reading and annotations) and quantitative data (e.g., bibliographic data), where an interpretative act is needed for the description to become a thick description.     Figure 2:  Annotation tool with data edit form (i) and  aggregated annotations in a dynamic table (ii)    Ongoing data analysis and graph creation: Another main aspect of qualitative research is the ongoing iteration between grasping the research material and creating new connections or qualities. This intensive work with research data can be done by comparing, following associations, close reading, or – as previously described – by distant reading of aggregated or related elements. While using a semantic graph as a network of the phenomenon, two different examples can be demonstrated which use the linkage between qualitative and quantitative data: Semantic browsing and querying the network. To represent relevant parts of the graph, aggregations and inferences are created for main entities (e.g. 1,200 aggregated descriptions of persons) to thickening the research data and browsing through the network (Figure 3, i). Additionally, a query tool enables to query and aggregate the semantic graph (Figure 3, ii).    Figure 3: Semantic Browsing through the aggregated information on page about person (i) and analysis tool with results in dynamic table (ii).     Discussion and outlook This paper demonstrates the possibility of linking qualitative and quantitative data by using semantic graph technologies to create thin descriptions. Therefore, the advantages of the interpretative act of thick descriptions are considered by allowing for ongoing iterations of analyzing the research material and creating the semantic graph in a formalized and unformalized way (enrichments, annotations, text descriptions). With semantic browsing, aggregations of information, annotation and querying the semantic graph, aspects of close and distant reading are addressed, thus offering new techniques for grasping the research material for qualitative research.  For the Digital Humanities, the focus on mattering of apparatuses offers the possibility to open the design space for digital tools to the diversity of epistemological practices in Humanities. Thereby, an engagement with the diversity of the Humanities comes to the front, enhancing the accountability of boundaries and possibilities of epistemological apparatuses in Digital Humanities. Acknowledgements  The authors would like to thank the research group around Semantic CorA, especially Marc Rittberger, Lia Veja, Kendra Sticht, Anne Hild, and Anna Stisser. The initial realization of the research environment Semantic CorA was supported by the German Research Foundation (DFG) and its further development is supported in the context of CEDIFOR by the eHumanities program of the German Federal Ministry of Education and Research (BMBF) no. 01UG1416C.   ",
       "article_title":"Linking Qualitative and Quantitative Research by Thin Descriptions through Semantic Graphs. Experiments in Apparatus Design with Semantic CorA.",
       "authors":[
          {
             "given":"Christoph",
             "family":"Schindler",
             "affiliation":[
                {
                   "original_name":"German Institute for International Educational Research (DIPF)",
                   "normalized_name":"German Institute for International Educational Research",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0327sr118",
                      "GRID":"grid.461683.e"
                   }
                }
             ]
          },
          {
             "given":"Basil",
             "family":"Ell",
             "affiliation":[
                {
                   "original_name":"Karlsruhe Institute of Technology (KIT)",
                   "normalized_name":"Karlsruhe Institute of Technology",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/04t3en479",
                      "GRID":"grid.7892.4"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "information architecture",
          "digital humanities - diversity",
          "linking and annotation",
          "corpora and corpus activities",
          "digitisation - theory and practice",
          "networks, relationships, graphs",
          "content analysis",
          "English",
          "semantic web",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Realm of Digital history  “Digital history” became a professional realm, because new ways to store, process, and study information entered the everyday historical research practices in Russia (Rosenzweig, 2010). Obviously, the “digitization” introduces new approach to the craft of the historian, when digitization became the indispensable stage of many studies (Volodin, 2015). Interdisciplinary framework of “digital history” is often placed in the context of a broader movement of the “digital humanities” (Schreibman et al., 2004; Schreibman et al., 2016). However, it should be emphasized that historical research has lots of specific features, and “digital turn” makes a significant impact on the research practices (Weller, 2013).   Historical research is a kind of reconstruction of the past on the basis of the extant monuments and documents, “remains” and “legends”. This reconstruction is based on three pillars: heuristics, criticism and interpretation. These basic practices today meet the challenges of the “digital age”. It's important to mention that  the “digital turn” in the profession of historical research was one of the four main themes of the International Congress of Historical Sciences in 2015. It seems that time has come to investigate the specificity of research practices and digital tools for professional historians.     Survey of “digital” in practice  The research is based on questionnaires and interviews of historians in Russia in 2015 and 2016 (178 respondents). In questionnaires and interviews I tried to ask not only “geeks” among historians, but also “pure” historians whom “digital turn” also touched (cf.: DARIAH Survey, 2015; Schreibman and Hanlon, 2010; Trinkle, 1999). First questions ask respondents to name main “digital practices” that they consider important. Such start helps to create “folksonomy” of practices and tools that historians define for themselves as “digital” (e.g. it’s not obvious whether text-editing is considered as digital practice, or email as research tool?). Then I started to ask questions about different fields of digital practices as we hypothetically classified in several groups: access, digitization, editing, mark-up, visualization, modeling/simulation, and others. Each group (or type of practice) includes different techniques and tools. We define techniques as more general term to call research procedure, when tools are linked to particular software or platform.    History in the “digital turn”  The instancy of the research is associated with a new stage in the development of information and digital technologies in history. “Digitizing” is not just a technical procedure now, but it becomes a preparation for further computerized analysis of historical sources, using different tools for visualization and analysis of historical information.   The “digital turn” in history requires a monographic study as well as a full systematization of historians’ research practices. We don’t know any systematic attempt to generalize the results achieved by “digital history”. This research project is designed to systematize the research practices (both visualization and analysis of historical data). This study tries to “catch” the development of contemporary digital practices, and we hope to make classification that will summarize achievement  in the field of “digital history”. Summarizing the answers, it’s possible to draw several valid observations: one insight and three dichotomies.     Insight: Digital is “Invisible”  It looks that a prediction which was vividly pronounced in “A New Companion to the Digital Humanities”  that “a decade or two from now, the modifier “digital” will have come to seem pleonastic when applied to the humanities” is much closer. Most of answers were concerned with some concrete experience of usage of digital tools, mentioning that they are so “natural” or “spontaneous”. The time has come when historians are really in the majority of cases are so-called “digital natives” (even if not in age, but at heart).   The usual remark on state-of-art in historical digital practices was that so-called new ICT (Information and communications technology) became usual, familiar, and mostly routine ICT. The increase of computing power broadens digital practices form tables and databases to full-text search in enormous online collections, and then to visualizations from GIS to 3D (cf.: Gregory, 2014).    Heuristics dichotomy: Analog versus Digital  Digital in Russian literally means numeric (like chiffre or numerique in French, or Zíffern in German) without any link to fingers, but strongly connected to quantitative research and computing. That’s why for main historians “digital history” strongly refers to quantitative history or cliometrics. History as one of \"traditionally print-based disciplines\" (Hayles, 2012, 1) is deeply connected to studies of analog primary sources. This dichotomy has two implications: on the one hand, historians usually see no principle differences between analog original and digital copy, but, on the other hands, they mainly prefer to study analog archives, because it shows their professional skills of access to rare documents.    Criticism dichotomy: Capta versus Data  The “digitization” became the usual and common practice of collecting information. But it's important to mention that for many respondents digitization is almost the same practice as “writing out” or “making notes”, not planning the future possibilities of processing this “capta”.    Distinction between “capta” and “data” in digital history became indicative. If capta is “taken” actively while data is assumed to be a “given” (Drucker, 2011), in digital history capta explains the “catch” of the researcher in terms of his/her sampling of primary sources as well as a critical attitude to the choice of the sources.     Interpretation dichotomy: Close versus Distant   The distinction between “close reading” and “distant reading” came to digital history from computer linguistics (Moretti). This dichotomy explains the conscious choice between brains or processors in solving different research problems. And main present question (or prospect) is that to what extent main practices of history can be robotized? This vision of the future is usually mixed with vague hope that at least interpretation will remain in the researches’ hands and heads. It also brings up an issue of possibilities of digital infrastructure for analytical level.   The aim of the lasting research of digital practices is to create a list of solutions and techniques developed in real research practice, and thus to disseminate such solutions among historians and researchers of the past. The classifier of digital methods and tools will help to choose the most appropriate solution for particular research aims.   ",
       "article_title":"“Digital” in practice: survey of Russian historians' research practices",
       "authors":[
          {
             "given":"Andrei",
             "family":"Volodin",
             "affiliation":[
                {
                   "original_name":"Moscow State University, Russian Federation",
                   "normalized_name":"Lomonosov Moscow State University",
                   "country":"Russia",
                   "identifiers":{
                      "ror":"https://ror.org/010pmpe69",
                      "GRID":"grid.14476.30"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-05",
       "keywords":[
          "digital humanities - diversity",
          "history of Humanities Computing/Digital Humanities",
          "digital humanities - facilities",
          "encoding - theory and practice",
          "historical studies",
          "digitisation - theory and practice",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction With over 85 million print units sold in 2014 for the top 300 comic book titles only, the comics industry is reaching a new high for the first time since 2007 (before the economical crisis). And this doesn’t include the increasingly popular graphic novels or the increasingly more accessible digital comics. The resurgence of comics and the establishment of the graphic novel as a literary genre prompted Humanities scholars to turn their attention on comics as a medium. In this paper, we address the difficulty of creating a digitized corpus by using a crowdsourced approach for annotating comic books. The resulting XML-based encodings could assist not only researchers, but publishers and collection curators equally.   Motivation Our approach should provide Digital Humanities (DH) scholars with a (currently missing) structured, annotated corpus; this should enable or speed up research related to the comics and sequential art theory: identifying the rhythm of the narration based on the shape, size or number of panels and its relation to the depicted action, investigations about the style of comics authors, historical periods, cultural movements etc. Curators and collectors (professional or amateur) of physical or online comics collections would be provided with a structured content which could be more easily integrated within their collections or databases. This may assist them into enlarging public or private databases of characters or comics series and enable the creation of artefacts such as comic books dictionaries, search indices and dictionaries of onomatopoeia. A certain number of projects are already in place and could greatly benefit from the creation of comic books annotations. We mention here the Grand Comics Database   http://www.comics.org/  (an online database of printed comics), Comic Book Database   http://comicbookdb.com/ , Digital Comics Museum   http://digitalcomicmuseum.com/  (a collection of scanned public domain comics from the Golden Age) or the Catalogue of the  Cité Internationale de la Bande Dessinée et de l’Image   http://www.citebd.org/  from Angoulême.  From a publishing perspective, current standard specifications related to digital comics, such as EPUB’s Region Based Navigation (Conboy et al., 2014) and Metadata Structural Vocabulary for Comics (Ichikawa et al., 2014) are taking care exclusively of the presentation layer (i.e. rendering a publication on a screen device). But the artistic nature of comics and the great potential digital comics have already showcased allow us to go beyond simple content presentation. We believe that the data we are collecting will allow publishers and digital comics authors to create better, enhanced content and in the end a superior reading experience.   Crowdsourcing Annotations for Comics Participants to our crowdsourcing experiment (Azavea, 2014; Sharma, 2010) are digital comics readers. Previous research has identified expertise sharing, belonging to a community and helping with a research project as strong motivating factors for crowdsourcing participants (Dunn et al., 2012). In addition, our industrial partner will incentivize participants with product vouchers for their platform   Actialuna – Sequencity: https://www.sequencity.com .  The tasks we propose are organized around a set of questions regarding a series of comics-related topics (Eisner, 1985; Ichikawa et al., 2014) on which computer algorithms are not performing well enough: complex page layouts, identification of narration elements (characters, places, events, objects), stylistic elements (balloon shapes, onomatopoeia, movement lines).   Figure 1. The 4 key annotation themes  We aggregate the answers (Feng et al., 2014; Snow et al., 2008) taking into account the reliability of an annotator in a given context (task difficulty, user experience with the task, type of question) and the agreement between the annotators (Nowak et al., 2010). A quality score is thus generated for each annotation, with the best of them being selected as solutions. We subsequently are able to generate the ComicsML encodings (Walsh, 2012). This XML derived format is particularly useful since it’s based on the already widespread Text Encoding Initiative (TEI), allowing for declarations of page structure and composition, panels, characters, text (in all the varieties hosted by the comics medium: different types of balloons, diegetic text, onomatopoeia), events and even panel-to-panel transitions.  Page structure The annotators are presented with a simple interface (Fig. 2) in which they will have to make a choice between a set of suggested grid layouts. These layouts are the output of applying the automatic frame extraction algorithm developed by (Rigaud et al., 2011). Alternatively, for complex page layouts, they will be asked to draw the page layout themselves.   Character identification At this step, we ask the “crowd” to simply enumerate all the characters they can identify in the current page. Characters are identified by reading their names in the text, recognising them from experience or simply giving a general statement about the character (e.g., “ masked man” may be referring to  Batman). Using state of the art symbolic learning algorithms we fusion generic and specific information (e.g. if  Batman and “masked man” both have high quality scores in the same context, they will both denote the same concept and will be considered as valid annotations).    Places identification We ask the annotators to simply enumerate all the places they can recognise on the current page. We are particularly looking for named places (e.g.,  Gotham City, NY, planet Mars), but will also ask the annotator to mark any generic place that he might consider important for the scenes in the page (e.g., “ the interior of a bank“ [in case of a robbery], “inside a space ship” [in case of a battle in space]). These are exactly the kind of very specific annotation tasks for which state of the art image recognition algorithms are expected to fail.    Events identification This is yet another highly specific recognition task. Annotators are asked to describe the most important events occurring in the page. The solutions generated at this step, together with the annotations obtained in the previous steps will be used to further build the ComicsML encoding of the page. Comics scholar Scott McCloud stresses the role of ellipsis (“the blood in the gutters“ – the space between two panels) as an artistic mean for authors to engage their readers, and describes a typology of these transition spaces (McCloud, 1993). ComicsML allows us to declare such transitions through the #ana attribute of the cbml:panel element, giving us the possibility to investigate their use and their distribution across different cultural spaces (France/Belgium, Japan/Korea, USA).    Figure 2. Annotation interface (drawing page structure-left, narration elements-right)    Non-visual cues Comics are a special medium, making use of the visual to depict all other non-visual senses, with the help of different drawing “tricks”, such as:  Smoke coming out of a cigarette may engage the reader’s smelling sense Onomatopoeia form a particular language of their own; comics and especially manga authors have proven great creativity when it comes to expressing different sounds via stylised text (e.g., “ POW!” – punch, “ BAM!” – gunshot)  Horizontal lines around a car suggest the car is moving at high speed, while around a ball, they express the ball’s movement  Researchers could study, for instance, the drawing style of an author and his use of non-visual cues, and go as far as creating onomatopoeia dictionaries for comics (to our knowledge, such dictionaries already exist for manga, but not for American nor European comics). At the end of this stage, we should be able to generate a reasonably complete ComicsML encoding of the current page (see Fig. 3).   Figure 3. A fragment of the ComicsML encoding for the page presented above     Conclusions We have presented the outline of our crowdsourced annotation system for comics, as well as details of how we have designed our tasks, having in mind three main aspects: the limits of current digital comic book formats, the specifications behind the ComicsML metadata schema and theoretical principles of comics as a medium (Eisner, 1985). Last, we have presented the way in which the collected results are merged into the final ComicsML encoding and have briefly discussed some potential applications.  ",
       "article_title":"Adding Semantics To Comics Using A Crowdsourcing Approach",
       "authors":[
          {
             "given":"Mihnea",
             "family":"Tufis",
             "affiliation":[
                {
                   "original_name":"Pierre and Marie Curie University, Paris 6 (UPMC), France",
                   "normalized_name":"Sorbonne University",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/02en5vm52",
                      "GRID":"grid.462844.8"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-05",
       "keywords":[
          "information retrieval",
          "xml",
          "data modeling and architecture including hypothesis-driven modeling",
          "encoding - theory and practice",
          "English",
          "audio, video, multimedia",
          "crowdsourcing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Over the last two decades, an important amount of work has been carried out by cultural heritage institutions to make their collections available online. How are these digitized collections discovered, discussed and shared on the Web?   “The future of online digitized heritage: the example of the Great War” is a research project conducted by the BnF, the BDIC and Télécom ParisTech as part of the Cluster of Excellence, Pasts in the present, Investissements d'avenir, réf. ANR-11-LABX-0026-01 (Valérie Beaudouin, Philippe Chevallier, Lionel Maurel, Josselin Morvan, Zeynep Pehlivan, Peter Stirling).    The digitized heritage around the First World War (WW1) is an ideal area for such analysis: many digitized collections, centenary anniversary (2014–2018) and an important activism around the Great War. Family, local and militant history are the main motivations of persons that get involved in the history of WW1 (Offenstadt, 2010). Who are the users who publish or discuss around WW1 on the web? Are they amateurs, experts, academics? How do they publish, share and comment digitized documents? These questions matter for the development of Digital Humanities, but also to those in charge of managing the collections. We conduct an exploratory analysis to understand how the activity around the war is visible in the digital space. Our research consists of two steps:  Identification and categorization of web sites dedicated to WW1 and network analysis of the links between those sources, in order to draw an overall cartography of Web activity on WW1 and to identify the position of amateurs. Focus on one of the main nodes in the web cartography (a forum dedicated to WW1), to analyze the forms of circulations of digitalized documents.  We also conducted a series of semi-structured interviews with participants.   Corpus and methodology The web is ephemeral: working on web archives allows us to rely on a stabilized corpus, which guaranties the reproducibility of the research (Brügger, 2013). Bibliothèque Nationale de France (BnF), in charge of archiving the French web, created a specific web archive collection dedicated to the centenary of the WW1, based on web sources chosen by librarians. The process of archiving is regularly repeated. Our research relies on the November 2014’s archive (9 698 633 Urls for a total size of 800 Gb). The first step of our analysis consists of generating an oriented network graph on web archives to study the relations (materialized by hyperlinks) between web sources related to WW1. We can consider a link as a pragmatic activity of citing or referencing sources (documents, web sites etc.), although in our approach we are not able to qualify the exact nature and function of the link (Saemmer, 2015).  To make the large-scale graphs readable, the nodes in the graph correspond to the  seed urls chosen by librarians and all the data crawled from each url is agregated to it. Librarians, in charge of web archiving, qualify the producers (or authors) of the websites. Depending on this categorization, we will distinguish institutional websites (public and official, red) and personal websites (personal and associative, blue).     Mapping WW1 on the Web The network graph allows to evaluate the place of “amateurs” websites compared to institutional sources.  Basic characteristics of our network are calculated using Gephi (Bastian et al., 2009). The network consisted of 514 nodes and 3713 edges with an average degree of 7.22. The average network distance between all pairs of nodes (average path length) is 2.78 edges with a diameter (longest distance) of 8 edges. The clustering coefficient (the degree to which they tend to cluster together) is 0.27 and the modularity index is 0.28. Overall, WW1 French network is made of highly connected pages (∼2.7 edges per node) and shows small-world scale-free network properties (Humphries et al., 2008; Watts et al., 1998) with high clustering coefficient, short average path length and a degree distribution following a “power-law” (smallworldness index = 15). More than half of the sources (52%) come from personal websites that are involved in WW1 as a serious leisure, but not as a profession. The institutional sites are less present (36 %).  To detect influential actors on our network, we use the  degree centrality which is simply the number of direct relationships that an actor has, the sum of outgoing and incoming links. The network is visualized in Figure 1 by using Gephi, Force Atlas 2 algorithm (Jacomy et al., 2014) as layout with node sizes proportional to their degree centrality.     Figure 1.Cartography of web sites dedicated to WW1 (degree>30)  The two main actors are centenaire.org, the official site dedicated to the Centenary, on one side and pages1418.mesdiscussions.net, a web forum managed by amateurs, on the other. Around them, we can distinguish two clusters: the red gathers the institutional sites, while the blue (bottom) gathers all the personal web sites that are intensely interconnected. The forum (Pages 14-18) has a specific position: although it is immersed in the middle of the amateur sphere, it is well connected to institutional sources, because users of this forum are mediators to institutional ressources. The forum constitutes a community of practice (Lave and Wenger, 1991): questions and answers on one side and discussions on the other are two kinds of interactions that allow to share and elaborate knowledge collectively (Conein and Latapy, 2008). Thanks to the forum, a lot of personal websites emerged, each of them dedicated to a specific regiment of foot. Experts of those regiments are the authors of those website that accumate documents (public and personal documents and photographies) on each soldier, each battle, each place occupied.  Degree centrality considers only direct relationships, we also use Hubs and Authorities, known as HITS (Kleinberg, 1999) algorithm. A hub is defined as an actor that points to many other actors and an authority is defined as an actor pointed by many others. HITS algorithm calculates two scores for each actor, hub score and authority score, in a mutually reinforcing way based on the idea that a good authority must be pointed to by several good hubs while a good hub must point to several good authorities (Table 1).     Top 5 Authorities Authority Score Top 5 Hubs Hub Score   memoiredeshommes.sga.defense.gouv.fr 0.0199 pages14-18.mesdiscussions.net 0.0379   centenaire.org 0.0135 centenaire.org 0.0276   crid1418.org 0.0127 guerre1418.fr 0.0251   chtimiste.com 0.0125 combattant.14-18.pagesperso-orange.fr 0.0175   gallica.bnf.fr 0.0124 verdun-meuse.fr 0.0163   Table 1 : Hubs and Authorities Some authorities, like memoiredeshommes.sga.defense.fr and gallica.bnf.fr, have a very specific profile: as documents warehouses, they receive a lot of links but do not point to other resources. The Centenary website is at the same time a top level authority and hub. The forum, pages1418.mesdiscussions.net, and other personal websites are hubs that point to a relatively large number of authorities.   Forum activity of citations Based on the specific position of the forum, we decided to focus on it. This forum, founded in 2004 by an amateur, has gradually become a reference site in terms of exchanges and discussions on the WW1. It is a highly active platform with about 400,000 messages in 10 years and about 18,000 subscribers.   Figure 3. Number of messages published in the forum by year  This forum was archived by BnF in January 2015 and we rely on this corpus for our work. For analysing the activity of citing, we classified the citations into four categories:   Message_Citation: using a part of previous message  Quote: text inserted using another source Links: hyperlink by using 'link' tag Images: hyperlink by using 'img' tag  As shown by Figure 4, while the usage of quote and message_citations stays stable, the usage of links and images increases over time according to the increase of digitized documents available on line.    Figure 4. Citations distribution over time  In the corpus, we identified 255,374 image or link citations, an average of 1 citation for 2 messages. We extracted their domain name. The ten more cited domains, which represent 60% of total links, are shown in Table 2.    Netloc Fréquence   images.mesdiscussions.net 91990   pages14-18.mesdiscussions.net 9636   www.memoiredeshommes.sga.defense.gouv.fr 8984   gallica.bnf.fr 6721   www.asoublies1418.fr 4979   usera.imagecave.com 4274   74eri.canalblog.com 4019   www.servimg.com 2674   imageshack.us 2592   largonnealheure1418.wordpress.com 2407   perso.orange.fr 1916   www.casimages.com 1588   www.memorial-genweb.org 1574   www.hiboox.fr 1492   www.pages14-18.com 1387   pagesperso-orange.fr 1372   perso.wanadoo.fr 1200   albindenis.free.fr 1129   images.imagehotel.net 1041   images4.hiboox.com 1016   Table 2. Most frequent hosts extracted from url citations The most notable result is the importance of image hosting services. Instead of giving a direct link to an online source, people use hosting services (ex: images.mesdiscussions.net). We can estimate that more that 100 000 citations (40% of total) are of this kind where almost half of them point to images. Users are not confident with the life of web sources. They prefer to download and post the picture than to point to a link which may disappear. The image, in this case is directly available and visible into the post.  Secondly, the forum itself is the most cited website: due to his long history and intense activity, a lot of questions have already been answered. An activity of knowledge management consists in answering a question by signaling an old topic about the same topic.  Memoire des hommes and  Gallica are the most cited institutional sites. The first one is a gold mine for genealogists who search for ancestors dead during the war and for historians interested in the battles and regiment history. Gallica is a huge warehouse (4 millions of documents): there is a need to identify what kind of documents people are looking for. We extracted the titles of the documents cited and made a content analysis with Iramuteq (Reinert, 1993) (Figure 4); we identified three kinds of documents: photographs and newspapers, official documents (“journaux officiels”, “décrets”) and documents related to the history of regiments (“historiques des Régiments”) from the department of Defense.     Figure 5. Text Mining clustering of title documents cited from Gallica  Users of the forum also cite a lot of personal web sites dedicated to specific aspects of the war: the history of the soldiers from a specific regiment or squadron for example.   Conclusion  To understand what internet users do with digitized archives, we explored systematically the websites discussing WW1. We developed an original methodology combining web and text mining methods applied to web archives. This method is transferrable. It includes expert advice to qualify the relevant variables for qualifying and selecting sources. Our analysis shows a great involvment of amateurs (more than half of the websites) in the memory of WW1. They participate to a network of personal websites that gives a specific vision of WW1, more focused on soldiers, regiments, geographic places, objects, remains of war, close to micro-history. A the core of this network, we find the forum, which is the place for interactions and discussions on documents, data, interpretations.  In doing research with digitized resources, amateurs collectively increase their professionalism. Although they do not have the status of academics, they acquire methods in exploring and citing their sources, commenting and sharing with other users. Those amateurs, in doing research with digitized resources, play a major role by discovering and citing institutional heritage documents, adding to their value and creating networks and resources.  ",
       "article_title":"The Great War on the Web: the Making of Citing and Referencing by Amateurs",
       "authors":[
          {
             "given":"Valérie",
             "family":"Beaudouin",
             "affiliation":[
                {
                   "original_name":"Telecom ParisTech, France",
                   "normalized_name":"Télécom ParisTech",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/01naq7912",
                      "GRID":"grid.463717.0"
                   }
                }
             ]
          },
          {
             "given":"Zeynep",
             "family":"Pehlivan",
             "affiliation":[
                {
                   "original_name":"Telecom ParisTech, France",
                   "normalized_name":"Télécom ParisTech",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/01naq7912",
                      "GRID":"grid.463717.0"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "hypertext",
          "media studies",
          "internet / world wide web",
          "user studies / user needs",
          "digitisation, resource creation, and discovery",
          "English",
          "networks, relationships, graphs",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Science gradually developed into an established sociocultural domain starting from the mid-17 th century onwards. In this process it became increasingly specialized and diversified. Here, we investigate a particular aspect of specialization on the basis of probabilistic topic models. As a corpus we use the Royal Society Corpus (Khamis et al. 2015), which covers the period from 1665 to 1869 and contains 9015 documents   Of these 205 years, 159 years actually contain documents (mean = 56.7, median=36, sd=61.6, min=12, max=444) .  We follow the overall approach of applying topic models to diachronic corpora (Blei and Lafferty 2006, Hall et al. 2008, Griffiths and Steyvers 2004, McFarland et al. 2013, Newman and Block 2006, Yang et al. 2011) to map documents to topics. Probabilistic topic models (Steyvers and Griffiths 2007) have become a popular means to summarize and analyze the content of text corpora. The principle idea is to model the generation of documents with a randomized two-stage process: For every word      w   i     in a document  d select a topic      z   k     from the document-topic distribution    P (   z   k   | d )   and then select the word from the topic-word distribution    P (   w   i   |   z   k   ) .   Consequently, the document-word distribution is factored as follows:     P (   w   i   | d ) =  ∑  k    P     w   i   |   z   k     P     z   k   | d   .     This factorization effectively reduces the dimensionality of the model for documents, improving their interpretability: Whereas    P     w   i   | d     requires one dimension for each distinct word (10s of thousands) per document,    P     z   k   | d     only requires one dimension for each topic (typically in the range of 20-100). Topics are thus not given explicitly for each document, but constitute  latent variables: A variety of approaches exist to estimate the document-topic and topic-word distributions from the  observable document-word distributions. We use Gibbs-Sampling as implemented in Mallet (McCallum 2002).  For the preliminary analysis in this paper, we process documents as is, without segmenting them further into pages, only excluding stop words but not performing lemmatization or normalization in order to stay reasonably close to the original source. We experimented with the number of topics ranging between 20 and 30, reporting here results on 24 topics. Cursory analysis of multiple runs with different seeds (Steyvers and Griffiths 2007) shows that the resulting topics are rather stable.   Table 1 displays the top words for the topics with manually assigned labels and their overall percentage of occurrence. We can roughly distinguish four groups of topics; three non-thematic groups and one thematic. The first group comprises topics arising from documents in  Latin and  French, some of which are also translated into English. The second group  Formulae and  Tables relates to highly formalized modes of information presentation. The third group of topics is also clearly non-thematic but relates to general scientific processes:  Observation and  Experiment both contain rather general verbs and adjectives in addition to nouns.  Events contains words describing remarkable events.  Headmatter includes formulaic expressions typically occurring at the beginning and end of documents that are letters. All topics in this group are relatively frequent. Finally, the topics in the fourth group ( Geography through  Chemistry), consisting mainly of nouns, indeed have a fairly clear thematic interpretation.    Label Words %   Latin  quae quam sed ab sit vero hoc ac sunt esse qui etiam autem pro erit inter quo aut sive 6.4   French  la le les des en du par dans qui il une qu pour ou ce sur ne au je 1.3   Formulae cos equation sin equal series point equations number line terms form values curve 4.7   Tables  weight water oo oz parts gr grain io grains fat increase weights grs passed urine specific 1.7   Observation great made make parts found body time small part water nature long good put find 10.4   Experiment present general subject case results similar nature author state result cases fact 7.3   Events great time account stone ground house fire letter place miles found side stones 5.9   Headmatter years year author society age number time royal life great letter account part letters 5.4   Geography  water sea tide high found river coast north land tides miles height surface great level 3.1   Meteorology  day ditto rain wind cloudy weather fair clear april year days night march july june 3.2   Botany  leaves plant plants tree tab bark folio foliis trees seeds seed flowers species fruit leaf 2.9   Reproduction  cells animal blood fluid eggs membrane found egg part animals ova size young 3.0   Cells  fibres structure form surface portion cells anterior part section side posterior 2.7   Paleontology  part bone bones teeth surface upper side lower anterior length posterior tooth large 2.5   Physiology  blood heart muscles part animal nerves vessels left parts stomach bladder body 5.5   Galaxy  distance position stars star obs small hill double equatorial vf diff st magnitudes nebula 1.6   Terrestrial Magn.  observations needle ship magnetic direct force made variation observed north diurnal 2.6   Solar System  sun time observations moon made observed difference observation clock latitude 5.5   Thermodyn.  air water heat temperature experiments tube experiment glass made time mercury 4.2   Mechanical Eng.  made length weight end diameter iron instrument experiments brass part point line 4.5   Electromagn. force electricity current wire action body power direction fluid motion surface effect 3.8   Optics  light rays glass eye red colours spectrum colour surface lines angle white blue object 3.7   Metallurgy  water acid salt grains quantity iron found solution colour substance experiments gold 4.8   Chemistry  acid water solution gas oxygen hydrogen carbonic cent action obtained salt potash 3.4    Table 1: Top words and percentages for topics  To investigate topical trends in the corpus we follow the approach in (Hall et al. 2008), by averaging the document-topic distributions for each year    y  :     P (   z   k   | y )   =     1 / n    ∑    d   j   ∈ y    P (   z   k   |   d   j   )     with    n   the number of documents in a year.  Figure 1 shows a selection of five topics with the most pronounced change over time. Interestingly, some of the major changes occur for non-thematic topics: The topic  Observation declines sharply from over 30% to less than 1 %. The topics  Experiment and  Formulae on the other hand increase starting around 1750. This indicates a substantial paradigm shift over time. Indeed, as Gleick (2010) vividly describes, the early stages of the Royal Society were largely devoted to observing and reporting about natural phenomena. The non-thematic topic  Latin reaches its peak in the early 18 th century, and the thematic topics  Cells and  Chemistry show a clear increase with the beginning of the 19 th century.    Figure 1: Major topical trends for selected topics  To gain a better understanding about the correlation of topics, we cluster them hierarchically on the basis of the Jensen-Shannon divergence between the topic-document distributions:    P   d | z   = P   z | d   /  ∑  j    P   z |   d   j         Topics that typically co-occur in documents have similar topic-document distributions, and thus will be placed close in the tree.   Figure 2: Hierarchical clustering of topics by their topic-document distribution  The resulting tree in  Figure 2 indeed identifies meaningful subgroups. Cutting the tree into six groups -  Nature, Latin, Medicine, Astronomy, Engineering, Matter - allows us to investigate the overall topic distribution over time ( Figure 3 with  Latin left out):     Figure 3: Distribution of topic groups over time  The topic group  Nature comprising reports of all kinds of natural phenomena (Gleick 2010) clearly decreases over time, which is partially to be attributed to the strong decrease of the topic  Observation in this group. The topic groups  Medicine and  Astronomy increase over time, whereas the topic groups  Engineering and  Matter also generally increase but with some intermediate peaks. Similar to the overall trends at the level of individual topics ( Figure 1), the biggest overall change occurs in the 2 nd half of the 18 th century.  Looking at the individual trends together,  Figure 3 clearly indicates topical diversification: Until around 1770, the dominance of the topic group  Nature leads to a highly skewed distribution of topic groups, whereas after 1770 topic groups are distributed much more evenly. The amount of skew can be characterized by the Shannon-Entropy:     H (   P   y   ) = -    ∑  k    P     z   k   | y       l o g   2   P     z   k   | y       of the year-topic distributions    P (   z   k   | y )   (Hall et al. 2010), with highly skewed distributions having low entropy. Indeed as can be seen in  Figure 4 (left), the entropy ( ent) increases fairly consistently during the 18 th century and levels out during the 19 th century, reflecting a general increase of topical diversity over time.  It is interesting to compare this with the mean entropy of the  individual document-topic distributions ( ment):       H   m e a n       P   y     ≔ 1 / n    ∑    d   j   ∈ y    H (   P     d   i     )     with    n   the number of documents      d   j     in year    y  . This measure decreases over time, i.e., while the overall topical diversity increases, the individual documents become more specific in terms of their topic distributions.  The difference between the entropy of year-topic distributions and mean entropy of individual document-topic distributions,    J S     P   y     = H (   P   y   ) -   H   m e a n   (   P   y   )    is the Jensen-Shannon divergence, which is usually applied to two distributions, generalized to the    n    topic distributions of all documents published in year  y. The two opposing trends of these quantities lead to a constantly increasing Jensen-Shannon divergence, with a particularly sharp increase between 1750 and 1800.  Figure 4 (right) depicts similar trends based on the 24 individual topic distributions. At this level, the year-topic entropy ( ent) shows less of a clear trend, but mean entropy ( ment) also clearly decreases, and consequently the Jensen-Shannon divergence clearly increases. Thus, at both levels of abstraction we can observe a clear diversification of the topics assigned to the individual documents. This strongly indicates a growing separation of individual scientific disciplines over time.    Figure 4: Entropy (ent), mean Entropy (ment), and Jensen-Shannon Divergence for topic groups (left) and individual topics (right)  As an alternative perspective on topical entropy  Table 2 gives examples of authors with more than 20 papers. The first three authors have the lowest entropy. The dominating topics for Cayley and Owen clearly characterize their main theme of work. Conversely, Rev. John Swinton’s top topic  Headmatter (62%) does not really reflect the overall theme of his publications (Orientalism), but rather their style as letters to members of the Royal Society – the dominant form of publication in this period. The second three authors have the highest entropy, their three top topics together amount for less than 50% of their overall topic distribution. However, they do characterize the main line of work of the authors in question fairly well.     author Papers Ent Ment Jsd Years Top Topics   Arthur Cayley 30 1.26 1.12 0.14 1850-1866 Formulae   Richard Owen 26 1.83 1.58 0.25 1843-1869 Paleontology   John Swinton 35 2.50 2.05 0.45 1753-1774 Headmatter   John Davy 58 4.05 3.33 0.72 1800-1856 Experiment, Chemistry, Physiology   William Watson 39 4.03 3.09 0.95 1739-1778 Events, Observation, Botany   Edmond Halley 65 3.93 2.75 1.18 1683-1731 Solar System, Observation, Latin    Table 2: Authors with minimum entropy (top) and maximum entropy (bottom)  In this paper we have analyzed the progression of topics in a corpus of the Royal Society of London. Our main result is the observation that the overall mixture of topics becomes more diverse over time, while the topics of individual documents become more specialized. These two opposing trends lead to a topical fragmentation of scientific discourse, which can be quantified by means of the generalized Jensen-Shannon divergence between the topic distributions of individual documents per time period. We are currently working on consolidating our analysis, experimenting with documents segmented into pages, focusing the analysis on different text types, and more carefully evaluating the resulting topic models (McFarland et al. 2013).  Of course, topic models only provide one, rather broad perspective on diversification of domain specific language. We plan to apply our approach also to other levels of linguistic analysis, such as terminology or grammar. ",
       "article_title":"Topical Diversification Over Time In The Royal Society Corpus",
       "authors":[
          {
             "given":"Peter",
             "family":"Fankhauser",
             "affiliation":[
                {
                   "original_name":"IDS-Mannheim, Germany",
                   "normalized_name":null,
                   "country":"Germany",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Jörg",
             "family":"Knappen",
             "affiliation":[
                {
                   "original_name":"Universität des Saarlandes, Germany",
                   "normalized_name":"Saarland University",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01jdpyv68",
                      "GRID":"grid.11749.3a"
                   }
                }
             ]
          },
          {
             "given":"Elke",
             "family":"Teich",
             "affiliation":[
                {
                   "original_name":"Universität des Saarlandes, Germany",
                   "normalized_name":"Saarland University",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01jdpyv68",
                      "GRID":"grid.11749.3a"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-24",
       "keywords":[
          "semantic analysis",
          "corpora and corpus activities",
          "data mining / text mining",
          "historical studies",
          "content analysis",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" We present a way of disaggregating the concept of anonymity and linking it to particular democratic goods. We demonstrate its value in the particular context of studying online commenting by contrasting three distinctive commenting regimes used by the Huffington Post (HuffPo) in the period January 2013 - March 2015. But we think this may have wider relevance for scholars looking at online communication, as modes of identity disclosure are an unavoidable yet consequential design feature of online communication platforms. In this paper we disaggregate three aspects of anonymity: traceability, which we argue relates to inclusion; durability, which influences deliberative quality through varying levels of communicative accountability; and connectedness, which ties users to real-world relations.   Decisions about online architectures have a crucial influence on the quality of communication, but are often adopted by default or with regard to other factors. In the realm of online commenting, we observe a trend of commenting platforms being outsourced to the Facebook commenting API. These changes are often framed and justified in terms of a shift from an anonymous but easily abused environment, to one in which users operate under real-name identities. When we think in terms of a choice between anonymous and real-name architectures, it seems that there is a simple trade off between the goods and dangers associated with anonymity, such as trolling on the one hand and the freedom to express one’s views without fear of recrimination on the other, and the goods and dangers of real-name communication, which ties users more closely to discursive norms of civility but which also risks reproducing offline power relations.  We propose to analyse the concept of anonymity in three dimensions. We argue that cross-platform connectedness is associated with increasingly personalised communication as well as less communicative engagement between individual users; durability (continuity) of identity is associated with greater levels of civility, reduced trolling, and higher quality of deliberation (more reason-giving, etc); traceability is associated with exit - that is, people worried about traceability leads people to make a binary decision, to opt out, and the increasingly pervasive traceability promotes exit. Thinking of anonymity in these terms helps resolve some apparent trade-offs: it may be possible to capture the benefits of communicative accountability without the drawbacks of either the abusive space of easy anonymity or of the exposure to offline power relationships associated with real-name spaces.  Disaggregating anonymity Anonymity exists on one end of a spectrum of degrees of disclosure of identity. In the context of commenting, anonymity means contributions to a discussion are not visibly linked to any particular commenter. Pseudonyms - names chosen by commenters - allow users to keep their real identity private if they wish, yet allows them to maintain a persistent alternate identity in the context of the forum in question. You might have an identity as a commenter on the  Guardian that you keep separate from your professional networks and from your social networks. Real name comments, obviously enough, appear under your real name, not a pseudonym.   While this is a useful starting point, many scholars analysing the different degrees of disclosure on online platforms have sought more fine-grained distinctions (Marx, 1999; Ruesch and Märker, 2012). We favour splitting online anonymity into three dimensions: traceability, durability, and connectedness.   Traceability refers to the extent to which your contributions can be traced to your real identity. Traceability is distinct from disclosure of identity to fellow commenters. You can make comments under a pseudonym and yet it may be possible (with some effort) for advertisers or the National Security Agency (NSA) to trace your real identity. Nissenbaum, for instance, argues that anonymity online, in the sense of ‘conducting one’s affairs, communicating, or engaging in transactions anonymously in the electronic sphere... without one’s name being known’, is undermined by technologies that have made it possible to track and / or piece together (‘infer identity from non-identifying signs and information’) the real identities of citizens online even when they are withholding their names or using pseudonyms (Nissenbaum, 1999). Because traceability is strictly distinct from the question of anonymity or pseudonymity  with respect to other commenters, it does not help us grasp the relation between disclosure of identity and discussion quality.   Durability refers to the ease or difficulty with which online identities can be acquired and changed. Where new pseudonyms are easy to create, online identities are disposable; if you acquire a reputation for abusive or untrustworthy behaviour you can just create a new pseudonym and start again. As Resnick and Friedman put it, cheap pseudonyms create ‘opportunities to misbehave without paying reputational consequences’ (Resnick and Friedman, 2001). Users are more likely to stick with a particular name, exposing them to the reputational consequences of their behaviour. The durability dimension is particularly important for the possibility of holding commenters accountable for claims they make, enabling challenges in terms of consistency, and exposing uncivil or abusive commenters to sanctions.   The third dimension has to do with  connectedness or bridging across different platforms and contexts. The most visible example of cross-platform connectedness is sites enabling users to login or register through a third party - typically Google or Facebook - rather than filling in a site-specific form. Connectedness also involves reputation, but is a global rather than local reputation. The durability or stickiness of an online identity is a necessary condition for building a local reputation, but it need not be connected to the wider reputation, a cross-referenced, cross-platform (including real life) reputation, of the sort you would want if you were renting out your apartment to someone you didn’t know (as in Airbnb).    The Experiment The two changes introduced by HuffPo provided us with a natural experiment. We collected more than 50 million comments on more than 50,000 articles featured on the HuffPo front page in the period January 2013 - March 2015.  The first of these phases allowed anonymous commenting. At this time, the platform experienced aggressive ‘trolling’ and the use of multiple accounts. In December 2013 HuffPo moved to regulate its forum by requiring users to authenticate their accounts through Facebook. On the face of it, little changed in this pseudonymous environment: user names and avatars remained, but behind the scenes Facebook’s database helped weed out fake accounts. Users did not have to create a new online identity, give up their old pseudonyms, or change the appearance of their HuffPo commenter profile. We read this first change as primarily about durability of identity.  In June 2014 HuffPo changed to commenting through Facebook, meaning that HuffPo user profiles were  replaced by Facebook profiles in a ‘real name’ environment. In this phase, comments appear below the line of the news article under the user’s real name, as well as - depending on a user’s privacy settings - appearing simultaneously on their Facebook page. This may have the effect that users comment on the HuffPo but speak to an audience located on Facebook. We read interpret this change as a move to more integrated and connected online identities.    While we found more durable online identities were associated with greater civility but less participation in online commenting, we also observed a shift in the sorts of issues on which users most frequently comment. This might be normatively encouraging: the relative rise in articles tagged Gay Voices, we speculate, may be a result of the inhibition of hostile and offensive commenters. Here greater civility seems to promote more participation and inclusive discussion. However, we also note a more general shift away from conflictual and politicized topics and towards ‘safer’ topics of celebrity, lifestyle and consumer issues. Turning to the second commenting change, we found that the Facebook phase exhibited markedly less argumentative engagement relative to the 'pseudonymous’ phase, as evidenced both by an overall reduction in the proportion of replies, and by measures of deliberative quality (Fredheim et al., 2015a, 2015b). This finding points in the same direction as a recent Pew survey that found people were less likely to discuss a controversial issue on social media than in face to face conversation (Hampton et al., 2014).  With these findings in mind, we argue that the pseudonymous phase harnessed the beneficial effects of increased durability, without introducing the costs of connectedness. More broadly, we can link these dimensions of identification to particular democratic goods. The dimension of traceability seems to be associated with inclusion and exclusion, as users make a binary choice to stay on the platform. The dimension of durability or continuity of identity is associated with deliberative quality in so far as (theoretically) it exposes users to communicative accountability and (empirically) is associated with a higher density of reason-giving in commenting. The dimension of connectedness ties users to real-world relations. This favours civility, but it also promotes what Schudson calls ‘sociable’ conversation rather than more adversarial ‘democratic’ conversation (Schudson, 1997). We argue that durable and disconnected identities are more conducive to productive, issue-based debate between heterogeneous users. This bears on discussions of polarization. It also bears on the drive towards integration of social media and news discussion.   ",
       "article_title":"Anonymity and Online Discussion: A New Framework for Analysis",
       "authors":[
          {
             "given":"Rolf",
             "family":"Fredheim",
             "affiliation":[
                {
                   "original_name":"CRASSH, University of Cambridge, United Kingdom",
                   "normalized_name":"University of Cambridge",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/013meh722",
                      "GRID":"grid.5335.0"
                   }
                }
             ]
          },
          {
             "given":"Alfred",
             "family":"Moore",
             "affiliation":[
                {
                   "original_name":"CRASSH, University of Cambridge, United Kingdom",
                   "normalized_name":"University of Cambridge",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/013meh722",
                      "GRID":"grid.5335.0"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-02",
       "keywords":[
          "English",
          "internet / world wide web"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" One of the most frustrating challenges facing practitioners of Dance is the need to use spoken/written language to reference non-verbal movement. The non-verbal to verbal, and vice versa, is not a challenge isolated to practitioners of dance, but is a frustration shared amongst researchers working to represent movement through technological means. Perhaps it is enough to allow the movement, non-verbal as it is, to speak for itself. Then again, it is often through verbal language that we can make meaning from movement. The intersection of language and movement is the point at which meaning-making enforces the mind-body connection, and it is often how embodied experience is transmitted. Two HCI research projects that have studied this connection between words and movement as a means to the classification and automatic recognition of movement, ARTeFACT and POEME, are now collaborating in a new project: Schrifttanz.  ARTeFACT seeks to enable automatic recognition, identification, annotation, and retrieval of movement-based data from streaming video. A lofty goal that has been approached through the generation and analysis of both codified and abstract movement using motion capture data collected at 120 Hz with a Vicon 8-camera motion capture system and a modified Plug-In Gait full body marker set with 38 infrared reflecting markers placed on performers. Early research included instances of iterative choreography created from accelerometer data and descriptions of movement through verbal language by observers (Coartney and Wiesner, 2009). The second phase of the project involved the capture of over 200 codified dance ‘steps’ in various genres, the development of an ontology, and the creation of IDMove, a tool through which we were able to automatically identify and name dance movement from single dancers (Wiesner et al., 2011, 2014). The third phase ventured into the realm of identifying abstract movement that represents the conceptual metaphor CONFLICT, as introduced by Lakoff and Johnson (2003). This data, derived from 7 dances about conflict and 19 CONFLICT terms, 396 different sections of movement, each lasting from 2-120 seconds, were captured and categorised. Critical reviews (written) about the dance works were also collected. Through statistical analysis of written and danced texts about CONFLICT, we distinguished body parts frequently used, structural preferences (e.g. stage spacing), and choreographic time spent on the different CONFLICT terms in total and per dance (Wiesner et al., 2015). The findings were validated by a small ‘crowd-sourced’ experiment, and movement ‘rules’ were developed per term in order to enable identification of a concept through movement (terms include victim, attack, surrender, struggle, conquer, hero, victory, survive, etc.) (Wiesner and Stalnaker, 2015). Concordances and collocation studies aided in the investigation of the intersections of words and movement, through the phenomenological approach of dancers’ descriptions and perceptions of viewers in the form of the reviews written by dance critics. A final step has been to align the non-verbal and verbal output with concepts used in Laban Movement Analysis (Body, Effort, Shape, and Space). ARTeFACT has collected a wealth of data from captured movement, from written descriptions and articles based on the dances, and from a broad LMA perspective. In the future, various modes of comparison (e.g. the metaphor PEACE, general language studies, etc.) will further test the system. Other researchers have incorporated LMA into their study as a means to identify movement (including studies on rats’ play) (Foroud and Pellis, 2003), yet their focus is on deconstruction. POEME and ARTeFACT coincide in that rather than provide a deconstruction of the movement they both seek to characterise the essence of that movement, an approach inspired by LMA. In the case of ARTeFACT, this is accomplished by describing the relations between parts of a movement that accompany the portrayal of abstract concepts in dance. For POEME, it means finding computational means to summarize a movement experience so that it can be studied as a whole. Where ARTeFACT has taken a more actively analytical approach - which resulted in a set of rules that describe stereotyped motion consistent with conceptual metaphors - POEME has taken a more abstract approach, letting data collection from a large number of human responses gradually expose relationships between verse and movement. POEME (Portrayal of Ephemeral Movement Experiences) is a mobile website (http://poeme.iat.sfu.ca) that interprets movement data, captured through a MARG sensor array commonly found in smartphones, into insightful, witty and whimsical poetry. The goals of POEME are twofold: 1) to create a playful game that can inspire movement exploration and 2) to explore new methods of classifying the nuances of bodily experiences through poetry and in turn understand how bodily knowledge can more easily be transmitted and articulated through words. POEME, inspired by the Japanese form of poetry known as haika no renga (comic, collaborative poetry), involves the social creation of poems through turn taking (Basho, 1998). Creating these linked poems requires participants to respond to previous stanzas with original verse creating a ‘movement poem’, an interleaved work of verses of words and phrases of movement. We believe, as Herbelot notes, that poetry can be “analysed along the usual dimensions of prosody, syntax, semantics, etc.” (2015). In POEME, each verse follows a rigid template that follows the form of a Parts of Speech Poem (PoSP). This PoSP template allows for very simple production of verses. Each poem begins with a noun, followed by two adjectives, two verbs, and an adverb. POEME composes verses from a large dictionary of English language words, based on measurements taken from prior movement responses. In an initial study, the system was trained by collecting movement responses to human- and machine-composed verses. In a later study, training was focused on two wordlists that depict different themes of movement: the ‘stillness wordlist’ (75 words that relate to little or no dynamic change in movement, e.g. frozen, still, serene); and a ‘motion wordlist’ (250 words related to the mechanics or physics of motion, e.g. buoyant, centripetal, accelerated). Using a Naive Bayes algorithm, POEME can differentiate between stillness and motion with 97% accuracy (Cuykendall et al., 2016). Schrifttanz is a unique project that combines the scale of data collection, which POEME offers, with the nuanced language model of ARTeFACT. In Schrifttanz we explore if POEME can recognise conceptual differences in movement, specifically concepts related to CONFLICT, which have been studied in detail during the creation of the ARTeFACT system. Three dancers train the POEME agent by recording movement based on the rules defined in ARTeFACT. These recorded movement sessions are used to model conceptual relations to movement in POEME. This allows us to generalise previous findings from ARTeFACT into a new sensor modality. Also, a Laban/Bartenieff Movement Analysis using the rules-based movements establishes a secondary set of language elements and validates distinctions between movement rules. A computational agent in POEME learns to classify these movements according to the metaphor terms and their associated movements generated by the rules. We then compare POEME’s ability to classify these movement metaphors with existing capabilities in ARTeFACT to validate and generalise previous findings and establish the reliability of the POEME system. Concurrently, POEME composes verses comprised of natural language words gathered through the written articles and reviews based on dances representing the conceptual metaphors identified in ARTeFACT. Free form responses to these verses are collected using POEME’s normal training procedure. A machine learning-process associates features of movement with the words. This allows POEME to generate relevant responses to movement. We explore methods of supplementing POEME’s verse generation by including the conceptual metaphor model. Words, like movements, have layers of meaning and can be recombined to create a multitude of sentences or sequences that all vary in their underlying meaning. As philosopher Mark Johnson states, “It is true that when we read, we read words. But words have meanings, and meanings go far beyond words” (2008). In Schrifttanz, the same could be said for bodily movement. POEME’s use of current mobile technology and the powerful interactive engine the POEME project has developed enables us to draw from a broader population in order to study and validate the findings of ARTeFACT. Synergistically, ARTeFACT offers to POEME the ability to advance its training mechanisms while it supports the representational nature of the data generated. The two projects are pieces of a puzzle that seeks to establish a better understanding of the relationship between movement and language through the particular and the universal.  ",
       "article_title":"Schrifttanz: Written Dance/Movement Poems",
       "authors":[
          {
             "given":"Susan L.",
             "family":"Wiesner",
             "affiliation":[
                {
                   "original_name":"University of Maryland, United States of America",
                   "normalized_name":null,
                   "country":"United States",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Shannon",
             "family":"Cuykendall",
             "affiliation":[
                {
                   "original_name":"Simon Fraser University, Canada",
                   "normalized_name":"Simon Fraser University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0213rcc28",
                      "GRID":"grid.61971.38"
                   }
                }
             ]
          },
          {
             "given":"Ethan",
             "family":"Soutar-Rau",
             "affiliation":[
                {
                   "original_name":"Simon Fraser University, Canada",
                   "normalized_name":"Simon Fraser University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0213rcc28",
                      "GRID":"grid.61971.38"
                   }
                }
             ]
          },
          {
             "given":"Rommie L.",
             "family":"Stalnaker",
             "affiliation":[
                {
                   "original_name":"Bite Dance, San Diego, United States of America",
                   "normalized_name":null,
                   "country":"United States",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Thecla",
             "family":"Schiphorst",
             "affiliation":[
                {
                   "original_name":"Simon Fraser University, Canada",
                   "normalized_name":"Simon Fraser University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0213rcc28",
                      "GRID":"grid.61971.38"
                   }
                }
             ]
          },
          {
             "given":"Karen",
             "family":"Bradley",
             "affiliation":[
                {
                   "original_name":"University of Maryland, United States of America",
                   "normalized_name":null,
                   "country":"United States",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "creative and performing arts, including writing",
          "software design and development",
          "English",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The HathiTrust Research Center (HTRC) aims to facilitate large-scale computational text analysis of the contents of the HathiTrust Digital Library (HTDL) through data services and analytical tools. We conducted a study of current and potential users of the HTRC to investigate how scholars integrate text analysis into their research. Our study aims to inform the development of HTRC services and also to generate deeper insights into scholarly research practices with large-scale digitized text corpora.   Background Studies on the use of digital content by humanities scholars, ranging from humanities cyberinfrastructure (ACLS, 2006) and patterns in scholarly practices (Brockman et al., 2001; Palmer and Neumann, 2002; Green and Courtney, 2015), to discipline-specific studies (Zorich, 2012; Babeu, 2011; Rutner and Schonfeld, 2011), reveal that scholars acquire and analyze digital content in multi-faceted ways. Several investigations particularly examine scholarly uses of digital tools (Frischer et al., 2006; Toms and O’Brien, 2008; Gibbs and Owens, 2012). Computational text analysis dates from the beginnings of humanities computing (Hindley, 2013), and the resources of the ARTFL Project (Argamon et al., 2009; Horton et al., 2009), MONK (Unsworth, 2011), Wordseer (Muralidharan and Hearst, 2013), Voyant and TaPOR (Rockwell et al., 2010), and Lexos (LeBlanc et al., 2013), among others, inform the current work of the HTRC to provide a secure computational and data environment for researchers to conduct analyses of content from the HathiTrust Digital Library. Our study builds on an earlier user needs assessment conducted for the HTRC and its Mellon Foundation-funded Workset Creation for Scholarly Analysis project. That earlier study analyzed interviews and focus groups in order to identify capabilities needed in large text corpora to facilitate scholarly research use (Fenlon et al., 2014). These desired capabilities included the ability to create and manipulate collections as reusable datasets and research products, the ability to work at different units of analysis, and access to highly enriched metadata (Green et al., 2014; Fenlon et al., 2014).  Our present study especially builds upon that previous investigation by examining the text analysis research practices of current and potential users of the HTRC.   Research Design  Goals Our study’s primary goals are:  To analyze current scholarly research practices with textual corpora to identify user requirements for HTRC services; to develop illustrative use cases of text analysis research for shaping training curricula; and to obtain information for guiding the development of HTRC research services in the University of Illinois Library’s Scholarly Commons and similar digital scholarship centers.  While the findings of this study specifically will inform the development of services to meet the needs of HTRC users, it also contributes broader insights into how to develop similar digital resources and research services for computational text analysis.   Methods We conducted fifteen semi-structured interviews with students, faculty, researchers, administrators, and librarians who pursue work that includes text analysis, or have familiarity with text analysis methods. Some participants were recruited at professional conferences for digital humanities and libraries, while others were active in HTRC user group forums. Several of the interviewees had previously interacted with the HTRC, and most had experience with the HTDL. The participants were from various disciplines — including English, Anthropology, History, and Computer Science —and ranged from newcomers to digital humanities to long-time researchers.  We performed an initial analysis of the interview data through open coding and will continue detailed qualitative analysis using ATLAS.ti. Data was independently coded by the authors to ensure inter-coder reliability. While we are still actively analyzing interview data, we identified several preliminary themes discussed here. These themes include strategies for obtaining and managing data, research workflows and results, collaborations, and teaching.     Analysis and Discussion  Data Acquisition and Management Several respondents characterized text analysis research as being time-intensive in spite of the speed of computational tools. One interviewee noted, ‘It’s funny, often people think, “Oh we have it digitized, now it’s useful.” Scholars realize that you have a lot more work to do after that. And that can often slow projects down terribly.’ The interviewees indicated that gathering, managing, and manipulating text data comprised a considerable portion of their work. An interviewee explained, ‘I think the biggest challenge is data, getting good data to work with. I think people underestimate the problems and difficulties in doing that.’ Interviewees also expressed a desire for improved ways to identify and extract the content they needed, especially when navigating large-scale collections to find the volumes, pages, or passages relevant to a research project. As one interviewee remarked, ‘Even if you had somehow structured your texts, I would be saying, “What was left out? How do I bring it back in?”’    Research Workflows and Results Several interviewees described the potential of text analysis to challenge previously held understandings of text, as differences between human and computational readings emerged. One respondent noted, ‘There are many cases in which the computer is at least as good—if not better—a reader than humans are. That’s very difficult for people to accept... sometimes the computer gets it right and it bears looking at that difference. So we kind of want to get that new ground truth on this kind of work.’ Many researchers highlighted the importance of interpretive work in understanding how the tools interact with the text, and characterized the interactions as dynamic. One respondent observed, ‘I yearn for workflows where the scholar could actually set their own tokenization rules.... It would be a way that we could create less language-specific [rules] or control the language specificity of the algorithm. I think that is the real need.’ Several respondents highlighted the importance of tools that flexibly fit into various stages of the research process, and also are accessible to users of different skill levels. Interviewees also suggested enhancements specific to the HTRC, which included expanded visualization capabilities, improved generation of statistics about text corpora, and better ability to handle languages other than English.    Research Collaborations Interviewees repeatedly cited collaboration and research support, both virtual and in-person, as important. Many interviewees worked with digital humanities initiatives, and reported that their local resources ranged from limited technical support to well-resourced research centers. For some interviewees, online support communities— such as Digital Humanities Questions and Answers or Stack Overflow — also were significant. Interdisciplinary collaborations between departments and across institutions emerged as the most prominent kind of partnership, but interviewees also noted the challenges that such collaborations pose. As one interviewee explained, ‘Collaborations between institutions: much more difficult. There’s money, there’s institutional blockages, and then anything over half a dozen people, it gets complicated very quickly. And so the people dynamics get very complicated.’ Some respondents noted that these collaborations affected their research practices and acquisition of research resources.   Interviewees reported that their collaborations with libraries ranged from non-existent to critical partnerships. Many saw the library as a key space because ‘the library is actually the one functioning interdisciplinary space on a university campus.’ Collaborations with the HTRC and digital repositories for working with data also were important to respondents.   Teaching and Training Interviewees mentioned their active efforts and intentions to incorporate computational text analysis into their teaching. Some remarked on institutional constraints that make it difficult to incorporate computational tools into curricula. As one respondent explained: ‘I once imagined teaching a class in which students learn to script and actually run analyses against data, but I was told, basically, that that class isn’t a humanities class anymore—that belongs in computer science.’ Some stated that the courses that they currently teach may not require or allow for the incorporation of computational analysis. Yet others noted that there is only a limited amount of technical or scientific skills that a humanities student could realistically master within a short period of time, with one interviewee noting that ‘you can only get people to learn so much about the math; as much as they can learn, they should — at the same time, it’s hard.’  Although the demand from students for learning about computational text analysis was, overall, reported to be increasing, some interviewees noted that they are constrained by not only limited resources, but also uncertainty as to how to carry out such activities. One interviewee reported prevailing sentiments that the digital humanities ‘doesn’t even fit anywhere,’ leading to the question of whether ‘there should be a whole separate department that’s digital humanities,’ or to offer training within existing curricula.    Conclusion  The immediate aims of this study are to generate an updated framework of user requirements that will guide the development of the HTRC’s educational programming and research support services and also to inform forthcoming Mellon Foundation-funded development of the HTRC Data Capsule. But our preliminary findings also provide insights into scholars’ needs as they increasingly incorporate text analysis in research and teaching. These findings also reveal how digital scholarship centers, information professionals, and providers of digitized content can best support scholarship as digital humanities resources evolve.   Acknowledgements We thank Megan Senseney, Angela Courtney, Nicholae Cline, and Leanne Mobley for their collaboration in this study.   ",
       "article_title":"Scholarly Requirements for Large Scale Text Analysis: A User Needs Assessment by the HathiTrust Research Center",
       "authors":[
          {
             "given":"Harriett Elizabeth",
             "family":"Green",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana-Champaign, United States of America",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Eleanor Frances",
             "family":"Dickson",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana-Champaign, United States of America",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Sayan",
             "family":"Bhattacharyya",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana-Champaign, United States of America",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-05",
       "keywords":[
          "interdisciplinary collaboration",
          "digital humanities - institutional support",
          "GLAM: galleries, libraries, archives, museums",
          "user studies / user needs",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The Oulipo, or Ouvroir de littérature potentielle, is a group of writers in Paris who for over fifty years have experimented with algorithmic techniques for writing and reading literature. Raymond Queneau, one of the group’s co-founders, proposed a method in 1964 for analyzing the syntactic structure of texts written in French, and he believed that the method, which he called matrix analysis, could provide a measure of an author’s style.  What can matrix analysis contribute to stylometry? Apart from its origins as a form of computational play for play’s sake (Wolff, 2007), matrix analysis offers an alternative to lexically based techniques for authorship attribution such as Burrow’s Delta (2002). Rybicki and Eder reported that Delta does not work as well for texts written in French as for those in English and German (2011). Antonia, Craig and Elliott have shown that analyzing the frequencies of lexical n-grams where n > 1 does not usually yield very good results (2014). Researchers have developed syntactical methods based on bigrams of labels for a simplified parsing of texts (Hirst and Feiguina, 2007) and on correlations between semantics and the structures of dependent and independent clauses (Allison et al., 2013). In the latter study the researchers concluded that their definition of style “entailed […]  a method for looking for it” (28). The early Oulipo would have agreed with this approach. Recognizing that an author aware of how he or she used words syntactically might apply Queneau’s matrix analysis to change his or her “manner,” François Le Lionnais (the other co-founder) claimed that matrix analysis could serve as a “literary prosthesis” exemplifying the vocation of the group (Bens, 2005: 246). For Le Lionnais, the most important focus of the Oulipo should be the synthesis of new possibilities for understanding literary phenomena (Oulipo, 1973: 17). Matrix analysis enables the identification of significant syntactical patterns for further inquiry into an author’s style. These patterns would be difficult to ascertain without a method like the one developed by Queneau.   Queneau devised a grammatical schema of the French language for describing the construction of word pairs using a system similar to linear algebra (1964). He began by dividing all elements of speech into two categories: signifiers, which include nouns, adjectives, and verbs (except  avoir and  être); and formatives, which include everything else ( avoir,  être, pronouns, articles, conjunctions, prepositions, adverbs, interjections, etc.). Given a sentence, one can construct two matrices where the first matrix contains all formatives and the second all signifiers:     If a sentence contains two consecutive formatives or signifiers, one can use a unitary element to construct the matrices:    By adopting the conventions that neither (1 × 1) nor (Y × 1) + (1 × Z) are allowed within a sentence, one avoids uninteresting or redundant word pairs.  Queneau proposed observing the distribution of formatives and signifiers in a text using the relation F + Uf = S + Us, where F is the number of formatives, S the number of signifiers, Uf the number of unitary elements paired with signifiers (1 × Z), and Us the number of unitary elements paired with formatives (Y × 1). Noting that even if an author like Flaubert worked tirelessly to vary how he wrote, Queneau believed that this distribution could serve as a “potential” but unconscious indicator of the author’s style (1965: 319).  In order to test Queneau’s hypothesis, I applied his matrix analysis to a corpus of 328 nineteenth-century French novels from the ARTFL-FRANTEXT database using Helmut Schmidt’s part-of-speech tagger (1995). Figure 3 is a biplot of a principal components analysis of scaled values for F, S and Uf (Us is excluded to avoid collinearity) and the graph shows that works by the authors Jules Barbey d’Aurevilly, Alexandre Dumas and Honoré de Balzac cluster separately whereas works by George Sand form distinct clusters.    Table 1 indicates the results of using support vector machines with a radial basis function kernel to build a classification model for the texts (Kuhn, 2015). Sixty-seven percent of the corpus was used for training the model with 10-fold cross-validation. A one-against-one method was used for classifying the test set (Karatzoglou, 2004: 7-8). The results show a moderate authorial signal in the works of Barbey d’Aurevilly and weaker signals for Dumas and Balzac.     To build a better model using matrix analysis, one can observe the distribution of bigrams of matrix analysis pairs in the corpus. Given the labels  F for (Y × 1),  S for (1 × Z), and  B for what Queneau called a biword (Y × Z), one can transcribe a text into a series of these letters. For instance, the sentence from George Sand’s  Indiana (1832):  (Toute × 1) (sa × conscience), (c’ × 1) (était × 1) (la × loi); (toute × 1) (sa × morale), (c’ × 1) (était × 1) (son × droit). can be represented as  FBFFBFBFFB. With the feature set of bigrams  FF,  FB,  BF,  BB,  BS,  SB,  SS,  SF, and  FS, one can analyze its distribution within the corpus. Figure 4 is a biplot of a principal components analysis of the corpus with the nine bigrams as variables and suggests that at least some authors do have measurable differences in style.     Table 2 shows the results of building a model with support vector machines to classify the texts by author according to the distribution of bigrams.    The model can identify the styles of Barbey d’Aurevilly and Dumas with very good accuracy, and it can detect authorial signals in works by Flaubert and Balzac. The model does not do well identifying works by Sand because they seem to evince two distinct styles (as suggested by Figure 4). The small cluster of works by Sand on the left side of the graph include  François le champi,  Elle et lui,  Le Château des désertes,  La Mare au diable,  Consuelo,  Indiana,  Lélia, and  La Comtesse de Rudolstadt. The predominance of formatives in this cluster is perhaps indicative of a more conversational style: such an hypothesis would require further analysis.   Compared to other classification methods based on wordlists, matrix analysis does not deliver as high a level of accuracy. Table 3 summarizes the sensitivity and specificity of several classification models with different statistical techniques for the five selected authors using the  classify() function of the R software package  stylo (Eder et al., 2013):     To minimize the effects of semantic variation, the wordlists for classification with  stylo were culled 100% (only those words that appear at least once in every text were included).  Despite the low accuracy of matrix analysis, it is possible to identify sample sentences that exemplify an author’s style with sequences of bigrams. In Figure 4 the left group of Sand’s texts clusters near the vectors for  FF,  BF and  FB. Scanning the texts for the highest relative frequency of these bigrams yields sentences such as this from Sand’s  Consuelo (1842):  (Il × faut) (que × 1) (je × sache) (comment × 1) (elle × 1) (se × tient), (ce × 1) (qu’ × 1) (elle × fait) (de × 1) (sa × bouche) (et × 1) (de × 1) (ses × yeux). BFBFFBFFBFBFFB  The syntax of this sentence as schematized by matrix analysis contains within it the syntax of the previously quoted sentence, inviting further inquiry into how Sand constructed her texts. Although Sand most likely did not think of her writing style in the terms conceived by Queneau, matrix analysis represents a method for thinking about style that not only measures how words are used but can also inform potentially the act of writing and reading. Lexically-based techniques consider texts as “bags of words” with structure-less frequencies, but matrix analysis approaches texts as objects that have undergone a process of development. As an Oulipian procedure, matrix analysis allows the reader to detect reproduced and reproducible patterns through an interactive process of textual exploration.  Queneau’s matrix analysis represents an analytical method for defining style that classifies texts according to their structure. The Oulipo in the 1960s made a distinction between the quality of works of literature and the potentiality of the methods used to create works of literature (Bens, 2005: 80). Practitioners of computational text analysis can observe a similar distinction between the accuracy of text classification and the potentiality of classification methods for understanding literature. If style implies how words are used more than what words are used, stylometry should seek to better understand how words are used. The Oulipo provides us with an example of this kind of inquiry into computationally revealed text structures. Queneau performed small experiments with matrix analysis, but Le Lionnais imagined the possibility of harnessing machines to support the necessary calculations on a larger scale (Bens, 2005: 246). Following the Oulipian notion of “plagiarism by anticipation” (Oulipo, 1973: 23), we can understand matrix analysis as a precursor of stylometry in the digital humanities. ",
       "article_title":"Oulipian Stylometry",
       "authors":[
          {
             "given":"Mark",
             "family":"Wolff",
             "affiliation":[
                {
                   "original_name":"Hartwick College, United States of America",
                   "normalized_name":"Hartwick College",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0288qta63",
                      "GRID":"grid.418410.8"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "french studies",
          "history of Humanities Computing/Digital Humanities",
          "data modeling and architecture including hypothesis-driven modeling",
          "corpora and corpus activities",
          "stylistics and stylometry",
          "authorship attribution / authority",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction This study discusses the methodology used in The Digital model of Andalusia's Late Gothic Heritage project to develop new models of heritage interpretation through the application of GIS and Graph visualization to provide new perspectives of Andalusia's heritage by considering social, political, economic and cultural evolutions. Given the special period context considered, late 15th and early 16th centuries, the project used a variety of sources to relate heterogeneous historical data on different subjects in order to create a historical spatial database and to respond questions such as: How was the construction process in Andalusia between 1433 and 1560? What period and region had more constructive activity? How did the opening of the Andalusia´s eastern border modify the dynamism of the territory? What features are more common in each time period? What professionals have worked together on a building or quarry? Is this reflected in the architectural language produced? Andalucía's territory is the consequence of a huge numbers of different cultures that passed through it - from Roman and Islamic to Christian. During the 15th century, Spain had profound transformations, both political and cultural. The Christian reconquest led to the centralization of the cultural production in major cities generating important flow of new knowledge. Also, the consolidation of the Iberian Peninsula´s borders and the growth of contacts with Europe, and in a near future with America as well, helped to improve the cultural expansion and exchanges between the political leaders, intellectuals, scientists and technicians (García C., 2011).  These transformations are documented by a large number of sources that usually focus only on one particular aspect. In this sense, some sources are about professionals that worked at the quarries, others are about building construction and labor contracts, or about journeys and meetings that the workers had had. On one hand, we are dealing with the data from researchers that studied the biography of a particular professional or studies that have been dedicated to the constructive evolution of a city or a building. On the other hand, we are analyzing historical cartography sources that show different territorial changes and stages over time (kingdoms, dioceses, borders, etc).  Thereby, our goal is to build a spatial-temporal database capable of linking information that at first glance may seem unrelated. Visualizing and relating these attributes through an information system on cultural heritage has steered our work in two directions: the creation of systems built around the entities, and the implementation of analyses to observe and interpret their relationships. In this study we demonstrate how all different historical information could be organized and structured in two types of digital models - GIS and graphs - that will are applied to our case study and allow us to generate a more comprehensive and flexible understanding of the phenomenon of the late Gothic period as a complex system through a combined knowledge of space, time and actors. Working with these two tools has provided new perspectives at the Late Gothic heritage, creating new groups and subgroups of entities, and new relationships that could be easily translated to other case studies. We also created different categories - for each technology - designing two models of organizational structure. On the one hand, we use GIS with a spatial approach, the space is the product and simultaneously the producer of a series of relationships (Lefebvre, 2000) whose analysis can be performed using the alphanumeric attributes of each spatial entity or its topology. On the other hand, using graphs we have an abstract approach to visualizing a network of professionals and works over the Late Gothic period. The creation of the two models is because each of these tools requires a specific way of organizing data. While GIS works with a system with fixed relational databases that support SQL \"join\" operations, using it as the main query language and organizes spatial data in layers and attributes, Graph model uses the NoSQL system (Not Only SQL) and organizes data in nodes and edges (Robinson et al., 2013).   2. GIS model The process of creating the GIS model will be developed in eight dynamic and interrelated phases: database design schema (figure 1); collection, processing and data selection; data entry and analysis in  ArcGIS software; production model views; assessment of the problem; data interconnection; generating queries and reports; system development documentation and dissemination of the model (Ferreira Lopes and Pinto Puerto, 2015). The longest stage of the process is the creation of spatial entities - data entry - around 75% of the time and effort undertaken in the research will be used to collect, treat and create the data so that they can later be analyzed. The big difficulty lies in its accuracy. For certain entities, the maps and information of the 15th and 16th centuries does not allow us to reach an urban scale precision which forced us to work with a territorial scale. That is the case, for example, of the kingdoms, buildings, quarries or paths layers (figure 2).     Figure 1: Andalucía's Late Gothic Heritage database schema    Figure 2: Andalucía's Late Gothic Heritage SDI, more than 100 buildings, 4.000km of paths, administrative and dioceses borders (which has changed through time), 14 quarries, and others entities  Also, some gaps are constantly present in the attributes; in many cases we have incomplete information, which somewhat limited our analysis of certain data. That is the case, for example, of some  \"date\" attributes. However, working with GIS allows us to take these gaps temporarily - once it offers an easy way to edit and add new data, which is one of the main advantages that the tool offers. Our SDI are created in the  ISO 19100 series standards,  Open Geospatial Consortium standards and the recommendations of the guidelines of  INSPIRE and  LISIGE in order to be extended, edited and viewed by other researchers.    3. Graph Model The construction process of the Graph Model has been developing in three phases: 1) data collection, 2) scheme creation and 3) queries and analysis. This paper will deal with the proposal and outline gathering held in conjunction with the  Culture Plex Lab at the University of Western Ontario in Canada.  Our \"starting point\" was the Cathedral of Seville. Knowing that the construction of the Cathedral was one of the events most responsible for the flow of knowledge at that time and knowing that a lot of professionals had worked or had some kind of work relation with it, this strategy was more realistic and reachable, initially (figure 03).  To create the Graph model we are using the software  Sylva DB  (de la Rosa et al., 2013) developed by the CulturePlex Lab, which has allowed us to create a scalable and flexible way to organize, structure, manage, visualize and analyze our mass of data. With all the data in  SylvaDB, we can see the links between different professionals and works, as well as between quarries and workers, parts of building and professionals, quarries and buildings. This tool has redirected the process in a way that otherwise would have been too taxing in terms of time and computation - we were now not required to create innumerable tables and charts to collect all the information.  After having a clear starting point and already with a certain amount of professionals collected - about 300 workers - the next step was the creation of a schema through which we discern graphic patterns. At this moment, in the Graph model we have 1.000 relationships and 850 nodes (figures 04, 05 and table 1).   Figure 3: Sylva´s print screen showing the Andalusia´s Late Gothic Network Graph model    Figure 4: This graph shows the masters builders who had worked in different activities concerning to the cimbor of the Cathedral of Seville    Figure 4: This graph shows the masters builders who had worked in some chapels    Table 1: This table shows the professionals who worked in different chapels. We can see the importance of some of them - the ones who more chapels worked in    4. Conclusions Different research methods based on new technologies applied to understanding the same phenomenon provide a greater depth of the problem. In this sense, the use of multiple methods has allowed us to achieve three important aspects: to promote different perspectives on the subject allowing a wider view about the object of study; include a large variety of variables in the study; provide multiple analyses of the same concept, which increases the validity of the research that remains open and upgradeable. Therefore, what we seek is to provide new methods but also new approaches that do not override other traditional systems, but enrich the discussion on the past and its relationship with the inheritance.   Funding This project is associated with the project and R+D+I (Research, Development and Innovation)  “A Digital Information Model for the Knowledge and Management of Immovable Cultural Heritage” funded by the Spanish Ministry of Economy and Competitiveness and led by the lecturer  Francisco Pinto Puerto.   ",
       "article_title":"Seeing Andalucia's Late Gothic heritage through GIS and Graphs",
       "authors":[
          {
             "given":"Patricia",
             "family":"Ferreira Lopes",
             "affiliation":[
                {
                   "original_name":"University of Seville, Spain",
                   "normalized_name":"University of Seville",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/03yxnpp24",
                      "GRID":"grid.9224.d"
                   }
                }
             ]
          },
          {
             "given":"Francisco",
             "family":"Pinto Puerto",
             "affiliation":[
                {
                   "original_name":"University of Seville, Spain",
                   "normalized_name":"University of Seville",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/03yxnpp24",
                      "GRID":"grid.9224.d"
                   }
                }
             ]
          },
          {
             "given":"Antonio",
             "family":"Jimenez Mavillard",
             "affiliation":[
                {
                   "original_name":"University of Western Ontario",
                   "normalized_name":"Western University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/02grkyz14",
                      "GRID":"grid.39381.30"
                   }
                }
             ]
          },
          {
             "given":"Juan Luis",
             "family":"Suarez",
             "affiliation":[
                {
                   "original_name":"University of Western Ontario",
                   "normalized_name":"Western University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/02grkyz14",
                      "GRID":"grid.39381.30"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-14",
       "keywords":[
          "visualisation",
          "cultural studies",
          "maps and mapping",
          "cultural infrastructure",
          "medieval studies",
          "ontologies",
          "data mining / text mining",
          "networks, relationships, graphs",
          "geospatial analysis, interfaces and technology",
          "spatio-temporal modeling, analysis and visualisation",
          "art history",
          "digital humanities - facilities",
          "spanish and spanish american studies",
          "digitisation - theory and practice",
          "knowledge representation",
          "English",
          "interdisciplinary collaboration",
          "databases & dbms",
          "historical studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Authorship studies have long played a central role in stylometry, the popular subfield of DH in which the writing style of a text is studied as a function of its author’s identity. While authorship studies come in many flavors, a remarkable aspect is that the field continues to be dominated by so-called ‘lazy’ approaches, where the authorship of an anonymous document is determined by extrapolating the authorship of a document’s nearest neighbor. For this, researchers use metrics to calculate the distances between vector representations of documents in a higher-dimensional space, such as the well-known Manhattan city block distance. In this paper, we apply the minmax metric – originally proposed in the field of geobotanics – to the problem of authorship attribution and verification. Comparative evaluations across a variety of benchmark corpora show that this metric yields better, as well as more consistent results than previously used metrics. While intuitively simply, this metric generally displays a regularising effect across different hyperparametrizations, and allows the more effective use of larger vocabularies and sparser document vectors. In particular the metric seems much less sensitive than its main competitors to (the dimensionality of) the vector space model under which the metric is applied. Most authorship studies in computer science are restricted to present-day document collections. In this paper, we illustrate the broader applicability of the minmax metric by applying it to a high-profile case study from Classical Antiquity. The ‘War Commentaries’ by Julius Caesar ( Corpus Caesarianum) refers to a group of Latin prose commentaries, describing the military campaigns of the world-renowned Roman statesman Julius Caesar (100-44 BC). While Caesar must have authored a significant portion of these commentaries himself, the exact delineation of his contribution to this important corpus remains a controversial matter. Most notably, Aulus Hirtius – one of Caesar’s most trusted generals – is sometimes believed to have contributed significantly to the corpus. Thus, the authenticity and authorship of the Caesarian corpus is a philological puzzle that has persisted for nineteen centuries. In our paper, we shed new light on this matter.    Benchmarking To properly evaluate the performance of the novel Ruzicka minmax metric, we turn to a publicly available benchmark corpora: the multilingual datasets (Dutch, English, Greek, and Spanish) used by the 2014 track on authorship verification in the PAN competition on uncovering plagiarism, authorship, and social software misuse. This track focused on the “open” task of authorship verification (as opposed to the closed set-up of authorship verification). Each dataset holds a number of “problems”, where given (a) at least one training text by a particular target author, (b) a set of similar mini-oeuvres by other authors, and (c) a new anonymous text, the task is to determine whether or not the anonymous text was written by the target author. A system must output for each of the problems a real-valued confidence score between 0.0 (“definitely not the same author”) and 1.0 (“definitely the same author”). By outputting the value of 0.5, a system can specify that it was not able to solve a problem. For each dataset, a fully independent training and test corpus are available (i.e. the problems, nor authors and texts in both sets do not overlap). Systems are eventually evaluated using two scoring metrics which were also used at the PAN: the established AUC-score, as well as the so-called c@1, a variation of the traditional accuracy-score, which gives more credit to systems that decide to leave some difficult verification problems unanswered. In the full paper, we offer a complete evaluation of all datasets: for the sake of brevity, this paper is restricted to a representative selection of results. As common in text classification research, we vectorize the datasets into a tabular model, under a ‘bag-of-words’ assumption, which is largely ignorant of the original word order in document. Unless reported otherwise, we use character tetragrams below (Koppel et al., 2014), which yield generally acceptable results across corpora. We experiment with a number of different vector space models, the results of which can be summarized as follows:  plain  tf (where simple relative frequencies are used);   tf-std, where the  tf-model is scaled using a feature’s standard deviation in the corpus (cf. Burrows’s Delta: Burrows, 2002);   tf-idf, where the  tf-model is scaled using a feature’s inverse document-frequency (to increase the weight of rare terms).  …  In our experiments, we focus on the Ruzicka ‘minmax’ distance metric, a still fairly novel algorithm in the field of stylometry. Just as the Euclidean or Manhattan distance, this metric will calculate a real-valued distance score between two document vector A and B as follows:    While the formula below uses the tf-model, the Ruzicka distance can of course be easily applied to other vector space models too. In our paper, we will offer a intuitive assessment of the desirable properties of this metric (e.g. in comparison to Burrows’s Delta).   General Imposters Framework (GI) In our experiments, we make amongst others use of the General Imposters Method, a bootstrapped approach to authorship verification which has recently yielded excellent results. Fitting the verifier on the train data involves two steps. First, we calculate a distance score for the anonymous document in each problem, using Algorithm 1, in order to determine whether the anonymous text was written by the target author specified in the problem:    Thus, during  k iterations (default 100), we randomly select a sample (e.g. 50%) of all the available features in the data set. Likewise, we randomly select  m ‘imposter’ documents (default 30), which were not written by the target author. Next, we use a  dist() function to assess whether the anonymous text is closer to any text by the target author than to any text written by the imposters. Here,  dist() represents a regular, geometric distance metric, such as the Manhattan or Ruzicka metric. The score returned by Algorithm 1 has been characterized as a ‘second-order’ metric, because it does not rely on the rather comparison of document vectors. The general intuition here, is that we do not just calculate how different two documents are; rather we test whether the stylistic differences between them are consistent (a) across many different feature sets, and (b) in comparison to other randomly, sampled documents.  In the second stage, we attempt to optimize the distance scores returned by Algorithm 1, in the light of the specific evaluation measures used. We apply a score shifter (Algorithm 2), which attempts to define a ‘grey zone’ where the results seem too unreliable to output a score (cf. c@1):    Through a grid search of different values between 0 and 1 for p1 and p2, we determine the settings which yield the optimal AUC x c@1 on the train data. In Fig. 1, we plot the optimal results which could be obtained on the train problems in the data set of Dutch Essays, for a specific combination of a metric and a vector space model. We ran the experiment 20 times, with increasing vocabulary truncations (e.g. the 1000 most frequent tetragrams). The results demonstrate how the Ruzicka minmax metric returns the most stable results across the experiments and clearly has a regularizing effect across different hyperparametrizations. In the full paper, we will present a complete evaluation of this system on all the PAN datasets, which in most cases yields surprisingly competitive scores on the test data, even without much corpus-specific parameter tuning. In the table below, we show the test results for Dutch essays corpus in terms of the AUC x c@1. The best combination reaches a AUC x c@1 of 0.886 on the test data (combination of  minmax and  std), whereas the best individual system submitted to PAN 2014 only reached 0.823 on that test dataset. Using randomized significance tests, we will additionally demonstrate the regularizing effect of the Ruzicka distance across vector spaces; its strong performance is also evident from Table 1.    Figure 1: Optimal results on train corpus          Vector Space / Metric Euclidean Manhattan Minmax   Tf 0.676 0.698 0.837   Tf-Idf 0.720 0.750 0.854   Tf-Std 0.614 0.701 0.886      Table 1: Final test results (AUC x C@1)   Corpus Caesarianum To further illustrate the applicability of the Ruzicka metric for authorship problems in traditional philology, we also report a stylometric case study concerning the  Corpus Caesarianum. This  Corpus is a group of five commentaries Caesar’s military campaigns:    Bellum Gallicum, the conquest of Gaul, 58 to 50 BC;   Bellum civile, the civil war with Pompey, 49 to 48 BC;   Bellum Alexandrinum, the campaigns in Egypt etc., 48 to 47 BC;   Bellum Africum, the war in North Africa, 47 to 46 BC   Bellum Hispaniense, a rebellion in Spain, 46 to 45 BC.   The first two commentaries are mainly by Caesar himself, the only exception being the final part of the  Gallic War (Book 8), which is by Caesar’s general Aulus Hirtius. Suetonius, writing a century and a half later, suggests that either Hirtius or another general, named Oppius, authored the remaining works. We will report experiments which broadly supports the Hirtius’s own claim that he himself compiled and edited the corpus of the non-Caesarian commentaries. Figure 2, for instance, shows a heatmap-like visualisation, in which Hirtius’s Book 8 of the  Gallic War clearly clusters with the bulk of the  Alexandrian War (labeled  x).    Figure 2: Minmax-based clustermap of 1000-word samples of the  Corpus Caesarianum.    ",
       "article_title":"Authorship Verification with the Ruzicka Metric",
       "authors":[
          {
             "given":"Mike",
             "family":"Kestemont",
             "affiliation":[
                {
                   "original_name":"University of Antwerp, Belgium",
                   "normalized_name":"University of Antwerp",
                   "country":"Belgium",
                   "identifiers":{
                      "ror":"https://ror.org/008x57b05",
                      "GRID":"grid.5284.b"
                   }
                }
             ]
          },
          {
             "given":"Justin",
             "family":"Stover",
             "affiliation":[
                {
                   "original_name":"University of Oxford, United Kingdom",
                   "normalized_name":"University of Oxford",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/052gg0110",
                      "GRID":"grid.4991.5"
                   }
                }
             ]
          },
          {
             "given":"Moshe",
             "family":"Koppel",
             "affiliation":[
                {
                   "original_name":"Bar-Ilan University, Israel",
                   "normalized_name":"Bar-Ilan University",
                   "country":"Israel",
                   "identifiers":{
                      "ror":"https://ror.org/03kgsv495",
                      "GRID":"grid.22098.31"
                   }
                }
             ]
          },
          {
             "given":"Folgert",
             "family":"Karsdorp",
             "affiliation":[
                {
                   "original_name":"Meertens Institute, The Netherlands",
                   "normalized_name":"Meertens Institute",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/05kaxyq51",
                      "GRID":"grid.450081.8"
                   }
                }
             ]
          },
          {
             "given":"Walter",
             "family":"Daelemans",
             "affiliation":[
                {
                   "original_name":"University of Antwerp, Belgium",
                   "normalized_name":"University of Antwerp",
                   "country":"Belgium",
                   "identifiers":{
                      "ror":"https://ror.org/008x57b05",
                      "GRID":"grid.5284.b"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "natural language processing",
          "classical studies",
          "stylistics and stylometry",
          "authorship attribution / authority",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Motivation Digital humanities needs tools that better support the core processes of humanistic inquiry. This includes support for handling uncertainty and incompleteness in the data, for interactive exploration, and for fluidly moving between close and distant reading (Drucker 2011; Jänicke et al., 2015; Caviglia, Ciuccarelli, and Coleman, 2012; Uboldi and Caviglia, 2015).  The Khepri tool presented here is part of a project to develop a modular set of components that take these requirements into account, and can be connected and configured to respond to the needs of a particular humanities task and data. Khepri targets data stored as Linked Data (Heath and Bizer, 2011), a set of scalable standards that has gained widespread adoption particularly in the sphere of cultural heritage.    Development process To ensure the tools developed meet the needs of humanities users, they are being developed iteratively, utilizing participatory design in case studies, as advocated by the field of design science (Hevner et al., 2004; Peffers et al., 2007; Wieringa 2009). The task of the computer scientist is to see beyond these individual studies; to identify common components allowing the tools to generalize beyond the projects under scrutiny.  To date, a variety of collaborations have been embarked upon, from the prosopographical study of the Republic of Letters   , through supporting engagement with WW1 primary sources (Mäkelä, Törnroos, et al., 2015), to developing a contextual network for Finnish fiction (Mäkelä, Hypén, and Hyvönen, 2013). Together, these span a range of research questions, and types of data.  Through these collaborations, a prevalent process of inquiry was identified – the need to explore and contrast differently constrained subsets of a dataset. For instance, this might be looking at the correspondence networks of different individuals and comparing them, or looking at how possible values of a linguistic variable behave with respect to each other as well as associated metadata. To support this process, Khepri utilizes the view-based paradigm (Mäkelä, 2010), where data is presented simultaneously from different perspectives, with each perspective acting both as a visualization as well as a means to constrain what is shown. A proper implementation of the paradigm also allows for speedy informed variation of parameters, and thus interactive exploration.  Because the views interact in a defined way, they can be developed as separate components targeting major visualization classes such as geographical, temporal or statistical. Each individual Khepri instance can then select from these the views suitable for that particular use.  Thus far, most of the work has been preparatory, with the functionalities simulated through ad-hoc disconnected components, tied together and supplemented by manual work of the computer scientist. However, now a first complete tool for a particular task has been developed. This instance has been configured for historical sociolinguistics.    Khepri for historical sociolinguistics Historical sociolinguistics is the study of language in relation to social factors through time (Nevalainen and Raumolin-Brunberg, 2003). A possible research question would be to chart the role of gender, age and socioeconomic status in the diffusion of the English progressive (as in  I am writing). From the viewpoint of the Khepri tool, this is interesting because it requires combining access to unstructured text with access to the structured (meta)data describing their authors.  This is also the area where current tools fall short, for while corpus tools (e.g. CQPweb (Hardie, 2012), Korp (Borin, Forsberg, and Roxendal, 2012) and WordSmith   ) enable querying texts by linguistic features, they poorly support walking from the texts to the attributes of the authors. On the other hand, tools for visually exploring structured data (e.g. Palladio   , Europeana4D  http://www.tinyurl.com/e4d-_project  and RAW   ) do not support interacting with text corpora.  This makes research currently very labor-intensive. For instance, if one wishes to study the aforementioned progressive, one first searches for instances of  -ing in the corpus using a corpus tool. The instances are then exported into Excel to analyze them and eliminate false hits such as gerunds (My favourite hobby is writing). Next, the number of hits produced by each person is calculated using another sheet that lists the authors by gender, age, socioeconomic status and time period. These numbers are then exported for statistical analysis and visualization. Because the corpus texts, spreadsheets, visualizations and statistical analyses are not connected to each other, the exploration and interpretation of the observations is cumbersome and time-consuming at every stage.     Figure 1. the Khepri for historical sociolinguistics interface     The user interface configuration of Khepri for historical sociolinguistics The Khepri interface for historical sociolinguistics is depicted in Figure 1. The interface is divided into three columns, with the views contained in each having different primary purposes.  On the left are views aimed primarily at producing a subset of interest. The first view is for text search. Below the query, matching keywords from the data are presented for evaluation. Notice that two sets of counts are given. One shows the overall amount of hits for a keyword in the corpus, while the other takes into account constraints set in other windows. This way, the view acts not only as a selector, but also as a statistical breakdown of the current subset.  Below the keyword search view, the user can add metadata views. Here for example, a view visualizes and allows one to constrain the data through the lens of the author's education.  The second column shows the items in the current subset. Matches are shown in their textual context, with metadata and additional context available on mouse-over. While tuned for close reading, this view also acts as a filter. Clicking on an item removes it from the current subset. For linguistic research, this is important as the inclusion or exclusion of a particular example of a phenomenon may depend on contextual cues and background knowledge that cannot be defined as search parameters, but require manual evaluation. When focusing on close reading, the column can be expanded to occupy the whole right-hand side of the interface. Expanded, the view shows additional metadata, such as the author and year of the texts. The view can also be sorted according to these properties, as well as grouped by them, so that for example only a listing of the authors, or the linguistic types (e.g. different words ending with  -ing) is shown, with the individual matches revealed by expanding.  To further help in keeping a close reading task organized, the interaction between this view and the constraining views has been designed so that it is easy to temporarily restrict the matches shown to only those from e.g. a particular spelling, or a particular social class.  Finally, the column on the right is intended primarily for visualization. In fact, it can visualize and contrast multiple subsets of the data. To facilitate this, the first two columns are subsumed in a tabbing container, with each tab containing the query state of a single subset. In the example of Figure 1, these are spelling variants of the negated auxiliary verb  cannot (written separately, contraction, written together).  By default, the frequency of each subset is visualized as its own line chart. However, numerous options affecting this are provided, drawn from best practices in the field (Hinneburg et al., 2007). For example, separate lines can be graphed for each of the values of a particular metadata property. In Figure 1 for example, each chart contains lines for male and female writers, showing that the use of the form “can not” seems to follow an approximately linear decline for men, but not for women.  To prevent misinterpretations arising from small samples, each graph can be accompanied by a dotted logarithm representing the size of the corpus as a whole for that metadata value. The interface also supports bootstrapping to visualize confidence intervals. As this takes considerable time to calculate, it should only be enabled when a seemingly significant discovery needs verification.  The interface also offers alternative charts. For example, when comparing possible values of a single linguistic variable, the area chart visualization shown in Figure 2 is appropriate. In addition, a motion chart visualization (Figure 3, inspired by the static scatterplots in Nevalainen, Raumolin-Brunberg, and Mannila (2011)) is provided, used to see how different individuals relate to the variable under study, and even how they change their use through time. In line with the view-based querying paradigm, all visualizations also act as selectors, enabling delving deeper into interesting phenomena. Through them, one can for example constrain the instance list to show only usage by women in a particular timespan, or in the case of the motion chart, even the use of a single individual.     Figure 2. Area charts showing the relative proportions of “can not” (blue), “cannot” (yellow) and “can’t” (red) by time and gender     Discussion and future work Khepri for historical sociolinguistics is the first complete version of the tool. It is also only in its second iteration, and will continue to improve based on feedback. However, it has already been received with excitement, enabling research that was previously too time-consuming to attempt. With the architecture of the tool now in place, other instances will soon follow, targeting next the Republic of Letters and Finnish fiction use cases. This can be said because all the views created are actually generic, and can be pointed to different data by reconfiguring. For example, text search is also useful for locating individuals or books, while the metadata facets directly target structured data already. The views requiring most modification are the statistical charts, but even here work will be fine-tuning to match differing metrics. Correspondingly, any visualizations developed for other scenarios can be imported here, to for example visualize language phenomena on maps.    Figure 3. Motion chart showing how many percent of individual writers use the form “cannot”    ",
       "article_title":" Khepri - a Modular View-Based Tool for Exploring (Historical Sociolinguistic) Data  ",
       "authors":[
          {
             "given":"Eetu",
             "family":"Mäkelä",
             "affiliation":[
                {
                   "original_name":"Aalto University, Finland",
                   "normalized_name":"Aalto University",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/020hwjq30",
                      "GRID":"grid.5373.2"
                   }
                }
             ]
          },
          {
             "given":"Tanja",
             "family":"Säily",
             "affiliation":[
                {
                   "original_name":"University of Helsinki, Finland",
                   "normalized_name":"University of Helsinki",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/040af2s02",
                      "GRID":"grid.7737.4"
                   }
                }
             ]
          },
          {
             "given":"Terttu",
             "family":"Nevalainen",
             "affiliation":[
                {
                   "original_name":"University of Helsinki, Finland",
                   "normalized_name":"University of Helsinki",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/040af2s02",
                      "GRID":"grid.7737.4"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "visualisation",
          "software design and development",
          "corpora and corpus activities",
          "user studies / user needs",
          "linguistics",
          "English",
          "semantic web",
          "interface and user experience design",
          "interdisciplinary collaboration",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Psychiatric practice during the nineteenth century was closely engrained in the spaces of the asylum. Envisioned as “therapeutic instruments” in and of themselves, asylums boomed all around the world: some were purpose-built, some were existing buildings repurposed as asylums. But how did these institutions really function? How was psychiatry practiced here? Our proposal reports a work in progress that visualizes the evolution and workings of a nineteenth century Ottoman asylum. Located in Istanbul, the capital of the Ottoman Empire, it was used as the state mental hospital between 1873 and 1922. The building is originally a sixteenth century imperial complex by the famous Ottoman court architect Mimar Sinan (1489-1588) and is still standing albeit currently under heavy and damaging restoration. Research shows that during the time of its use as an asylum, the building changed significantly to enable its medical function. Indeed, it is precisely through the changing of the building one can demonstrate the medicalization and modernization of Ottoman psychiatry during this period. In this sense, our case study does not visualize an \"ideal\" asylum nor is it a search for the \"original\" complex; on the contrary, it analyzes a neglected and messy phase of an existing building's life. Traditionally perceived as a “corruption” in the original structure, we take the period it was used to treat the insane as a story worth being told.  The information is gathered from a detailed research of primary archival and printed sources involving multiple disciplines: medicine, psychiatry and architecture. These sources include: (1) Ottoman and Turkish official documents kept in the Prime Minister's Archives: Correspondences between state departments giving details about the running of mental institutions, spatial interventions, and concerns over public health; (2) Medical publications of the period: Visitor accounts by physicians, medical writings discussing approaches in psychiatry, and comparative reports on the current conditions of asylums around the world; (3) Popular media: Newspaper articles and travel accounts. The project is located at the intersection of architectural and medical history; however, it is primarily using the toolset of architectural history. Partly stemming from the demands of the field and partly due to the nature of the project itself, one of the major concerns is the visualization of findings. In line with architects' particular understanding of visualization, the first resort was experimenting with various 2D and perspectival illustration techniques. 2D depictions (i.e. plan, façade, elevation) have been the mode of representation for centuries in the field of architecture. Perspective was a welcome addition with the development of linear, or mathematical, perspective in the fifteenth century. Computational 3D modeling and visual simulation (rendering) has become the norm starting from the 1990s. Despite its popularity in architectural practice and education, the use of 3D computation has, until recently, been relatively underutilized in architectural history. Scholars in architectural history and in the neighbouring fields of art history and archeology have recently been using methodologies and approaches to integrate innovative technologies in historical research. From virtual reconstruction to web-based panoramas, geospatial modeling to photogrammetry, the virtual creation of static environments to interactive ones, these studies appear in multiple platforms and venues related to digital humanities, digital art history, architectural computation etc. In the mainstream publications of architectural history, an important milestone was the publication of an article that featured real-time interactive simulations of the Roman Forum presenting a hypothetical reconstruction of a funeral ceremony in the Journal of the Society of Architectural Historians (JSAH) (Favro and Johanson, 2010). Another outstanding project that addressed issues of time and movement was \"The Virtual Monastery\" that focused on a single structure to analyse the workings of a building type through ages (Bonde et al., 2009).  Visualizing the findings In our project, we faced several challenges to demonstrate the findings with methods traditionally used in architectural history. Ottoman mental facilities in general, and our case study in particular (especially the period that we are focusing on), is largely understudied compared to the aforementioned examples of the Roman Forum or monasteries. All the findings come from primary sources that are incomplete in nature. Moreover, the sources are predominantly textual with limited visual resources describing various states of the building. The historical layers added to the building render conventional visualization methods ineffectual. As a response to these challenges and the resulting complicated presentation of findings, the project employs an interactive 3D simulation toolset built within Unity 3D that allows visualizing hypothetical spatial reconstructions and trajectory tracking all continuously referring to temporal data and incorporating primary documents. Our interactive simulation framework builds on these efforts creating the ability to navigate in four dimensions simultaneously and without restriction:  1 Temporal dimension: Instead of the traditional interest in finding out about the “original” building, this approach looks at the  life of a building and how it changed over time. The method we propose for Unity is to implement a time slider component into the viewing user interface (UI). This component controls the visibility of architectural changes at distinct time periods. In this project, we have identified four specific points in time wherein primary sources identify distinct architectural states. The timeline UI is enabled by including time data within the model data. In the 3D modeling or 3D data acquisition phase, model elements are tagged with custom properties coded to our tool. These custom properties represent abstract metadata defining the start and end date for the object(s) (for temporal properties), the orientation point for the object(s) (for geospatial locating), as well as any descriptive information all of which become native to the file.    2 Reconstructed trajectories: Instead of treating space as an isolated entity, this approach captures the  life in the building. By tracing the movement of the occupiers of the space, we know more about their lives. The everyday life in the asylum consisted of the acts and movements of its occupiers, in this case, patients, doctors and the staff. By tracing their movements and visualizing them, it becomes possible to have a better understanding of the daily routines (eating, cleaning, sleeping etc.) in addition to the medical treatments that took place through a certain reorganization of space and time. This setting is particularly illuminating as it was assumed at the time that the life and routines of patients in the asylum were crucial components of the healing process.The user interface (UI) enables diagrammatic circulation indicators for each of the four points in time to be visible at the viewer’s discretion.    3 Cross-referencing data: In addition to the ability to preview time and movement, the interactive functionality in Unity allows the viewer to select the information relevant to the simulation viewed at any time. The ability to combine text as hyperlinked visual overlays and graphic information enables important reference texts and images to be associated with spatial and temporal elements and viewed when desired. One advantage of this presentation method is the ability to break free from the linear structure of a textual manuscript. One can interact with the information presented in a nonlinear way (albeit guided). By cross-referencing various types of information, we aim to create a system that binds or connects these documents and data sets in meaningful ways.     Figure 1: Simulation User Interface Mockup (B. Ozludil, U. Thompson, A. Wendell). Photograph source: Sihhat Almanaki, 1933.  We do not intend the spatiotemporal model to be the  primary outcome, but rather a platform inviting interpretation and scholarship through the combined textual and visual data. In line with the ontological approach adopted in digital humanities, the ever-evolving outcome is a “knowledge representation, a hypothesis” not a re-creation of reality (Bonde et al., 2009). The relationship between the representation and its referent also calls into question producing knowledge with partial data, which is not uncommon in (conventional or digital) historical scholarship. Both possibilities and risks exist in 3D visualization with partial data. 3D models give the opportunity to relate to the building without the trained eye to \"read\" 2D representations. Risk is related: 3D creates a sense of absolute masking the hypothetical elements within. Being aware of this danger, the project aims to visually clarify the “known” versus “unknown” using graphic coloration indicating the extent of “known” detail.  The tool we propose visualizes temporally-located spatial data that allows multiple readings and interpretations, and that is open to manipulation, rather than a mere representation of a space. Doing so will open the approach to more disciplines and interested parties, and while still technical in nature, will provide a platform for historical spatial research. This work can be shared with other digital humanities scholars employing Unity or similar tools. Integrating innovative technologies in historical research has the potential to change the ways in which we conceptualize and tackle the problem at hand. In other words, rather than being \"tools\" to accomplish what is predetermined, they open up new ways to think and to produce knowledge.  ",
       "article_title":"Digging the Aboveground: Visual Archeology of an Asylum",
       "authors":[
          {
             "given":"Burcak",
             "family":"Ozludil Altin",
             "affiliation":[
                {
                   "original_name":"New Jersey Institute of Technology, United States of America",
                   "normalized_name":"New Jersey Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05e74xb87",
                      "GRID":"grid.260896.3"
                   }
                }
             ]
          },
          {
             "given":"Augustus",
             "family":"Wendell",
             "affiliation":[
                {
                   "original_name":"New Jersey Institute of Technology, United States of America",
                   "normalized_name":"New Jersey Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05e74xb87",
                      "GRID":"grid.260896.3"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-17",
       "keywords":[
          "spatio-temporal modeling, analysis and visualisation",
          "visualisation",
          "art history",
          "historical studies",
          "English",
          "audio, video, multimedia",
          "interface and user experience design",
          "geospatial analysis, interfaces and technology"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" In 1993-94 Roy Rosenzweig and David Thelen conducted a set of surveys that resulted in the publication of a seminal work titled  The Presence of the Past. The team of historians, museum curators and students surveyed the American public about their interaction with history in order to understand “better ways of connecting academic historians with larger audiences” (Rosenzweig and Thelen, 1998). Their study concluded that Americans’ interest in history related to those events and groups that intersected with their own lives. They universally trusted museums because they could see the objects and images that were the “stuff” of history, but family stories, letters, and photographs defined history for most of the survey participants. With the publication of the survey, Rosenzweig and Thelen entered a historiographic discussion that centered on ways to incorporate history from the “bottom up” and to make the museum experience more interactive (Wallace, 1996; Hayden, 1997; Gardner and LaPaglia, 2004; Lewis, 2005; Gordon, 2010; Anderson, 2012; and Falk and Dierking, 2012).  Almost twenty years after Rosenzweig and Thelen conducted their survey, William G. Thomas, Patrick D. Jones and Andrew Witmer “advanced a movement to democratize and open history” with the founding of the History Harvest Project at the University of Nebraska (Thomas, Jones, and Witmer, 2013). Incorporating their own digital history experience into the classroom, Thomas, Jones, and Witmer developed student projects that operated at the “intersection of digital history and experiential learning.” Beginning in 2010 and continuing to the present, students worked with specific communities to organize History Harvest events at which they photographed objects and scanned images offered by individuals from their personal collections. The photographs and scans were uploaded into an Omeka database and organized into collections and digital exhibits. The project’s aim is to “make invisible archives and stories more visible.” The founders argue that to be successful, a History Harvest must be “organic, grassroots, and local.” At the University of Central Florida (UCF), a Regional Initiative for Collecting History, Experiences, and Stories (RICHES™) Project has advanced the concept of student-public engagement through the use of History Harvests to collect “invisible archives and stories” by making the presentation of the digitized material interactive and democratizing the ability of site users to utilize the data in the collections to develop their own narratives. This transformational addition to the efforts to democratize history provides the user with the data, the tools to analyze the data, and the digital space to organize the data in order to create a narrative. Working in Florida, students have access to a history that celebrates more than 500 years of interaction between Native Americans, Europeans, and Africans. The geographical position of Florida with its extensive coastline means that the history of the area has always been global. Interaction between the people of Caribbean Islands and those living in Florida pre-dated European claims for the region and expands the scope of scholarship. Successive waves of Spanish, French, British and American settlers altered the landscape and created new borderlands that continued until well into the twentieth century. Currently, the third most populous state in the nation, with an economy that ranges from agriculture to tourism to space exploration and high tech industries, Florida’s history is deep, diverse, and largely unexplored.  Students in graduate classes in Public History at UCF have completed three History Harvests and three additional Harvests are planned for 2016. The largest of the completed Harvests was accomplished through a partnership between the Introduction to Public History class, the Oviedo (Florida) Historical Society and a RICHES partnership tech firm, EZ-Photo-I/O Track. The students worked with the historical society to understand the history of the 137-year-old agricultural community that was transformed by the founding of UCF and the influx of thousands of students and faculty only three miles from the center of town. Based on the information provided by the historical society and a National Endowment for the Humanities-funded, society-published book (Adicks and Neely) the students conducted sixteen oral histories with descendants of the original settlers. In order to generate community awareness of the upcoming History Harvest students participated in public events and obtained permission to post information on the community and historical society social media sites. Building on the planning documents produced by previous student-led History Harvests, they organized the release forms, prepared to obtain the stories associated with items brought for scanning, and worked with EZ-Photo staff to facilitate the scanning process. More than 500 images, documents, three dimensional items, pamphlets, and scrapbooks were scanned on the day of the History Harvest. They told the story of citrus grove owners, farm laborers and fruit packers, migrants and immigrants to the community, schools, churches, and community social life.    Oviedo History Harvest, 2015  The UCF/RICHES History Harvest pushed the use of the collected data to a new, level by placing the material in RICHES Mosaic Interface™  https://richesmi.cah.ucf.edu . RICHES MI was initially created with the help of a National Endowment for the Humanities Startup Grant, and is an Omeka archive with multiple custom plugins and a front end that is a unique mechanism for searching the archive. RICHES MI users can search the database using natural language, tags, and topics, and browse by collection categories to maximize their search results; analyze the results of their search using the RICHES-developed “Connections” feature to show the relationship between the returned item and other items in the database; and visualize their results on a Google map and through digital exhibits, map overlays, and other visualizations. Users can save their search results in a Bookbag where they can annotate individual items, store items in folders, map the items collected in folders on a Google map and a timeline, and analyze the connections between items in the folders. They can also develop a narrative using the story board feature to organize their data. Finally, a search in the Omeka archive provides the user with links to digitized primary and secondary sources relevant to the individual object as well as links to other databases or websites that might be useful to the researcher.     Schematic of RICHES Functionality  Student Harvests, like their counterparts elsewhere, also create digital exhibits to organize the collected images and documents into an interpretative narrative. Faculty and students gain much through their participation in the History Harvests. Students acquire digital skills in metadata writing, archiving and exhibit creation; they learn to conduct oral histories and plan events; and they develop an understanding of the connection between local history and national/global issues. Each student’s work is cited by name in the metadata and in digital exhibits. Faculty participants are organizing larger projects (i.e. Parramore Project and Glass Bank Project) that include History Harvests and are expected to result in scholarly and pedagogical publications.    History Harvests in Omeka  The next three projects, which are funded by a National Endowment for the Humanities “The Common Good: The Humanities in the Public Square” grant, will push the boundaries of History Harvests further: students will be using the oral histories and the collections obtained through History Harvests to produce a documentary film on an African American community’s struggles with urban development, to understand a coastal community’s relationship to an iconic building, and to provide evidence for two MA thesis projects dealing with Orlando’s LGBTQ community. The African American community project pairs students in an undergraduate history class with students at a predominantly black high school to explore changes that the construction of public buildings, entertainment and sports venues, an interstate highway, and the proposed expansion of UCF to a downtown campus have had on what was once a vibrant and cohesive African American community. The material collected will be used by a second undergraduate Honors class to produce a documentary film. This will be the fourth student produced film supervised by the course faculty. The second History Harvest will focus on an iconic building, the so-called Glass Bank, in Cocoa Beach, Florida. Constructed in the 1960s, the Glass Bank was demolished in 2014, but not before a partnership between RICHES and the Institute for Simulation and Training produced a 3D scan of the building. Recreated as it existed in 1963, the Glass Bank will serve as a vehicle for collecting images, artifacts and stories about the building as a center of community activity and economic development. See the video of the 3D scan  https://www.youtube.com/watch?v=DtVETzCND4M Finally, students will work with the GLBT Virtual Museum and the Orange County Regional History Center to harvest images, oral histories, and artifacts that tell the story of the gay destination, Parliament House, and the effect of the HIV/AIDS epidemic in Orlando.   The RICHES project builds upon the insights of Public History scholarship and Digital scholarship to understand the complex past of local communities and to connect those histories to larger narratives. Using History Harvests to build a bridge between individuals, families, and local museums and historical societies and the academic community of students and faculty provides a service to the public and experiential learning for students. Awareness of the interaction between local and the national/global events enables students to consider their own scholarship in new ways. Placing the material harvested in an interactive database like RICHES Mosaic Interface democratizes the history collected by providing context, analytical tools, and space for organizing personal narratives.  ",
       "article_title":"Harvesting History: Democratizing The Past Through The Digitization Of Community History",
       "authors":[
          {
             "given":"Connie Lee",
             "family":"Lester",
             "affiliation":[
                {
                   "original_name":"University of Central Florida, United States of America",
                   "normalized_name":"University of Central Florida",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/036nfer12",
                      "GRID":"grid.170430.1"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "folklore and oral history",
          "digital humanities - pedagogy and curriculum",
          "teaching and pedagogy",
          "digital humanities - diversity",
          "historical studies",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Digital Humanities (DH) is becoming increasingly a collaborative community of practice, a move encouraged both by the scale, scope and complexity of projects (L. Siemens and Burr, 2013) and by granting agencies with programs such as Digging into Data (2013), the new Bilateral Digital Humanities Program between the National Endowment for the Humanities’ Office of Digital Humanities and the Deutsche Forschungsgemeinschaft, Germany’s research office (2014), Canada’s Social Sciences and Humanities Research Council’s Partnership Grants (2013), and many others. This trend is further supported through efforts such as Fair Cite (2012) and the Collaborators’ Bill of Rights (Off the Tracks, 2011) to more fully recognize project contributions through multi-authorship citation practices, University of Virginia’s Praxis Program which provides collaborative project experience for graduate students (The Praxis Program at the Scholars' Lab, 2011a, 2011b, 2012, nd), and individual DH project charters outlining guidelines for team work (Hjartarson, Fast, and Hasenbank, 2011; Ruecker and Radzikowska, 2007). These are all exciting developments that will reap long-term contributions at the individual, project, and DH as a community of practice levels and beyond. At the same, more work is needed to understand how these teams function and the types of supports which are needed to coordinate the research, people and tasks to ensure successful outcomes at these levels (Dombrowski, 2013; Lyall, Bruce, Marsden, and Meagher, 2013). With this knowledge, DH teams will be better able to anticipate benefits and challenges associated with collaborations and develop processes to maximize benefits while minimizing associated challenges.  At present, much of the body of knowledge about academic team functioning and best practice guidelines has been developed through a reflection process at a project’s completion (For example, see Bracken and Oughton, 2006; Bryan, Negretti, Christensen, and Stokes, 2002; Kishchuk, 2005; Lawrence, 2006; National Endowment for the Humanities Office of Digital Humanities, 2010). These reviews often focus on the actual research work accomplished with little discussion of associated processes that supported the work, communication patterns, and other factors that supported (or not) successful outcomes. As a result, some hard-earned lessons are forgotten or minimized through the passage of time, but might be captured if this type of reflection occurs during a project’s life.  As a large project in terms of team membership, budget, scope, disciplinary perspectives, and project length and research integration, Implementing New Knowledge Environments (INKE) provides a unique perspective to explore the nature of collaboration (INKE, 2012a). INKE’s primary research focus is the exploration of e-books and their potential from a variety of perspectives, including interface design, modeling and prototyping, user studies, and textual studies (INKE, 2012b). Further, this collaboration is examining the “understanding, creating, and evaluating research structures that will allow academic and non-academic (including industry partner) members of our research team to work together in ways that meet the needs of the research and development cycles of the entire INKE group” (R. G. Siemens, Siemens, Cunningham, Galey, Ruecker, and Warwick, 2012, p. 7) See R. G. Siemens et al. (2009) for the full grant application.. To that end, INKE has undergone yearly reflections on the nature of collaboration within the project with an objective to better understand the ways to support large-scale research collaborations as they unfold and communicate these lessons to other projects for consideration.   This paper contributes to our understanding of how research teams function by reviewing INKE’s six years of experience in collaboration and with a view to articulate best practice guidelines (L. Siemens and INKE Research Group, 2010, 2012b, 2012c, 2012d, 2012e, 2013). Members of the administrative team, researchers, graduate research assistants and others are asked about their experiences collaborating within INKE on an annual basis in order to understand the nature of collaboration and ways that it may change over a grant’s long-term life. The interview questions focus on understanding the nature of collaboration, advantages and challenges associated with it within INKE’s context. These interviews allow the researcher to explore topics more fully and deeply with probing and follow up questions while participants reflect on their own experiences and emphasis those issues which are important to them (Marshall and Rossman, 1999; McCracken, 1988; Newell and Swan, 2000; Rubin and Rubin, 1995). This paper focuses on a summary of the first 5 years of the grant-funded work. At the time of writing this proposal, final data analysis is being completed, but clear patterns are emerging and, after final analysis, these will form the basis of my presentation. While the grant application suggested a stable team of active researchers and partners, the reality has been very different. Due to a variety of reasons (L. Siemens and INKE Research Group, 2012a, 2013), change and transition have been constant within INKE, which has led to sub-research group re-organizations and the creation of new ones. Concurrently, new researchers, administrative leads and partners joined the team. And as is always the case when working with student research assistants and post-doctoral fellows, sub-research areas were continually recruiting and training new ones as others moved on to other opportunities.  Grounding this change has been several constants that have ensured the research has been able to continue effectively and efficiently despite the transitions. First, the governance documents outlined clear articulation of roles and responsibilities which became especially important for ensuring that new researchers, partners, and administrative understood and enacted the nature of collaboration and accountability within INKE. At the same time, these documents provided guidance for ensuring that processes for change, transition, planning and reporting, and decision making were considered and thoughtful while remaining responsive to changing circumstances (L. Siemens and INKE Research Group, 2012c). The use of basecamp, an online project space, and an updated project website further reinforced these processes by providing an ongoing repository for messages, documents, data, and publications, all important knowledge for current and new researchers and partners. Second, multiple communication channels, such as annual birds-of-a-feather gatherings, attendance at other conferences, both formal and in-formal face-to-face administrative and sub-research area group meetings, conference calls, and online project spaces, ensured that team members met on a regular basis to exchange information about and participate in research projects with the other sub-research areas, ensuring highly collaborative work. As INKE nears the end of its 7 years of funded research, the team is anticipating both winding down this focused work on e-books and exploring next research steps, building from its successes (INKE, 2014). While the research has been intellectually challenging and not without its administrative issues, INKE team members report positive experiences in terms of the collaboration, associated outcomes, and connections to the DH and traditional humanities communities. As measures of this positive spirit and connections, they are extending their collaboration into associated research areas as well as contemplating another large-scale research project.  This research will make several contributions to the knowledge base about the nature of collaboration within the DH community of practice. First, this research contributes to efforts to make work patterns and relationships more explicit and understand those factors that tend to predispose them to success, and perhaps, more importantly, to avoid those which may lead to problematic interactions. Already, lessons from INKE are informing other projects’ collaborations (Nowviskie, 2011; The Praxis Program at the Scholars’ Lab, nd). Second, INKE’s experience demonstrates that these types of team research projects require skills not typically taught in graduate school, including project management and collaboration within a targeted and integrative research environment, which differs from that of curiosity-based one. This reinforces the call to enlarge graduate training beyond purely disciplinary to these larger collaborative skills to ensure that students are prepared for both academic and alternative academic posts (Berman, 2011; Carr, 2012; Leon, 2011; Nowviskie, 2010; Powell, Bouchard, Dalgleish, Keenan, McLeod, and Thomson, 2013; L. Siemens, 2013; Spiro, 2010).  ",
       "article_title":"Change, Transition and Governance: Lessons from a Long-Term, Large Scale DH Collaboration",
       "authors":[
          {
             "given":"Lynne",
             "family":"Siemens",
             "affiliation":[
                {
                   "original_name":"University of Victoria, Canada",
                   "normalized_name":"University of Victoria",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04s5mat29",
                      "GRID":"grid.143640.4"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-02",
       "keywords":[
          "project design, organization, management",
          "digital humanities - institutional support",
          "English",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The nineteenth century provides a perfect setting for a digital humanities class as a result of the similarities between the industrial and digital revolutions and between the proliferation of print and periodicals and the rise of blogs and Twitter. Arthur Conan Doyle’s Sherlock Holmes stories are likewise the perfect subject matter: most of Holmes’s cases revolve around the technology of the day, from cabinet photographs in “A Scandal in Bohemia” to the typewriter in “A Case of Identity.” These connections enable students to learn about an earlier time period and literature while also historicizing their own technological moment. The resurgence in popularity of Holmes adaptations in recent years—Sherlock (BBC), Elementary, and Mr. Holmes, to name just a few—emphasizes these connections and also brings students into the classroom.  “Digital Tools for the 21st Century: Sherlock Holmes’s London” (taught from the Fall of 2014 through the Fall of 2015) is an introductory digital humanities class that uses Holmes stories as a corpus on which to practice a wide variety of basic digital humanities methodologies and tools, including visualizations, digital archives and editions, mapping, and distant reading. “Digital Tools” unites theory and practice with a tripartite structure for each unit, which I have dubbed “Read, Play, Build.” First, students read articles from books and blog posts about the pros and cons of each methodological approach. They then examine current projects to discuss the ways that each enhances scholarly fields and poses new research questions. Each unit concludes with an in-class lab component, in which they build a small project using a well-known tool. For example, in the archives and editions unit, students examine Jerome McGann’s “Radiant Textuality,” discuss the importance of preservation, access, and challenging the canon, examine The Rossetti Archive (and many others), and use the tool Juxta Editions to create their own, fully transcribed digital edition of a Sherlock Holmes story of their choice, using page images of the original printing in The Strand Magazine to learn about XML, basic bibliography, and best editorial practices. Likewise, in the unit on distant reading and topic modeling, they read Franco Moretti’s “The Slaughterhouse of Literature” and Ted Underwood’s blog posts on topic modeling, examine Mining the Dispatch to see that methodology in action, and topic model all 56 Sherlock Holmes short stories and analyze the trends in a blog post.  Although this class uses Holmes stories as base texts, it also situates these stories in their historical and cultural context by examining Victorian digital humanities projects from fields other than English. Students explore the Proceedings of the Old Bailey, a searchable archive of all court records in the Old Bailey from 1674-1913, to learn about crime, class, and the legal system. They also examine the David Livingstone 1871 Field Diary to learn about empire and Charles Booth’s poverty maps from 1898-1899 from the Charles Booth Online Archive to learn about socioeconomic conditions in London and to compare Conan Doyle’s fictional London to the actual city.  This paper will present the class in greater depth and will provide examples of the digital projects students collaboratively created—from contributing to the marginalia project Book Traces to making digital narrative maps of Holmes stories with Mapbox—in order to provide new models for student scholarship and their role in the future of the English departments and the humanities. It will also discuss the importance of the Holmes stories as the corpus for the class, as the character of Holmes himself provides a useful model for a digital humanist, especially when students may be unused to thinking about data in a humanities context, as he combines data science with humanities skills of close reading, archiving, and a love of literature and music. Teaching Holmes with digital tools lets students build on the traditional humanities skills of close reading, understanding patterns, and using archives, and augments that scholarly toolkit by guiding students to a better understanding of rhetorical patterns and spatial significance, while also teaching them about collaboration, interdisciplinarity, and public humanities. In accordance with its public humanities focus, the course’s materials, including the syllabus and assignments, are publicly available on the class’s website (https://hawksites.newpaltz.edu/dhm293/). By melding research with project-based learning, this course enables students to engage with research and Victorian history more actively than is common at the introductory level. ",
       "article_title":"Read, Play, Build: Teaching Sherlock Holmes through Digital Humanities",
       "authors":[
          {
             "given":"Joanna Elizabeth",
             "family":"Swafford",
             "affiliation":[
                {
                   "original_name":"SUNY New Paltz, United States of America",
                   "normalized_name":null,
                   "country":"United States",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016",
       "keywords":[
          "digital humanities - pedagogy and curriculum",
          "teaching and pedagogy",
          "literary studies",
          "english studies",
          "English",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction This paper draws on results of a three-year ethnographic study funded by the Andrew W. Mellon Foundation and the Royal Netherlands Academy of Arts and Sciences, which explored research practices, challenges, and directions in contemporary digital humanities (DH). Within this broader set of questions and results, the current paper focuses on findings related to humanists’ strategies for developing competencies in digital scholarship. The study was conducted from 2010 to 2013 at twenty-three educational, research and funding institutions in the United States and Europe, and it included case studies, surveys, in-depth interviews, and observations. The study involved 258 participants, including researchers, faculty, students, university administrators, librarians, software developers, policy makers, and funders. For more information about the methodological design and results of the study see  Amongst Digital Humanists: An Ethnographic Study of Digital Knowledge Production (Antonijević, 2015).    Results The majority of humanists consulted in this study reported awareness of methodological and epistemological benefits of digital research tools and methods, but also a lack of opportunity to acquire skills and knowledge that would enable them to reach beyond the “search and access” level of digital scholarship. As one assistant professor of art history put it, “I haven’t used technology in my research in a pervasive way to really, really think about epistemological issues. I’m not opposed to using technology to analyze, but I haven’t had a chance to learn it.”  Humanists commonly identify lack of time as one of the main impediments to developing digital research skills. One aspect of this is the learning curve, and a perception that the time needed to learn new tools and methods slows down their established research process. Another root of researchers’ lack of time for developing computational skills stems from the structure of disciplinary incentives and rewards in humanities disciplines. Respondents underscored that when training sessions on digital research tools and methods were organized at their universities, hardly any of the tenure-track faculty attended them: “We are not rewarded for doing that. What we are rewarded for is publishing, and going to one of those sessions takes away [time] from our publishing. So, there’s a lot of resistance“ explained an assistant professor of linguistics.  With digital skills still having an unrecognized status in their disciplines and departments, interviewees said the only organized educational initiatives at their disposal were training sessions at university libraries, to which they had mixed reactions. While some found the library sessions “eye-opening” the majority did not regard them as helpful. Commonly, respondents pointed that the librarians focused on digital tools and resources, while field-specific research questions exceeded their scope. The respondents held that only their peers understand epistemological and methodological complexity of particular research problems, so they considered working with colleagues as a more effective way to develop digital skills than attending library workshops or instruction sessions. In the same way, respondents identified as the preferred type of instruction the one that does not profess to teaching about digital technologies, but about a specific humanities subject area, introducing digital tools and methods along the way. Attending library and similar workshops was thus seen as less effective then attending academic conferences where peers present results achieved through digital methods and tools. Learning by example inspires humanists to discover new tools and methods, and to apply them in their own work. As a professor of Romance languages and literature put it, “in an ideal world, I would like to see humanists teaching other humanists how to conduct research using the enhancements of digital tools; I guess that’s as opposed to we bring the IT people in to teach us. I think that it should happen the way that we teach other things in the humanities, through collaboration with one another.” Dissatisfaction with existing educational initiatives might be the reason that the majority of humanities scholars consulted in this study reported not having any formal technology-related instruction, which is consistent with Siemens’ (2013) findings. Instead, the respondents reported that they predominantly relay on informal channels, such as word of mouth, to learn about digital research tools and methods: “I do everything on my own, I ask around. It feels serendipitous, I sort of bump into it, or I hear a friend talk about it, or a colleague will shoot me an e-mail. It’s not organized or strategic at all” related an associate professor of philosophy. This informal learning path is linked to immediate and specific research problems scholars are facing, which makes it preferred over workshops and similar efforts where learning is often decontextualized from practice. This method also successfully makes use of one of the scholars’ most scarce recourses—their time. It enables them to direct learning efforts towards tools, methods, and subjects of particular interest to them. Informal learning also often takes place through engagement with students. The respondents explained that teaching prompts them to expand knowledge of digital tools and methods, and students challenge them to be up-to-date with technology. Even those respondents who described themselves as “technological dinosaurs” identified the need to take up digital technologies in the classroom. In addition to class interaction, more formal initiatives where students teach faculty take place in digital humanities centers, where graduate students often work as tutors. Students consulted in this study believed that this reversal of instructional roles facilitated their understanding of the didactic principles, motivating them to develop their own pedagogic strategies: “You’re doing work with people who are the smartest people on the earth in their particular discipline. On average, they don’t like to admit that they don’t know something. So, it’s not trying to force things upon them, but it’s trying to present things in the same way that you would pedagogically teach a difficult concept. If I can present it it in such a way that leads them to discovering on their own, they have a sense of ownership of the idea, so that’s “teaching the old dog new tricks” if I can say that.” [A graduate student]   Conclusion Findings of this study indicate that library initiatives and units should not remain the main locus of digital scholarship in the humanities. Instead, digital scholarship needs to be part of humanities departments and wider university initiatives, since “digital humanities will ultimately matter, or not at all,  inside the department” (Liu, 2009: 21; italics in the original). As long as humanists’ interaction with digital tools, methods, and resources is treated merely as a technical skill that can be taught by non-expert personnel, it will be difficult to achieve more substantial transformations and to motivate academics about digital scholarship. As Raley (2014) points out, “academic service staff providing skills-based training […] and performing service work for “clueless arts and humanities scholars” can tell us something about both the field and the university” (p. 7).  Instead of skilled-based training, humanities education in digital scholarship needs a comprehensive framework encompassing epistemological, methodological, technical, and sociocultural aspects of digital knowledge production. These include developing understanding of digital and other types of data, fostering critical reflection on digital objects of inquiry, comprehending the influence of algorithmic processes on humanities investigations, and so on. Similarly, digital methods training should include systematic deliberation on methodological decisions influencing research process and results, epistemological and ethical challenges of digital scholarship, as well as making choice of digital tools and methods as best suited for specific research questions. This study reveals that humanists favor and best learn in practice, when instruction is closely related to their area of study and when it unfolds organically, through collaboration with colleagues and students. Therefore, initiatives for developing competencies in digital scholarship among senior scholars should use a variety of collaborative learning strategies. Furthermore, these educational activities should not restrain the generative potential of digital scholarship in the humanities through the exclusive focus on research themes, methods, and skills recognized in the DH field. Humanities scholars do not necessarily need or want to be digital humanists; they do, however, need and overwhelmingly want to be scholars competent at teaching and conducting research in the digital age. As a program officer in a major humanities foundation consulted in this study pointed out, the goal should be “to fund training for scholars even if they don’t want to be a digital humanist in the sense that they’re building their own tools and their programming, but more along the lines of users of digital technology within their own research.” This kind of funders’ support to digital scholarship in the humanities is vital. Equally vital is the need that education in digital scholarship becomes administratively recognized as part of scholars’ professional development included in their paid time and activity, as well as in their promotion dossiers.  ",
       "article_title":"Developing Competencies in Digital Scholarship Among Humanities Scholars",
       "authors":[
          {
             "given":"Smiljana",
             "family":"Antonijevic Ubois",
             "affiliation":[
                {
                   "original_name":"Penn State University, United States of America",
                   "normalized_name":null,
                   "country":"United States",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-09",
       "keywords":[
          "digital humanities - pedagogy and curriculum",
          "teaching and pedagogy",
          "digital humanities - institutional support",
          "cultural infrastructure",
          "anthropology",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Letters are an important historical source: First, they may contain comments from contemporaries about the most different events, persons, publications, and issues. Second, letters allow insights about connections and networks between correspondence partners. So, questions occur which can only be answered across the borders of scholarly letter editions due to the fact that these editions are usually focussed on partial correspondences (of a certain person or between two specific persons). But this requires time-consuming searches across various letter editions. This has been a well-known problem for quite some time, now (Bunzel, 2013: 117) and has already evoked work on a few databases dedicated to correspondence, like e.g. “Early Modern Letters Online”.   Early Modern Letters Online (EMLO),  http://emlo.bodleian.ox.ac.uk/, is the one with the largest databases, but also includes data from other databases like “Circulation of Knowledge and Learned Practices in the 17th-century Dutch Republic”,  http://ckcc.huygens.knaw.nl/. Besides this, there also exist more focused databases like “Exilnetz33”,  http://exilnetz33.de.   But these databases have some limitations: firstly, they focus on specific research questions, time periods, geographic areas, or certain material.  Secondly, they don’t provide an open, standard-based and well-documented way to provide and update data. Furthermore, the data can be searched and displayed, but only on a simple website. In none of the existing databases dedicated to edited letters it is possible to query and retrieve the data via an Application Programming Interface (API) and under a free license for subsequent use.  The mentioned methodic problem has lead Wolfgang Bunzel, who works in the field of research about the Romanticism, to request: ‘the creation of a decentralized, preferably open digital platform, based on HTML/XML and operating with minimal TEI standards, which is extensible in different directions and allows for existing web portals and websites to contribute at the lowest possible cost. This doesn’t request some kind of super structure which covers the entire amount of letters from the Romantic era (which could not be estimated exactly, anyway) but rather an intelligent linking system, which associates existing documents with one another. The creation of such nexus will naturally lead to research options reaching from searches for persons and places to specific keyword-based searches [...]’ (Bunzel, 2013: 123)   Please note, that this is my own translation into English.   With “correspSearch” ( http://correspSearch.bbaw.de) this paper will present a web service, which takes a step in this direction by aggregating metadata of letters from various (digital or printed) scholarly editions and providing them collectively via open interfaces (Fig. 1). In doing so, it is independent from specific research questions as well as from temporal, geographic, or thematic limits.  The basis of the web service are digital indexes of letters provided by the editions in the “Correspondence Metadata Interchange Format” (CMIF), that has been (and will further be) developed by the Correspondence Special Interest Group of the Text Encoding Initiative (TEI).   The CMIF is maintained in a GitHub repository:  https://github.com/TEI-Correspondence-SIG/CMIF   The CMI format is based on the TEI Guidelines and allows to interchange metadata of letters, postcards etc. between scholarly editions by restricting and normalizing the essential elements of a communication act, namely sender, addressee, dates and places of writing and receiving. Besides the consistent TEI XML encoding, interchange will be enabled by usage of ISO dates and authority controlled identifiers. Sender, addressee, sender’s place as well addressee’s place are identified unambiguously using authority IDs, as e.g. provided by the Library of Congress.   For using authority files in scholarly editions, cf. Stadler 2012  When reading the indexes of letters the web service retrieves the most common authority controlled IDs from the Virtual International Authority File (VIAF). This way, IDs from different authority files are mapped onto one another. Up till now, the web service supports VIAF, GND (“Gemeinsame Normdatei” from the Deutsche Nationalbibliothek) as well as the authority files of the Bibliothèque nationale de France (BNF), the Library of Congress (LC), and the National Diet Library (NDL) in Japan. As for place names the web service uses “GeoNames”.  The scholarly editions themselves provide such digital indexes of letters in CMI format, online and under a free license (CC-BY 4.0). For this purpose the CMI format and its creation process is extensively documented on the correspSearch website including a FAQ section.   See  http://correspsearch.bbaw.de/index.xql?id=participate   After providing a CMI file online, it is only necessary to register its URL for the web service. After that the file is automatically retrieved by correspSearch periodically (and in that way updated, if necessary).   The aggregated letter indexes are searchable on the correspSearch website by correspondent, location, and date. Correspondent and location can be specified according to their role in the communication process. Search results are displayed based on the metadata of the individual letter, together with biographical details. Letters from digital editions are directly linked. Apart from the website an API has been implemented which allows for automatic requests to the web service.   See  http://correspsearch.bbaw.de/index.xql?id=api   In this scenario, the results are provided in TEI-XML in the described CMIF under a CC-BY 4.0 license, thus ensuring and facilitating further use and processing of the search results. Furthermore, the web service offers BEACON files as well as an experimental TEI-JSON output.    Thanks to the API it is possible to automatically refer or even link from one digital letter edition to related letters provided by other editions. This function was already implemented in a prototype for the digital scholarly edition “Schleiermacher in Berlin 1808-1834” (Fig. 2).   The first version of the scholarly digital edition “Schleiermacher in Berlin” will be published in the next months by the Berlin-Brandenburg Academy of Sciences and Humanities.  This feature helps researchers avoid methodological problems when interpreting a piece of correspondence: When analyzing a letter they usually consider the preceding and following letters in the correspondence between the sender and addressee, as well. However, their interpretation often does not include the letters which the correspondents send to or receive from  other persons. With this feature the background of historical correspondences can be easily explored.  Via the API scholars can also exploit the data basis by usage of their own innovative technologies as well as of technologies which the web service itself does not yet support technically. Therefore, with a sufficiently extended data basis and the suitable software it will be possible to perform research on e.g. social or correspondence networks based on correspSearch.  For example: the developers of the visualisation tool “nodegoat” imported the data in their application to visualize a correspondence network:  http://correspsearch-test.nodegoat.net/viewer.p/4/136/scenario/1/geo/fullscreen   Furthermore, the correspSearch API was connected with the web service “XTriples”, developed by the Academy of Sciences, Humanities and Literature in Mainz (Germany). Thus the results can be converted into RDF and provided for further analyses with the help of semantic web technologies.   http://xtriples.spatialhumanities.de. One prototype configuration is available under  http://xtriples.spatialhumanities.de/examples.html      The web service correspSearch and the CMIF are still under development. In the future it should be possible to search also for mentioned persons, events, publications etc. For this purpose the enhancement of the CMI file is currently discussed (Dumont, 2015). Also additional authority files will be supported, e.g. the Getty Thesaurus of Geographic Names.   http://www.getty.edu/research/tools/vocabularies/tgn/    The web service correspSearch was granted the “Berlin Digital Humanities Award 2015” (First Prize, endowed with 1.200 €).   Fig 1: Operating principle of the web service correspSearch    Fig 2: Screenshot of the digital scholarly edition “Schleiermacher in Berlin 1808-1834” (published soon), which presents letters to and from the theologian Friedrich Schleiermacher  The digital scholarly edition queries the correspSearch API for other letters from and to August Boeckh around the date of the letter displayed (10 september 1810). If there is a result, the edition provides links to these letters. ",
       "article_title":"correspSearch - A Web Service to Connect Diverse Scholarly Editions of Letters",
       "authors":[
          {
             "given":"Stefan",
             "family":"Dumont",
             "affiliation":[
                {
                   "original_name":"Berlin-Brandenburg Academy of Sciences and Humanities, Germany",
                   "normalized_name":"Berlin-Brandenburg Academy of Sciences and Humanities",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/05jgq9443",
                      "GRID":"grid.420264.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-05",
       "keywords":[
          "linking and annotation",
          "standards and interoperability",
          "scholarly editing",
          "English",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" New technologies can now be used to fabricate old ones. With rapid prototyping techniques, a nineteenth-century mechanism can be downloaded from an online library, translated into code, fed to a 3-D printer, and used to repair a watch, all in about an hour. While such 3-D fabrication techniques tend to fetishize objects, this talk proposes an alternative: “prototyping the past,” or remaking technologies that no longer function, no longer exist, or may have only existed as fictions, illustrations, or one-offs. Conceptually, prototyping the past explains why technologies matter by approaching them as representations  and agents of history. Practically, it creates media that function simultaneously as evidence  and arguments for interpreting the past. Yet most important, it does more than re-contextualize media history in the present. It integrates that history into the trajectories of design practice.   This full circuit through digital and analog manufacturing corresponds rather provocatively with a “materialist turn” in media studies, where, since at least the late 1990s, scholars have stressed the importance of media as physical entities that resist or diffract interpretations as often as they facilitate them.   For examples of this work, see Kittler (1999), Gitelman (1999, 2006, 2014), Bowker and Star (2000), Sterne (2003, 2012), Galloway (2006), Kirschenbaum (2008), Vismann (2008), Chun (2011), Ernst (2012), Parikka (2013, 2015), and Starosielski (2015).  Writing about phonography, Lisa Gitelman asserts, “At certain levels, media are very influential, and their material properties do (literally and figuratively)  matter, determining some of the local conditions of communications amid the broader circulations that at once express and constitute social relations” (2006, p. 10). Somewhere between technological determinism   Technological determinism asserts that technologies cause cultural and social phenomena. For more, see Postman (1992).  and social constructivism,   Social constructivism asserts that cultural and social phenomena precede or shape the development of technologies. For more on social constructivism, see Pinch and Bijker (1984). For an overview of technology and cultural criticism, see Sayers (2014).  this assertion accounts for how—as both representations  and agents of history—media play an active role in reproducing the past. Following Gitelman, in the following paragraphs I situate media history in rapid prototyping research, looking specifically at the effects of fabricating tactile models to “prototype the past.” While instrumental uses of rapid prototyping tend to fetishize objects, encourage commodification, foster nostalgia, or ignore the political dimensions of material culture, I argue that rapid prototyping can help researchers demonstrate how social and cultural complexity is intricately entangled with the historical particulars of design.   Put this way, the combination of media history with rapid prototyping corresponds with how Wendy Chun describes the aims of the  New Media, Old Media anthology, which focuses on the “actualities of the media itself” and “the experience . . . of being entangled within it” (2005, p. 9).    Ultimately, what distinguishes rapid prototyping from most other approaches to media history is its investment in not only translating between 2-D and 3-D modes of trial-and-error production but also communicating across a spectrum of media that operate simultaneously as evidence and arguments for future work. Put simply, prototyping the past is more than re-contextualizing media history in the present. It is an opportunity to meaningfully integrate that history into the social, cultural, and ethical trajectories of design practice.   More common in engineering and architecture than the humanities, rapid prototyping entails producing materials through a combination of computer numerical control (CNC) machines—such as 3-D printers (additive manufacturing) and routers (subtractive manufacturing)—with manual approaches to wood, paper, clay, etc. The aim is to subject a 3-D model to repeated feedback throughout the development process. In this sense, the design cycles are small, not grand. Also, the models are versioned. Instead of working toward a single model, multiple models are maintained throughout production. This approach is steeped in “design-in-use,” which privileges situated activity over some ideal model or user (Botero, 2013). Through design-in-use, a prototype is treated like a congealed dialogue between interested groups.  With the above in mind, this talk explains why scholars of media history may wish to prototype the past. It builds on especiwork by Kraus (2009), Balsamo (2011), Ratto (2011), Buechley (2012), Perner-Wilson (2012), Ames (2014), Hjorth (2014), Jungnickel (2014), and Rosner (2014)., and It also they in part corresponds with arguments published in “New Old Things” (2012), by Elliott, MacDougall, and Turkel. There, Elliott  et al. argue that “matter [is] a new medium for historical research. Working with actual, physical stuff offers the historian new opportunities to explore the interactions of people and things” (2012, p. 122). Also, rapid prototyping may frame prototypes may be understood as  situations for interpretation, without creating exact reproductions of historical artefacts (127). By extension, using matter as a medium for historical research need notneed not fetishize the past. Instead, itt becomes a time and space to interpret the material intricacies of technological design, both now and then.   Perhaps most important, prototyping the past allows scholars to remake technologies that no longer function, no longer exist, or may have only existed as illustrations, fictions, patents, or—fittingly enough—prototypes. TheseOne appeal of prototyping the past is that remade technologiesremade technologies may be circulated as tangible reminders of what was forgotten or destroyed. Yet prototyping the past also affords critiques of what is ready to hand. That is,  it refuses to take historical materials at face value. Through trial and error, it tests the plausibility of historical claims. After all, what is depicted in a text may contain redactions, deliberate omissions, purposeful obfuscations, or accidental occlusions. Using historical materials to prototype a technology amplifies the meaningfulness of these absences.   Put this way, prototyping the past is intertwined with close reading. However, its emphasis on physically remaking historical technologies expands hermeneutics to include the centrality of translation and tacit knowledge toin media history. By re-contextualizing historical technologies in the present, prototyping also accentuates differences across time. What was once an innovation in the 1860s becomes a relic in 2015. Alternatively, these differences across time may turn things of the past into the stuff of present-day current curiosity., thus complicating distinctions between old and new.  Rather than transcending such differences, prototyping the past grounds media history in a particular thing and the interpretations it affords. Following the work of Karen Barad, such grounding posits prototypes as entanglements of meaning with matter by attending to the substance of “fine-grained details” (Barad , 2007, p. 90). Here, neither meaning nor matter can be relegated to a concept or abstraction. Again, situations are significant. And prototyping reminds scholars of that significance. It is an embodied process involving frustration and surprise, trial, and error, and it highlights how technologies do not emerge effortlessly from the brilliant minds of inventors or makers. Prototyping also reminds scholars that 1) the sources of meaning are forever unstable and under dispute, 2) historical materials are not complete“total” works but rather compositions of parts that change—degrade, rot, morph, warp, break, or swell—over time, 3) prototypes can be de- and remanufactured without significant material consequence, and 36), as noted earlier, materials resist or diffuse as many interpretations as they facilitate.  Example Application: Kits for Cultural History Based at the Maker Lab in the Humanities (“Lab”) at the University of Victoria, the Kits for Cultural History (“Kits”) remake technologies from the past, package them in bespoke containers, contextualize them with historical materials, and encourage people to experiment with them. Comparable to Heathkits, the Kits include components and guides for assembly. However, the guides are steeped in cultural history and do not assume a single approach to assembly. By design, this resistance to uniformity is essential, since the Kits focus on technologies that are inaccessible today. These technologies are not found in galleries, libraries, archives, or museums; they no longer function as they once did; or they were never actually built or mass-manufactured. Such inaccessibility necessarily entails a degree of uncertainty where research is concerned. Rather than approaching this uncertainty at a remove,  the Kits prototype absences in the historical record and prompt audiences to examine the contingencies of that record. Anchored in design-in-use, this method approach presents prototypes as negotiations or situations for interpretation, not replicas.   The Kits’ design cycle may be visualized as follows:    Figure 1: Design Cycle for the Kits  Once the Lab selects a technology for remaking, we historicize it through archival materials. Informed by existing theoriesy, we then speculate about absences in the record and determine how those absences might manifest in tactile form. Next, we model, fabricate, and assemble the technology’s component parts into prototypes, which we test and share with other researchers. After feedback, the Lab writes about the process and related history. When bundled together, the writing, prototyping, and testing refine ourthe research, and the cycle is repeated until we deem a Kit persuasive. Once a Kit is ready for circulation, we publish it in tactile form, as a repository of digital files, and through an exhibit. With these, the Lab also authors articles about the Kit’s contribution to media history, theorhistory, and design. We treat these publications—the tactile Kit, repository, exhibit, and article—equally as elements of scholarly communication.  Throughout this cycle, we ask several questions of what we are prototyping: 1) Who made it? For whom? When? 2) How was it made? How did it work? How was it used? 3) Do any instances of it exist? If so, where are they? Can they be handled? 4) Under what assumptions was it made and used? 5) How might prototyping it shape design in the future?  We have used this process to produce three Kits thus far: an Early Wearable Kit (for an electro-mobile skull stick-pin from 1867), an Early Magnetic Recording Kit (for experiments involving steel wire, telephones, and carriages during the late 1890s), and an Early Optophonics Kits (for a reading aid patented in 1919).  Findings: Rapid Prototyping and Media History, Together The Kits support the following observations about the articulation of CNC techniques with media history: 1) prototyping the past demands methods from the humanities, engineering, and fine arts; 2) prototyping is not always futurist or restricted to forecasting; 3) 3-D media such as tactile models are not more persuasive than 2-D media such as illustrations; both include exaggeration and omission; 4) history remains inaccessible even with access to physical materials, which neither resolve issues of absence nor guarantee certainty about the past; 5) prototyping the past may actually resist nostalgia; 6) as with any research method, prototyping is not immediate and cannot access “real history”; 7) prototyping may be premised on  not replicating history—on what we should  not repeat; 8) prototyping tests suspicions of history by grounding them in fine-grained details of matter and meaning; and 9) prototyping the past need not aim for a rational history without remainders. Instead, it can recognize how the technologies we use to reproduce history exceed our control and understanding. Indeed, the speculative elements of prototyping can be anchored in the specificities surrounding historical absences—of what we cannot prove or do not know for sure but certainly shapes us.   Acknowledgments The Social Sciences and Humanities Research Council, the Canada Foundation for Innovation, and the British Columbia Knowledge Development Fund have supported this research.  ",
       "article_title":"Using Computer Numerical Control Techniques to Prototype Media History",
       "authors":[
          {
             "given":"Jentery",
             "family":"Sayers",
             "affiliation":[
                {
                   "original_name":"University of Victoria, Canada",
                   "normalized_name":"University of Victoria",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04s5mat29",
                      "GRID":"grid.143640.4"
                   }
                }
             ]
          },
          {
             "given":"Tiffany",
             "family":"Chan",
             "affiliation":[
                {
                   "original_name":"University of Victoria, Canada",
                   "normalized_name":"University of Victoria",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04s5mat29",
                      "GRID":"grid.143640.4"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "project design, organization, management",
          "cultural studies",
          "creative and performing arts, including writing",
          "media studies",
          "data modeling and architecture including hypothesis-driven modeling",
          "GLAM: galleries, libraries, archives, museums",
          "digitisation, resource creation, and discovery",
          "historical studies",
          "digitisation - theory and practice",
          "3D Printing",
          "archives, repositories, sustainability and preservation",
          "English",
          "audio, video, multimedia",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction This study aims to grasp the regional differences in the musical characteristics inherent in the traditional Japanese folk songs of the Chugoku district (the westernmost region of Japan’s largest island of Honshu) by extracting and comparing the characteristics of each area by conducting quantitative analysis in order to promote digital humanities research on traditional Japanese folk songs. We have sampled and digitized the five largest song genres within the music corpora of the  Nihon Min’yo Taikan consisting of 1,794 song pieces from 45 Japanese prefectures, and have clarified the following three points by extracting and comparing their respective musical patterns (Kawase and Tokosumi 2011): the most important characteristics in the melody of Japanese folk songs is the transition pattern, which is based on an interval of perfect fourth pitch; regionally adjacent areas tend to have similar musical characteristics; and the differences in the musical characteristics almost match the East-West division in the geolinguistics or in the folkloristics from a broader perspective.  However, to conduct more detailed analysis in order to empirically clarify the structures by which music has spread and changed in traditional settlements, it is necessary to expand the data and do comparisons based on the old Japanese provinces (ancient administrative units that were used under the ritsuryo system before the modern prefecture system was established).   Procedure In order to digitize the Japanese folk song pieces, we generate a sequence of notes by converting the music score into MusicXML file format. We devised a method of digitizing each note in terms of its relative pitch by subtracting the next pitch height for a given MusicXML. It is possible to generate a sequence  T that carries information about the pitch to the next note:  T = ( t 1,  t 2, … ,  t i, …,  t n). An example of the corresponding pitch intervals for  t i can be written as shown in Table 1. We treat sequence  T as a categorical time series, and execute n-gram analysis by conducting unigram, bigram, and trigram patterns to clarify major transitions and their trends in the Chugoku district.    Table 1: Corresponding Pitch Intervals    Overview of Data In order to quantitatively extract pitch transition patterns from Japanese folk songs, we sampled and digitized folk songs included in the  Nihon Min’yo Taikan. We sampled all the songs in the music corpora included in the Chugoku Region volume (1966). Table 2 shows the statistics about the song pieces from each area in the Chugoku district. Figure 1 is a map of the provinces in the Chugoku district.    Table 2: Basic Statistics for Number of Songs for Each Province    Figure 1: Geographical Divisions of the Chugokuk District Under the Old Province System    Results  Frequency of the First Transition Figure 2(a) shows the usage frequency of the first transition patter (unigram). The graph implies that pitch transitions occur almost equally in both the ascending and descending directions. Figure 2(b) is a cumulative relative frequency diagram of the first transitions for ascending and descending order that confirms the trends for each interval. The profile shows that most of the voice range does not extend beyond the interval of perfect fifth (±7).   Figure 2: (a) First Transition Frequency and (b) Cumulative Relative Frequency Diagram  Based on studies of musical psychology, step-by-step changes in pitch height (in other words, pitch interval transitions that always involve ascending or descending order) are an indispensable factor by which humans perceive melodic patterns (Scruton 1997). Therefore, to extract the structures buried under repetition of notes, we will exclude the 0 intervals from sequence  T in order to seek the pure changes of ups and downs within the melody.    Overall Summary of Bigram Patterns Table 3(a) shows the top 40 bigram patterns. These components do not exceed the range of the perfect fifths (±7), and the values of the chains (intervals) also add up to a pitch interval between -5 and +5. In addition, we find that transition patterns that add up to 0 ( t i +  t i +1 = 0), transition patterns that add up to ±5 ( t i +  t i +1 = ±5, which is the interval of the perfect fourth), and patterns that include the interval of the perfect fourth in either component (e.g. patterns that form ( t i,  t i +1) = (±5, *), (*, ±5)) also appear with high frequency.    Table 3: Frequency of Occurrence of (a) Bigrams and (b) Trigram (Top 40 Patterns)    Overall Summary of Trigram Patterns Table 3(b) shows the top 40 trigram patterns. These components do not exceed the range of perfect fourths (±5), and the feature that stands out is that some of the top-ranked patterns are patterns with a single pitch transition attached to bigram. Most of the transition patterns fall in the range of the intervals of perfect fifths (±7). Furthermore, there are many high-ranked patterns in which the beginning two transitions or the last two transitions add up to form either the same degree of pitch ( t i +  t i +1 = 0 or  t i +1 +  t i +2 = 0) or a perfect fourth pitch ( t i +  t i +1 = ±5 or  t i +1 +  t i +2 = ±5).     Discussion  Koizumi’s Tetrachord Theory To speed things along, we will mention Koizumi’s tetrachord theory (1958). The tetrachord is a unit consisting of two stable outlining tones (nuclear tones) with the interval of a perfect fourth pitch, and one unstable intermediate tone located between them. Depending on the position of the intermediate tone, four different types of tetrachords can be formed (Table 4).    Table 4: For Basic Types of Tetrachords  Using a bigram model representing pitch transitions, all four types of tetrachords can be expressed as follows in ascending order:  min’yo (+3, +2),  miyako bushi (+1, +4),  ritsu (+2, +3), and  ryukyu (+4, +1). Depending on the positions of the three initial pitches in a tetrachord, six transition patterns can be considered in perceiving a tetrachord in two steps (bigram). Therefore, the amount of tetrachords within two steps can be obtained by counting the pairs of 24 transition patterns in sequence  T.    Table 5: Number of Transition Patterns for the Four Tetrachords    Province Similarities Based on the Frequency of Occurrence of Tetrachords Table 5 shows the frequency that the 24 transition patterns for the four tetrachords appear for all 12 provinces. Here, we did hierarchical cluster analysis on the 24 transition patterns for each province to objectively demonstrate similarities in the 12 provinces. The dendrogram in Figure 3 shows hierarchical cluster analysis results. When calculating distances between each element, we normalized the frequency that the tetrachords appear, and used the Euclidean distance and the algorithm from the Ward method.   Figure 3: Cluster Analysis Results  If we look for  h = 60 (a height where there are three vertical lines) and trace them to the individuals, this partition of three clusters separates the 12 provinces into  C 1 = {Aki, Suo},  C 2 = {Mimasaka, Bizen, Bitchu, Nagato, Inaba, Hoki, Izumo, Oki}, and  C 3 = {Iwami, Bingo}, which almost classifies the Chugoku district into geographically close groups. Meanwhile, it also stands out that only Nagato is separately geographically isolated.    Sea Route Connecting the Provinces The cluster analysis results did not turn out to match the famous two ancient divisions based on the main road: San’yo = {Mimasaka, Bizen, Bitchu, Bingo, Aki, Suo, Nagato} and San’in = {Inaba, Hoki, Izumo, Iwami, Oki}. However, instead it is possible to consider each cluster based on sea routes (Figure 4). Since ancient times, the Chugoku district has been an area travelers passed through when traveling from the Kansai district (a center of regional culture in Japan) to get to Kyushu region, which had close relations with China and Korea. Since there wasn’t much flat land and there are many mountain ranges, sea routes played an important role in transportation. The existence of the Ota River, a major river and also a sociocultural and economical foundation in the Chugoku district, supports the grouping of the two provinces in  C 1. From an oceanic commerce point of view, the north and south sea routes that link the Nagato and Kansai regions also support the grouping of providences in  C 2. It is also possible to say that the Gonokawa River, which functioned as a traffic relay point between San’yo and San’in, supports the grouping of provinces in  C 3. In this way, we find that rivers have important functions that exert musical influence as well as indicate boundaries between districts and geographical areas.    Figure 4: Sea Routes Map of the Chugoku District     Conclusions In this paper, we digitized the melodies of traditional Japanese folk songs in the Chugoku district, and quantitatively analyzed its pitch transition patterns using n-gram modeling, and did a classification experiment based on the frequency of occurrence of the tetrachords to see the differences in each province. As a result, we constructed the possibility that the melodic features in the Chugoku district spread by land and sea routes based on actual music data analysis. However, we should be able to describe the relationships influencing musical culture between regions in detail if we develop this analysis on a nationwide scale. In further research, it is possible to clarify the structural commonalities and differences between areas by conducting analysis on musical corpora nationwide including folk song pieces from neighboring regions such as the Kyushu, Shikoku, and Kansai regions, which has not really been pursued in existing humanities research fields. We believe this will empirically clarify the musical culture phenomena by which music spreads and changes.  ",
       "article_title":"Regional Classification of Traditional Japanese Folk Songs from the Chugoku District",
       "authors":[
          {
             "given":"Akihiro",
             "family":"Kawase",
             "affiliation":[
                {
                   "original_name":"National Institute for Japanese Language and Linguistics, Japan",
                   "normalized_name":"National Institute for Japanese Language and Linguistics",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/02caqc891",
                      "GRID":"grid.471854.b"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-16",
       "keywords":[
          "information retrieval",
          "music",
          "cultural studies",
          "digital humanities - diversity",
          "data mining / text mining",
          "historical studies",
          "English",
          "asian studies",
          "audio, video, multimedia"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Background Richard Wagner’s leitmotifs (Whittall, 2003), the compositional technique and its aesthetic implications as well as the individual motives, their musical characteristics, meanings, and transformations through the works, have been subjected to a wide variety of interpretations throughout different cultural and historical situations. The popular Wagner discourse in particular – in work introductions, guidebooks, programme notes, commentaries and motivic annotations in libretti and piano scores – focused on the extraction, description and presentation of leitmotifs in order to convey their interpretation of Wagner’s work. Thus, the phenomenon ‘leitmotif’ cannot only be defined as a compositional technique, but also as a reception practice (Grey, 1996; Rümenapp, 2002; Thorau, 2003). This study aims at a large-scale comparison of these reception practices that reaches across cultures and time periods and is able to not only describe general characteristics of sources, but to compare interpretations down to conceptions of individual motives. Semantic Web technologies (Berners-Lee et al., 2001; Bizer et al., 2011), with the enhanced capabilities of linking, searching and structuring of data they provide, may facilitate this undertaking. This paper describes an ontology that models the structure and content of leitmotif interpretations, which shall enable the semantic annotation of sources and the structured representation of the interpretations they contain. A particular focus lies thereby on the methodological considerations and design decisions that stand behind it.    2. Methodological Considerations Semantic Web Research and Musicology are two distinct domains, each with their own conventions, terminologies, and patterns of thought and discourse. Attempts to integrate them thus have to deal with the following questions: “How do [musicological] methods translate into methods of Semantic Web Research?” (Kummer, 2011) Can we conceive of a musicological ontology, which not only forms the basis to represent discourse in novel ways, but also works as an analytical device? How can we take advantage of the commonalities and accommodate the differences between methodologies and ways of thinking, such as different approaches to the presentation of knowledge and the construction of complexity, and the confrontation of different values of flexibility, ambiguity, and metaphoricity on the one hand and explicit definitions and taxonomies on the other? There are several key strategies by which the described ontology aims to achieve compliance with both domains: 1) a historical, critical and philosophical ‘awareness’ within the terminology and its specifications, as well as the connection to a shared vocabulary in the form of musicological encyclopaedias; 2) the incorporation of flexibility, ambiguity and nuance into the explicit model; 3) a direct correspondence between the ontology and both the document and “thought structure” of the interpretations it models.    3. Related Work Existing approaches that apply Semantic Web technologies to music or musicology are mostly concerned with organising and retrieving musical data. The Music Ontology aims at modelling the “music production workflow”, and focuses on managing “music-related data” of recorded and foremost popular music (Raimond, 2007). Since this domain specification also shapes the terminology and definitions it employs, its application in the scholarly context of music history can be problematic: it does not comply with the terminological standards and conventions of musicology, and does not provide sufficient concepts to describe the variety of data that contextualises music and that is vital for music-historical study. The “musicSpace” project (Bretherton et al., 2009) aimed at enhancing the possibilities of musicological research by “integrating access to many of musicology’s leading data sources”, gathering factual information from this data and metadata and making this information accessible and searchable via a single interface. While sharing common goals and objectives, the project at hand aims at going beyond factual information and at analysing the structure and content of a specific type of musicological discourse, modelling assignments people attribute to a musical work throughout the history of its interpretation. Related approaches can also be found in different disciplines, such as Cultural Heritage and Museology (Doerr, 2003): Traditional approaches in musicology tended to treat works as self-contained conceptual objects; therefore, the reception or interpretation history of a musical work can, to a certain extent, be seen in correspondence to the provenance of an object. Relevant are furthermore ontologies that aim at modelling narratives or, more general, the content or structure of sources in different contexts, such as the OntoMedia ontology (Jewell et al., 2005), Story Fountain (Mulholland et al., 2004), or Curate (Mulholland et al, 2012).   4. Structure and Design Analyses and interpretations of musical works, even though their content, analytical approach etc. might be completely different, on a basic level often rely on similar structures and constituent elements. These commonalities increase within the “specialised” analytic or interpretative discourse that is concerned with, for instance, a particular composer, a particular form, style or genre, which shares patterns and conventions of discourse and a specified vocabulary. This ontology, therefore, aims to model leitmotif interpretation as a special case of musicological discourse: its design is based on an analysis of representative source documents, and its structure and scope is informed by the structure and content of these interpretations and their presentation principles.  The ontology negotiates between several layers that model different dimensions. The first contains bibliographical metadata on the source documents, while the second concerns the source type and the corresponding internal document structure. The third level comprises the content structure, and is most significant for the comparisons of the different leitmotif interpretations.  Guidebooks, such as Hans von Wolzogen’s  Thematischer Leitfaden (Wolzogen, 1876), are already highly structured documents: They follow the chronology of the work, and “narrate” the music by extracting leitmotifs and describing their appearances, interactions and transformations through the course of the work. This interpretational concept of a leitmotif is, however, not the same as the compositional concept: they are integrated, self-contained constructs, abstractions from the musical context that tie together a musical phenomenon and its interpretation. They are represented as a collection of several constituent elements, such as a notation example fixing the motive’s shape, and a descriptive name defining its semantic reference (see figure 1). Which constituent elements exactly form a leitmotif concept differs between sources, and is characteristic for the source type or the interpretation approach. However, whereas within one guide, leitmotif concepts are stable and easily identifiable, the comparison between a large number of sources undermines permanent constructions: the same name can be found applied to different musical phenomena, or the same musical shape can be assigned different names; and even seemingly small nuances can point to significant differences in interpretations and aesthetic attitudes: the names ‘Schicksalsmotiv’ (Wolzogen, 1876) and ‘Schicksalsfrage’ (Porges, 1882), for instance, imply a completely different relationship between the name and the musical phenomenon: while the former establishes a denoting connection, with the motive acting as a sign for its semantic reference, the second establishes a metaphoric connection between the musical structure and the name, as the rhetoric characteristics of the ‘Frage’ (‘question’) parallel the musical characteristics of the motive. The varied combinations of constituent elements in different sources further complicate a comparison: how can we compare leitmotif concepts that don’t share any features? The ontology applies several strategies in order to cope with these difficulties: 1) Rather than providing a fixed definition of an interpretational leitmotif concept and its structure, the notion of a leitmotif concept is defined through its relationship to its constituent elements. Thus, the ontology does not prescribe a particular structure in which the actual instances have to be fitted, but stays flexible and can accommodate varying interpretational approaches. 2) The ontology introduces three meta-constituents, that are assigned in the analytic process and that shall introduce a common basis for comparison: the reference to a point in the work, the reference to a semantic sphere, and the reference to a basic musical shape (see figure 1). 3) Within these three categories, there is a set of relationships that aims at expressing a differentiated spectrum of influence and independence, identity and contrast, similarity and difference. In addition to the described structural features, the ontology also introduces different levels of abstraction, so that leitmotif interpretation is modelled as a special case of musicological discourse in general and the ontology remains potentially extensible to accommodate other forms of music interpretation and discourse.     Figure: Possible constituents of a leitmotif concept    5. Conclusion The described ontology forms the basis of a semantic annotation of the digitised sources, which shall facilitate the systematic comparison of a wide variety of leitmotif interpretations and enable to answer questions such as the following: Which musical shapes have been associated with a certain semantic sphere? How has a certain set of musical characteristics been classified in different interpretations and reception contexts? Being able to reorganise and query the reservoir of leitmotif concepts according to such questions can provide insights about larger contexts, patterns and constellations and form the starting point of more detailed investigations. Furthermore, beyond this particular analysis, it forms a first step towards a new structure of musicological discourse, as it allows to link elements of musical analyses, interpretations or reception documents together in novel ways.   ",
       "article_title":"Modelling Music Reception: An Ontology For Representing Interpretations of Richard Wagner's Leitmotifs",
       "authors":[
          {
             "given":"Carolin",
             "family":"Rindfleisch",
             "affiliation":[
                {
                   "original_name":"University of Oxford, United Kingdom",
                   "normalized_name":"University of Oxford",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/052gg0110",
                      "GRID":"grid.4991.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "semantic web",
          "semantic analysis",
          "music",
          "linking and annotation",
          "data modeling and architecture including hypothesis-driven modeling",
          "ontologies",
          "knowledge representation",
          "archives, repositories, sustainability and preservation",
          "content analysis",
          "English",
          "bibliographic methods / textual studies",
          "text analysis",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The division between quantitative and qualitative approaches is fundamental to the study of the past. Quantitative approaches are predominantly used to study statistical sources in fields such as historical demography and economic history where large numerical databases are available. These approaches are well suited to situations where large volumes of digital statistics are available and are very good at finding relationships between variables. They can be criticised on two levels: firstly, while good at identifying relationships they are poor at establishing the causal mechanisms that cause them. Secondly, and more fundamentally, most information about the past is not in numerical form and thus cannot form part of a traditional quantitative analysis, thus many relevant factors cannot be included within the analysis. Qualitative sources, particularly texts, are richer in both the range of material available and in the amount of detail it provides about the conditions in which people lived. They are however much more complex to work with and traditionally required close reading which is slow and selective. Digital technologies offer the potential to overcome this divide and make use of the combined advantages of both types of source, making use of statistical sources to identify patterns and relationships, and textual sources to help explain the patterns found. This paper presents an example of this based on infant mortality in Victorian and Edwardian England and Wales. Infant mortality refers to deaths before the age of one, usually expressed as an infant mortality rate (IMR) of infant deaths per thousand births. An orthodox story is that infant mortality decline was brought about by government action on sanitation (Szreter, 1991; Woods et al., 1988; Woods and Shelton, 1997). More recently Gregory (2008) used a GIS database and demonstrated that infant mortality decline actually started before the Public Health Acts, that decline was earlier and steeper in rural areas of the south and east of England than anywhere else, and that rural parts of the north, Wales and the West Country had among the lowest rates of improvement. He was, however, unable to explain this variation with the data available. This paper uses a combination of statistical and textual sources to attempt to provide explanations for infant mortality decline with an emphasis on rural areas.    Figure 1: Temporal trajectories and geographical locations of the seven latent classes  In the quantitative stage, as many geographically disaggregate independent variables as possible were assembled to explore what other factors seemed to be associated with infant mortality decline. Analysing change over time and space for multiple variables is difficult. A technique called Latent Trajectory Analysis to do this but combine it with the use of GIS (Nagin, 1999). Figure 1 shows how this techniques was used to group districts into seven clusters based on their temporal characteristics, and then map where rural districts with similar trajectories are found. This confirms that there was a clear geography to infant mortality decline. The biggest improvements occurred in rural areas in the south and east in the clusters described as ‘Fenland’ and ‘Mercia.’ Areas showing the lowest declines, which in some cases actually got worse, were in the north and west in the clusters described as ‘Upland’ and ‘Heath and Moor.’  Further analysis reveals statistically significant relationships between infant mortality change and rates of female TB and a less strong relationship with female literacy. These relationships have been shown elsewhere in the literature, however our findings show that these factors were important in rural areas as well as in urban ones. Interestingly convincing relationships could not be found with population density or fertility, these would be expected from the literature. The other striking finding was that the most important independent variable in every model was ‘decade’, in other words, change over time that none of our other variables could account for. Fundamentally this tells us three things: first that there are some very interesting geographical patterns to infant mortality decline, second that this is related to some other factors, particularly female health and education, and third that even though we have assembled what we believe to be as many quantitative independent variables as it is practical to use, these are unable to account for much of the change in infant mortality over this period.  The relative lack of explanatory ability by the quantitative analysis is hardly surprising given the lack of variables on a whole range of topics that may be associated with improving infant health, including: sanitary conditions, access to midwives and healthcare, attitudes towards breast-feeding, access to safe cows’ milk, and so on which have never been captured in statistical form. Thus, if we are going to improve our understanding we need to explore non-quantitative sources. Of particular interest here is the British Library’s Nineteenth Century Newspapers collection, a corpus of over 50 newspapers, most of which are available for series that cover most of the century. The corpus is at least 30 billion words although, to date we have been working with individual newspaper series which are usually hundreds of millions of words.     Figure 2: Crude death rates from disease classes and frequency of instances of these diseases in the Era  Figure 2 shows the use of one newspaper, the Era, to explore the relationship between interest in a variety of diseases associated with the young, classed according to whether they are respiratory diseases, food and water-bourn diseases, and diseases associated with crowding (Woods, 2000). The frequency with which these diseases are mentioned in the corpus is compared with the death rates from the diseases. Different disease types show quite different results. Interest in diseases of crowding follows the decline in these diseases, particularly associated with the decline of scarlet fever and typhus. Food and water death rates rise overall while interest in them seems to fall, while deaths from respiratory diseases are broadly flat but there is a major increase in interest in them in the 1880s and 1890s. We can then explore the collocates to these terms, the words that are found close to them. This shows four major classes of collocates: first, other words associated with disease, included other disease names, and words such as ‘died’, ‘attack’ and ‘epidemic.’ Second there are symptoms of disease, third are brand names of medicines typically found in advertisements, and fourth there are military terms which are found in the 1850s and are related to the Crimean War.  To allow us to explore the geographies within themes within the newspapers we have developed a technique we call concordance geoparsing (Rupp et. al., 2014). This involves firstly extracting concordance lines around the search-term. These concordance lines are then geo-parsed using the Edinburgh Geoparser (Grover et. al., 2010) which identifies place-names and matches them to a gazetteer to allocate them to a grid reference. The results are then explored to check for errors which are corrected with the corrections added to an updates file. In this way we can cumulatively remove geoparsing errors and have confidence in its results.    Figure 3: Locations associated with disease instances from the Era  Figure 3 shows the place-names associated with the diseases in the Era. It is noticeable that these are concentrated in urban areas but there are exceptions to this. We will present comparisons between the number of references to a disease and the number of deaths from it in a district, showing how media interest was not related to disease severity.  Taking this work further, we will take regional newspapers from rural areas that experienced the highest and lowest declines and explore these for collocates and places that are associated with diseases themselves and a wide range of potential causal factors that may be related to them. In this way we will be combining the descriptive strengths of statistical data with the explanatory power of textual sources, using text to work around insufficiencies of data and making use of the temporal and geographical detail in both. Acknowledgement: The research leading to these results has received funding from the European Research Council (ERC) under the European Union’s Seventh Framework Programme (FP7/2007-2013) / ERC grant “Spatial Humanities: Texts, GIS, places” (agreement number 283850).  ",
       "article_title":"Combining Corpora and Statistics using Geographical Technologies: New Evidence on Nineteenth Century Infant Mortality Decline in England and Wales",
       "authors":[
          {
             "given":"Paul",
             "family":"Atkinson",
             "affiliation":[
                {
                   "original_name":"Lancaster University, United Kingdom",
                   "normalized_name":"Lancaster University",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04f2nsd36",
                      "GRID":"grid.9835.7"
                   }
                }
             ]
          },
          {
             "given":"Ian",
             "family":"Gregory",
             "affiliation":[
                {
                   "original_name":"Lancaster University, United Kingdom",
                   "normalized_name":"Lancaster University",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04f2nsd36",
                      "GRID":"grid.9835.7"
                   }
                }
             ]
          },
          {
             "given":"Catherine",
             "family":"Porter",
             "affiliation":[
                {
                   "original_name":"Lancaster University, United Kingdom",
                   "normalized_name":"Lancaster University",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04f2nsd36",
                      "GRID":"grid.9835.7"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-29",
       "keywords":[
          "corpora and corpus activities",
          "historical studies",
          "English",
          "geospatial analysis, interfaces and technology"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Over half a century ago Briet (1951) famously asked \"Qu'est-ce que la documentation?\" She proposed a definition much more fluid than the scraps of writing on paper we usually associate with the term: any object, even an uninscribed stone, becomes a document as soon as it is used to communicate some fact (e.g. in a museum’s geological collection). In a similar vein we argue in this paper for a renewed consideration of the nature of text in the digital realm, or rather the nature that has been imposed upon it both by information-technological and methodological constraints. Text is by its nature both discrete and continuous. The glyphs of a writing system are combined into words, which are arranged into phrases, sentences, paragraphs, quotations, and so on; these discrete formations can be arbitrarily complex, but they are communicated through the atom of the glyph. On the other hand, very many elements of the meaning of a text—e.g. narratological elements such as the theme of isolation in  Do Androids Dream of Electric Sheep?, intertextual references and referents (Miola 2000), the concept of  writerly text (Barthes, 1974), or even the visual presentations of individual glyphs, defy discrete boundaries. Yet the technologies we have used overwhelmingly for the large-scale production of text—from the printing press to signal transmissions and thence to digital models—treat text as a code of discrete characters; moreover, once text moves from “print” to “signal” (Petzold, 2000) it becomes specifically a single stream of these discrete characters. This can already present some problems to scholars who wish to remediate texts not coded by machine, e.g. manuscripts, to the digital domain for humanistic enquiry. To date scholars who encounter these problems generally consider them an acceptable trade-off for the benefits of access, exchange, and computational tractability that the digital medium offers.   If we want our digital model of a text to extend beyond the semiotic registration of a single stream of characters, however, problems emerge. McGann (2004) summarizes this well: \"Print and manuscript technology represent efforts to mark natural language so that it can be preserved and transmitted. It is a technology that constrains the shapeshiftings of language, which is itself a special-purpose system for coding human communication. Exactly the same can be said of electronic encoding systems. In each case constraints are installed in order to facilitate operations that would otherwise be difficult or impossible.” Everyone who has ever worked on the remediation of a physically-inscribed text into the digital medium has implicitly conceptualized it at some point as a single stream of discrete characters, simply because this is how we have always known text, ultimately, to be represented digitally. Transcription is thus exactly the sort of constraint to which McGann refers: there is a single stream of text, and only once it exists can it be subdivided and classified as having certain descriptive, structural, or logical characteristics, or even relationships to other portions of text. The model of text as a single stream of discrete characters also informs the concept of XML markup. While this is not stated explicitly anywhere of which we are aware, the underlying assumption is plain in, e.g., Goldfarb’s discussion of the roots of SGML (1996) or Renear’s definition of text encoding (2004). As Pierazzo (2011) notes, “The editor will first transcribe a primary source, thereby creating a transcription; this transcription will be corrected, proofread, annotated, and then prepared for publication.” The predominant model for the process of annotation to which Pierazzo refers is TEI-XML, that is, embedded markup within the hierarchical structure required by XML. Conceptually a tree structure expressed in XML can be perfectly reorderable, and so not bound to a single valid serialization. With text encoding, however, the ordering constraint remains and must be preserved, however marked up it is and subdivided into branches. Standoff markup schemes similarly assert a single stream—each scheme relies on the stability of the range notation it uses for the underlying text stream—but the markup itself is reorderable. This reorderable property of standoff markup takes us a small step closer to our goal: to recast and represent digital text as something more than a single stream of discrete tokens.   Standoff markup essentially applies a graph model. This is the feature of standoff that makes it so valuable to its proponents—in a graph, unlike a tree, overlapping ranges of markup are easily handled. Some of these standoff markup elements might even include alternative readings to a given text range, or a reference to a different text that elucidates the contextual meaning of the range in question. From there it is only a small step to the conception of a text that is itself more complex than a single stream. Such conceptions emerge already with the different implementations of variant graphs for collations of “multi-version documents” (Schmidt and Colomb, 2009; see also Dekker et al., 2015; Andrews and Macé, 2013; Jänicke et al., 2014). While so far the variant graph has been used only for word-by-word comparisons of text versions, the concept can be extended radically farther. We will demonstrate this, building on prior work in this direction (e.g. Marcoux, Sperberg-McQueen, and Huitfeldt, 2013). We will show in our paper, for instance, how a graph-based digital text can model collections of stories preserved in manuscripts, without giving primacy to any one ordering of the story sequence; that is, it can contain both the collated text of each story and the variety of story order within the tradition. Likewise the same narrative element, inserted into multiple stories, can be represented as such.  The representation of information as entities with relationships between them—in other words: as a graph—is not a new idea. RDF works exactly this way, and the theoretical ideas behind hypertext are not dissimilar (cf. Nelson, 1993). The ease with which complex graphs can be modelled in software, however, has massively increased in the last five years. Moreover, it is now reasonably straightforward to create an initial graph representation from a TEI-encoded XML file, and nearly as straightforward to produce multiple “flavors” of TEI XML from a single graph—for example, a document-oriented representation alongside a logical-text-oriented representation, such as those created in the Faust edition project (Brüning et al., 2012). Graph representation does not relieve us from the constraint that digital text must be formed from discrete atomic units (characters) and discrete compound digital units. What it does allow us to do, however, is to digitally model the shades of meaning, ambiguity, and uncertainty of text—the aspects of a text’s meaning or interpretation that are not by their nature discrete at all. So far, uncertainty of interpretation has been all but explicitly avoided in the scholarly digital space. McGann (2004) observes: “In the case of a system like the Text Encoding Initiative (TEI), the system is designed to ‘disambiguate’ entirely the materials to be encoded.” The 1700+ pages of the TEI guidelines bear witness to the difficulty of this task; element names and their intended usages are defined as precisely as possible, with illustrative examples.  The use of tags and attributes in a TEI-encoded file is itself an act of interpretation—it is precisely in the labelling of the elements of an encoding and of the relationships between those elements that much of the scholar’s interpretation of the encoded text lies. This is equally true of a graph encoding. The prolixity of the Guidelines is in fact a result of the attempt to constrain the scholar’s interpretation of a given tag to a disambiguated, and thereby discrete, set of meanings. Rather than shrinking from uncertainty of meaning, however, rather than resolving it in an occasionally misleading or (paradoxically) unhelpful attempt to ease computational analysis, how much better if the uncertainty can be retained in the model, in a manner that is nonetheless machine-parseable? Interpretation remains inherent in the labels that are chosen for the properties in a graph model; these may be taken from the TEI, or they may reflect another interest of the scholar. Either way, a graph-based text model by its nature includes computationally tractable information about how these properties relate to each other.  In representing “fuzziness” or ambiguity of interpretation, we can perhaps follow the lead of those who have struggled with similar problems of representation of time. If text can be represented as a graph, mostly-sequential but with scope for fluidity, then interpretative elements—say, picking out the theme of isolation within the text—can be represented as spanning a continuous or disjoint range of text sequences, beginning not before word A but certainly by word B, and continuing at least as far as word C but certainly not beyond word D. And so on. If scholars disagree on the correct identification of a person in a historical text, both hypotheses can be represented without innate primacy being given to one over the other, because a graph does not enforce a single ordering of its elements. The text model we are advocating, then, amounts to a form of knowledge representation; in representing “the text”, we are actually representing a collection of our knowledge, understanding, and interpretation of that text, in a form that can be analyzed and processed by a machine. The more we can encode texts as computable knowledge, the greater the power computational methods will have in textual scholarship of all forms. ",
       "article_title":"Qu’est-ce qu’un texte numérique? A New Rationale for the Digital Representation of Text",
       "authors":[
          {
             "given":"Joris J.",
             "family":"Van Zundert",
             "affiliation":[
                {
                   "original_name":"Huygens Institute for the History of the Netherlands - Royal Netherlands Academy of Arts and Sciences",
                   "normalized_name":"Royal Netherlands Academy of Arts and Sciences",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/043c0p156",
                      "GRID":"grid.418101.d"
                   }
                }
             ]
          },
          {
             "given":"Tara L.",
             "family":"Andrews",
             "affiliation":[
                {
                   "original_name":"Universität Bern, Switzerland",
                   "normalized_name":"University of Bern",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02k7v4d05",
                      "GRID":"grid.5734.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-05",
       "keywords":[
          "xml",
          "data modeling and architecture including hypothesis-driven modeling",
          "encoding - theory and practice",
          "scholarly editing",
          "knowledge representation",
          "English",
          "networks, relationships, graphs",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction This paper addresses how text-mining, machine-learning and information retrieval algorithms from the field of artificial intelligence can be used to analyze Art-Research archives and conduct (art-) historical research. To gain quick insight into the archive, two aspects are focused on: relations between groups of people using community detection, and global content changes over time using topic modeling. For such archives pre-tagged ground-truth collections are generally not available, and the archives are often too large, geographically distributed, and not always available in digital formats to build such a ground-truth at reasonable costs. To develop and test the validity and relevance of existing tools, close collaboration was established between the AI researchers, museum staff, and researchers in CREATE, a digital humanities project that investigates the development of cultural industries in Amsterdam over the course of the last five centuries.   Data The research draws on two datasets. The principal dataset is the digitized archive of the Stedelijk Museum Amsterdam, a renowned international museum dedicated to modern and contemporary art and design. The archive of the Stedelijk Museum Amsterdam contains documents from the period 1930-1980. The corpus is a static collection of approximately 160.000 text documents that were digitized using OCR. The second dataset is drawn from Delpher, developed by (Koninklijke Bibliotheek Nederland, 2015). Delpher provides a collection of digitized newspapers, books and magazines that is available for research. A selection of newspapers was made that is used as an additional dataset for this project. Only articles from 1930-1980 that resulted from the query ”Stedelijk Museum” AND ”Amsterdam” were used, forming a set of 18.290 articles.   Methodology The following methodology uses two approaches to obtain a quick and detailed overview of the content of a digitized archive that contains unstructured information. The first one focuses on the relations between named entities and aims at finding communities in the relation network. The second approach uses time based topic-modeling to get an overview of content changes over time. Finally, a name extraction method is presented that is able to handle multiple causes of name variations.  Relation networks and community detection In its most basic form, a relation between two named entities can be said to exist when they occur together in the same document. The strength of a relation can be characterized by the number of documents in which both named entities occur. When all the co-occurrences are found, a relation network can be constructed. In addition, sentiment analysis can be done to further characterize a relation. A sentiment score is assigned to each document, indicating the sentiment content of the document. No distinction is made between positive and negative sentiment polarity. The hypothesis is that relations between individuals with a high sentiment are more interesting than relations with a low sentiment. This is because sentiments around trigger-events are often higher than around common-day events. A lexicon based approach is used with lists of language specific sentiment words. The sentiment score of a document is then given by the sigmoid of the count of the sentiment words in the document, normalized by the number of words in the document. Finally, community detection algorithms can be applied to the relation network. These types of algorithms aim at finding clusters of groups of entities that have dense connections between members of the clusters and sparse connections with members of other clusters (Fortunato, 2010). The relation weight measure that is used to calculate the communities, is taken as the product of the strength of the relation, i.e. the number of documents where both entities occur in, and the average sentiment score of the documents of a relation. It was found that combining these two measures, resulted in more meaningful communities.   Time based Topic Modeling In the next approach, topic modeling algorithms are applied to analyze the information content and their evolution over time. Topic modeling tries to discover the underlying thematic structure in a collection of documents. Non-Negative Matrix Factorization (NMF) is being used as a tool for topic modeling (Arora et al., 2012). NMF is an unsupervised method where a matrix is approximated by two low rank non-negative matrices. The extracted semantic feature vectors have only non-negative values and are sparse so they are easily interpretable. Furthermore, NMF is shown to generate more consistent results over multiple runs (Choo et al., 2013), compared to other tools used for topic modeling such as LDA (Blei et al., 2003).  The approach suggested in (Vaca et al., 2014) uses a time-based collective matrix factorization based on NMF and is used in this project. It extends NMF by introducing a topic transition matrix that allows to track topics as they emerge, evolve and fade over time.   Name Extraction The following method was used to extract named entities from a collection of documents in order to build the relation network. It handles different causes of name variations such as OCR induced errors commonly found in digitized document collections, spelling mistakes, name abbreviations and first and last name combinations.  The method makes use of lists of name variations. Starting from a set of names extracted from a name database, such as RKDArtists and (RKD, 2015), the document collection is searched for possible name variations. These variations are found by searching for the last name using a fuzzy search. The similarity between the group of tokens around the found last name, and the original name is then calculated as a similarity score. The similarity score calculation is based on the idea described in (Song and Chen, 2007), which uses a n-gram set matching technique. The lists of name variations can then be evaluated manually or a threshold on the similarity score can be used to identify name variations that correspond to the original name. The method using a threshold of 0.9 on the similarity score was tested on 50 randomly chosen names. The average precision was found to be 81 percent.    Results A relation network was constructed for the document collection of the archive of the Stedelijk Museum Amsterdam. Only artists with the graphic artist qualification in the RKDArtists and database were used. The methods were implemented using available open source software libraries such as the Apache Lucene text search engine library (The Apache Software Foundation, 2015) and the Gephi platform (Bastian et al., 2009). The standard community detection feature in Gephi was used, which is based on the Louvain method (Blondel et al., 2008). The result is shown in Figure 1. The color of the relation between the nodes indicates the average sentiment score of the relation, starting from blue (neutral) to red (high sentiment content). Communities such as group exhibitions, art movements or a group of artists closely related to the museum director, could be identified with the help of a museum expert.   Figure 1: Found communities for graphic artists in the archive of the Stedelijk Museum  The time based topic modeling algorithm suggested in (Vaca et al., 2014) was implemented in MATLAB and Java. The algorithm was applied to both the archive of the Stedelijk Museum Amsterdam and newspaper articles from the Delpher database. The results are visualized over time in the form of stacked topic rivers (Wei et al., 2010), shown in Figure 2 and Figure 3. Several exhibitions and events could be identified and are annotated on the chart.   Figure 2: Time based topic modeling for the archive of the Stedelijk Museum Amsterdam    Figure 3: Time based topic modeling for Delpher newspaper articles    Conclusion This paper discusses two approaches to gain insight into a digitized archive. Relation networks of persons with community detection are considered, relying on a robust name extraction method. Furthermore, the evolution of content over time can be explored using time based topic modeling. For the humanities researchers in this project, the main aim was to asses the research potential of computational analysis of digitized art archives in general, and the Stedelijk Museum in particular. Two types of preliminary research questions were developed to do so. The first type had to do with identifying patterns of change and continuity, across time and place. These include for instance tracing the position of the Stedelijk Museum as an intermediary in Dutch design industries, or the development of the Stedelijk Museum as an increasingly international player. The second type of question is less concerned with general historical patterns, and more with specific art-historical research questions, regarding for instance (networks of) particular artists, artworks or exhibitions. But before we could start asking such questions to digitized art-historical archives, the quality and accessibility of the texts needed to be established. Secondly, specific methods needed to be explored and adapted in order to clean, identify, retrieve, extract, and structure the texts. The first results presented in this paper demonstrate that even though they may not be clean at the first try or capture all historical nuance, they do help archives to open up and show unexpected relationships and patterns, to answer specific questions, and to get connected with other relevant sources, such RKDartists and Delpher. The community detection in relation with sentiment mining, the topic modeling and name extraction method developed in this project therefore provide a solid basis for the next step in assessing the research potential of art-historical archives: developing in-depth case studies, again in close collaboration with art-historians and historians, allowing the archive to speak up in unprecedented ways, offering access to hidden story lines that subvert and augment prevailing historical narratives.  ",
       "article_title":"SMTP: Stedelijk Museum Text Mining Project",
       "authors":[
          {
             "given":"Jeroen",
             "family":"Smeets",
             "affiliation":[
                {
                   "original_name":"Maastricht University, Netherlands, The",
                   "normalized_name":"Maastricht University",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/02jz4aj89",
                      "GRID":"grid.5012.6"
                   }
                }
             ]
          },
          {
             "given":"Johannes C.",
             "family":"Scholtes",
             "affiliation":[
                {
                   "original_name":"Maastricht University, Netherlands, The",
                   "normalized_name":"Maastricht University",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/02jz4aj89",
                      "GRID":"grid.5012.6"
                   }
                }
             ]
          },
          {
             "given":"Claartje",
             "family":"Rasterhoff",
             "affiliation":[
                {
                   "original_name":"CREATE, University of Amsterdam, Netherlands, The",
                   "normalized_name":"University of Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04dkp9463",
                      "GRID":"grid.7177.6"
                   }
                }
             ]
          },
          {
             "given":"Margriet",
             "family":"Schavemaker",
             "affiliation":[
                {
                   "original_name":"Stedelijk Museum Amsterdam, Netherlands, The",
                   "normalized_name":"Stedelijk Museum Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/006a5r035",
                      "GRID":"grid.500970.b"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "information retrieval",
          "art history",
          "GLAM: galleries, libraries, archives, museums",
          "data mining / text mining",
          "historical studies",
          "archives, repositories, sustainability and preservation",
          "English",
          "networks, relationships, graphs"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Among the most popular platforms for digital humanities development projects is GitHub.   This work emerges out of Rice University’s John E. Sawyer Seminar on Platforms of Knowledge in Wide Web of Worlds, supported by the Andrew W. Mellon Foundation.  GitHub provides web-based hosting for coding and other collaborative projects, building on the Git version control system. Founded in 2008, GitHub now hosts approximately 31 million repositories and 12 million users, making it “ the largest online storage space of collaborative works that exists in the world” (GitHub, 2016; Orsini, 2013). Using GitHub, developers can fork (copy) a public repository to their own account, change the code, and submit a pull request to share the modifications with the repository owner, who can then “merge” the code with the original. GitHub also enables users to create profiles, “follow” others, “star” projects and “watch” them evolve, serving as a social network for developers (Brown, 2014).  Digital humanities (DH) researchers are drawn by GitHub’s support for version control and collaboration, as well as by its free hosting for publicly available projects. A range of digital humanities projects use GitHub, including:  Software Writing projects Taxonomies and community documentation Websites Datasets Course materials Syllabi Research notes  What digital humanities researchers are adopting GitHub, and why? What are the benefits and risks of employing GitHub for digital humanities work?  Academia’s growing reliance on GitHub requires careful consideration. As a for-profit company, GitHub does not necessarily operate with the interests of academics at heart. Yet it provides services that would be difficult for scholars to secure themselves, and it enables collaboration. To develop an informed view of GitHub and similar services, we need to establish clear criteria for evaluating platforms. In choosing a platform for a web project, Quinn Dombrowski recommends considering functionality, familiarity, community, support and cost (Dombrowski, 2013). We would add support for openness and sustainability as core criteria for digital humanities platforms. Through an initial case study of GitHub, we will examine these criteria for evaluating DH platforms:  Functionality: GitHub offers several features that make it attractive to researchers, particularly open science advocates. Karthik Ram suggests that Git (and by extension GitHub) supports open science by providing decentralized version control; attributing changes to authors; supporting distributed backup of data; enabling projects to branch in new directions; collecting feedback through issue trackers; and facilitating reuse through forking (Ram, 2013). Likewise, Konrad Lawson touts the power of GitHub in facilitating “collaboration without collaboration” (easily modifying someone else’s code through forking, and contributing that code back through a pull request) and detailed credit for contributions (Lawson, 2013c; Lawson, 2013a). While GitHub can be used for a range of texts, from syllabi to code, it’s not necessarily well suited for all uses. For example, Mark Sample notes the significant labor and potentially low rewards in putting syllabi into GitHub (Sample, 2012a). Lawson observes that using GitHub for writing projects requires overcoming a fairly steep learning curve, using plain text (or a converter), creating short documents, and dealing with limited support for non-textual files (Lawson, 2013d). Familiarity/ease of use: As Lincoln Mullen points out, GitHub’s learning curve poses a barrier to entry for some potential collaborators (Mullen, 2012). Indeed, participants discussing how to make it easier for women to contribute to Programming Historian identified the publication’s reliance on GitHub as an obstacle (Crymble et al, 2015). To what extent do the challenges of using GitHub limit its adoption in the humanities? Community: As more digital humanists adopt GitHub, it becomes even more attractive, since you are more likely to find collaborators and to gain recognition for your work. Yet the wide adoption of GitHub may reduce diversity and increase dependency on a commercial platform. Not everyone wants to participate in this community. Lawson points to several social and cultural obstacles to GitHub enabling richer academic collaboration, including reluctance to embrace “forking” as means of building on another’s work; fears of plagiarism; concerns that the original voice of the author will be lost; anxiety that transparency will reveal one’s scholarly flaws; and worry that ideas will be stolen or misused (Lawson, 2013b).  Support: With such a large community, new users can turn to a number of resources to learn how to use GitHub. Some university IT groups offer limited support for GitHub, but in general users are left to secure their own support. Cost/business model: GitHub uses a “Freemium” business model in which it hosts public repositories for free and charges companies for private repositories (Brown, 2014). It also offers up to five free private repositories to academic researchers and twenty to research groups. While free holds appeal, should the digital humanities community be concerned about becoming dependent on a platform developed by a for-profit company? As Sample warns, “History suggests that relying too much on a commercial service with interests that do not necessarily align with our own is no way to sustain the work of the humanities”(Sample, 2012b). GitHub has attracted $350 million in venture capital and is now valued at about $2 billion, so it faces pressure to generate a profit (Gage, 2015). Klint Finley argues that GitHub’s business interests may work against its open source mission, pointing to SourceForge as an example of an open source software site that went astray (Finley, 2015). When SourceForge was acquired, it began to display junky third-party ads that misled people into downloading malicious software, prompting projects such as GIMP and VLC to leave. While GitHub is not funded through ad revenue, its business model could change under pressure from investors.  Support for openness: Compared to some web platforms that claim user-produced content as their own, GitHub articulates an open approach to intellectual property: “Your profile and materials uploaded remain yours”(GitHub, 2015). But should we be concerned about clauses reserving the right to remove content and requiring users to defend and indemnify GitHub against suits alleging that their content violates the law? Does GitHub’s model of providing free public repositories lead some users to share work that they otherwise would keep private? Sustainability: GitHub is not meant to be a preservation repository, and it is easy to delete a public repository (Bergman, 2012). However, Git’s distributed, decentralized approach to versioning provides protection against data loss, since everyone who contributes to a GitHub project has a local copy of the code (Finley, 2015).   In addition to these criteria, we will also consider the significance of factors such as accessibility and multilingualism. In performing this research, we are first identifying digital humanities users by 1) searching for publicly available GitHub accounts associated with presenters at the last three Digital Humanities conferences and 2) searching for GitHub accounts associated with Digital Humanities centers listed on CenterNet. To understand patterns of collaboration and code reuse, we will analyze publicly available statistics for selected users such as number of commits, branches, releases and contributors, as well as networks connecting users. We will survey GitHub DH users to understand how and why they use GitHub, its strengths, and its weaknesses. We will also conduct interviews with selected GitHub users. Where possible, we will use GitHub to share ongoing work about this project.   https://github.com/lms4w/githubproject  By analyzing public GitHub statistics and gathering insights and information from users, we will illustrate how GitHub is being used in the digital humanities community and develop principles for evaluating platforms.  ",
       "article_title":"Evaluating GitHub as a Platform of Knowledge for the Humanities",
       "authors":[
          {
             "given":"Lisa",
             "family":"Spiro",
             "affiliation":[
                {
                   "original_name":"Rice University, United States of America",
                   "normalized_name":"Rice University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/008zs3103",
                      "GRID":"grid.21940.3e"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-05",
       "keywords":[
          "project design, organization, management",
          "digital humanities - facilities",
          "user studies / user needs",
          "cultural infrastructure",
          "archives, repositories, sustainability and preservation",
          "English",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Huge amounts of comics are being published on a daily basis. However, the textual data that are circumscribed in comics’ speech balloons and thought bubbles are highly unstructured, and very hard to automatically extract and digitize. Several automatic techniques have been used to automatically detect the shapes, sizes, and contents of speech balloons and thought bubbles leading to automatic text digitization and localisation in comics (Rigaud et al 2013; Ho et al 2012). Such advances open new horizons for a myriad of text mining applications to comics, namely, automatic indexing, searching, recommendation and visualization.      Named Entity Recognition (NER) is a task of information extraction under text mining that aims to identify in-text references to concepts such as people, locations and organizations, mainly in unstructured natural-language text. NER is very useful for text indexing, text summarization, question answering and several other tasks that enhance the experience between humans and literature. In a previous study, we developed a simple and original method of Unsupervised Named Entity Recognition and Disambiguation (UNERD) (Mosallem et al 2014) to automatically extract names of people, locations and organizations from French and English (Abi Haidar et al., 2016) newspapers. We then used the text coordinates in the ALTO XML format to automatically locate and highlight the named entities detected by UNERD on the scanned image of the newspaper (Abi Haidar et al 2014). Last, we used UNERD to detect and visualize named entities from French newspaper during the period of the first world war (Abi Haidar et al 2016). The main challenges encountered in unsupervised NER have been identified and addressed with our original UNERD method when applied on English and French newspapers and French literature. These challenges include but are not limited to named entity disambiguation, named entity boundary detection, and domain-specific dictionary attribution. In comics, in addition to the aforementioned challenges, the case-insensitive text extracted from comics adds the challenge of proper noun detection that we used to handle using POS tagging. We used TreeTagger (Shmid 2005) POS tagger to detect proper nouns, however and like other POS taggers, it works only when applied on case sensitive text. Here, we use a variation of our UNERD method to detect names of fictional characters in a  database of unstructured text from comics that has been recently digitized by our partners at the L3i Labs using an active contour model for speech balloon detection (Rigaud et al 2013). Au lieu of POS tagging for proper noun extraction, we filter stop words that constitute a 5000 word list of most frequent French words. For the dictionary, we use  the freebase dictionaries of fictional characters and characters in fiction novels from Freebase. The comics digitised data amounts to around 8000 case-insensitive words in the French language. We evaluate our method's precision by analysing the predicted character names.  Out of 182 predictions of character names made by UNERD, 113 names were correctly predicted. This precision of 62% cannot be compared to that obtained in our previous results with UNERD when tested on case sensitive textual data. We also did not compute the coverage since we do not have any gold standard or any annotated data. In the table below, we see the most frequent characters that were automatically detected. The terms ‘FLIP’ and ‘HUM’ could be discarded by adding a list of onomatopoeic terms to the list of stop words thus improving precision to 68%. Our precision compares well to results presented in Cornolti's meta-study of Named Entity Recognition/Entity Linking resulting in a precision of 69% (Cornolti et al 2013).The extracted character names are then associated with page numbers to help with the automatic indexing of characters that is otherwise very costly in time and human resources. Co-occurrences of character names in the same page are used to detect dialogues or interactions between the characters. Such information, along with the mere occurrences of character names, are used by our partner, Actialuna, in order to enhance digital comics recommendation. Our method can be applied to extract entities from other case-insensitive texts by simply adapting the dictionary to the targeted text. Our results are preliminary and have been tested on a tiny database that is currently being updated.   Frequency Entity   2 CASTOR   2 COW-BOY   2 CYBORG   2 Doc   2 FRED   2 Keuf   2 Lamisseb   2 LET   2 LONGUE-VUE   2 PHILÉMON   2 PiLOT-BOAT   2 PRESIDENT   2 RAT   2 Stock   3 HUM   3 JEAN-MICHEL   3 JOHN   3 KID   3 LEO   3 Li   3 MIN   3 NEAR   3 NEW-YORK   3 PiLOTE   3 PIRANHA   3 Traffic   4 ACHETé   4 BILL   4 DEMON   4 DOC   4 ZiG   5 MAN   5 STEVE   8 BOY   8 CINDY   8 DOLLY   8 FLIP   12 NEMO   14 COCO   14 SUPER   16 ALFRED   ",
       "article_title":" Automatic Detection of Characters in Case Insensitive Text in Comics  ",
       "authors":[
          {
             "given":"Alaa",
             "family":"Abi Haidar",
             "affiliation":[
                {
                   "original_name":"LIP6. Univeristy of Pierre and Marie Curie (UPMC), France",
                   "normalized_name":null,
                   "country":"France",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Jean-Gabriel",
             "family":"Ganascia",
             "affiliation":[
                {
                   "original_name":"LIP6. Univeristy of Pierre and Marie Curie (UPMC), France",
                   "normalized_name":null,
                   "country":"France",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-15",
       "keywords":[
          "information retrieval",
          "data mining / text mining",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Understanding how the spoken language is represented in novels over time, and how this relates to gender and other characteristics of the represented speaker and the author is a key question in the Digital Humanities. Previous work has explored such questions lexically, focusing for example on differences in word choice between male and female authors. Yet while such lexical stylistic approaches have been computationally sound (Argamon 2003, Olsen 2005, Yu 2014, Rybicki 2015), a purely lexical approach is known to have serious methodological dangers. Individual lexical items are highly conflated with topic, genre, author idiosyncrasies, and era, making it difficult to draw general conclusions, particularly over long time periods. An even greater problem is that this approach essentializes gender a priori (Bing & Bergvall 1996), neglecting how the complex interplay between an author and the characters they portray creates the performance of linguistically gendered writing. We propose to draw from methods in social psychology (Newman 2008), network analysis (Schwartz 2013), and corpus linguistics (Biber 1991) to offer two innovations in the analysis of novels. The first is a new metric for characterizing dialogue, called  dialogism, that references Bakhtin’s theories of novels and dialogue (1935). This measurement uses abstract grammatical features in the text to characterize the extent to which it is dialogic. By using categories like parts of speech, our method avoids the genre-specific and era-specific problems of individual words. Our second innovation is a computational analysis of the performance of gender within dialogue. We explore the relationship between the gender of the characters portrayed in novels and their language, which illuminates the performative aspects of gender.  Although we do not entirely escape the entrapments of gender as a defining authorship trait in our analysis, we do use it to develop deeper understanding of dialogue as a whole rather than simply a tool of reifying stereotypes. From our innovations, we propose to answer three important questions: 1) what is the composition of the dialogic landscape in fiction? 2) what characterizes dialogue linguistically? and 3) how is author gender and the gender performed by characters reflected in the language of dialogue?  Data and Dialogue Extraction Using three corpora of novels from the beginning of the romantic period to the present day, we construct a corpus that contains 1,106 novels from 1782 until 2011. This corpus is largely composed of the standard English canon, with the pre-1900 portion drawn from Chadwyck-Healey, and therefore exhibits the bias present in canonical authorship, containing 851 male-authored novels and 255 female-authored novels. We introduce and use a new dialogue extraction system to locate quoted text based on a series of rules and regular expressions.Then, using a number of high-precision patterns such as <QUOTE>-<PRONOUN>-<VERB>, we assign speaker gender to quotes associated with gender-disambiguating pronouns ( he, she, etc.) or proper names that can be reliably distinguished from gendered namelists. We evaluate our system on texts hand-labeled for quotes and speaker identity (Austen’s  Pride and Prejudice (He 2013), Cooper’s  The Spy, and Fitzgerald’s  The Great Gatsby), resulting in average quote extraction of 94.5% of quotes at 95.4% precision and gender attribution of an average of 44.4% of the extracted quotes at 93.1% precision.    The Dialogic Landscape We first statistically evaluate the distributions of extracted dialogue to see whether novels have become more dialogue driven over time. Controlling for novel length (Figure 1), we measure mean quotes per thousand words per decade, finding a steady increase over time regardless of author gender (Figure 2). Overall, male authors add roughly 1 quote per 20 years (r2 =.80); female authors 1 quote every 30 years (r2 = .64).    Figure 1: Average novel length over time    Figure 2: Quotes per thousand words by author gender over time  Next, we examine the relative attention that authors pay to characters and gender, using dialogue as our lens. From the extracted speaker-assigned quotes we calculate the proportion of male- to female-spoken dialogue per novel. This allows us to understand the gender of the characters portrayed in our texts and how this composition has changed over time.    Figure 3: Mean normalized ratio of male- and female-spoken quotes by novel over time by author gender, bucketed by 50 year intervals  We compute the mean of a normalized ratio of words spoken by male versus female characters per novel, bucketed by 50 year intervals (Figure 3). This ratio is normalized such that it forms a continuous spectrum centered at 0, with +1 signifying that male characters spoke twice as often; -1 that female characters did. Here, we see that male authors tend to write male-spoken dialogue and female authors female-spoken dialogue, but that overall, female-authored dialogue tends to be close to balanced (mean = -.04) while male-authored dialogue is far from it (mean = 2.5). An interesting leap towards balanced portrayal happens among male authors at the beginning of the 20th century, influenced largely by authors such as Henry James, E.M. Forster, and Booth Tarkington.   Linguistic Characterization of Dialogue To discover the underlying linguistic differences between narration and dialogue, we perform Multi-Dimensional Analysis (MDA) on the dialogue in our corpus, using a slightly modified set of Penn Treebank part-of-speech tags as distinguishing features. While it is not a classification algorithm, MDA isolates factors such that data with similar features are grouped together. These factors contain features that are positively or negatively correlated with one another. Effectively, even though no class labels are used, these factors reveal strong feature relationships for dialogic text.  Based on this analysis we propose a new dialogue metric,  dialogism, that is robust to lexical choice and transportable across corpora. The new metric is constructed from the ten factors (out of 16) with r2 > 0.5 when used to separate narration from dialogue and considers both positive indicators and negative indicators of dialogue, shown in Table 1. Figure 4 shows the high overall separation between narration and dialogue that our metric achieves (Kolmogorov-Smirnov value = 0.89).    Table 1: Positive and negative contributors to dialogism score  + -   Present tense verbs, bare verbs, modals, 1st/2nd/”it” pronouns, Wh-pronouns, interjections, existential there, adverbs  past tense verbs, 3rd person pronouns, gerunds, particles, nominalizations, determiners, Wh-determiners, prepositions and subordinating conjunctions, adjectives       Figure 4: dialogism scores for narration and dialogue   Examing dialogism over time, we find that the distance between narration and dialogue has increased since the 18th century, and that, as a whole, novels are becoming more dialogic. At the same time, visualizing the outlying data quantifies the effects that shifts in literary style, for instance, modernism in the early 20th century, had in terms of dialogism.    Figure 5: Difference in dialogism scores for dialogue and narration over time for texts within 1 (73.1%), 2 (93.5%), and 3 (99.5%) standard deviations of the mean difference.    Gender in Dialogue: Performance and Authorship We finally turn to the performative aspects of gender: how do authors perform gender through the speech of their gendered characters? Which authors most significantly differentiate their male and female characters through dialogue?  Using the same corpus, but subsampling to balance for speaker gender, we isolate author gender effects and use these results combined with the original data to isolate speaker gender effects. This analysis reveals that the differences between male- and female-authored dialogue cannot be accounted for solely based on either author gender or on the genders of the speakers they portray. Author gender accounts for 64% of the variation between male- and female-authored dialogue while other speaker gender effects account for 36%. However, because the standard deviation of this ratio is so high (23%), this is an indication that some authors are more heavily influenced by their own genders and some more by the characters they portray.    Figure 6: Authors whose male and female characters are significantly differentiated by dialogism score at p < .0005  Digging deeper into the question of which authors are better than others at differentiating male and female characters through dialogue, we perform t-tests on the male- versus female-spoken quotes for each author. Shown in Figure 6 are the authors who differentiate their male and female characters through dialogue at p < .0005. Notably, while most authors portray female characters as more dialogic in speech, a small minority do the opposite (above, Maria Cummins), a trend that also holds at higher p thresholds.   Conclusion The development of the novel is marked by an increased use of dialogue over time. This suggests that the deepening of characters was accompanied by, or may be an effect of, a shift of attention towards performative modes of characterization. Moreover, this transformation to a more dialoguedriven structure bears a gendered dimension, suggesting that the depiction of sociality by female novelists favored a more realistic gender balance than the predominately male social models favored in maleauthored texts. Our work suggests that the presence of any gendered language in the text may be contingent upon the mode of performance adopted by the author, regardless of gender, an observation supported by the variation in both dialogue cast composition and dialogism at the beginning of the 20th century, when a shift in literary style occurred. Further, the strict delineation of midnineteenth century sociality into the gendered public and private spheres, as represented by the novel, is itself a deeply gendered understanding of sociocultural codes more true of maleauthored texts than femaleauthored ones. The substantial effects that speaker gender has on dialogue indicates that perhaps not only are novels intrinsically dialogic, but that dialogue itself is intrinsically performative. Thus, the performative nature of a novel is itself deepened by the degree to which its’ author differentiates the characters through a gendered performance of dialogue.   Acknowledgements This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE-114747. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. The authors thank their anonymous reviewers, the Stanford LitLab, and the Stanford NLP group for their helpful feedback.  ",
       "article_title":"The Dialogic Turn and the Performance of Gender: the English Canon 1782-2011",
       "authors":[
          {
             "given":"Grace",
             "family":"Muzny",
             "affiliation":[
                {
                   "original_name":"Stanford Universtiy, United States of America",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          },
          {
             "given":"Mark",
             "family":"Algee-Hewitt",
             "affiliation":[
                {
                   "original_name":"Stanford Universtiy, United States of America",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          },
          {
             "given":"Dan",
             "family":"Jurafsky",
             "affiliation":[
                {
                   "original_name":"Stanford Universtiy, United States of America",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-18",
       "keywords":[
          "literary studies",
          "natural language processing",
          "english studies",
          "gender studies",
          "linguistics",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The problems at hand In the context of the current onslaught cultural artefacts in the Middle East face from the iconoclasts of the Islamic State, from the institutional neglect of states and elites, and from poverty and war, digital preservation efforts promise some relief as well as potential counter narratives. They might also be the only resolve for future education and rebuilding efforts once the wars in Syria, Iraq or Yemen come to an end; and while the digitisation of Archaelogical artefacts has recently received some attention from well-funded international and national organisations, particularly vulnerable collections of texts in libraries, archives, and private homes are destroyed without the world having known about their existence in the first place.   For a good example of crowd-sourced conservation efforts targetted at the Armenian communities of the Ottoman Empire see the  Houshamadyan project, which was established by Elke Hartmann and Vahé Tachjian in Berlin in 2010 and launched an “Open Digital Archive” in 2015. Other digitisation projects worth mentioning are the  Yemen Manuscript Digitisation Project (University of Oregon, Princeton University, Freie Universität Berlin) and the recent “Million Image Database Project” of the  Digital Archaeology Institute (UNESCO, University of Oxford, government of the UAE) that aims at delivering 5000 3D cameras to the MENA region in spring 2016.    Early Arabic periodicals, such as Butrus al-Bustānī’s  al-Jinān (Beirut, 1876–86), Yaʿqūb Ṣarrūf, Fāris Nimr, and Shāhīn Makāriyūs’  al-Muqtaṭaf (Beirut and Cairo, 1876–1952), Muḥammad Kurd ʿAlī’s  al-Muqtabas (Cairo and Damascus, 1906–18/19) or Rashīd Riḍā’s  al-Manār (Cairo, 1898–1941) are at the core of the Arabic renaissance ( al-nahḍa), Arab nationalism, and the Islamic reform movement. These better known and—at the time—widely popular journals do not face the ultimate danger of their last copy being destroyed. Yet, copies are distributed throughout libraries and institutions worldwide. This makes it almost impossible to trace discourses across journals and with the demolition and closure of libraries in the Middle East, they are increasingly accessible to the affluent Western researcher only.   In many instances libraries hold incomplete collections and only single copies. This, for instance, has caused even scholars working on individual journals to miss the fact that the very journal they were concerned with appeared in at least two different editions (e.g. (Glaß, 2004) see (Grallert, 2013; Grallert, 2014)).   Digitisation seemingly offers an “easy” remedy to the problem of access and some large-scale scanning projects, such as  Hathitrust,   It must be noted that the US-based HathiTrust does not provide public or open access to its collections even to material deemed in the public domain under extremely strict US copyright laws to users outside the USA. Citing the absence of editors able to read many of the languages written in non-Latin scripts, HathiTrust tends to be extra cautious with the material of interest to us and restricts access by default to US-IPs. These restrictions can be lifted on a case-by-case basis, which requires at least an English email conversation and prevents access to the collection for many of the communities who produced these cultural artefacts; see  https://www.hathitrust.org/access_use for the access policies.   the  British Library’s “Endangered Archives Programme” (EAP),  MenaDoc or  Institut du Monde Arabe produced digital facsimiles of numerous Arabic periodicals. But due to the state of Arabic OCR and the particular difficulties of low-quality fonts, inks, and paper employed at the turn of the twentieth century, these texts can only reliably be digitised by human transcription (c.f. Märgner and El Abed, 2012).   For the abominable state of Arabic OCR even for well-funded corporations and projects, try searching inside Arabic works on Google Books or HathiTrust.  Funds for transcribing the tens to hundreds of thousands of pages of an average mundane periodical are simply not available, despite of their cultural significance and unlike for valuable manuscripts and high-brow literature. Consequently, we still have not a single digital scholarly edition of any of these journals.  On the other hand, gray online-libraries of Arabic literature, namely   al-Maktaba al-Shāmila ,   Mishkāt ,   Ṣayd al-Fawāʾid  or   al-Waraq , provide access to a vast body of, mostly classical, Arabic texts including transcriptions of unknown provenance, editorial principals, and quality for some of the mentioned periodicals. In addition, these gray “editions” lack information linking the digital representation to material originals, namely bibliographic meta-data and page breaks, which makes them almost impossible to employ for scholarly research.    Our proposed solution With the  GitHub-hosted TEI edition of  Majallat al-Muqtabas    For a history of Muḥammad Kurd ʿAlī’s journal  al-Muqtabas (The Digest) see (Seikaly, 1981) and the readme.md of the project’s  GitHub repository.   we want to show that through re-purposing well-established open software and by bridging the gap between immensely popular, but non-academic (and, at least under US copyright laws, occasionally illegal) online libraries of volunteers and academic scanning efforts as well as editorial expertise, one can produce scholarly editions that offer solutions for most of the above-mentioned problems—including the absence of expensive infrastructure: We use  digital texts from  shamela.ws , transform them into TEI XML, add light structural mark-up for articles, sections, authors, and bibliographic metadata, and link each page to facsimiles provided through  EAP and  HathiTrust; the latter step, in the process of which we also make first corrections to the transcription, though trivial, is the most labour-intensive, given that page breaks are commonly ignored by  shamela.ws’s anonymous transcribers. The digital edition (TEI, markdown, and a web-display) is then hosted as a GitHub repository with a  CC BY-SA 4.0 licence for reading, contribution, and re-use.   The text of  al-Muqtabas itself is in the public domain even under the most restrictive definitions (i.e. in the USA); the anonymous original transcribers at  shamela.ws do not claim copyright; and we only link to publicly accessible facsimile’s without copying or downloading them.    We argue that by linking facsimiles to the digital text, every reader can validate the quality of the transcription against the original we can remove the greatest limitation of crowd-sourced or gray transcriptions and the main source of disciplinary contempt among historians and scholars of the Middle East. Improvements of the transcription and mark-up can be crowd-sourced with clear attribution of authorship and version control using .git and GitHub’s core functionality. Such an approach as proposed by Christian Wittern (2013) has recently seen a number of concurrent practical implementations such as project  GITenberg led by Seth Woodworth, Jonathan Reeve’s  Git-lit, and others.  In addition to the TEI XML files we provide structured bibliographic metadata for every article in  al-Muqtabas (currently as BibTeX). The TEI edition will be referencable down to the word level for scholarly citations, annotation layers, as well as web-applications through a documented and persistent URI scheme.  In order to contribute to the improvement of Arabic OCR algorithms, we will provide corrected transcriptions of the facsimile pages as ground truth to interested research projects starting with  transkribus.eu.  To ease access for human readers (the main projected audience of our edition) and the correction process, we also provide a  basic web-display that adheres to the principles of  GO::DH’s Minimal Computing Working group. This web-display is implemented through an adaptation of the  TEI Boilerplate XSLT stylesheets to the needs of Arabic texts and the parallel display of facsimiles and the transcription. Based solely on XSLT 1 and CSS, it runs in most internet browsers and can be downloaded, distributed and run locally without any internet connection—an absolute necessity for societies outside the global North.    Figure 1: The web-display of  Digital Muqtabas based on TEI Boilerplate.   Finally, by sharing all our code, we hope to facilitate similar projects and digital editions of further periodicals. For this purpose, we successfully tested adapting the code to  ʿAbd al-Qādir al-Iskandarānī’s monthly journal  al-Ḥaqāʾiq (1910–12, Damascus)    On the history of  al-Ḥaqāʾiq and some of its quarrels with  al-Muqtabas see (Commins, 1990:118–22).   in February 2016.    Conclusion The paper will discuss the challenges cultural artefacts, and particularly texts, face in the Middle East. We will propose a solution to some of these problems based on the principles of openness, simplicity, and adherence to scholarly and technical standards. Applying these principles, our edition of  Majallat al-Muqtabas improves already existing digital artefacts and makes them accessible for reading and re-use to the scholarly community as well as the general public. Finally, we will discuss the particular challenges and experiences of this still very young project (since October 2015).   ",
       "article_title":"The journal _al-Muqtabas_ between _Shamela.ws_, HathiTrust, and GitHub: producing open, collaborative, and fully-referencable digital editions of early Arabic periodicals---with almost no funds.",
       "authors":[
          {
             "given":"Till",
             "family":"Grallert",
             "affiliation":[
                {
                   "original_name":"Orient-Institut Beirut, Lebanon (Lebanese Republic)",
                   "normalized_name":"Orient Institut Beirut",
                   "country":"Lebanon",
                   "identifiers":{
                      "ror":"https://ror.org/03wd03705",
                      "GRID":"grid.506474.4"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "linking and annotation",
          "xml",
          "encoding - theory and practice",
          "historical studies",
          "near eastern studies",
          "archives, repositories, sustainability and preservation",
          "authorship attribution / authority",
          "English",
          "crowdsourcing",
          "copyright, licensing, and Open Access"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The North Carolina (USA) Jukebox project transforms an inaccessible audio archive from the 1930s, of historic North Carolina folk music collected by Frank Clyde Brown, into a vital, publicly accessible digital archive and museum exhibition. Led by Trudi Abel, a librarian in the Rubenstein Special Collections Library at Duke University, and Victoria Szabo, a faculty member in Visual and Media Studies and Information Science + Studies at Duke, this interdisciplinary, collaborative effort also involves scholars in music and folklore, music and preservation librarians, digital media specialists, descendants of the original performers, and contemporary musicians who play this music professionally today (Archives Alive Initiative | Trinity College of Arts and` Sciences, 2016).  As a teaching experience associated with the Library’s Archives Alive initiative, the project offers opportunities for students from arts, music, computer science, and engineering programs to learn about the collection and develop an exhibition from initial concept to execution, and to do so in collaboration with a diverse set of mentors and collaborators who help them understand the histories and technologies involved, as well as stakes of their presentation choices. As a ongoing archival project, it demonstrates the challenges and opportunities inherent in working out a major library archive and preservation effort alongside a live curricular intervention and planned public exhibition. As a research project, it offers scholars in media studies a firsthand view of how material recording and playback technologies and their affordances help shape subsequent cultural histories, and affect what we can recirculate and share today. Taken together, these strands demonstrate that introducing digital cultural heritage project development as a shared objective enriches student learning, encourages library archiving and preservation projects to consider their public facing dimensions as they construct new resources, and offers digital humanities and media studies scholars meaningful opportunities to collaborate with colleagues in historically minded disciplines around new forms of scholarly production – in this case data-driven exhibitions at the Mountain Music Museum in Western NC, at the Rubenstein Special Collections Library at Duke, and online.   Figure 1. NC Jukebox Project Advertisement  Our project begins with the songcatcher himself. In the 1930s Frank Clyde Brown, Duke Professor of English, and co-organizer of the North Carolina Folklore Society (1913) as Zeke Graves in our Library tells us, began recording and archiving Western North Carolina folk music (Graves, 2015). Following in the tradition of folklorist Alan Lomax, and songcatcher/musician Bascom Lunsford as chronicled by Loyal Jones, along with other famous songcatchers of the period, he drove around region capturing a range of singers and songs using the technology available in the period, a notebook and Dictaphone equipped with first wax cylinders and later aluminum cylinders (Jones and Forbes, 1984). Like us, Brown involved his students in the project as well, encouraging them to capture songs and research their origins. Today most of those recordings are still housed on wax cylinders and glass disks in the Duke Libraries, and in the Library of Congress, with about 400 songs having already been converted to digital formats. The rest are being converted as part of a substantial Council of Library and Information Resources grant. In addition to learning about Brown, histories of the music, songcatching, and folklore practices of the period, students in a Fall 2015 NC Jukebox course began to work closely with the 400 digitized recordings we currently have available, developing metadata, transcribing songs, and organizing their materials in spreadsheet, blog, and database form. Our project also explores biographies of the singers, transcribes the songs as heard on the tapes in in comparison to other versions, and traces the Scotch‑English history and contemporary analogues of the songs themselves through research in Child’s  Ballads and other key sources (Child et al., 2001). In addition, we have begun to demonstrate change over time and space through maps, patterns, flows, timelines, and networks of the music - a kind of distant listening, or viewing of its collection and presentation, with more to come. In the physical exhibits, interactive touchscreens, period photos, and hybrid analog‑digital audio playback machines – a radio, a Jukebox, and perhaps a 78‑playing phonograph – will invoke the historical conditions of production and reception of the music for diverse audiences.  As an historiographical research project, NC Jukebox is also offering opportunities to explore firsthand how social and material conditions affect the writing of cultural history itself. Over the course of this project we have learned about the history of songcatching and folklore as social and academic practices designed to verify expectations about a specific kind of musical heritage. Brown died before his work could be compiled into the published versions of his work. As his papers reveal, not all of the songs he collected were included in the final, posthumous collection of his work (Guide to the Frank Clyde Brown Papers, 1912-1974, 2016). His subsequent editors, like other before them who were seeking a pure musical tradition descended from that of the Scots-Irish settlers in the region, as seen in Ritchie and Orr’s  Wayfaring Strangers, for example, picked and chose songs to include in the published work (Ritchie, Orr and Parton, 2014). Their criteria are (helpfully for future researchers) sketched out in their editorial notes, which are also in the archives. Songs that were too popular, had been published, were too religious, or, perhaps most significantly, were from African American traditions, were excluded from the published collection, even if familiar from other sourdes like the Library of Congress Checklist of Recorded Songs (U.S Library of Congress, 1942). On another note, we also experienced the lingering effects of musical and cultural segregation first-hand as a class when our well-intentioned musical guest, a contemporary folksinger who is helping keep the Mountain Music traditions alive today, explained that he was going to perform a song as he had heard his “colored” neighbors sing it growing up (McKinney, 2015). This moment became part of a class conversation about representing tradition while at the same time acknowledging our contemporary perspectives upon it in how we frame the music.  This project is also about the history of recording and reproduction technologies, which has deeply affected the content. Encountering recordings made from wax cylinders and glass disks, which included a white-glove visit from Special Collections as well as examining photographs of Brown and other “in the field,” encouraged conversations around our continuously evolving standards and expectations for archiving and reproduction. Our students had to consider whether it was more “authentic” to leave in the hisses and crackles that had made it into the third generation audio files they were listening to, or whether they could and should attempt to clean up the sound quality so it was closer to the “original” source – the singers themselves. Were we archiving the archives, or the ur-performances? Our students also learned about and from Charles Bond, the onetime Duke undergraduate (and now lawyer in San Francisco) who serendipitously took it upon himself in the 1980s to transfer some of the existing recordings to reel-to-reel tapes, using a moog synthesizer to clean up some pops and crackles.    Figure 2. Wax Cylinder from the FCB collection  Further, we discovered how the limits of the recording medium itself reveal the priorities of the documentarians and archivists involved. Wax disks could only record 6-7 minutes of a song, as the  Federal Cylinder Project editors note (Gray and Schupman, 1990). Songcatchers might record just one stanza of a song, rather then the whole performance, a fact that highlights that it was the intellectual process of abstracting data from the performance, to be converted to written notation and lyrics, rather than the performance itself, that was most valued for academic folklore purposes. The recordings were data to mine rather than songs to hear. We have also confronted challenges to our efforts in repatriation and exhibition as we began to develop our downloadable “Greatest Hits” of the FCB collection. Because of copyright restrictions on published songs, in the end we may need to limit our final song choices to the purely folkloric – putting us in some cases right back in line with Brown’s purity-seeking editors! We have even begun to wonder about the NC Jukebox idea itself as a title and concept, given that the term jukebox didn’t come into common parlance until the 1940s, as we learned from  Jukeboxes:An American Social History, and the music we are sharing was most likely shared at the time either locally or over live radio, as our guest Terry McKinney told us (Segrave, 2002). Our presentism is a both a problem and an acknowledgement of our own historicity and subjectivity.  NC Jukebox is serving as a prototype for future “Archives Alive” projects at Duke University in terms of pedagogical approach, access to primary and secondary source materials from Special Collections, community engagement, and digital platform and exhibition development. As a digital heritage project designed to serve multiple audiences, the digital and onsite exhibition components are being built with an eye towards multiple display formats and locations for a single set of materials within a grown growing database of content. This includes exhibition in a regiona museum organized by McKinney himself (Neufeld, 2015). It also necessitates metadata standards and library infrastructure, a conversation that is ongoing with our Library staff. For the exhibits in Western North Carolina and at the Rubenstein Library we have websites, Omeka exhibits, and interactive web graphics, as well as the downloadable playlist and physical exhibits and listening stations. For the archive, however, we are working with Library and technology partners on a more sustainable and flexible content management system in Drupal that draws content from the more permanent institutional repository solution, which will serve as the substrate for future development as well as, we hope, drive later installations. Our hope is that subsequent generations of students, librarians, and scholars will be able to build upon what we have done in bringing the archives alive. ",
       "article_title":"The North Carolina Jukebox Project: Archives Alive and the Making of Digital Cultural Heritage",
       "authors":[
          {
             "given":"Victoria",
             "family":"Szabo",
             "affiliation":[
                {
                   "original_name":"Duke University, United States of America",
                   "normalized_name":"Duke University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00py81415",
                      "GRID":"grid.26009.3d"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "folklore and oral history",
          "music",
          "project design, organization, management",
          "digital humanities - pedagogy and curriculum",
          "media studies",
          "digitisation, resource creation, and discovery",
          "historical studies",
          "archives, repositories, sustainability and preservation",
          "English",
          "audio, video, multimedia",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" In this paper I will showcase the results of topic-modelling research undertaken during my 2015 visiting fellowship at Victoria University Wellington (VUW), New Zealand. This research was undertaken in collaboration with staff at the Alexander Turnbull Library, National Library of New Zealand (ATL), and was subsequently applied to the Open Philology Project (OPP) in the Department of Digital Humanities at the University of Leipzig, Germany, and to Classical Persian corpora in collaboration with the Roshan Institute for Persian Studies at the University of Maryland (UMD), USA. The paper emphasizes how humanities researchers, even those who do not have deep scripting knowledge, can be enabled not only to understand, but also to modify the topic-modelling process, adapt it to their corpus- or language-specific needs, and then use the results of this process for further qualitative research. Topic-modelling in digital humanities research is a means to an end that should always help to answer a specific research question. In this way the paper follows an alternative route to previous interactive topic-modelling research, for example Hu et al. (2014) at the University of Maryland. Instead of asking how the method of topic-modelling can be improved by user-input, the author of this paper focuses on the ways in which we can improve the results of topic-modelling while also improving the humanities researcher’s understanding of the method and its variables.  After a brief introduction to the research projects in Wellington and Leipzig and to topic-modelling itself, the paper will summarize the limitations of topic-modelling with special emphasis on how to determine an ideal number of topics, as well as a short discussion of morphosyntactic normalization and the use of stop-words. It will then suggest a researcher-focused method of addressing these limitations and challenges in topic-modelling by introducing the Shiny topic-modelling application developed by the author based on R, Shiny, and J. Chang’s LDA library and C. Sievert’s LDAvis library. The paper will then briefly demonstrate the applicability to the different use-cases at ATL and OPP, which deal with very different fields and languages, including English, Latin, Ancient Greek, Classical Arabic and Persian. The paper will finish by stressing how digital humanities research results and practices can be improved by enabling humanities researchers who focus on more traditional and qualitative analyses of the corpora to use the quantitative method of topic-modelling as a macro-scope and faceting tool; effectively calling humanities researchers back to their driving seats.   During my research stay at VUW I worked with the Research Librarian for cartoons at ATL, Dr Melinda Johnston, on a mixed-methods-based analysis of the reactions of cartoonists and New Zealand print publications to the Canterbury Earthquakes in 2010 and 2011. ATL is part of the National Library of New Zealand, an institution that is interested in making the country’s cultural heritage more accessible to a digital audience and researchers. Within the short project I attempted to automatize the detection and analysis of cartoon descriptions created by ATL and over 100,000 abstracts produced by Index New Zealand (INNZ); all items were published between September 2010 and January 2014. The INNZ-data could be retrieved as a double-zipped XML file from INNZ’s webpage and ATL’s item descriptions could be queried using the Digital New Zealand (DNZ) API. During the project it became apparent how a topic-modelling approach, first applied by the author in VUW’s Digital Colenso project, could considerably speed up the finding of earthquake-related descriptions and abstracts.    Figure 1: Percentage of earthquake-related articles and cartoons published in three New Zealand newspapers in the period from December 2010 to December 2011 generated using LDA-topic modelling. Also showing combined earthquake score.  The results were so impressive (see e.g. Figure 1 and forthcoming article by Johnston, M. and Koentges, T. in  Alexandria: The Journal of National and International Library and Information Issues) that the author decided to apply it to Latin and Greek literature in Leipzig’s OPP project. OPP has a text collection of over 60 million Greek and Latin words, and has recently begun to add Classical Persian and Arabic texts. It is one of the core interests of OPP to produce methods that can compete with more traditional approaches and that can swiftly generate results on big data. OPP is maintained and organized using eXistDB, the CTS/CITE-Architecture developed by the Homer Multitext Project, and additional web-based tools and services, including GitHub repositories and Morpheus, a Greek and Latin morpho-syntactic analyzer. This structure enables researchers to use a CTS-API to retrieve their desired text-corpora or specific texts. In a first evaluation run of the topic-modeller, 30,000 Classical Arabic biographies have been used (see Figure 2).     Figure 2: LDAvis visualization of the result of an LDA-topic model of 30,000 Classical Arabic biographies, showing a topic that can be used to detect biographies of women.  In both research institutions, OPP and ATL, researchers applying more qualitative methods complemented the process and evaluated results. In what follows the quantitative method topic-modelling will be explained briefly. Topic-modelling is “a method for finding and tracing clusters of words (called “topics” in shorthand) in large bodies of texts” (M. Posner, 2012). A topic can be described as a recurring pattern of co-occurring words (M. Brett, 2012). Topic models are probabilistic models that are often based on the number of topics in the corpus being assumed and fixed (D. Bley, 2012). The simplest and probably one of the most frequently applied topic models is the latent Dirichlet allocation (LDA). Success and results of LDA rely on a number of apriori-set variables: for instance, the number of topics assumed in the corpus, the number of iterations of the modelling process, the decision for or against morpho-syntactic normalisation of the research corpus, and how stop-words are implemented in the process. Furthermore, its interpretation is often influenced by how the topics are graphically represented and how the words of each topic are displayed.  While Sievert has found already a very convincing solution for the latter (2014), the former is often out of the hands of the qualitative traditional researcher and any bigger modifications would have to be implemented by a computer-savy researcher. However, topic-modelling is often not an end in itself, rather it is a tool used to help answer a specific humanities research question or to facet large text-corpora so that further methods can be applied to a much smaller selection of texts. Traditional researchers often have to continue to work with topic-modelling results, but may not always be aware of the bias that the apriori-set variables have brought into the selection process. One possible way to bridge this gap between researcher and method is to involve the qualitative researcher earlier by providing them with agency in the topic-modelling process. The author of this paper used R and the web-application framework Shiny to combine Chang’s LDA- and Sievert’s LDAvis-libraries with DNZ/CTS API requests and language-specific handling of the text data to create a graphical user interface (GUI) that enables researchers to find, topic-model, and browse texts in the collections of ATL, INNZ, OPP, and UMD. They can then export their produced corpus and model, so they can apply qualitative methods on a precise facet of a large text corpora, rather than the whole text corpora itself, which contains texts that are irrelevant for answering the researcher’s specific research question. On the left side of the GUI, the researcher can set the following variables: a) search term(s) or CTS-URN(s); b) the source-collection; c) certain stopword lists or processes; d) additional stop-words; e) the number of topics; and f) the number of terms shown for each topic in the visualization. The application then generates the necessary API-requests, normalizes the text as desired by the researcher, applies Chang’s LDA-library, and finally presents a D3 visualization of the topics, their relationship to each other, and their terms using Sievert’s LDAvis and dimension reduction via Jensen-Shannon Divergence & Principal Components as implemented in LDAvis. The researcher can then directly and visually evaluate the success of their topic-modelling and use the settings on the left like the settings of a microscope to focus on certain significant relationships of word co-occurrences within the corpus. Once they have focused their research tool, they can then export visualizations, topics, and their corpus for further research (see Figure 3) and teaching (see Figure 4).    Figure 3: GUI for Topic-Modelling ATL and INNZ descriptions.  The approach also enables educators to use the topic-modelling results to identify sight-readings for language-teaching purposes. The passages are paired based on the profile of their document-topic values and Perseid’s Morpheus API is used to mark vocabulary not-yet-known by the learners. Unknown vocabulary links to an online dictionary, using markdown (see Figure 4).   Figure 4: Sight-Readings based on the passage Thuc. 1.26.1 with unknown vocabulary marked up.  The author hopes that this paper demonstrates how topic-modelling, in all its complexity, can be opened up to, and positively influenced by, more traditional researchers without advanced computer skills, enabling them to answer specific research questions based on large text corpora. ",
       "article_title":"Researchers to your Driving Seats: Building a Graphical User Interface for Multilingual Topic-Modelling in R with Shiny",
       "authors":[
          {
             "given":"Thomas",
             "family":"Koentges",
             "affiliation":[
                {
                   "original_name":"University of Leipzig, Germany",
                   "normalized_name":"Leipzig University",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/03s7gtk40",
                      "GRID":"grid.9647.c"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "information retrieval",
          "visualisation",
          "cultural studies",
          "morphology",
          "classical studies",
          "corpora and corpus activities",
          "other",
          "archives, repositories, sustainability and preservation",
          "bibliographic methods / textual studies",
          "text analysis",
          "data mining / text mining",
          "digital humanities - multilinguality",
          "digitisation, resource creation, and discovery",
          "knowledge representation",
          "English",
          "interface and user experience design",
          "digital humanities - institutional support",
          "GLAM: galleries, libraries, archives, museums",
          "multilingual / multicultural approaches"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Replacing “How something is made, with a view to finding out what it is” with “How something is made, with a view to making it again” – the Essence with the Preparation – is linked to an option that’s completely antiscientific: in reality, the starting point of the Fantasy [of the critic’s writing of a novel] isn’t the Novel (in general, as a genre), but one or two novels out of thousands.  -Roland Barthes,  The Preparation of the Novel, Session of December 9, 1978, 13.   The Literariness of Topic Modeling  This short paper reports on the progress of my attempt to construct a reading of topic modeling using state-of-the-art literary criticism. I argue that dominant digital humanities understandings of topic models assume some of the characteristics of literature most essential to twentieth-century criticism – counter-factuality, a mediated form that is ultimately separable from aesthetic characteristics, and an efficient, self-enclosed, total form. More specifically, I show that topic models also tend to be read by digital humanists according to the assumptions, protocols, and caveats we accord to the interpretation of realist fiction. While often revealing and productive, many digital humanists’ uses of topic modeling are indebted to assumptions about the literariness and fictionality of topic models that we have yet to fully understand. Drawing on work by Stephen Ramsay, Johanna Drucker, Alan Liu, and others that theorizes continuities between the values of literary criticism and computational processes, I suggest that we temporarily set aside the idea that topic modeling reveals the \"contents\" of a set of novels (or of any other corpus). Instead, drawing on Roland Barthes’ late work on  The Preparation of the Novel, we might rethink topics as preparatory notes written by no one, as an imaginary archive whose contents furnish a productively alienating, too-perfect map of the novel’s preparation. In  Preparation, Barthes moved away from his earlier work’s emphasis on totalizing interpretations of literature’s meaning to think about models of the text that allow for a more partial and slow view of the process of meaning creation.   See Buurma, R.S. and Heffernan, L. (2014). “Notation After ‘The Reality Effect’: Remaking Reference with Roland Barthes and Sheila Heti.”  Representations 125:1 80–102. doi:10.1525/rep.2014.125.1.80.    Topic modeling has the potential for helping us towards a Barthesian reimagination of the novel's reader as the novel's writer, of the search for the fantasy origins of a novel as a method that pulls us away from formal totality and a form-content divide. While this reorientation comes out of literary studies, I also suggest that it might have applications for more instrumental uses of topic modeling outside the realm of the humanities, in which assumptions about topics as equivalent to a document set’s “contents” also tend to draw on our conventions for reading realist genres.     Fictionality and the Topic Model  The past few years have seen the rapid popularization of topic modeling among humanist scholars in general, and among scholars of literature in particular.   Blei, D. M. (2014). Topic Modeling and Digital Humanities.  Journal of Digital Humanities (April 8, 2013).  http://journalofdigitalhumanities.org/2-1/topicmodeling-and-digital-humanities-by-david-m-blei/ Erlin M (2014). The Location of Literary History: Topic Modeling, Network Analysis, and the German Novel, 1731-1864, p 55-90. In Matt Erlin (ed. and introd.) and Lynne Tatlock (ed. and introd.),  Distant Readings: Topologies of German Culture in the Long Nineteenth Century, Rochester, NY: Camden House; Goldstone, A. and Underwood, T (2014). The Quiet Transformations of Literary Studies: What Thirteen Thousand Scholars Could Tell Us.  New Literary History: a journal of theory and interpretation 45:3; Jockers, M.L. and Mimno, D (2013). Significant Themes in 19th-Century Literature.  Poetics 41:6, 750–69. doi:10.1016/j.poetic.2013.08.005; Laudun, J. and Goodwin, J. (2013). Computing Folklore Studies: Mapping over a Century of Scholarly Production through Topics.  Journal of American Folklore 126:502, 455-475; Meeks, E (2013). The Digital Humanities Contribution to Topic Modeling.  Journal of Digital Humanities, 2:1.  http://journalofdigitalhumanities.org/2-1/dh-contribution-to-topic-modeling/; Rhody, L.M. (2013). Topic Modeling and Figurative Language.  Journal of Digital Humanities.  http://journalofdigitalhumanities.org/2-1/topic-modeling-and-figurative-language-by-lisa-m-rhody/; Rhody, L.M. (2016). The Story of Stopwords: Topic Modeling an Ekphrastic Tradition.  Digital Humanities 2014,  Lausanne, Switzerland. Accessed January 3, 2016.  http://www.lisarhody.com/the-story-of-stopwords/ and Tangherlini, T.R. and Leonard, P (2013). Trawling in the Sea of the Great Unread: Sub-Corpus Topic Modeling and Humanities Research.  Poetics 41:6 (December 2013): 725-749. doi:10.1016/j.poetic.2013.08.002.     The literature on topic modeling abounds in stern and salutary warnings about the limits and dangers of topic modeling for humanistic study. One can read about the dangers of introducing algorithmic black boxes into literary research, the concern that literary scholars are unprepared to fully (or even partially) interpret the topic models and their related data, and the worry that they fail to understand even the interpretive choices made during corpus preparation. Part of the worry derives from a larger assumption that topic modeling \"reveals\" the \"contents\" of novels. We assume that literary critics dipping their toes into topic modeling will shed their traditional interpretive caution in the face of the algorithm’s authority, and will misunderstand the un-semantic nature of topics or accept meaningless correlations as meaningful. I want to suggest that all such warnings are relevant only given a very limited understanding of what a topic model is, its imagined relation to the corpus from which it derives, and the goals of the model’s interpreter. These warnings do usefully help us think about some of the invisible interpretive choices we make when we choose chunk and clean documents, apply stoplists, select a number of topics to train, and - most importantly, assign semantic labels to unsemantically generated topics. And yet these warnings assume either that topic models aspire to be mimetic maps of the corpuses they model or that technologically unsophisticated interpreters of topic models imagine that this is the case.   Benjamin Schmidt warns, for example, that \"simplifying topic models for humanists who will not (and should not) study the underlying algorithms creates an enormous potential for groundless — or even misleading — “insights.”\" Schmidt worries that a pair of assumptions about topic models – that they are \"coherent\" and \"stable\" – \"let humanists assume that the co-occurrence patterns described by topics are meaningful; topics are useful because they describe things that resemble “concepts,” “discourses,” or “fields.”\" Schmidt is worried, that is, that the appearance of semantic meaning we find in \"good\" topics will seduce humanists into thinking that they have discovered the \"contents\" of novels – whereas what topic modeling really offers us is exactly a non-semantic machine indexing of a set of texts about which our approaches tend to be based on ground assumptions about semantic meaning. Benjamin Schmidt, “Words Alone: Dismantling Topic Modeling in the Humanities.”  The Journal of Digital Humanities 2:1 (Winter 2013), http://journalofdigitalhumanities.org/2-1/words-alone-by-benjamin-m-schmidt/    This is not surprising; the assumption that topic models are a realist genre is pervasive in literature on topic modeling, literary and otherwise.   One recent paper, for example, describes good topics with the example of “trout fish fly fishing water angler stream rod flies salmon…” explaining that the topic “is specific. There is a clear focus on words related to the sport of trout fishing. It is coherent. All of the words are likely to appear near one another in a document. Some words (water, fly) are ambiguous and may occur in other contexts, but they are appropriate for this context. It is concrete. We can picture the angler with his rod catching trout in the stream. It is informative. Someone unfamiliar with the topic can work from general words (fishing) to learn about more unfamiliar words (angler). Relationships between entities can be inferred (trout and salmon both live in streams and can be caught in similar ways).” (Boyd-Graber J, Mimno D and Newman D (2014) Care and Feeding of Topic Models: Problems, Diagnostics, and Improvements. In: Airoldi E M, Blei D, Erosheva E A, Fienberg S E (eds.),  The Handbook of Mixed Membership Models and Their Applications. Boca Raton, FL: CRC Press, Taylor and Francis Group, 2015.)    Yet if we relieve ourselves of this constraint and instead substitute a more plausible frame – the topic model’s fictionality – we will be able to enjoy a wider range of relations between model and corpus.  In place of assuming that topic models belong to the realm of realism, then, we might pay more attention to the generative uncertainty of topic modeling and to its literal fictionality. Topics are probabilistically-created formations, and the algorithm that generates topic models is based on the enabling--but crucially, counterfactual--\"assumption that documents have multiple topics.\" (Boyd-Graber et al., 2015). By looking at the documents we offer it, the algorithm generates topics that, in given proportions, compose each document. (Or, rather, it generates the probability that a certain percentage of words in every given document were generated by a given particular topic.) Topics, of course, don't actually exist prior to the documents that generate them; they don't actually exist independently in the same way the documents at all. They are, in a certain sense, fictions. Topics are things that might have existed – but didn’t! - given the existence of the document set in question. While we can and sometimes do relegate this fact to the realm of methodology, the fictionality of topics is crucial to remember for any literary-critical uses of topic modeling, for it reminds us that these models offer us a view of our document set radically at odds with any other more literal sources of a novel we might use – such as an author’s notes towards a novel, or a catalog of the virtual or actual library of books a novelist brings to the writing table, or even the looser sense of social \"discourses\" that exist prior to novels and which we might imagine in part \"composing\" a novel.   As Boyd-Graber et alia note, \"Topic models are based on a generative model that clearly does not match thway humans write. However topic models are often able to learn meaningful and sensible models.\" (2014: 15).    Using a few targeted examples drawn from topic models of corpuses of nineteenth-century novels of varying sizes and comparing them to some examples of nineteenth-century novelists’ notebooks, I suggest that reimagining topic models as fictional notes might be not just a theoretical exercise but a practical way of conceptualizing the relation between topic model and corpus.    ",
       "article_title":"The Preparation of the Topic Model",
       "authors":[
          {
             "given":"Rachel Sagner",
             "family":"Buurma",
             "affiliation":[
                {
                   "original_name":"Swarthmore College, United States of America",
                   "normalized_name":"Swarthmore College",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/012dg8a96",
                      "GRID":"grid.264430.7"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "genre-specific studies: prose, poetry, drama",
          "teaching and pedagogy",
          "literary studies",
          "natural language processing",
          "english studies",
          "digital humanities - nature and significance",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The digitization of large databases of works of arts photographs opens new avenue for research in Art History. For instance, collecting and analyzing painting representations beyond the relatively small number of commonly accessible works was previously extremely challenging. In the coming years, researchers are likely to have an easier access not only to representations of paintings from museums archives but also from private collections, fine arts auction houses, art historian photo collections, etc. However, the access to large online databases is in itself not sufficient. There is a need for efficient search engines, capable of searching painting representations not only on the basis of textual metadata but also directly through visual queries. In this paper we explore how convolutional neural network descriptors can be used in combination with algebraic queries to express powerful search queries in the context of Art History research.   Context and Method This project is part of project called  Replica, conducted in collaboration with the Cini Foundation in Venice. This project is based on two parallel developments, the digitization of the Cini Foundation’s photo library, a collection of about a million photographs of paintings, engravings, plastic arts and architecture (1300 - 1900) and the creation of a dedicated search engine allowing for searches for visual patterns in this database. As the digitization is currently ongoing, the results reported in this paper are performed on a subset of only 39 000 paintings. However, the progressive densification of the database, including a large set of so-called minor painters, should, in the coming months, unfold the full discovery potential of this search engine.  The field of visual pattern recognition has been recently transformed by the surprising performances of so-called deep learning approaches using Convolutional Neural Networks (CNN). CNN are multi-layers architectures used for supervised learning, especially for object classification. Each layer is representing an operation on the previous layer: convolution layer, fully-connected layer, pooling layer, regularization layer, etc. These networks have many parameters (filter parameters, fully-connected weights) that can be learned via backpropagation in a supervised manner. Traditionally, the input (first) layer is a full raster image and the output (last) layer is a vector representing the score of the input image for each class. By showing the network some labeled images and comparing the network’s output to the desired label, one can update the parameters of the model. The theory of deep neural methods have been known for decades, and were already successfully applied with the first convolutional neural networks in the 90s to digit recognition (LeCun et al., 1989). However, their computational complexity and the necessity of important amount of training data have seen them being ignored for a long time. With very large datasets available like  ImageNet, and GPU computation being more accessible, there has been a sudden surge of interest in deep methods (Deng et al., 2009).  In 2012, a convolutional neural network shattered the competition in a difficult 1000 class object recognition challenge, attaining the impressive result of a top–5 error of 15.3% compared to 26.3% for the runner-up (Krizhevsky et al., 2012). Ultimately, this work had an important impact on the machine vision community starting the so-called deep learning revolution. A clear manifestation of this trend was that just a year later at the next iteration of this object recognition challenge, almost every entry was based on CNNs as well. Despite being trained to recognize a precise set of classes. It has been observed that some of the learned parameters of the CNNs will most likely be similar across different datasets. For instance, the first convolutional layer usually learns various edge detectors and basic filters. Some researchers have evaluated the representative power of CNN trained for a specific task to other problems. Using a model that outperformed the others in 2012 on the  ImageNet data, results seemed extremely promising, suggesting that task transfer is possible (Donahe et al., 2014).  To calculate the descriptors of our search engine, we use a pre-trained Convolutional Neural Networks similar to the one described in (Donahe et al., 2014). Each painting of our database is associated with 1000 features, corresponding to the last convolutional layer of the pre-trained Convolutional Neural Network. These features are thought to represent high-level characteristics directly usable for the classification tasks. Through this process each painting is associated with a single point in a high dimensional space. When a single image query is sent to the search engine, the results are simply shown, ranked by their distance to the query.  However, similarity between paintings could not be the results of single homogenous distance. To enable the users to specify the kind of similarity they want to explore, a more refined language has been introduced. Searches take the form of  algebraic formulas in which the user can add or subtract examples. For performing such searches, we use a binary support vector machine (kernel Radial Basis Function). In the cases where no negative examples are provided, a one-class-support vector machine is used (in the case of a query with a single image, this corresponds to a simple nearest neighbor algorithm). The rest of the paper shows examples of such algebraic queries and the corresponding results.     Examples of queries and results The classic principles for classifying visual similarities in Art History include various dimensions like recurrence of particular pictorial patterns or common compositional structures. As a query illustration, the first criteria of classification chosen is the search for common ‘dominant and multiple pictorial motif’ in the composition. One classic example in this typology is the  Still life, featuring for instance only a large bouquet of flowers. The development of this subject has a long history, from the late sixteenth century, before arriving at its codification during the seventeenth-century. The results of a query with Juan de Arellano’s  Still life of flowers (Fig. 1) include other famous interpreters of the genre, almost identical in composition and also close in terms of chronological and pictorial influences: de la Corte, Snyders, Casteels. However, there are also seventeenth-century painters, Gentileschi, Régnier, Bonito, Vouet, who, while not painting Still lifes, are characterized by the same tonality of  chiaroscuro typical of this precise moment in history of art. Without further information the similarities found by a single image query include various families of resemblances combining pictorial patterns and color tones.   To focus only on the pictorial motif of flowers excluding any paintings with figures, we subtract one of the paintings by Gentileschi to the initial query (Fig. 2). We obtain all the ‘key painters’ in this genre including for instance Daniel Seghers. He does not paint a real Still life but flowers around a sacred figure, the Virgin, one of the first subjects, probably invented by Jan Brueghel the Elder. It is probably from this initial subject that evolves the  Still Lifes with flowers. So, in this case, the algebraic query recovered the evolution of a specific pictorial motif with its significant variations during the Seventeenth century.    Figure 1 : A query of a Still Life of Flowers by Juan de Arellano returns several paintings with flowers but also other subjects    Figure 2 : By subtracting to the flower painting the Finding of Moses by Orazio Gentileschi, only painting featuring flower are returned  Another criteria of classification in Art History are structural analogies between compositions (Gombrich, 1960). Structure of the composition is understood here in a geometrical sense, with the reoccurrence of similar geometrical patterns in various paintings. The formal analysis of paintings have classically focused on such kinds of structural similarity (Focillon 1964; Didi-Huberman 1996). The  The Gallery of Archduke Leopold painted by David Teniers the younger, in 1639, is also known to have four different variants. This painting is considered a reference of a long tradition of paintings subjects featuring cabinet of art lovers and collectors, a well studied genre (Findlen 1996; van der Veen 1993). A single query returns variations of the same painting (Staatsgalerie Schleissheim, Münich; Prado, Madrid; Kunsthistorisches Museum, Wien), but also painting featuring squares within squares (Fig. 3). For instance on  The Ambassadors depart by Vittore Carpaccio and Baptism of St Libertus by Colijn de Coter, squares are on the floor or on the wall. To refine further our search and find the “good neighborliness” (Warnke 2000; Freedberg 1989) of David Teniers structure, we can try to exclude these two examples by subtracting them to the initial query. Results of such a query exclude now interior scenes featuring geometrical squares but now include various scenes of the  Passion of the Christ organized as sequences of “squares”, non featuring any elements in perspective (Fig. 4). In a third attempt, we can now exclude those by substracting them to the initial query and reinforce the focus on search by adding variant of the first painting by Teniers. Indeed, all the first results feature now paintings with galleries of collectors with examples of the most important authors of the genre, thus facilitating the study of their mutual influence (Fig. 5).    Figure 3: A query with the The Gallery of Archduke Leopold by David Teniers the younger (1639) gives a first results four variants of the same painting by the same author. The following results include various kind of painting which same some similarities with the initial query but are not representing the same subject    Figure 4: When The Gallery of Archduke Leopold is subtracted with The Ambassadors depart by Vittore Carpaccio and Baptism of St Libertus by Colijn de Coter a series of paintings only containing hierarchy of embedded squares are returned. The formula has isolated a specific characteristic in the feature space when the presence of a multiple squares is the most specific trait    Figure 5: In order to search specifically for the paintings containing paintings, the two painting representing the stories of the Passion of the Christ can be subtracted. Most results now feature paintings in which paintings are present    Perspectives  Pattern recognition methods have made extremely impressive progresses in the recent years, thanks to the advances of convolutional neural network and the advent of very large databases of images. As Art History deals with the study of the migration of patterns, this is surely a great opportunity for designing new tools to search through large databases of paintings photographs. This paper is a first examination of the typologies of search use cases that could be envisioned combining convolutional neural network features and simple algebraic formulas. This initial study illustrates that this new way of expressing queries allows for the incremental definition of various kinds of “similarities” between paintings. Convolutional neural networks features manage to capture many dimensions of similarity between paintings, including composition, colors and also common iconographic elements. Combined with a simple language for expressing the specificity of the traits that the user looks for, it could enable new powerful search tools that may in turn have important impact on history of art studies.   ",
       "article_title":"Visual Patterns Discovery in Large Databases of Paintings",
       "authors":[
          {
             "given":"Isabella",
             "family":"di Lenardo",
             "affiliation":[
                {
                   "original_name":"DHLAB - EPFL, Switzerland",
                   "normalized_name":null,
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Benoit",
             "family":"Seguin",
             "affiliation":[
                {
                   "original_name":"DHLAB - EPFL, Switzerland",
                   "normalized_name":null,
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Frédéric",
             "family":"Kaplan",
             "affiliation":[
                {
                   "original_name":"DHLAB - EPFL, Switzerland",
                   "normalized_name":null,
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "renaissance studies",
          "art history",
          "English",
          "image processing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Introduction At the DH2014 conference it was explained how the  ZoomImagine software (Z-editor) allows zooming on units of text and contextual information (z-lexias) in order to explore layers of meaning that support critical readings of literature (Armaselu 2014). Changing perspective or analytical viewpoints by using different types of “magnifying glass” is also possible (Figure 1).     Figure 1. Z-text layout. Levels of z-lexias (center). Changing perspective (bottom right) In this study the results of two experiments will be discussed in which the Z-editor was used to explore cross-overs between text and image and history and literature in descriptions of imaginary cities and imaginary depictions of existing cities. The first case concerns readings of the  Invisible Cities (1972) in which Italo Calvino (1923-1985) described Marco Polo’s accounts of visits to cities to Kubla Kahn, Emperor of the Tartars. The second one discusses designs for and historical depictions of the citadel of the city of Groningen in the Netherlands in an atlas of the Flemish engineer Pierre Lepoivre (1546-1626). We will demonstrate that the Z-text model is both suitable for analyzing critical readings and artistic impressions of the Invisible Cities as for assessing different levels of historical evidence of visualizations, such as maps, drawings and designs of existing cities. Furthermore, it allows for navigating in an associative way through imaginary cities of which features might have inspired both Calvino’s literary writings as Lepoivre’s designs. Contemporary artists still inspired by Calvino seem to delve into a collective visual memory of “cities”, in a way Renaissance engineers linked their designs to representations of ideal cities and new fortress towns, from Campanella’s City of the Sun to Zamośź in Poland. The Z-text model might stimulate to explore digital past and future identities of imaginary and real cities. We will conclude with a discussion of the potential of the Z-editor interface for general debates within the digital humanities.  The Invisible Cities of Italo Calvino The publication of  Le Città Invisibili in 1972 by Italo Calvino was directly followed by comments by literary critics and by visualizations of artists inspired by the poetical descriptions of the imaginary cities in the imageless book. The division into 11 themes associated with the city that return each 5 times in the book -City and Memory, City and Desire, City and Signs, Thin Cities, Trading Cities, Cities and Eyes, Cities and Names, Cities and the Dead, Cities and the Sky, Continuous Cities and Hidden Cities- has resulted into various interpretations of these imaginary urban spaces, such as “rhizomatic space” Kerstin Pilz (2003) and “city of strata” Sambit Panigrahi (2014). The zooming functionalities of the editor and Z-text model make it possible for instance to analyze Calvino’s description of the continuous city of Leonia that each days breaches through its boundaries of deposited waste at its circumference with these contemporary interpretations (Figure 2).  Figure 2.  Invisible Cities Z-text. Level 1 (left), 2 (right). Calvino's text unfolding along different lines of “stratification”, the \"new-old\" divide and the \"textual-visual\" metamorphosis (the visual \"becoming\" of the text). The first level (left) contains fragments (z-lexias) conveying the meaning of \"new\" (through words like \"new\", \"renew\"). By zooming-in (right), further elements from deeper levels can be surfaced, i.e. fragments entailing the idea of accumulation of \"old\" (including words such as “yesterday”, \"past\", “residue”, “remains”, \"garbage\", \"refuse\") or visual representations not belonging to the original text but imagined by various artists.  Another \"rhizomatic\" expansion imagined via the model - along with the \"contextualization\" line ‐ enables other readings of Calvino’s work, both by critics and himself. Calvino in some occasions looks through the lens of the writer and in other moments through the eyes of the reader or editor and states “that the author’s view no longer counts” (Baldi, 2015; Calvino, 2015: 41-42). Calvino sometimes puts the Invisible Cities directly into context by referring to works that similarly are inspired by Marco Polo’s travels, such as the poem  Kubla Kahn (1798) of Samuel Taylor Coleridge or  The Message from the Emperor (1919) by Franz Kafka. In other occasions references are indirect, for example when Calvino notes that the atlas of Kubla Kahn contains images of “lands visited in thought, but not yet discovered or founded: New Atlantis, Utopia, the City of the Sun [..]” (Calvino, 1997: 147). On a more detailed level the possibility of sideways movements becomes important when Calvino reacts to all those critics who underlined the importance of the closing sentence by claiming that the Invisible Cities is “a many facetted book” with “various possible ‘conclusions’” (Calvino, 1983: 41). By combining the zooming functionality with the representation of contextual information a Z-text becomes a multidimensional space of analysis (Figure 3).     Figure 3.  Invisible Cities Z-text. Level 2 (top left), 3 (bottom left), 5 (right). “Multidimensional” exploration by switching perspectives. One can zoom-in to get the visual representation of a fragment (compare Figure 2) or, instead, change the perspective and zoom-in with a contextual magnifying glass and expand Calvino's text with Kafka's or Calvino's own comments on  Invisible Cities (Kafka, 2012; Calvino, 1983).  Such an interface may be useful for the analysis of the Invisible Cities when Calvino observes: “And yet, all these pages put together did not make a book: for a book (I think) is something which has a beginning and an end [..] It is a space which the reader must enter, wander round, maybe lose his way in, and eventually find an exit, or perhaps even several exits, [..]” (Calvino, 1983: 38). If we enter our concentric city of Leonia again, new associative perspectives pop up. We recognize Calvino’s reference to Kafka’s story of the Emperor’s messenger wanting to report about the death of Kubla Kahn by trying - breaking through the walls surrounding the palace, “the center of the world, piled high with its own refuse.” (Kafka, 2012: 28) However, another of Calvino’s references, the one to Campanella’s concentric walled City of the Sun allows to link the form of Leonia with images of built and not built fortified cities that are related to our second experiment dealing with levels of historical evidence of designs of citadels in the atlas of Lepoivre (compare Figure 5). This comparison is of interest since Calvino once considered a theme “Cities and Form”, but decided to merge it with those of other cities (Calvino, 1983: 38).  Imaginary Depictions of An Existing City: Groningen (Netherlands) Renaissance ideal-fortress cities were ideally radial-concentric cities with a polygonal perimeter with angular bastions. Therefore circumscribing and inscribing circles played an essential role in designing fortified cities and citadels. The design of fortifications was not just functional. Fortification atlases were cultural artifacts amongst other books on architecture such as the Renaissance Vitruvius editions that often contained visual interpretations of the city described in the lost original manuscript. In this context also the four drawings based on the designs of the Italian engineer Bartolomeo Campi of the city and citadel of Groningen in the Atlas of Lepoivre of ca. 1624 must be seen (Heuvel, 1994: 1998) (Figure 4).     Figure 4.  Lepoivre Atlas. Z-text layout (top), level 4 (bottom left); interpretation (bottom right). The imaginary space of historical representation can be traversed through layers corresponding to decreasing levels of historical evidence (top left) or to different perspectives (Campi/Aleotti and Agustino variants, top right).  The detail of the bastion was probably directly copied after Campi’s design of 1570 (Heuvel, 1994). Lepoivre included a drawing after Campi’s design for a hexagonal citadel, but a pentagonal one was executed. The third drawing shows Groningen with the hexagonal citadel that never existed in that form and the fourth as a part of the Siege of Groningen in 1594, when the citadel that existed was already dismantled (Heuvel, 1998: 200). The Z-editor not only allows moving along the four different levels of historical evidence, but also to position these drawings (that we could call nowadays historical falsifications) between more and less imaginary depictions of existing and non existing cities (Figure 5).     Figure 5.  Imaginary Cities Z-text. Level 1 (left), 2 (top right), 3 (bottom right). Traversal through the space of forms, following multiple exploration paths and connecting imaginary cities in literature (Calvino, Campanella) and in architectural treatises and theory (Vitruvius) with imaginary representations of existing cities in design (Lepoivre).  The Z-text model and the digital humanities  The Z-text model meets a wide array of new reading and analysis practices discussed in relation to the digital humanities. While the zooming function supports bridging distant and close reading by scalable reading (Mueller, 2012) the combination with contextualization on the various planes to read text and image from various perspectives coincides with notions of deep reading (Birkets, 1994). This combination allows for the creation of multiple levels of meaning and supports a continuously process of reinterpretation from multiple perspectives, contributing this way in developing digital hermeneutic methods (Capurro, 2010). ",
       "article_title":"Invisible Cities In Literature And History: Interfaces To Scalable Readings Of Textual And Visual Representations Of The Imaginary",
       "authors":[
          {
             "given":"Charles van den",
             "family":"Heuvel",
             "affiliation":[
                {
                   "original_name":"Huygens Institute for the History of the Netherlands, Netherlands,",
                   "normalized_name":"Huygens Institute for the History of the Netherlands",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04x6kq749",
                      "GRID":"grid.450092.a"
                   }
                }
             ]
          },
          {
             "given":"Florentina",
             "family":"Armaselu",
             "affiliation":[
                {
                   "original_name":"Centre virtuel de la connaissance sur l’ Europe (CVCE), Luxemburg",
                   "normalized_name":null,
                   "country":"Luxembourg",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-02",
       "keywords":[
          "visualisation",
          "literary studies",
          "linking and annotation",
          "digital humanities - nature and significance",
          "historical studies",
          "scholarly editing",
          "knowledge representation",
          "English",
          "italian studies",
          "interface and user experience design"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" This paper introduces a digital humanities project at the Max Planck Institute for the History of Science (Max-Planck-Institut für Wissenschaftsgeschichte, MPIWG) that aims to unlock the treasure chest of local knowledge written in the genre of Chinese local gazetteers for computer assisted analyses.  In the past two decades, a great amount of historical documents have been digitized and put on the Web to enable easy access for scholars around the globe. In parallel, the amount of searchable full-text versions of historical texts has increased, which opens the possibility of text mining the contents for large scaled analyses. Many works in this direction have been proposed and got recognition, while they are also criticized for drawing conclusions from seemingly imprecise results due to the restriction that their algorithms have no knowledge about the meanings of the different pieces in a text (Jockers, 2013; Google Ngram Viewer; Chen et al., 2007). An alternative approach is thus to first “teach” computers what each pieces of a text means before asking computers to run automatic analyses. Such “teaching” is done by tagging, or called markup. Many digital humanities projects have been using TEI, a standard for text encoding based on XML, to tag their research materials (TEI; Flanders). In the Local Gazetteers Project, since the genre organizes knowledge in a very structural way, we are also using tagging to teach computers the meanings of texts in order to turn them into data tables to enable computer assisted analyses including GIS mapping. However, since the amount of texts is huge, we also propose a research data repository for scholars to collaborate in this project and to aggregate their results.   What are the Chinese Local Gazetteers? The Chinese local gazetteers is a genre of texts that has been produced in China consistently from the 10 th century on to even today. Most of them are compiled by local officials as a major means to collect and aggregate historical, social, and geographical knowledge of an administrative region for governing purposes. There are at least 8,000 titles of pre-1949 local gazetteers still extant today. They cover almost every well-populated region in historical China.   Despite being compiled by different officials for different regions, the local gazetteers have developed a pretty consistent structure of “describing” local knowledge. Most gazetteers contain the following chapters: history, geography, local government, infrastructure (buildings, schools, temples, bridges), local products (grains, plants, animals, drugs, commodities), people (local officials and celebrities), and literature. The vast number, the longue durée, the width in geographical coverage, together with the extensive and consistent selection of topics have made the local gazetteers major sources for knowledge about regions for scholars from later periods. To date, several digitization projects on the local gazetteers have been conducted by either commercial vendors or public institutions, and thousands of titles have been made available in searchable full texts for scholarly access.    To name just a few of such digital local gazetteers project, there is first the  Chinese Local Gazetteers Database (Zhongguo Fangzhii Ku) by a commercial vendor Erudition, second open access projects such as CTEXT.org with OCR-ed full texts of the gazetteers, and third national projects in China such as the Electronic Local Gazetteers by the Beijing National Library.                  The Project’s Goal Local gazetteers are well studied, but the vast amount of information contained within also makes scholars struggled to study them analytically. We noticed that the local gazetteers often organize knowledge by listing the items: list of temples, lists of flora and fauna, lists of local officials, etc. (See Figure 1 for an example.) This characteristic makes the local gazetteers a database by nature.  The goal of this project is to transform this genre into a scholarly enhanced database in order to enable new forms of digital historical analyses. By turning texts (of lists of things) into data (tables), it will be much easier to aggregate the rich knowledge written in all the extant local gazetteers to compile a database for historical China on local knowledge across geographical regions and time periods. Research-oriented queries, visualization of query results, and further large scaled analyses will then be more easily realized with such a database.    Figure 1 Two pages from the chapter of “local products” from Fujian tongzhi (Qing, 1737). The pages list items according to categories. The bigger fonts represent the names of the items, while the smaller fonts are descriptions of the items. (Image source:  http://ctext.org/library.pl?if=en&file=50305&page=80.)     Our approach We have identified and developed a set of digital tools to help this process. They are: (1) an  extraction interface that helps historians to tag and to transform digital texts and their built-in structures into  data tables; (2) a  research data repository where historians can  share and publish their data collected via the extraction interface; (3) a set of  digital analytical, visualization, and analysis tools that can be applied on the collected data for posing research questions.    I. An extraction interface for transforming digital texts into data             Due to the well-formatted organization of the local gazetteers, some scholars have tried to write computer programs to parse the digital texts in order to collect data for their own research (Mitchell, 2015; Chang, 2015). However, due to the fact that each of the local gazetteers has slightly different formats, it is difficult to write generic computer programs that can work on all the gazetteers. The China Biographical Database project (CBDB) tackled this problem by building an interactive user interface that allows scholars to describe observable writing patterns of the text in regular expressions (Wikipedia, 2015) – The Smart Regex Tool. The compiled regular expressions can be saved, run against other texts, and modified to fit slightly different writing patterns. CBDB used this interface and efficiently collected 250,000 records on local officials from 290 local gazetteers using only 420 man-hours (Pang et al., 2014). We inherited this interface from CBDB and further improved it so that scholars can define different tag sets and regular expressions to capture information relevant to topics they are interested in. Figure 2 to Figure 6 are the step-by-step screenshots of how we use this interface to tag a text on local product and to export the result into a table. The resulting table contains not only the names, categories, descriptions, alternative names, and usages of the products, but also the source gazetteer, the chapter name, and page number as well as geographical coordinates for mapping purpose.   Figure 2 A sample digital texts for “local products” for Figure 1. The categories are circled in blue color, while the names of products are circled in pink.     Figure 3 Step 1 of turning texts into data: Break the text into records (rows) via the help of the Smart Regex tool.     Figure 4 Step 2: Adding shared categorical information to each row.    Figure 5 Step 3: Tag further information that you want to capture.    Figure 6 Step 4: Export the tagging result as a table.    II. Compiling a global database from local records: A research data repository Our extraction interface provides a semi-automatic way of transforming texts into data tables. Nevertheless, compiling a global database through this interface will still take a lot of time. We envision it to be a collaborative work among scholars with different research interests and specialties, and we eagerly need to be a way to give credits to scholars who contribute the data in the academic world. We found that the philosophy behind the open source software Dataverse, developed by Institute for Quantitative Social Sciences at Harvard University with the philosophy of “dedicated to sharing, archiving, and citing research data” (IQSS, 2015), matches our needs and will allow our project to meet the open access policy of the Max Planck Society and the “Berlin declaration”.   Please see the declaration at http://openaccess.mpg.de/Berliner-Erklaerung.   After a text is tagged in our extraction interface, a user can publish the data to LGDataverse, a Dataverse instance we set up for this project. A citation link to the data along with the contributors will be shown on the LGDataverse page, urging any scholar who wishes to use the data to cite it in their publications (Figure 7). We are still working on a way to aggregate the produced tables into one database in order to enable joint queries on records drawn from different gazetteers.    Figure 7 A screenshot of LGDataverse.    III. Computer assisted analyses on large scales By transforming texts into data tables with predefined shared schema, it enables computers to easily process and analyze the data on large scales with better accuracy. We envision there can be multiple tools connected to our data, and scholars can choose which tools to use based on their research needs.  At the moment we are using the open source software PLATIN GeoBrowser (http://platin.mpiwg-berlin.mpg.de/) to create geospatial visualization for our data with one click away. PLATIN doesn’t just display the geospatial distribution of the data on a map but also provides an animated timeline and user-defined pie charts, which we found very helpful for historians for preliminary analysis (Figure 8 and Figure 9).    Figure 8 LGMap service (based on PLATIN) with Pie Chart function for analyzing categorical data.    Figure 9 LGMap service with Timeline function showing temporal distribution of two datasets.     Concluding Remarks This is an ongoing project. As a history of science study, we are collecting data from the chapter of local products in order to understand how local identities were constructed through the compilation of local products. In 2016, MPIWG will invite visiting scholars to use our digital tools to collect and analyze data of other topics from local gazetteers. This will also be a chance to further examine the model we proposed: from data collection from digital texts, aggregation of data in a research data repository, to digital tools for analysis and visualization.   ",
       "article_title":"Compiling a Database on Historical China from Local Records: The Local Gazetteers Project at MPIWG",
       "authors":[
          {
             "given":"Shih-Pei",
             "family":"Chen",
             "affiliation":[
                {
                   "original_name":"Max Planck Institute for the History of Science, Germany",
                   "normalized_name":"Max Planck Institute for the History of Science",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0492sjc74",
                      "GRID":"grid.419556.a"
                   }
                }
             ]
          },
          {
             "given":"Zoe",
             "family":"Hong",
             "affiliation":[
                {
                   "original_name":"Max Planck Institute for the History of Science, Germany",
                   "normalized_name":"Max Planck Institute for the History of Science",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0492sjc74",
                      "GRID":"grid.419556.a"
                   }
                }
             ]
          },
          {
             "given":"Dagmar",
             "family":"Schäfer",
             "affiliation":[
                {
                   "original_name":"Max Planck Institute for the History of Science, Germany",
                   "normalized_name":"Max Planck Institute for the History of Science",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0492sjc74",
                      "GRID":"grid.419556.a"
                   }
                }
             ]
          },
          {
             "given":"Martina",
             "family":"Siebert",
             "affiliation":[
                {
                   "original_name":"Max Planck Institute for the History of Science, Germany",
                   "normalized_name":"Max Planck Institute for the History of Science",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0492sjc74",
                      "GRID":"grid.419556.a"
                   }
                }
             ]
          },
          {
             "given":"Jorge",
             "family":"Urzúa",
             "affiliation":[
                {
                   "original_name":"Max Planck Institute for the History of Science, Germany",
                   "normalized_name":"Max Planck Institute for the History of Science",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0492sjc74",
                      "GRID":"grid.419556.a"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "information architecture",
          "spatio-temporal modeling, analysis and visualisation",
          "visualisation",
          "maps and mapping",
          "data modeling and architecture including hypothesis-driven modeling",
          "databases & dbms",
          "digital humanities - facilities",
          "digitisation, resource creation, and discovery",
          "historical studies",
          "knowledge representation",
          "archives, repositories, sustainability and preservation",
          "content analysis",
          "English",
          "asian studies",
          "interface and user experience design",
          "geospatial analysis, interfaces and technology"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Imagine being faced with the choice of remaining in your hometown, living in what you have known all of your life in harshly oppressive conditions, or leaving, along with a number of your family and neighbors, headed North or West toward a land of promise and opportunity. This was the basic choice that many African Americans had in increasing numbers after 1865 when the emancipation proclamation was signed and a flurry of legislation passed in an attempt to equal the playing field for recently emancipated African Americans. By the early 20th Century, the Great Negro Migration was well underway, as African Americans were being driven off the land by the violence and intimidation of the Ku Klux Klan, natural disasters, and increasing levels of legal and extra-legal oppression and inequality in all sectors of life. They were also being pulled to the North and West by the perceived promise of increased opportunities in many aspects of life, the receipt of letters and communication from friends and relatives who had already made the move, and the perception of less racism in the North and West. By the end of World War I, The Great Migration had reached flood proportions with hundreds of thousands of African Americans moving to urban centers in the north such as Chicago, Il, Harlem, NY, the steel belt and the auto industry in the midwest, as well as out towards the great promise of the West. Those who left the south for the urban North knowingly or sometimes unknowingly became a part of the New Negro Movement , known later as The Harlem Renaissance, of the 1920s, and The Chicago Renaissance of the 1930s and 40s. Understanding the cultural, intellectual and political output of African Americans during this period is key to understanding American history and our contemporary society. But what was it like in the big city of the 20s? How did that change, if at all, from the big city of the 40s and 50s?Artistic production was huge during both these periods, how might similar issues of artistic production be discussed from within this virtual environment? There are a variety of ways to introduce our current generation of students to this vibrant aspect of African American culture that encompass both traditional and avant garde approaches. This project builds on two existing projects to engage today’s learners and teachers in ways not previously possible, and also intertwines modern technology and advanced visualization with traditional practices to create a learning experience like no other that is focused on  The Great Migration, and Beyond. Additionally, this paper explores, through the discussion and demonstration of the aforementioned project, ideas related to digital preservation, lack of diversity in Digital Humanities and introduces Digital Africana Studies as an engaging pedagogy.   The Great Migration and Beyond represents the first time that two major virtual environments focused on African American history and culture will be connected through both technology and theme. We recognize that there will be a number of challenges as we create rich historic landscapes and experiences while simultaneously creating a non-linear, visually and technologically unified journey through time and place related to African American life and culture.  The technology enabling this virtual world development is based on the Unity 3D game engine. Unity 3D is a platform for creating virtual worlds and games that accepts 3D assets or models using Open Standards format, meaning that assets can originate in nearly any 3D modeling tool and those entities can then be imported into Unity. The platform then enables us to distribute the final project to the spectrum of media devices (e.g. PC, Tablet, Game Console, Mobile,  and full VR environments such as the CAVE, CAVE2, or a Data Wall. Additionally, we plan to address how the environment can be experienced through the increasing number of peripheral devices such as the Oculus Rift, HTC Vive, Samsung Gear or even Google Cardboard. Key to the verisimilitude and distinctive look/feel of the Harlem/Bronzeville environment is the use of motion capture-based animation of the characters that the visitor encounters. While exploring the environment, visitors will encounter scenes depicting significant historic events of the time and place or common everyday occurrences where, those occurrences actually become a part of the evolving experience, in essence, a living part of a Theater of the Surround.  The Starting Point  The Virtual Harlem Project is a representation of Harlem, New York, as it existed during the 1920s Jazz Age, in 3D space, where objects such as buildings, interiors, automobiles and more are constructed using 3D modeling applications. This project is one of the earliest full virtual reality environments created for use in the humanities and certainly one of the first for use in an African American literature course. Virtual Harlem was originally conceived by Dr. Bryan Carter as part of his graduate work at the University of Missouri-Columbia in 1996, and developed by the Advanced Technology Center there who assisted him in designing it as a collaborative Virtual Reality (VR) environment and learning simulation in which participants learned about and experienced the Harlem Renaissance to supplement real world courses about the subject.  The Harlem Renaissance/New Negro Movement was a unique period in American history that occurred in the 1920’s, just after the end of World War I.   It was a time when Langston Hughes, Eubie Blake, Marcus-Garvey, Zora Neale Hurston, Paul Robeson, and countless other African Americans made their indelible mark on the landscape of American and international culture. This is a time when African Americans made their first appearances on Broadway; chic supper clubs opened on Harlem streets; riotous rent parties kept economic realities at bay while the rich and famous, both white and black, attempted to outdo each other with elegant, integrated soirees (Lewis)., and African American artists and entertainers were the toast of European Cafe Society.  Time Machine: Bronzeville has been in-development as a multi-modal recreation and immersive experience of the vanished and historically significant Bronzeville section of Chicago’s South Side, during The Chicago Renaissance period (1930–1950). Components of this project include a  computer game treatment, online immersive web destination, and  augmented reality gallery installation.  Through the use of digital 3D image creation and animation, game and web technologies, the visitor can explore the history, lore, and legends of Bronzeville during the defining events of the 20th century: The Great Migration, The Great Depression, The Chicago Renaissance, Jim Crow Segregation (American Apartheid), World War II, and the emergence of The Black Metropolis. Intuitively navigating the avenues, alleys and interior spaces of Bronzeville, and interacting with its residents, the visitor discovers the genius, ingenuity, and invention in all the arts and humanities, from painters’ studios and recital halls, juke joints and storefront churches, to lecture halls, theater stages and street corner soapboxes that distinguishes the vibrant and creative period of The Chicago Renaissance.   The Vision: An Expanded Harlem/Bronzeville World The immersive cityscapes of  Virtual Harlem and  Time Machine: Bronzeville are historical simulations and interpretations of the vanished Harlem and Bronzeville of the 1920s, 1930s and 1940s, recreated from extensive research of the photo and graphic records, the African American press archives, personal memoirs,  oral histories and statistics.  The richly detailed environments will be deeply embedded with informing and contextual media. The visitor’s movements and interactions with animated characters and objects will trigger radio broadcasts, phonograph recordings, ambient soundscapes, and links to other resources. The visitor will be able to interact with and manipulate objects in the environment to access the archive of historical photos, print documents and media clips that illuminate the events, persons, and significance of the creative, cultural, social and commercial engines that were Harlem and  Bronzeville in these periods. Extensive use is made of documentation gleaned from African American press, radio and film archives. The projected Harlem/Bronzeville complex will enable the visitor to explore 3D simulations of the vanished Harlem and Bronzeville communities. With a click/touch, select locations can be viewed through decades of change, allowing the visitor to witness the evolution of the neighborhoods, and to compare the historical views with the contemporary cityscape. 3D animated scenes and tableaux depict historical events, persons and places significant in understanding this period and its arts movement. In-world interactive maps and guides aid the visitor in navigating the terrains and times. Featured historical figures, locations, and significant events of Harlem and Bronzeville  include:  artists’ profiles and portfolios for Langston Hughes, Richard Wright, Arna Bontemps, Margaret Walker,  Gwendolyn Brooks, Inez Cunningham Stark, William Edouard Scott, Charles White, Archibald John Motley, Jr., Eldzier Cortor, William MacBride, Elizabeth Catlett, Gordon Parks, Horace Cayton, John Johnson, the Jones Brothers (Policy Kings), The Apollo Theater, the Cotton Club, Connie’s Inn, The Theresa Hotel, The Dark Tower, The South Side Community Art Center, Parkway Community House, South Side Writers’ Group, the American Negro Exposition, Louis Armstrong and Chicago Jazz, Thomas Dorsey, the “Father of Gospel Music”, blues artists, Contralto Mahalia Jackson, Katherine Dunham and Ballets Negres, the Beaux Arts Ball, The Skyloft Players, The Bud Billiken Parade, Artists’ and Models’ Ball, Savoy Ballroom, Regal Theater, Rhumboogie, Club DeLisa, among many others.   Timeline, Maps, and Infographics Through the Graphical User Interface Unity3d overlay (GUI), the visitor will be able to access a Timeline, maps and other infographics, presenting a multimedia chronology of The Great Migration, The Harlem Renaissance, and The Chicago Renaissance. Images and text, media clips and animations pop up as the cursor rolls over key points on the graph. Hot spots on the graph link to other components of the environment, enabling visitors to travel quickly around the space. The Timeline, maps and infographics also present contextual information about national and international events, as documented in the African American press and other media sources.  This prototype GUI will also enable users to save information, take snapshots and submit suggestions for integration of their own research for approval or suggestions to improve the experience.   Community-Of-Interest Another of our challenges will be to create a forum for a community-of-interest, encouraging inquiry and making connections, and a host site/server for live presentations, teaching/learning experiences, performances and discussions. We envision an expanding archive and repository for Humanities research and scholarship (Implementation Phase). The start-up phase will include the design and samples of supplemental materials and guides to aid educators, and researchers in making full use of the evolving technology components and capabilities of this multi-modal compendium of reference materials, image archive, bibliography, repository for and curation of visitor contributions, and library of, and portal to, essays and lectures by scholars. The immersive Harlem/Bronzeville complex presents an unprecedented contribution to the historical representation and interpretation of African Americans, uniquely enabled by current and emerging technologies. The design of the content and interactive engagement will make the Harlem/Bronzeville destination attractive to a wide spectrum of visitors, with international reach.   Enhancing the Humanities The evolution of video games, the excitement surrounding virtual reality with the launch of a number of VR headsets, the increased graphic and processing power of currently available personal computers and gaming consoles, the rise of low cost and accessible virtual environment and game development tools, and the focus on increased engagement in the classroom makes this a perfect time to consider ways that both these projects might focus on a much larger and unifying aspect of African American culture and transformative period of American history,  The Great Migration. The nationwide observance of the centennial of The Great Migration begins in 2016, and our project is intended to contribute to this multi-year observance.    The Larger Discussions This project, in addition to it representing an innovative use of emerging technologies to teach Africana content, also introduces three parallel discussions; digital preservation, lack of diversity in Digital Humanities and the pedagogy of Digital Africana Studies.    Lack of Diversity in Digital Humanities So what exactly is the Virtual Harlem/Bronzeville project? Since its inception, the Virtual Harlem Project has been called a variety of things to include a virtual learning platform, a collaborative learning network and a digital humanities project. Although it is most likely one of the oldest VR environments focused on African American life and culture, more specifically, that of the 1920s Jazz Age/Harlem Renaissance, until recently, Virtual Harlem has rarely been discussed as an example of diversity in Digital Humanities. This is rather odd given that the project is particularly focused on African American life and culture, that it was conceived by an African American scholar and that it was initially intended to be used in an African American literature course. Yet, discussions of Virtual Harlem tend to center on the project as a good example of advanced visualization, technology in the classroom, or even a digital humanities project, without much mention of it having an emphasis on diversity. There are a variety of reasons why this may be the case. One is that there are so few VR projects that deal with diverse topics that those that are tend to be discussed with regards to the technologies used, not necessarily the nature or content of the project. Virtual Harlem also represents the use of interactive technologies where students are encouraged to become active contributors to a much larger landscape that is accessible by other members of the class and by limited numbers who are not enrolled. Contributions to the environment are done in non-traditional ways, to include performance, 3D body scanning and motion capture. It is just this sort of non-traditional teaching, learning and use of advanced technologies that some, particularly within the Africana Studies scholarly community, sometimes tend to view with a level of scepticism. Possible reasons for this are complex and may in part be related to how traditionally, scholarly respect for the discipline has been connected to knowledge and experience of its scholars. When the focus is shifted towards the technology used, there may be a fear of losing part of that respect. Tara McPherson suggests that “politically committed academics ...engage technology and its production not simply as an object of our scorn, critique or fascination but as a productive and generative space that is always emergent and never fully determined” (McPherson 155). Virtual Harlem represents one of the myriad of technologies that are and can be used in the humanities to address the learning styles of this generation of students, to encourage students to make use of the variety of tools they have at their disposal to express their understanding of humanities content, and to help us all deal with increasing amounts of data and information directly and indirectly related to the humanities. There are, however, a number of exciting projects by a diverse set of scholars that are flying under the radar simply because these scholars tend not to publish their work in digital humanities journals, many are dealing with tenure and promotion and focusing their publishing efforts on that which those evaluating their dossiers are more familiar instead of pushing projects that some fear may call their scholarship into question simply because evaluating digital humanities projects is not always fairly done in every discipline. These scholars are caught in a catch 22. Their projects are an amazing example of the use of technology in the classroom, yet evaluators have a fundamental lack of understanding of what digital humanities is or how to deal with it for tenure and promotion. So underrepresented scholars typically have to publish in traditional venues while working on their digital humanities projects as side projects until tenure and promotion are earned. Funding for digital humanities projects dealing with diverse topics is also difficult. Statistics are difficult to come by, but judging from the articles published in the most difficult popular digital humanities journals and books, there are very few pieces published that document projects dealing with diverse topics, created by underrepresented scholars. That is a problem. So what can be done to diversify the field? What can be done to encourage young minority scholars to contribute their ideas to the growing body of digital humanities scholarship in an effort to strengthen the field? Answers to these questions are not easy, nor will they be addressed in the very near future. However, there are efforts being made to introduce underrepresented scholars to digital tools and projects that, in time, may filter into the classroom and eventually into scholarship of these groups. These activities include workshops funded by the National Endowment for the Humanities and the Mellon Foundation along with pedagogies being developed that inherently incorporate technology as a part of the teaching and learning process.   Digital Africana Studies  Digital Africana Studies closely parallels Digital Humanities in that it encourages scholars to use a variety of technologies to teach and research within their fields and seek connections to the outside world in an effort to understand the human condition. Furthermore, Digital Africana Studies, also encourages students to use a variety of digital tools to express their understanding of course content and find relationships between what they are learning, their lives and the world around them. Digital Africana Studies is a direct outgrowth of Afrofuturism, which is a theory that explores how people of African descent are represented in conversations of the future, whether that future be depicted through science fiction, the entertainment industry, popular culture or education, as well as a way of looking at the world, a canopy for thinking about black diasporic artistic production, a way of considering the presence of people of African descent within past, present and future Western society, as well as an epistemology that is thinking about the future, the subject position of black people, and about how that is both alienating and about alienation. So when I am asked, “what  is Virtual Harlem?”, I find it simultaneously strange, curious and sometimes insulting that some would try to categorize, label and subsequently only  see it through a relatively narrow lens, thus missing the larger, experiential aspect of the project. Digital Africana Studies is the pedagogical and practical application of Afrofuturism in that it seeks to explore how advanced technologies may be used in the classroom to support a more experiential environment, one that creates memorable  encounters with the culture and content of the course.   ",
       "article_title":"The Evolution of Virtual Harlem: Bringing the Jazz Age to Life",
       "authors":[
          {
             "given":"Bryan Wilson",
             "family":"Carter",
             "affiliation":[
                {
                   "original_name":"University of Arizona, United States of America",
                   "normalized_name":"University of Arizona",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03m2x1q45",
                      "GRID":"grid.134563.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-12",
       "keywords":[
          "games and meaningful play",
          "cultural studies",
          "digital humanities - diversity",
          "virtual and augmented reality",
          "agent modeling and simulation",
          "English",
          "audio, video, multimedia"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Background Commonplaces are a particular instance of historical text reuse (Dacome, 2004; Allan 2010; Blair, 2011). This paper describes our efforts at identifying commonplaces in the Gale-Cengage  Eighteenth Century Collections Online (ECCO) database. Given the size of this collection, as well as the state of the data in terms of its OCR output, identifying shared passages that exhibit the textual characteristics of commonplaces – e.g., are relatively short, repeated, and rhetorically significant – is a non-trivial computational task. In our previous work on text reuse, we came across numerous examples of textual borrowings and shared passages that we considered possible commonplaces (Allen et al., 2010; Horton et al., 2010; Roe, 2012). We expanded this work into a Digging into Data Round 3 project using similar methods to explore the more than 200,000 works contained in ECCO, a dataset that represents most of the printed literary and scientific output in Britain from 1700 to 1799  On our Digging into Data project, see  http://diggingintodata.org/awards/2013, and on ECCO, see  http://gdc.gale.com/products/eighteenth-century-collections-online/.  .   Previously we developed a sequence alignment algorithm for the identification of large-scale text reuse  See  https://code.google.com/p/text-pair/.  . This algorithm, called PhiloLine, generates a list of similar passages (based on a set of flexible matching parameters) shared between any two texts. This simple approach allows us to find borrowings and other instances of text reuse, from quotations to uncited passages and paraphrases, over large heterogeneous corpora (Edelstein et al., 2013). Historical text reuse detection is a burgeoning field within the digital humanities, whether focussed on literary allusion (Coffee et al., 2012; Coffee et al., 2014), paraphrase (Büchler et al., 2011), influence (Büchler et al., 2014), or networks of reprinting (Smith et al., 2015). While all these projects address text reuse in slightly different ways, the flexibility and scalability offered by PhiloLine, coupled with our familiarity with the system, offered significant advantages over other approaches. We thus aimed to use PhiloLine to compare the ECCO corpus to itself, compile a list of the most frequent shared passages, and from there evaluate these passages in order to build a database of potential commonplaces.    Eliminating duplicates The scope and scale of the ECCO dataset represented a major hurdle both in terms of computational expense and evaluation of the matching algorithm. Faced with more than 32 million pages of text, any manipulation of the data takes on significant proportions. To put this in perspective, comparing ECCO’s 200,000 documents to each other means making some 40 billion pairwise comparisons and then storing and evaluating the output. Fortunately, our focus on commonplaces requires us to dramatically reduce the number of comparisons. We needed, for instance, to eliminate duplicate or near-similar texts in order to reduce the number of documents for comparison. The most obvious method would be to compare all the words in each work, and define a similarity threshold beyond which we consider two works to be the same. But, given the unequal quality of the OCR in the ECCO dataset, the reliability of any algorithm meant to detect similarity between two texts is very low. Two identical texts, for instance, can potentially only share 20% of individual tokens due to the quality of the OCR. As a result, we decided to focus our efforts on comparing document metadata instead, as it is of excellent quality.   Our methodology consisted in comparing titles in the dataset using a cosine similarity algorithm (Singhal, 2011). For our purposes, we determined a minimal similarity index to automatically determine whether two texts were the same, that is to say a re-edition of the same work. Beyond a certain threshold score, the newest document in terms of date of publication is automatically flagged as a duplicate. If it so happens that the minimal score is not reached, but still remains high, we compare authors, and if these are the same, we similarly flag the most recent document as a duplicate:     The document with the oldest publication date serves as the “source” text. Using this method, we were able to reduce the size of the corpus by 43%, eliminating 88,850 documents. We were thus left with 116,700 unique texts on which to run our matching algorithm.    Detecting similar passages Similar passage detection requires a one-to-one document comparison. Rather than compare all 116,700 texts to one another, we decided to leverage Gale’s thematic divisions, and limit the comparison task to individual modules  Gale’s ECCO modules: History and Geography (17,950 works, reduced to 10,528); Social Sciences and Fine Arts (48,335 works, reduced to 30,498); Medicine and Sciences (15,636 works, reduced to 9,202); Literature and Language (53,351 works, reduced to 25,655); Religion and Philosophy (51,485 works, reduced to 29,962); Law (13,595 works, reduced to 7,726); and General Reference (5,198 works, reduced to 3,129). . While PhiloLine’s matching capacity is powerful, it is necessary to understand its underlying algorithm in order to configure the tool properly. Briefly, PhiloLine’s operational logic is to compare sequences of words, or n-grams, and determine the presence of a shared passage according to the number of common contiguous n-grams between two sequences. For example, the following text from Shakespeare’s  The Tempest is rendered by as a set of overlapping n-grams (where n = 3):     Trigram generation and stopword removal are thus the main parameters we apply to transform texts prior to the sequence alignment process. Once this is done, we proceed with the text-sequence comparisons. Below is an example of just such an alignment of sequences drawn from the Literature and Language module:    In this case, we note the perfect alignment, which PhiloLine detected because there are at least three contiguous trigrams in common between both passages.  Using these base parameters (overlapping trigrams with stopwords removed), we compared the ECCO corpus to itself on a module-by-module basis. The output of this comparison ranged from 3.5 million common passages in the Literature and Language module, to almost 17 million possible commonplaces in Religion and Philosophy. Identifying these common passages is thus only a first step. Even after significant duplicate reduction, the sheer scale of the passages that require post-processing evaluation is daunting.   From similar passages to commonplaces To attack this problem, we treat commonplaces generically as the repeated use of the same passage - more or less similar - in a minimum number of different authors. We began by grouping all source passages that were identical in order of frequency. Given that commonplaces are normally short expressions, at the most no longer than several sentences, we restricted our search to passages containing a minimum of five words and a maximum of 75:     A cursory glance at this list reveals several variants of the same passage that need to be merged in order to better represent a single commonplace. If we take the following passage from the Scottish poet James Thomson, for instance:   Then infant reason grows apace, and calls For the kind hand of an assiduous care. Delightful talk! to rear the tender thought, To teach the young idea how to shoot, To pour the freft infiruAion o'er the mind, 1150 To breathe enlivening spirit, and to fix The generous purpose in the glowing breast.   We notice that the reuse of this passage in other authors can vary significantly.   Gentleman of the Middle Temple (1775):  How glorious would her matron employments be, to  hear the tender thought, to teach the young idea how to Jhoot; to be at once the precept and example to her family of every thing that was good, every thing that was virtuous.    Mrs Lovechild (1790):  Happy the Mother \"Distilling knowledge through the lips of “love !”- '  Delightful talk! to rear the tender thought, “To teach the young idea how to shoot”, To pour the fresh inltrution o'er the mind !'Lines which will never cease to be quoted...  Given the variability in the reuse of any given passage, as well as the approximate quality of the OCR, we developed a new algorithm that could match similar passages in a way that was both precise, and yet more flexible than PhiloLine. The algorithm uses the same n-grams as PhiloLine, though they are constructed differently. Rather than use overlapping trigrams, as we do for sequence matching, here we use alternating bigrams for increased flexibility:    By skipping a word in the creation of these bigrams, we create n-grams that are both rarer than in-sequence bigrams, but also more common than in-sequence trigrams. In essence, these bigrams are flexible trigrams where the middle word is ignored. In this manner we can alleviate some of the issues that come from the dirty OCR. As there is a higher probability for a regular trigram to contain a wrongly identified letter, it has a higher chance of being unique, therefore making it less reliable for similarity matching than our flexible trigrams.  Finally, we needed to take into account the different lengths of these passages, as some are much longer than others. This variability led to the introduction of a coefficient that accounts for varying lengths, and allows us to automatically determine the minimum number of matching n-grams needed to establish similarity between two passages. For instance, if two passages of 30 words must share 4 bigrams, a passage of 30 words and another of 50 should share more bigrams to retain the same level of similarity. Using the above methods, we were able to merge various uses of a single source passage and assign them a unique identifier. We can therefore now identify the highest frequency commonplaces in the Language and Literature module (see Appendix 1), as well as the most ‘highly commonplaced’ authors, i.e., those that generated the most shared passages (see Appendix 2). These preliminary results suggest that commonplacing, as an intertextual reading/writing practice, was alive and well in 18th-century England. Digging into a dataset such as ECCO can thus offer us new perspectives from which to view and understand 18th-century print culture, provided we unearth more than we cover up.    Future Work We aim to release an interactive database of possible commonplaces in early 2016. The database will allow users to navigate the ECCO dataset via the commonplaces, most commonly cited authors and works, and visualize commonplace use and practices over time. We will also introduce several curated datasets that pre-date the 18th century, and that can act as a control on sources that fall outside the date boundaries of our data. These possible datasets include the King James Bible, Classical Latin texts, and EEBO-TCP  See  http://www.textcreationpartnership.org/tcp-eebo/.  . Further goals for this project include merging the module-specific results into one large pool of potential commonplaces that reach across disciplinary boundaries; developing a user interface that allows for commonplace curation as a form of crowdsourcing; and introducing non-English datasets for comparison in order to find instances of multi-lingual commonplace practices.   Appendix 1:     Appendix 2:     ",
       "article_title":"Digging into ECCO: Identifying Commonplaces and other Forms of Text Reuse at Scale",
       "authors":[
          {
             "given":"Glenn",
             "family":"Roe",
             "affiliation":[
                {
                   "original_name":"Centre for Digital Humanities Research, Australian National University, Australia",
                   "normalized_name":"Australian National University",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/019wvm592",
                      "GRID":"grid.1001.0"
                   }
                }
             ]
          },
          {
             "given":"Clovis",
             "family":"Gladstone",
             "affiliation":[
                {
                   "original_name":"The ARTFL Project, University of Chicago, USA",
                   "normalized_name":"University of Chicago",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/024mw5h28",
                      "GRID":"grid.170205.1"
                   }
                }
             ]
          },
          {
             "given":"Robert",
             "family":"Morrissey",
             "affiliation":[
                {
                   "original_name":"The ARTFL Project, University of Chicago, USA",
                   "normalized_name":"University of Chicago",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/024mw5h28",
                      "GRID":"grid.170205.1"
                   }
                }
             ]
          },
          {
             "given":"Mark",
             "family":"Olsen",
             "affiliation":[
                {
                   "original_name":"The ARTFL Project, University of Chicago, USA",
                   "normalized_name":"University of Chicago",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/024mw5h28",
                      "GRID":"grid.170205.1"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "english studies",
          "digitisation, resource creation, and discovery",
          "historical studies",
          "data mining / text mining",
          "English",
          "bibliographic methods / textual studies",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Most government and business sponsored geographic information systems (GIS) are employed to manage, track, and exploit labor and production. GIS has primarily been used to regulate political economies, “the concern for life is to administer it (life) through controls and regulations so that resources might be rightly apportioned” (Crampton, 2011). Therefore, governments and businesses use GIS is to produce abstract intelligence and knowledge about people and terrains, which they use to make decisions for distributing goods. These goods can include hospitals, parks, roads, military air strikes, etc. By understanding GIS as an engine for political decision making, we can begin to understand that maps are inherently political. However, GIS, because of its ubiquitous, malleable, and slippery nature, can be used for more than simply calculating statistics to influence the distribution of goods. Instead, GIS can be used to distribute ideas, concepts, emotions, and narratives (the qualitative data of our human actions) in order to act as a counter-map. Nancy Lee Peluso (1995) first defined counter-mapping as an effort to “appropriate the state’s techniques and manner of representation to bolster the legitimacy of “customary” claims to resources” (384). In this definition of counter-mapping, counter-maps contest governmental decisions on political economies by representing location specific knowledge that challenges the abstract statistical knowledge sanctioned by the state. Researchers usually create these counter maps by using participatory GIS (PGIS) methods. PGIS is a cartographic research method that asks members of a population of a mapped geographic area to become active producers in their own mapping. It asks the population members to map their own problems, wants, and needs. For example, Sarah Elwood (2006) has used PGIS with community members from low income neighborhoods in Chicago to help community members advocate for neighborhood improvements (197-208). Though this use of counter-mapping may be helpful to advocate for the redistribution of goods, I argue, it doesn’t do enough to contest the primary political functionality of GIS. GIS in these projects often simply replicates the dominant ideology of GIS - that its sole function is to be used to make decisions for the distributions of goods within political economies. This definition of counter-mapping has constrained our imaginations and the ability to play with the functions of GIS and to miss out on potential GIS poetics. The central question in consternation is: how can maps function in the age of electracy unconstrained by the ideologies of the paper map? Instead of focusing on the political and economic functionality of GIS, which I argue is a print centric way of thinking about GIS, what would it mean to create an electronic poetic discourse of GIS? How can GIS be used to map human emotion, desire, and truth? This type of reasoning resists the temptation to define GIS as a tool for deliberate decision making and to understand GIS as a medium for invention. Invention is the counter logic of GIS hermeneutics -- GIS heuretics. Gregory Ulmer (2002) states, “The purpose of the course is to approach electracy by trying to invent it (what I call “heuretics” --the use of theory to invent forms and practices, as distinct from “hermeneutics,” which uses theory to interpret existing works)” (4). How can we invent geography for the electronic apparatus? This question can be answered following the models provided by Ulmer. Ulmer invents a method for applying humanities discourse which relies “not on positivism but quantum relativity; not realism but surrealism.” Surrealism, then can act as theory that counters traditional positivistic and “rational” decision making. In the Surrealist Manifesto, Andre Breton (1924) writes: “(I)n this day and age logical methods are applicable only to solving problems of secondary interest. The absolute rationalism that is still in vogue allows us to consider only facts relating directly to our experience. Logical ends, on the contrary, escape us. It is pointless to add that experience itself has found itself increasingly circumscribed. It paces back and forth in a cage from which it is more and more difficult to make it emerge. It too leans for support on what is most immediately expedient, and it is protected by the sentinels of common sense. Under the pretense of civilization and progress, we have managed to banish from the mind everything that may rightly or wrongly be termed superstition, or fancy; forbidden is any kind of search for truth which is not in conformance with accepted practices” (2). We can see that, just as I’m searching for a theory and method that resists hermeneutic rationalizations of GIS, so were the surrealists resisting positivist rationalism, because these rationalisms limited creativity and human experience. Therefore, surrealism allows humanities researchers to infuse creativity back into the research milieu. Furthermore, one humanities method that may be applied to electronic map making, GIS, is psychogeography, an avant-garde method developed by the Situationists who were heavily influenced by surrealism. Guy Debord (1959) defines psychogeography as derive, meaning “to drift.” He defines derive as \"the study of the precise laws and specific effects of the geographical environment, consciously organized or not, on the emotions and behavior of individuals” (3). By implementing psychogeography, the researcher ignores the boundaries and zones of a city, disturbs the modern distance between researcher and space, and allows the researcher to gain the feeling of a place. By performing psychogeography, the researcher is allowed an intimacy with space and place which may change the conclusions he or she may make about a particular space had he or she relied solely on traditional GIS hermeneutics. Here, emotion has been injected into the research paradigm. This paper then, describes a method of using GIS tracking services to record a pscyhogeography performed in Bradenton, FL. Currently, there is a heroin epidemic happening in Bradenton, FL, particularly in the South Bradenton neighborhood, and there have been several maps issued by the government and local media agencies that track heroin overdoses in Bradenton, FL. To counter map these official and popular maps, I used Debord's drifting methodology which he outlines in “Theory of the Derive,” to perform three different drifts. My goal was to record the feeling of South Bradenton. These drifts were tracked using the GIS interface My Tracks mobile phone application by Google, Inc. During the drifts, My Tracks recorded my path as I drifted through South Bradenton, FL. Additionally, I used my cell phone to take images and videos of the neighborhood and I recorded notes, thoughts, and feelings into a notebook with time stamps. Once the drifts were completed, the My Tracks paths were exported to Google My Maps and the photos and notes were imported into the My Map and added to their corresponding places on the My Tracks path. This presentation will showcase these maps and contrast them to the government maps created about the heroin epidemic. My presentation then focuses on my analysis of the results, the usefulness of psychogeography for geographic research in the digital age, and further research needed for the development of a theory for electronic geographic research and methodologies. ",
       "article_title":"Countering Counter Mapping Methods: Constructing A Humanities GIS Methodology In the Age of Electracy",
       "authors":[
          {
             "given":"Clayton John",
             "family":"Benjamin",
             "affiliation":[
                {
                   "original_name":"University of Central Florida, United States of America",
                   "normalized_name":"University of Central Florida",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/036nfer12",
                      "GRID":"grid.170430.1"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-13",
       "keywords":[
          "maps and mapping",
          "English",
          "spatio-temporal modeling, analysis and visualisation"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Famine, as John Walter writes, was “…a recurring reality and ever-present fear” in the early modern period (Walter, 2015). The AHRC-funded project  Famine and Dearth in India and Britain, 1550-1800: Connected Cultural Histories of Food Security ( http://humanities.exeter.ac.uk/english/research/projects/famine/) examines the practices, discourses and literary modes through which societies in early modern India and Britain articulated their concerns about the availability and distribution of food (Mukherjee, 2014). A collaboration between the University of Exeter and Jadavpur University, Calcutta, the project draws upon a large body of texts written in languages including English, Latin, Persian, Bengali and Hindi for evidence about cultural responses to landscapes of famine and dearth. Its aim is to produce a digital resource that includes encoded extracts from the source materials and maps that reflect the variety and scope of identified responses to the landscapes encountered in the sources.   The application of digital methods to large corpora of thematic texts presents both opportunities and challenges, which will be examined in this work-in-progress paper. The dataset is highly diverse, not only in terms of the languages in which these texts are written but also in the types of documents that form our body of evidence. The source materials include chronicle histories, gazetteers, official correspondence, legislation, pamphlets, periodicals, plays, poetry, surveys and prose (fiction and non-fiction), and the project will publish excerpts from each of these categories. Our markup priorities (beyond the basic structure of the document) lie in how to encode the many themes surrounding famine and dearth that are present in our texts, which we need to extract in order to address the research questions of the project. We can use natural language processing tools to help us identify names and places, for instance (at least for some of the languages), but the range of terms used in these texts to describe the features of the landscape, the people, and the food situation are extremely varied. Inevitably there are also subtleties in the ways in which particular concepts are represented in the various languages of our source materials, as well as added complications such as variant spellings. The project uses the open source software GATE (General Architecture for Text Engineering,  http://gate.ac.uk/) to identify names and places in the texts written in English, and we would like to apply a similar process to the texts written in other languages. It is likely that we will create some of the gazetteers from scratch, in which case we would aim to make these available as part of the project’s outputs.  One of the most interesting challenges is in how to map the resulting data meaningfully. Exploring responses to landscapes of scarcity is a key research question of the project, and descriptions of such responses feature very heavily in some of our texts, particularly in the travel writings (see McRae, 2009 for a discussion of some of themes in travel writing of this period). The works of Peter Mundy, for instance, are full of rich descriptions of the places he visited, including very personal observations of the circumstances in which he found himself (Carnac Temple, 1914; Pritchard, 2011). We are particularly keen to place our work in the context of some of the projects that have taken place during the last decade on mapping emotional responses to landscapes at other periods and in different geographical areas. While many of the recent projects on emotional cartography use wearable technologies to measure and record responses to the landscape, such as Christian Nold’s 2006 Greenwich Emotion Map (an art project combining annotations with measurements of skin responses at different stages of a walk through the Greenwich area of London:  http://www.emotionmap.net/background.htm), we see potential in learning from, and building upon, their approaches to visualising the resulting data (Nold, 2009). Projects such as Kurt Jensen’s representation of Sterne’s  A Sentimental Journey, built using Neatline ( http://neatline.org/), have also suggested possible directions for representing some of the travel writings ( http://enec3120.neatline-uva.org/neatline/show/a-sentimental-journey), and our recent workshop on food security has helped to clarify the relevant user requirements ( http://foodsecurity.exeter.ac.uk/). However the question of how to integrate such a wide variety of sources and languages into useful and meaningful maps remains one of the most interesting and challenging aspects of the project. As such, it will be a key focus of our paper, and we anticipate that presenting the results of our experiments with this data could be helpful for other projects that are grappling with similar issues, potentially in very different subject areas.  ",
       "article_title":"Mapping Multilingual Responses To Famine And Dearth In The Early Modern Landscapes Of India And Britain",
       "authors":[
          {
             "given":"Charlotte",
             "family":"Tupman",
             "affiliation":[
                {
                   "original_name":"University of Exeter, United Kingdom",
                   "normalized_name":"University of Exeter",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/03yghzc09",
                      "GRID":"grid.8391.3"
                   }
                }
             ]
          },
          {
             "given":"Richard",
             "family":"Holding",
             "affiliation":[
                {
                   "original_name":"University of Exeter, United Kingdom",
                   "normalized_name":"University of Exeter",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/03yghzc09",
                      "GRID":"grid.8391.3"
                   }
                }
             ]
          },
          {
             "given":"Hannah",
             "family":"Petrie",
             "affiliation":[
                {
                   "original_name":"University of Exeter, United Kingdom",
                   "normalized_name":"University of Exeter",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/03yghzc09",
                      "GRID":"grid.8391.3"
                   }
                }
             ]
          },
          {
             "given":"Gary",
             "family":"Stringer",
             "affiliation":[
                {
                   "original_name":"University of Exeter, United Kingdom",
                   "normalized_name":"University of Exeter",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/03yghzc09",
                      "GRID":"grid.8391.3"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "maps and mapping",
          "xml",
          "encoding - theory and practice",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction This project is centrally concerned with using digital computation to analyze historical and comparative data of film credits in order to study the evolution of film production. Our hypothesis is that the elements of film production, as credited by the film itself, have a direct correlation with the historical and cultural background of film production. We want to identify the specific connection with each query, hence contributing to the understanding of the culture of film production in general.  In the introduction to  The Production of Culture and the Cultures of Production, Paul du Gay points out that culture and economy are mutually constitutive in the present day. Not only are most of popular cultural products manufactured by organized industries, but the production activities themselves also manifest cultural values (Gay, 1997). Our project is theoretically oriented toward the latter, the cultures of production. Although relatively less studied than the production of culture led by the theories of the Culture Industry (Horkheimer & Adorno, 1997), significant scholarship has been conducted to discern the cultural patterns of the film and television productions, such as John Thornton Caldwell’s book,  Production Culture: Industrial Reflexivity and Critical Practice in Film and Television (Caldwell, 2008). The primary data Caldwell and other researchers used are trade publications, interviews, and field observations. Sharing the same theoretical inclination of Caldwell, our research breaks the methodological new ground by analyzing the most conspicuous records of film production—the credits shown on film itself.   Influenced by fiction publication and theater programs in earlier years, film crediting convention has evolved throughout the history of cinema. Today, it is typical for an American film to open with a title sequence and end with a longer rolling credit crawl. The end credits for big production blockbusters are relatively lengthy, featuring both above-the-line categories such as director, producer, and cast, and below-the line categories such as production assistants, camera operators, electricians, special effects specialists, sound editors, etc. As records of film production, film credits contain a plethora of vital information, disclosing not only involved labor (personnel), and corporations (financiers, production companies, and distribution companies), but also procedures (e.g. special effects units and location shootings), and technologies (e.g. Panavision cameras and Chapman camera cranes).  The scholarly attention to film credits has steadily increased in recent years. Many focus on the credits’ aesthetic and philosophical values (de Mourgues, 1993; Moinereau, 2009; Tylski, 2009). Some use film credits as an evidence to study the Hollywood star system and labor hierarchy (Clark, 1995; Chisholm, 2000; Carman, 2008). Will Straw’s article “Letter of Introduction: Film Credits and Cityscapes” represents a significant contribution by linking the design of the title credit sequence to the socio-historical condition of film viewing – the development of city life after World War II (Straw, 2010). Two recent dissertations also offer interesting insights (Allison, 2001; Crawford, 2013). Allison’s research is statistical in nature but only confined to opening title sequences. Crawford’s dissertation presents multiple perspectives (aesthetic, legal, and industrial), convincingly justifying the importance of film credits.  This project focuses on actually accessing and analyzing film credits to generate insights into the evolution of film production. More specifically, we propose to employ large-scale computation to perform comparative analysis on film credits. Digital computation is necessary to carry the study of film credits to another level since physically examining the credits even for one film is a daunting task. The final crawl of  Dark Knight (2008), for example, runs for 7.5 minutes and contains hundreds of names and entities. The listed order and naming of some categories also differ from film to film. Besides increasing efficiency by substituting for human labor, digital computation also invites a large quantity of research subjects. These methods will potentially discover patterns across an extended period of time as well as across national and genre boundaries. We are confident that the potential of film credits as written records will be fully tapped with the aid of digital computation, and our research project will represent a major contribution to film studies.   Our project is mapped into two stages. For the first stage, we will pull available digital records of American films on the Internet to perform some statistical analysis. The second stage will see the expansion of selected works from American films to Chinese films. The Chinese film industry is chosen because of its dramatic development trajectory in the last 30 years. The industry was resuscitated in the 1980s from its near death in the Cultural Revolution (1966–1976) (zero output for three years), and only found itself slipping into financial crises throughout the 1990s in its attempt to balance political and commercial imperatives. Just when the industry was on the brink of collapse at the dawn of the new millennium, it picked itself up with full-scale capitalization while still maintaining substantial degree of governmental control. Charting an extraordinary growth curve to the present day, the Chinese film market has become the second largest in the world since 2012, and is on track to overtake the United States in 2017 or 2018 if the current growing pattern sustains. Tracing the information listed as credits over time and comparing to that of Hollywood will demonstrate the trajectory of both local changes and external influences in the development of Chinese commercial cinema. Because the digital records of Chinese film credits are not as readily available as their American counterparts, we will employ video-capture as well as optical character recognition technology to establish source data sets.    Preliminary Findings In this conference paper we will present some of our preliminary findings from the first stage. We have developed a software tool to aggregate and analyze records from International Movie Database (IMDB) for analysis. The application is implemented in Java and can parse IMDB records, extract named entities, conduct statistical and association analysis. One of our queries is the number of people listed under major credit categories as documented by the IMDB. We selected 20,690 titles since 1900 with at least one director and at least 10 cast members (actors and actresses). The average number of crew for each film over years is shown in Table-1. For 1900 to 2010, the film statistics are aggregated every 10 years. For example, “1900 to 1910” refers to films released between Jan. 01, 1900 and Jan. 01, 1910. Figure 1 shows the growth of cinematographer, composer, costume-designer, director, editor, miscellaneous workers, producer, production-designer, and writer categories. And Figure 2 shows growth in the number of actors and actresses.  From Figure 1, we can see the dramatic increase in the number of credited producers and miscellaneous workers (below-the-line film crew) over time. After years of steady increase, the number of producers took off around 1975, the same year that saw the release of modern blockbuster prototype  Jaws. Our findings put a direct link between the spreading of the blockbuster business model and the number of producers on board. The miscellaneous worker category displays an even steeper rise. Interestingly, the time of dramatic increase coincides with that of producers – around 1975, but its down size in the early 1990s has no parallels. This graph pinpoints key historical moments for further investigation in the area of union activities, film labor division and representation. Finally from Figure 2 we can see that the gap between male and female cast has remained steady over a century. The biggest gap, however, does not appear in recent years when superhero films reign at the box office – probably thanks to an increased awareness of gender equality – but in the period of 1925 to 1945. Our findings invite further scrutiny into the pre-Paramount Decree (1948) years to study gender dynamics in Hollywood. From this perspective, the conventionally-defined golden age of Hollywood studio era (1927–1960) (Bordwell et al., 1985) may not be as seamlessly continuous as once conceived.     Figure 1 Average number of crews excluding actors and actress per film    Figure 2 Average number of actors and actress per film     ACTOR ACTRESS CINEMATOGRAPHER COMPOSER COSTUMER DIRECTOR EDITOR MISC PRODUCER PRODUCTION WRITER Movie count   1900 to 1910 9.57 6.57 0.57 0.29 0.00 2.14 0.00 0.14 1.14 0.14 2.00 7   1910 to 1920 9.67 4.96 0.94 0.08 0.05 2.12 0.08 0.24 0.76 0.04 1.73 347   1920 to 1930 10.56 4.87 1.29 0.40 0.15 2.14 0.37 0.68 0.83 0.07 2.61 492   1930 to 1940 19.40 6.23 1.31 0.91 0.26 2.16 0.81 0.96 1.01 0.28 3.16 971   1940 to 1950 21.89 7.64 1.26 1.01 0.33 2.24 1.06 1.46 1.14 0.43 3.20 613   1950 to 1960 19.52 8.72 1.19 0.99 0.34 2.41 0.95 1.60 1.26 0.48 3.18 863   1960 to 1970 17.37 8.52 1.15 0.91 0.35 2.60 0.96 1.31 1.27 0.43 3.00 1045   1970 to 1980 16.59 7.90 1.01 0.84 0.29 2.50 0.82 1.97 1.49 0.30 2.60 1301   1980 to 1990 18.21 9.51 0.91 0.84 0.34 2.71 0.86 3.69 1.98 0.38 2.75 1559   1990 to 2000 18.47 11.06 0.95 0.83 0.38 2.96 1.04 5.86 2.90 0.34 2.89 2127   2000 to 2010 15.89 8.84 1.18 0.90 0.30 2.60 1.35 5.54 3.68 0.32 2.37 5458   2010 to 2015 15.47 8.26 1.29 0.85 0.25 2.55 1.36 4.35 4.50 0.31 2.37 5655   after 2015 18.04 9.50 1.17 0.65 0.30 3.04 1.11 5.37 5.57 0.37 2.47 252   Table 1 Average number of crew per film from 1900 to 2015   Summary In summary, these preliminary findings illustrate the mechanism of using computation to analyze the credits of a large number of films across historical periods. The inquiries described above all fall into the category of the “personnel.” There are three other categories for future considerations: companies, procedures, and technologies. We believe that the computation-aided study of film credits will detect patterns or gaps, which will serve as important clues and evidences to analyze the cultural dynamics of film production.   ",
       "article_title":"Computation-Aided Analysis on Film Credits",
       "authors":[
          {
             "given":"Li",
             "family":"Yang",
             "affiliation":[
                {
                   "original_name":"Lafayette College, United States of America",
                   "normalized_name":"Lafayette College",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/036n0x007",
                      "GRID":"grid.258879.9"
                   }
                }
             ]
          },
          {
             "given":"Weijia",
             "family":"Xu",
             "affiliation":[
                {
                   "original_name":"Texas Advanced Computing Center, University of Texas at Austin, United States of America",
                   "normalized_name":"The University of Texas at Austin",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00hj54h04",
                      "GRID":"grid.89336.37"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "cultural studies",
          "film and cinema studies",
          "English",
          "asian studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Robert Louis Stevenson (1850-1894) is known to have co-authored several of his works, including  The Dynamiter (1885), on which he collaborated with his wife Fanny van der Grift Stevenson (Stevenson and Stevenson 1885). Until recently, Stevenson scholars had comparatively little information about how the collaboration around  The Dynamiter had operated, with one narrative claiming prominence: the preface to the 1905 edition written by Fanny (after her husband’s death) which stated that she had invented the stories during an illness of his, and they had subsequently worked collaboratively to put them into written form (Stevenson 1905).   In this paper, we take insights gleaned from our previous work on the authorship of  The Dynamiter (carried out with collaborators Mingyuan Chen, Carlos Fonseca, Laura McAleese, Alba Morollón Díaz-Faes and Elizabeth Nicholas) to investigate another text,  The Wrecker (1892). (At the time of submitting this abstract, the authorship analysis of  The Wrecker had not yet been carried out, so we have supplied details about our previous study so as to demonstrate that the methods used are both appropriate and robust.) We used the R package Stylo (Eder et al. 2015) to apply Burrows’ Delta (Burrows 2002) to two reference corpora containing works known to have been solely authored by Fanny or Robert Louis, and used these as comparators against the individual stories in  The Dynamiter.   Visualizing the results in the form of a cluster analysis indicated that Burrows’ Delta performed well at separating out texts known to be authored by Fanny from texts known to be authored by Robert Louis (Fig 1.)   Fig. 1. Cluster analysis of Burrows’ Delta scores of the 450 most frequent words in texts by Fanny, in green and blue, by Robert Louis, in orange and black, and The Dynamiter, in red (pronouns deleted, no culling).  Our interpretation of these results was that the stories Fanny was most likely to have authored from  The Dynamiter  were “The Story of the Destroying Angel” and “The Fair Cuban”. However, it is important to note that while Fanny may well have had an originating role for the plot of many, if not all, of the stories – something which the Preface to the 1905 edition seeks to establish – when it came to the actual writing down of the stories, the “signal” from her linguistic signature was made less clear by the “noise” of her husband’s heavy editorial hand (something which is known from biographical and historical writings about their relationship).  Building on this earlier work on  The Dynamiter, the paper we propose here will examine another work co-authored by Stevenson:  The Wrecker, also a volume of short stories, which Stevenson co-authored with his stepson Lloyd Osbourne. Stylo will again be used, as will insights from the many  Dynamiter tests. These indicated, for instance, that deleting pronouns resulted in better separation of texts in the reference corpus, something we attributed to Stevenson’s tendency to write about predominantly male characters, which resulted in the prevalence of male pronouns. With the experience gained from attempting to find a solution to the “signal” vs. “noise” problem caused by Robert Louis’s proclivity to edit the work of his collaborators, we will also investigate how changes to additional parameters offered by Stylo – changes to the number of most frequent words considered, for example, and variations in text sample size – affect the results of the  Wrecker tests.  As mentioned above, our earlier results suggested that Robert Louis’s editing practices – he had a tendency to edit texts meticulously prior to publication – makes it hard to determine with certainty which sections were written, or initially drafted, by Fanny. A model of co-authorship in which the boundaries between one author and another are clear-cut – where the assumption is that one person is solely responsible for some sections and a second person is solely responsible for others – breaks down in a situation such as this, where the shared domestic space of two authors means that close consultation with – and iterative redrafting of the work of – one’s familial collaborator is not simply possible but likely. It underlines the need for stylometric analysis to be complemented with careful literary historical analysis in order to arrive at any meaningful conclusions. The broader significance of this work is that it is not only of relevance for the field of authorship attribution, but also for its recuperative potential in relation to figures who are less prominent within literary history, including women. Despite scholars' awareness of Fanny’s involvement with her husband's writing, for instance, there is still minimal research into how she influenced his work, and how other women in similar positions influenced their famous husbands' legacies. In bringing to light the omitted literary contributions from women and other family members, what emerges is the need for theoretical approaches capable of evaluating the gendered practices of literary studies and book history in contributing to these omissions. In analysing letters and journals, combing through stylometric data, and assessing biographical accounts, there is a risk of overlooking collaborations in favour of other types of partnerships (amanuenses, muses, and the like). A woman like Fanny van der Grift, who history has recorded as a keen writer, diarist, and editor, surpasses the labels that book history has offered her. While authorship attribution analysis offers one set of useful tools for breaking down these barriers, further critical engagement with our own gendered scholarly practices is necessary to more clearly understand how and why certain works and canons have become established in the way that they have. ",
       "article_title":"All in the Family: Testing Burrows’ Delta on Robert Louis Stevenson’s Collaboratively Authored Volumes The Dynamiter and The Wrecker",
       "authors":[
          {
             "given":"Anouk",
             "family":"Lang",
             "affiliation":[
                {
                   "original_name":"University of Edinburgh, United Kingdom",
                   "normalized_name":"University of Edinburgh",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/01nrxwf90",
                      "GRID":"grid.4305.2"
                   }
                }
             ]
          },
          {
             "given":"Robyn",
             "family":"Pritzker",
             "affiliation":[
                {
                   "original_name":"University of Edinburgh, United Kingdom",
                   "normalized_name":"University of Edinburgh",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/01nrxwf90",
                      "GRID":"grid.4305.2"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "english studies",
          "stylistics and stylometry",
          "authorship attribution / authority",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" This paper adopts a social-deterministic view of intellectual history in order to link changes in social and professional interactions to the intellectual character of a community. The formation of Australia’s economic history community between 1950 and 1970 is used to illuminate the link between these social and intellectual forces. The economic history field globally has experienced a number of changes over the twentieth century, and although some of these aspects are understood for the larger communities in the US and the UK (see Hudson, 2001; Lyons et al., 2008 for recent examples), Australia’s economic history field remains neglected. Social connections within the community were driven by contextual factors such as the post-WWII expansion of higher education, the growing emphasis on research output and the development of domestic PhD programs. These connections then developed in a multi-dimensional way throughout the 1950s and 1960s, with co-location and collaborative relationships emerging simultaneously between individuals. It is found that the increased density and multiplexity of ties in the community contributed to the development of a distinctive intellectual approach. This project combines the quantitative analysis of social and intellectual relationships between members of the field with the qualitative analysis of published works of economic history in this period. Though there is a long tradition of emphasising the importance of social interactions for the development of ideas (Kuhn, [1962] 1970; Mulkay et al., 1975; Whitley, 1984), there have been only limited attempts to visualise social relationships in intellectual history (for some examples, see Harvard University, 2015; Shakeosphere, 2015; Six Degrees of Francis Bacon, 2015), and even fewer attempts to relate changes in the social structure to changes in the intellectual character of the group.   Methodology This project links the social and knowledge networks for this community. The social network has been visualised through co-location and collaboration networks. The knowledge network has been visualised through citation analysis and is discussed further through the qualitative analysis of texts. The collaboration and citation analyses are based on a selection of key texts of Australian economic history for the period 1950 to 1970. Texts have been selected from prior wide reading of the subject, with further guidance from secondary analyses that focus on the literary aspects of the field. This has determined the key ideas, approaches and debates for this community, which has informed the selection of texts. Based on these texts, collaboration for this community has been recorded in three ways: co-authorship, contributions to edited works, and sub-authorship (acknowledgments). These networks are bonded-tie and valued, based on the number of separate texts that each pair of individuals collaborated on. Collaboration in this context represents ongoing, two-way interaction and the trading of theoretical insights and ideas (Laudel, 2002; Wang et al., 2014). Co-location has been used to map geographic proximity between two individuals, assuming that if they worked within the same university or faculty, they were more likely to have contact than those who were geographically disparate (Jaffe et al., 1993; Ponds et al., 2007). This network is also bonded-tie and valued: if two people were both employed by the same university for 2 years, their relationship is given a score of 2, and so on.  From the raw collaboration and co-location data, a network of key actors in the community has been determined to allow the combination of the different social networks. All authors of key texts and key collaborative works are included. Otherwise actors are included if they were involved in more than one type of social interaction in this community, for example if an individual was appointed to one of the key universities in this period, as well as involved in the sub-authorship network. From this, each social network map has been weighted based on the strength of that particular relationship. Co-location has been weighted by 0.5, because although geographic proximity may induce interaction between scholars, there is no guarantee. Sub-authorship and contributions to edited works are weighted by 2, and co-authorship is weighted by 4. The knowledge network has been partially analysed through citations, as these represent the ideas that are shared between different authors and the place of a text in the wider context of the field (Leydesdorff and Amsterdamska, 1990; Newman, 2010; Siler, 2013). The citation network for this project has been coded manually, as texts are generally non-digitised books and articles – features that make the use of the Social Sciences Citation Index (SSCI) inappropriate here. This network is directed, indicating the one-way transfer of ideas, and is valued for the number of times in each text that the citation is made. Supplementing the quantitative analysis of social and intellectual relationships is a qualitative component. The key texts in this community, as identified above, have been analysed using Lloyd’s (1995) framework, which classifies works of economic history based upon the author’s assumptions about how the economy operates (ontology) and the author’s methodology for gaining knowledge about the economy (epistemology). By analysing both qualitative and quantitative aspects, this methodology is able to link social and intellectual interactions with the ideas that emerge in the author’s texts.    Preliminary results The data has been visualised with  NetDraw. Although the community had some mobility between different locations, the co-location map shows that interactions were primarily between those in the same city. This includes those appointed to the Australian National University (ANU) (located in Canberra, and visualised as the large cluster on the right in figure 1), the University of Melbourne and Monash University (both located in Melbourne, and visualised as the cluster at the bottom of figure 1), or the University of Sydney and the University of New South Wales (both located in Sydney, and visualised as the cluster on the left of figure 1).     Figure 1: Co-location, 1950–1970  Collaboration was less geographically determined, though there was still greater interaction between those working at the same site. All co-authors were those who also had co-location ties (see figure 2). Contributors to edited works were also generally those that were geographically proximate to the editors in this period (see figure 3). Sub-authorship was the least geographically determined social network, responsible for most of the interactions between those in different cities (see figure 4). Having said that, the sub-authorship network was still clustered around the main institutions, with the ANU group experiencing the greatest frequency of sub-authorship in this period. Figure 5 shows that when each of these measures are combined, social interactions between members of the community in this period broadly followed co-location trends.   Figure 2: Co-authorship, 1950–1970    Figure 3: Contributors to edited works, 1950-1970    Figure 4: Sub-authorship, 1950–1970    Figure 5: Multiple measures, 1950–1970  Figure 6 shows the citation network for this community, with authors of those texts included in the citation analysis highlighted. There was a broad tendency for those that had strong interactions in the social network to have similar citation patterns. A good example of this is NG Butlin, WA Sinclair, N Cain and AR Hall, located together towards the centre of figure 6. In the social network, these individuals were part of the large ANU cluster, with co-location and collaboration interactions throughout this period. There is evidence to support the counterfactual as well: that a lack of social interactions with the core of the community led to distinctive citation patterns. BR Davidson and E Dunsdorfs are good examples here, located on the fringes of figure 6 below, and also with limited interaction in the social networks above. However, this is a loose relationship, with non-social factors such as choice of research topic and approach to the subject also converging citation patterns.   Figure 6: Citations, 1950-1970  The networks above show that there was a range of social and intellectual interactions in Australia’s economic history field between 1950 and 1970, particularly structured around key institutions and prominent figures. This was accompanied by the development of a distinctive intellectual approach for the community. The orthodox approach to economic history in this period focussed on national income accounting, domestic determinants of growth, urbanisation, and the treatment of economic agents in an abstract and aggregate way. Core proponents of this approach had multidimensional interactions in the social network, mostly centred on NG Butlin and his colleagues at the ANU. A number of alternative approaches existed in smaller ‘pockets’ in the community, which were also structured by social interactions. By combining the quantitative analysis of various social and intellectual relationships with the qualitative analysis of texts and ideas, this project links the social network and the knowledge network for Australia’s economic history community. Not only does this provide a pioneering attempt to combine SNA with the more traditional methods for intellectual history, it analyses the Australian economic history field in a dynamic and multi-dimensional way. Key results suggest an association between social interactions and the citation patterns adopted by members of the community in this period. Key social and intellectual relationships between members of the field also led to the development of a distinctive intellectual approach for this interdisciplinary field.  ",
       "article_title":"The Formation of Australia’s Economic History Community, 1950 – 1970: A Multidimensional Network Analysis",
       "authors":[
          {
             "given":"Claire",
             "family":"Wright",
             "affiliation":[
                {
                   "original_name":"University of Wollongong, Australia",
                   "normalized_name":"University of Wollongong",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/00jtmb277",
                      "GRID":"grid.1007.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-29",
       "keywords":[
          "visualisation",
          "historical studies",
          "English",
          "networks, relationships, graphs",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Biblissima—Bibliotheca bibliothecarum novissima—is a Digital Humanities project that brings together libraries and research institutions, whose goal is to provide a single access point to over 40 databases regarding medieval and Renaissance manuscripts and early printed books by the end of 2016. The project began late in 2012 with nine founding partners: the Department of Manuscripts at the BnF (Bibliothèque nationale de France), the Campus Condorcet, the CESR (Centre d'Etudes Supérieures de la Renaissance), the CIHAM (Histoire, Archéologie, Littératures des mondes chrétiens et musulmans médiévaux), the Centre Jean-Mabillon at the ENC (École nationale des chartes), the CRAHAM (Centre de Recherches Archéologiques et Historiques Anciennes et Médiévales. Centre Michel de Boüard), the IRHT (Institut de recherche et d’histoire des textes), the Digital Document Centre at the MRSH de Caen (Maison de la Recherche en Sciences Humaines), and the SAPRAT-EPHE (Savoirs et pratiques du Moyen Âge au XIXe siècle, Ecole pratique des hautes études). Funded by the French National Research Agency (ANR: Agence Nationale de la Recherche), Biblissima is concerned with the history of collections and the transmission of texts. In this respect, the project stands somewhat in line with the works of Antoon Sanders, who published his Bibliotheca Belgica Manuscripta in 1641-44, and Bernard de Montfaucon and his Bibliotheca bibliothecarum manuscriptorum nova, published in 1739, both of which are major inventories compiling lists of manuscripts held in many different libraries in Europe. In order to create an Online library of historical collections of France for the 21st century, our chosen solution was to develop a semantic web application. We released a prototype (http://demos.biblissima-condorcet.fr/prototype) in summer 2015 that provides unified access to a subset of two major iconographic databases: Initiale (http://initiale.irht.cnrs.fr/accueil/index.php) and Mandragore (http://mandragore.bnf.fr/html/accueil.html). An export of the metadata pertaining to the illuminations that depict geographical locations was used as a starting point for the development of our application. People (author or illuminator), institutions (libraries), work titles and places were aligned with the BnF authority records and, whenever possible, also linked to external vocabularies (GeoNames, VIAF, Pleiades, Getty thesauri). The next step involved generating dynamic web pages that describe a person, a work, an expression, a manuscript, a part of a manuscript or an illumination, and which include links to the corresponding descriptions in the source databases. This original data is enriched with longitude and latitude coordinates for geographical names that were acquired by aligning them with the equivalent GeoNames concepts. It is now possible, for example, to show on two different maps the places depicted in illuminations with the images of the corresponding folios, and the illuminations from manuscripts held in a particular institution. In addition, a viewer embedded in the page (we currently favour Mirador: https://iiif.github.io/mirador) shows the digitised manuscript or folio when available. The information needed to display these images is structured according to the Shared Canvas data model (http://iiif.io/model/shared-canvas), which is the foundation of IIIF (International Image Interoperability Framework: http://iiif.io). This information is passed to the viewer in form of JSON-LD manifests, which are generated from metadata and image files supplied by the partner institutions. The viewer itself is client-based and features a deep-zoom capability for the loaded images, the possibility to display associated metadata, and also the superposition of different layers, such as an image and the corresponding textual transcription. The data model behind the application is based on CIDOC-CRM (http://www.cidoc-crm.org) and FRBRoo (http://www.cidoc-crm.org/frbr_inro.html) and our data is already available in RDF format. Beyond taking into account the different levels of work, manifestation and item (using the classes F4 Manifestation Singleton for manuscripts as well as F3 Manifestation Product Type and F5 Item for early printed books), we intend to group manuscripts and early printed books as productions of certain expressions (class F2 Expression). Illuminations are understood as features (class E26 Physical Feature) placed on a folio or page, and provenance marks will be modelled in the same way. Based on the lessons learned through developing our own ontology, we will be able to give feedback about the data.bnf.fr, FRBRoo and SharedCanvas ontologies, as well as other theoretical foundations. We are currently working on extending this prototype, which was developed using the semantic web application framework CubicWeb (https://www.cubicweb.org/). In addition to manuscripts, we will be including early printed books, with a strong emphasis on book provenance, by integrating data from the following four sources: Esprit des Livres (database on auction and other sales catalogues, ENC), Bibale (database on historical book collections, IRHT), Europeana Regia (database on three important historical collections of the Middle Ages and Renaissance), as well as CR2I (Catalogues Régionaux des Incunables Informatisés, CESR) and CRIICO (CR2I Centre-Ouest, CESR). Resolved challenges include the creation of ARKs (Archival Resource Keys) for each manuscript (BnF and IRHT) and codicological unit (BnF). At a later stage, digital editions of inventories of manuscripts in TEI XML will be integrated as well. This short paper will present the latest version of the semantic web application for Biblissima’s data cluster. ",
       "article_title":"Biblissima - Following Medieval Manuscripts and Incunabula through their Existence via a Semantic Web Application",
       "authors":[
          {
             "given":"Stefanie",
             "family":"Gehrke",
             "affiliation":[
                {
                   "original_name":"Equipex Biblissima, France",
                   "normalized_name":null,
                   "country":"France",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "linking and annotation",
          "standards and interoperability",
          "GLAM: galleries, libraries, archives, museums",
          "ontologies",
          "digitisation, resource creation, and discovery",
          "cultural infrastructure",
          "English",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Widespread digitization of cultural heritage materials has presented scholars with unprecedented access to primary sources. For digital humanities scholars, who craft arguments from examination of primary sources, increased access to materials has been celebrated as the “democratization of historical research” (Bolick, 2006). Presumably, such changes in archival research environments have influenced how humanist scholars work. Indeed, recent research has confirmed that technological advancements have significantly impacted scholarly practices (Rutner and Schonfeld, 2012; Chassanoff, 2013). Yet  how humanists evaluate and use digitalsource materials to construct narratives is less well-understood.   In this paper, I report on findings from a one year qualitative study examining digital humanists’ scholarly use of one kind of digital source material - digitized archival photographs. Using a case study approach, I examine the practices and processes at play in the construction of historical evidence. This research is guided by the following questions:  How and why are digital humanists using digitized archival photographs in their research and teaching activities?  What factors and qualities matter to them in their experiences in the photographic digital archive?   The goal of this study is to provide an in-depth, holistic understanding of a complex interaction space made up of, but not limited to: digital surrogates of archival objects, user perceptions and attitudes, environmental constraints, and historical training and orientation. Empirical research in digital environments tends to focus on single components of the interaction (e.g., user and interface; user and artifact) as they relate to specific aspects of information behavior, or to conceptualize information use as the successful fulfillment of stated information needs. Yet such perspectives do not attend to the impact that ecological factors may have on user interactions with materials. Adopting a phenomenological stance enables a focus on understanding “how persons construct meaning” through examinations of their particular experiences with certain phenomenon (Wilson, 2002). In this study, exploring  how and why scholars use digital photographs helps to reveal the emergent qualities and attributes that make this experience meaningful for participants.   Semi-structured interviews were conducted with sixteen participants (9 men and 7 women) throughout the spring and summer of 2015. Recruited participants came from a variety of academic departments, including History, English, African-American Studies, American Studies, Classical Studies, and Musicology. Each participant pre-selected two examples of digitized archival photographs they had used in research and teaching. Two customized web pages, which I termed  Photograph Scenarios, were created in an attempt to replicate where possible their original experience viewing and encountering the digital photograph. I also collected supplementary materials related to each participant’s image use, including conference presentations, class presentations, course syllabus, dissertation chapters, and journal articles. Data were analyzed using open coding and thematic analysis in order to surface salient aspects of the experience related to interpretation and use. To strengthen and verify the analysis, triangulation across data sources was employed.      Figure 1: Photograph Scenario used in interview from Library of Congress collection (http://www.loc.gov/pictures/item/2002707085/)   The findings presented in this paper shed light on both practical concerns and intellectual challenges that surface throughout the experience of constructing historical evidence. Descriptive quotations from interview transcripts alongside examples of image use illustrate the functional ways humanists are using digitized archival photographs in their scholarly activities (e.g., to make historical assertions, to corroborate existing information, etc.). A typology illustrating how scholars use digital photographs is presented below.     HUMANIST USED PHOTOGRAPH TO….   INTERVIEW EXCERPT     Corroborate existing information  \"A lot of times we need to look at these historic photos to actually know what it actually looked like and not just our idea of what it looked like\"    Make historical assertions  \"And I was having trouble finding certain types of ferries that I know existed because of other records. You know, like you would find references to their being a rope ferry, but then you could never find a photo to see exactly what they meant by that.\"    Reference historical documentation  \"Because we were working on just one of the bridges, and as you can see from where they had, it's on the well, the left and the right side, it shows the different bridges that were there.\"    Elicit reactions from viewer  \"But I'd always try to find good touristy photos to show some good 60s and 70s touristy photos or whatever, just to make it livelier\"    Present community perceptions at time of creation  “…it’s sort of like what did the community, or the boosters of the community, think was important? Because, hey, it's what they were trying to put out for the public…”    Juxtapose against other sources  \"Yeah, and that's exactly what I did, is I put it in conversation with other images like it.\"   Table 1: Typologies of Use The cases presented demonstrate the extent to which material conditions of  experience (rather than the tools through which users discovered or accessed resources) can impact interpretive practices and further use.   One salient theme is the factors that motivate participants to use photographs. In one case study, a participant discovered a “vernacular body of images” that led them to collect similar types of visual sources. In another case, a participant selected their project topic after encountering multiple published images depicting what they term “radical masculinity” in the Communist party. They describe their discovery experience: “But then I started noticing there was a lot of- not this image in particular- but there were several other ones that were repeatedly published, of Communists bandaged and showing black eyes and stuff, after confrontations with the police or vigilantes, so that was the first thing I noted.”   Other cases illustrate the discrepancies between viewing digital photographs online and encountering them in person. In one case, a participant describes the advantages of obtaining a high quality scan from an archivist rather than accessing and using the “tiny tiny little print” displayed in the online collection. In another case, a participant attributes the discovery of a number of details that became central to their historical argument only after viewing a physical version of the photograph in person:  So, there are a whole number of things that I never would have seen [if I hadn’t also viewed the photograph in person], I think. And a whole range of- you know, the physical image has a whole affect to it that you don't get from a glowing screen.… I can look at the digital image from the [library], and zoom and zoom in to see it, but I never would have seen [this important detail] if I was just looking at a 2 inch by 3 inch digital version. Both of these examples demonstrate the importance of individual scholars’ interpretive experiences as a means for understanding the conditions that enable further use of these primary sources.  Digital humanities scholars undoubtedly face a number of practical and hermeneutic challenges in using source materials from digital archival research environments. As spaces of knowledge production, online archives must support numerous heterogeneous practices in order to remain useful and relevant. At the same time, navigating online archives to find and use sources requires digital humanists to be competent at varying levels, including: interfaces, digitization quality, overall orientation to the archive, and domain-specific heuristics (Yakel and Torres, 2003).   While there has been unquestionable growth in access to digital archival materials, there have been few empirical attempts to understand the factors and qualities that matter to scholars as they interact with digital sources. The case studies presented in this paper will inform design and development efforts in the digital humanities community by focusing on the end-users of these systems and their needs. A holistic, methodological approach which emphasizes scholars’ experiences evaluating, interpreting and using materials in scholarly activities allows for an exploration of the mediating factors underlying these practices. Such an orientation enables us to extend our attention beyond simply exploring the material constraints of resource discovery and access, which are often modeled on analog approaches to communication. Instead, this study broadens the emphasis to, in the words of Gregory Bateson, “the difference that makes a difference.” ",
       "article_title":"Constructing Evidence in the Photographic Archive: The Experience of Digital Humanists",
       "authors":[
          {
             "given":"Alexandra M",
             "family":"Chassanoff",
             "affiliation":[
                {
                   "original_name":"School of Information and Library Science, University of North Carolina Chapel Hill",
                   "normalized_name":"University of North Carolina at Chapel Hill",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0130frc33",
                      "GRID":"grid.10698.36"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016",
       "keywords":[
          "user studies / user needs",
          "digitisation, resource creation, and discovery",
          "knowledge representation",
          "archives, repositories, sustainability and preservation",
          "English",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Sea and Spar Between by Stephanie Strickland and Nick Montfort is a poetry generator that repurposes, combines and re-mixes words and phrases from Emily Dickinson’s poems with those taken from Herman Melville’s  Moby Dick into an (almost) endless sea of stanzas (the program produces about 225 trillion stanzas arranged on a toroidal surface). In his keynote speech to the U.S. Library of Congress, Stuart Moulthrop defined  Sea and Spar Between as both “an immensely long-form computational poem” and “a remarkably compact poem generator.” Transferring such a complex e-literary work into a different language and cultural context raises vital questions regarding the very nature of translation and adaptation in the digital age.  The first Polish version of the program, presented by us in 2013 at Electronic Literature Organization Conference in Paris and published online in 2014 in Techsty (both as a program and as a glossed code with Montfort's and Strickland's comments), greatly multiplied the distributive authorship of the work as a whole, revealing new culturally and linguistically determined aspects of code, grammar and style, and adding a complex layer of interdependencies.   See and Spar Between poses a translational challenge which in some languages might seem impossible to accomplish. Polish, our target language, imposed some serious constraints: one-syllable words became disyllabic or multisyllabic; kennings taken from Melville’s work required a different morphological, lexical and grammatical arrangement; and most of the generative rhetoric of the original (like anaphors) had to take into consideration the grammatical gender of Polish words. As a result, the javascript code, instructions that accompany the javascript file, and arrays of words that the poetry generator draws from, needed to be expanded and rewritten. Moreover, at several crucial points of this rule-driven work, the nature of Polish language forced us to modify the code.   In 2015, we started work on porting the  Sea and Spar Between generator from its javascript+html5 web browser context into XBOX 360 Kinect motion-sensing environment, where the input is controlled by user gestures. This adaptation, which will be available both in English and Polish will be semantically oriented. We aim to use Kinect affordances to program various haptic gestures recognized by the sensor in a way that ensures coherence between interactive gestures and the content of the poem, between the user’s haptic activity and their cognitive processes. Moving from one platform to another, from the Web to Kinect, will involve translating mouse and keyboard gestures (point-and-click, numerical input) into a gesture vocabulary of sensor technology. As an adaptation, our work aims at incorporating the dominant themes of Strickland and Montfort’s work into user movements during the multidimensional navigation of the work. If the generated stanzas are compared to fish in the ocean, the screen to an infinite canvas and the reader’s navigation to a sea voyage, then Kinect port transforms these very metaphors from subject of “translation” into a finite set of gestures that are in sync with the work’s semantics and the authorial intentions behind the generator. For this reason alone, the port to Kinect cannot be a translation of a more radical type, in which Dickinson and Melville are replaced with any other poets from any other language.  Once again, the question arises of what we translate/adapt/port when we translate a digital work of art for digitally enhanced venues and for an audience of “digital natives.”  In the course of translation of  Sea and Spar Between and its adaptation for different platforms, the process of negotiation between the source language and the target language involves the factors unseen in traditional translation. Strickland and Montfort read Dickinson and Melville and parse their readings into a computer program, which as a translation, or port, from Python to javascript, is already a derivative. This collision of cultures, languages and tools is amplified when transposed into a different language. The transposition involves the original authors of  Sea and Spar Between, the original translators of Dickinson and Melville into Polish, and us, turning the process into a multilayered translational challenge, something we propose to call a distributed translation. The forthcoming port to Kinect makes these issues even more challenging and exciting.   ",
       "article_title":"Translating Electronic Literature. Multicultural, Multilingual and Cross-Platform Encounters",
       "authors":[
          {
             "given":"Monika",
             "family":"Górska-Olesińska",
             "affiliation":[
                {
                   "original_name":"University of Opole, Poland",
                   "normalized_name":"Opole University",
                   "country":"Poland",
                   "identifiers":{
                      "ror":"https://ror.org/04gbpnx96",
                      "GRID":"grid.107891.6"
                   }
                }
             ]
          },
          {
             "given":"Mariusz",
             "family":"Pisarski",
             "affiliation":[
                {
                   "original_name":"University of Warsaw, Poland",
                   "normalized_name":"University of Warsaw",
                   "country":"Poland",
                   "identifiers":{
                      "ror":"https://ror.org/039bjqg32",
                      "GRID":"grid.12847.38"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "digital humanities - multilinguality",
          "games and meaningful play",
          "literary studies",
          "english studies",
          "media studies",
          "digital humanities - diversity",
          "other",
          "machine translation",
          "programming",
          "translation studies",
          "multilingual / multicultural approaches",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" After decades of building digital resources for humanities research, such as Digital Scholarly Editions (DSE), and making them available to researchers and the broader public, we are at the point where many of these resources can be connected to one another and are more and more accepted by the scholarly community. However, we also experience the challenge to maintain all the various Digital Scholarly Editions which were built on a diverse base of different technologies. This is especially complex as Digital Scholarly Editions are “living” objects. On the one hand that means that the content can be extended and refined continuously. Hence they are never finished. On the other hand the technological basis must be kept accessible, secure and running. Those two processes can be summarized under the term “data curation”. If we assume that a Digital Scholarly Edition not only consists of the marked up texts, mostly XML documents, but also of another layer on top of the XML documents, the functionality layer – all the interactive parts, the visualizations and the different views on the texts, indexes or other research material, such as images or audio documents – it is obvious that data curation can become an unlimited complex task. This functionality layer provides an enormous additional benefit to the texts. A Digital Scholarly Edition can be seen as a tool which is used to analyze the XML documents, thus as part in the research process which must be preserved to reproduce research results which often cannot be achieved without the functionality layer. A Digital Humanities resource usually undergoes a typical life cycle and is built by a team of team members with a variety of competences that are needed for each task:  Analysis of the sources to be edited (humanities scholars) Requirement Engineering (the whole project team) Design of the data or document model, choosing what standards to use (scholars, database-, markup-, metadata-specialists) Choosing, adopting, and/or developing software tools for transcription, editing and publishing (software developers, scholars) Installing and maintaining development servers and web servers (system administrators) Conceptual design and implementation of the web publication of the Digital Scholarly Edition (web designer, web developer, scholars) Preparation for long term access and archiving (documentation- and metadata-specialists) Service support and maintenance after project finished (data curators)  At each step of this life cycle decisions are made which have impact on the subsequent steps. The first two steps of the list constitute the foundation on which the whole Digital Scholarly Edition is built on, from the data model over the choice of software tools until the publication as well as data curation. Digital Scholarly Editions are sufficiently described from a methodically point of view regarding the document and text modeling (Pierazzo 2015, Sahle 2013). An analytical description from the technological point of view still is a desideratum. To make a comprehensive data curation possible a technological publishing concept which uses standardized components is needed. Such a concept can consist of standards for a formal project documentation, a description of the used technologies, the provided interfaces and APIs, a design paradigm for typical user interaction tasks, and many more. Standards on the data- and metadata-layer are broadly accepted and in use – one example are the Guidelines of the Text Encoding Initiative (TEI –  http://www.tei-c.org) – but they are still missing for the functionality layer.  A high standard critical Digital Scholarly Edition can only be built in a sustainable way and be maintained when it follows technological standards which still have to be developed. The paper will present a first tiny step of a proposal for a minimal standard from the technological point of view of a Digital Scholarly Edition. It focuses on experiences made during the last ten years working on XML-based Digital Scholarly Editions built with certain tools, such as eXistdb ( http://exist-db.org). Hence the proposed solution cannot be valid for all the different kinds of Digital Humanities scholarly resources.  A possible next step towards such a formal description could be to package those XML-documents together with the source code of the functionality layer in a standardized self-descriptive format. An option for this task could be the EXPath Packaging System ( http://expath.org/modules/pkg/), which works well for XML-based Digital Scholarly Editions and is widely used by Digital Scholarly Editions which are published using eXistdb. The main purpose of such a packaging system is not connectivity or interoperability rather than maintenance and data curation. The packaging system can be extended gradually to a technological publishing format which incorporates the aforementioned aspects such as a project description format.  A possible formal project description format for the documentation will consist of the following information:  The name of the project and all involved institutions and persons. The status of the project: planned, work in progress, published, or finished. The applied technologies and standards. The licenses, which are used for research data, source code and other components such as fonts, audio or video documents. Information about where to find the source code, if the source code is available under an open source license. Information about provided APIs and other interfaces to retrieve the research data and metadata in various formats (XML, JSON etc.) or get structured information about persons or places to be processed further in other contexts (In case of a correspondence edition metadata about the letters should be prepared in the Correspondence Metadata Interchange Format (CMIF) to be reused by  http://correspsearch.bbaw.de).  Contextual details about the data producers, how data are collected etc. (More at Faniel 2015) Canonical citation rules and instructions for persistent referencing of current parts and older versions of the research data. A standardized change log, which can be evaluated by other services.  Of course this list can be just a first suggestion and does not provide all the information that can be given about a project. The project description must be accessible under a standardized URL (e.g.  http://home.of.project/api/projectdescription) and can be serialized in different formats, such as XML or JSON, for further processing. That would allow a Digital Scholarly Edition to be registered at a central directory where all information and updates of various Digital Scholarly Editions which follow the same publishing model are collected automatically. Such a central directory does not exist yet. Currently existing directories collect information manually and describe projects externally, so changes and updates are harder to track.  The success of such a publishing model depends on pragmatic usage possibilities and a critical mass of Digital Humanities scholars and projects who publish their Digital Scholarly Edition using this publishing model. It is difficult to find a standardized, generic approach in the world of Digital Scholarly Editions as every project encounters a different set of problems and a different set of uses. Thus it is important as developers to not make too many assumptions about the nature of a project and further the development of a technological publishing standard in continuous exchange with the scholarly community and in very small steps which take into account the diversity across the Humanities. ",
       "article_title":"Sustainable publishing - Standardization possibilities for Digital Scholarly Edition technology",
       "authors":[
          {
             "given":"Alexander",
             "family":"Czmiel",
             "affiliation":[
                {
                   "original_name":"Berlin-Brandenburg Academy of Sciences and Humanities, Germany",
                   "normalized_name":"Berlin-Brandenburg Academy of Sciences and Humanities",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/05jgq9443",
                      "GRID":"grid.420264.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "information architecture",
          "project design, organization, management",
          "software design and development",
          "standards and interoperability",
          "programming",
          "English",
          "publishing and delivery systems",
          "interface and user experience design",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The ISMI project The Islamic Scientific Manuscripts Initiative (ISMI) is a project by the Max Planck Institute for the History of Science and the Institute of Islamic Studies at McGill University in Montreal to collect and make accessible information on all Islamic manuscripts in the exact sciences (astronomy, mathematics, optics, mathematical geography, music, mechanics, and related disciplines), whether in Arabic, Persian, Turkish, or other languages ranging in time from the 8th to the 19th century. Since 2007 the project has collected information on over 4000 texts existing in 14500 witnesses in 7500 codices, as well as information on over 2000 persons. A preliminary website with a subset of information on 130 codices is already available online ( https://ismi.mpiwg-berlin.mpg.de).      Figure 1: Online view of codex Peterman I 674 (Staatsbibliothek Berlin)    The project's goal is to create a catalogue database of all relevant manuscripts and record as much information on these manuscripts as available. The collected data contains basic bibliographic information but also paleographic and codicological information, and also information about the content of the texts and information about the uses of the manuscript and its users. The manuscripts sometimes and notes and colophons providing information about the reading of a text or the use of a text in teaching, about sponsors and the acquisition and ownership of the manuscripts over time. With this information the database not only provides a powerful bibliographical research tool for scholars in the field, but also helps to answer questions pertaining to the historical and social context of knowledge like: Was the author working as an isolated individual or as part of a scientific group? Was this a well-known text? Did it influence subsequent workers in the field? Was it studied at a court or in a school?   A manuscript database Cataloging old manuscripts is already a task that overwhelms standard bibliographical databases. There is the problem of anonymous authors at the same time as a proliferation of authors with the same or similar names. The integration of information from different sources is made difficult by libraries changing the names of their collections and their numbering systems or libraries themselves being incorporated or centralized into other institutions. Adding to that are the problems of handling of Arabic writing and the multitude of slightly different Arabic romanization systems. Other requirements also arose in the project early on: for example the need to record and present outdated information. In many cases authorship information has been misattributed widely and for long times so that scholars coming to the database looking for specific information may search under a wrong name or assume the database to be in error unless the common misattribution is also presented with arguments of why the old information is superseded.   Manuscripts as network of objects The database development started in 2006 with a new data model based on the idea of a network of flexible objects and relations. Objects can have arbitrary attributes which are text strings. The relations between objects are also like objects and also have attributes. The objects are things like an abstract  TEXT, a concrete  WITNESS and a real or imaginary  PERSON while relations like  is_exemplar_of connect a text and its witnesses and  was_created_by connects the texts and a person as its author. The same person can at the same time also be connected to other witnesses as a copyist or as a sponsor.      Figure 2: Data model showing relations between text, witness, person and codex objects.    The flexible nature of the relations made it easy to introduce new relation types as it became necessary in the research process, for example to record the documented reading of a manuscript or the misattribution of authorship. This concept of a network of objects with flexible relations, also called an attribute-graph exists in database products like Neo4J today but those were not available in 2006 which led to the development of a custom database called \"OpenMind\". The database software is Open Source, written in Java, uses a conventional SQL database backend and a Web-based frontend.   Challenges of networked data The network-like structure of data in the database makes it easy to add new relation types or new attributes to objects while at the same time making it more difficult to create simple forms for entering data for things that are composed of multiple objects in the data model. A form for a manuscript for example not only creates a witness object but also creates relations to a text object, multiple person objects, codex, collection, library and place objects, creating those objects if they do not exist. Visualising and querying such networks of data objects is also a new challenge where few established tools and concepts exist. Data can be projected into conventional tools like tables and spreadsheets researchers may be familiar with but these do not exploit the full potential of existing relations. Network visualisation tools and methods on the other hand make if often difficult to browse and search for specific items and require a careful selection of semantically relevant relations for the application of standard graph-theoretical measures. The project currently explores different tools and methods and gathered input from expert scholars in the field in a workshop in February 2016 to be presented at the conference.  ",
       "article_title":" An Islamic Manuscript Database as a Network of Objects  ",
       "authors":[
          {
             "given":"Robert",
             "family":"Casties",
             "affiliation":[
                {
                   "original_name":"MPIWG, Germany",
                   "normalized_name":null,
                   "country":"Germany",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "classical studies",
          "databases & dbms",
          "historical studies",
          "networks, relationships, graphs",
          "archives, repositories, sustainability and preservation",
          "English",
          "bibliographic methods / textual studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Rhyme is a key feature of many verse forms used throughout the history of English poetry. In its simplest forms, rhyme in English poetry can be understood as a connection between words at the end of lines of poetry, due to the similarity in the sounds of their final syllables. Rhyme suggests and enacts relationships between sound and sense in poetry: “the equivalence of the rhyme syllables or words on the phonic level implies a relation or likeness or difference on the semantic level” (Brogan et al., 2012). Internal rhyme can also connect words located elsewhere in the poetic line, although that is less common in English poetry. Rhyme connects lines of poetry through sound patterns which contribute to the tone, pace, and emotional effects of a poem. Because a common printing convention in the nineteenth century indented lines of poetry to match the rhyme scheme, rhyme was also a visual feature of printed poems, perceptible to the reader’s eye and cognition. Rhyme, along with meter, contributes to the memorable qualities of verse, and to the popularity of humorous forms like the limerick. Rhymed verse forms, such as the sonnet and ballad, were very popular in nineteenth-century British poetry, alongside unrhymed blank verse. Rhyme was used for different ideological and aesthetic purposes, as in the working-class ballads of the 1840s or Decadent revivals of French forms like the triolet and villanelle. But the rules of rhyme are not fixed. Throughout the nineteenth century, poets and critics debated which kinds of rhymes were allowed in the best poetry. In particular, some critics argued that imperfect rhyme (also called near rhyme) was allowable because so many poets used such rhymes (such as “love” and “prove”), while others argued for a more restrictive definition of perfect rhyme. These debates about rhyme are part of larger nineteenth-century discourses about prosody, the patterning of sound in poetry. Recent work in nineteenth-century studies has focused on “historical poetics,” the examination of nineteenth-century theories of prosody and their cultural impact (Hall 2011, Martin 2012, Rudy 2009). Such scholarship complicates transhistorical definitions of lyric poetry and its formal features by demonstrating how “prosody provided a way of thinking, a method of protest, of scientific argument and investigation, of negotiating gender, class, and national structures” (Martin and Levin, 2011: 153). But too often this work in historical poetics remains focused on theory separate from poetic practice: Yopie Prins, for instance, says bluntly that “practical application is not the point of historical poetics. There are other, more interesting questions” which for her encompass the relationship of prosody to larger philosophical and scientific discourses (Prins, 2008: 233). This paper offers an alternative, computational approach to historical poetics that will not only further our understanding of nineteenth-century theories of rhyme but also of their relationship to actual poetic practice.  Rhyming dictionaries, which serve as a resource to writers seeking rhymes for their compositions, serve as an importance source for understanding the changing rules of rhyme in the nineteenth century. In the eighteenth century, Edward Bysshe’s  The Art of English Poetry (1702) offered a small dictionary of rhymes, supplanted later in the century by John Walker’s  A Dictionary of the English Language, Answering at once the Purposes of Rhyming, Spelling, and Pronouncing, first published in 1775 with 41,000 words included in the rhyme dictionary. Walker’s dictionary was expanded and reprinted many times throughout the nineteenth and twentieth centuries, and it is mentioned in the letters and papers of many writers and poets as a standard text. But in the second half of the nineteenth century the popularity of poetry writing as a leisure activity among the growing middle class led to the publication of many new rhyme dictionaries, each of which offered different definitions of acceptable rhymes. These included J. E. Carpenter’s  A Handbook of Poetry (1868); Tom Hood’s  The Rules of Rhyme (1869), later republished as  The Rhymester; Or, the Rules of Rhyme; the American writer Samuel W. Barnum’s  A Vocabulary of English Rhymes, Arranged on a New Plan (1876); John Longmuir’s  Rhythmical Index to the English Language (1877); R. F. Brewer’s  Orthometry: A Treatise on the Art of Versification (1893); and Andrew Loring’s  The Rhymer’s Lexicon (1905).  This paper presents my current work in progress in operationalizing these nineteenth-century rhyme dictionaries as R scripts that identify rhymes in poetic texts based on the rules of a specific dictionary. The entries in these rhyme dictionaries consist of rhyme syllables that serve as the entry headings, followed by a list of homophone syllables, and then examples of words that rhyme with the entry heading. The rhyme syllables and words listed may be subdivided into categories, such as perfect rhymes and imperfect or “allowable” rhymes. Many of the dictionaries provide supporting evidence for the use of imperfect rhymes in the form of quotations from the works of British poets. For this project, all of the rhyme syllables, rhyme words, and rhyme categories are stored in csv files for each dictionary, organized by entry headings. In the interest of fidelity to the original historical dictionaries, and to facilitate comparative analysis, conflicting entries have not been normalized. Thus one dictionary might give very different rhyme syllables or words for a given entry than those listed in another; it is precisely this kind of historical variation in the rules of rhyme that this project allows us to explore. The R package data.table facilitates querying and joining data files for multiple dictionaries so as to compare their entries for the same rhyme syllable.  The technical section of the paper describes the key components of these scripts:   the last word of each line of the poem is extracted from the text and converted to reverse spelling; a series of regular expressions are used to enact the specific instructions each rhyme dictionary provides for its users to identify the rhyme syllable to look up, such as locating the vowel that precedes the consonant(s) of the final syllable (“ame”), unless the consonant(s) are preceded by a dipthong, in which case the first of the two dipthong vowels begins the rhyme syllable (“ound”);  a hash lookup for each rhyme syllable to the selected dictionary returns the perfect rhyme syllables, perfect rhyme words, imperfect rhyme syllables (if given), and imperfect rhyme words (if given);  matches among these returned values and the other rhyme syllables and words in the poem are sought; when located, a capital letter is used to mark the rhyme pattern, as is traditional in literary criticism (ABAB, etc); for those dictionaries that include imperfect rhymes, the ratio of perfect to imperfect rhymes in the poem is also recorded.  This project expands our understanding of nineteenth-century rhyme theory and practice in three ways: by enabling the identification of rhyme patterns in large sets of texts, we can expand our understanding of historical verse writing practices beyond canonical literary texts; by operationalizing the rules instantiated in different dictionaries, we can compare how they would have evaluated the rhymes chosen by particular poets; and by creating a database of the words included in these dictionaries, we can examine the consistency and variation of the recommended rhymes.  The final section of the paper presents a case study in the application of these scripts to literary texts, an analysis of the rhyme patterns used in the 1274 poems included in Edmund Clarence Stedman’s  A Victorian Anthology 1837-1895 (1895), as a sample dataset of poetry produced during the time when these rhyme dictionaries were circulating and when these rhyme theories were debated. Examining the frequency of different rhyme patterns and the vocabulary of rhyme common in Victorian poetry opens up new paths for analyzing rhyme practice, rhyme theory, and the lexical fields generated by the use of rhyme. This project thus contributes to current work in nineteenth-century studies and historical poetics by computationally analyzing the theory offered by rhyme dictionaries with the actual practices of nineteenth-century poetry.  ",
       "article_title":"Exploring the Rules of Rhyme: Operationalizing Historical Poetics",
       "authors":[
          {
             "given":"Natalie M.",
             "family":"Houston",
             "affiliation":[
                {
                   "original_name":"University of Massachusetts-Lowell, United States of America",
                   "normalized_name":"University of Massachusetts Lowell",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03hamhx47",
                      "GRID":"grid.225262.3"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "genre-specific studies: prose, poetry, drama",
          "literary studies",
          "english studies",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" In this paper we present a technique to enable the historical study of ideas instead of words. It aimed at assisting humanities scholars in overcoming the limitations of traditional keyword searching by making use of context-specific dictionaries. The elaboration of this technique was the result of a successful collaboration between the History Department of Utrecht University (UU) and the Research Department of the Koninklijke Bibliotheek, National Library of the Netherlands (KB), executed by the authors of this paper during Huijnen's period as Researcher-in-residence at the KB in 2015.  The aim of this collaborative project was twofold: first, to create a method for dictionary extraction from a representative text corpus, based on existing methods and algorithms. Second, to find a way of executing dictionary searches in the KB's digitized newspaper archive and visualizing the results. Both components of the project were tested and evaluated by means of a case study on the impact of American scientific management theories in the Dutch public sphere during the first half of the 20th Century. Using the approach described here, we were able to discover and analyze shifts in the way the modernization of Dutch business and economy was discussed during this period. We would not have been able to achieve the same results by means of traditional historical scholarship alone.  Historical newspapers have traditionally been popular sources to study public mentalities and collective cultures within historical scholarship. At the same time, they have been known as notoriously time-consuming and complex to analyze. The recent digitization of newspapers and the use of computers to gain access to the growing mass of digital corpora of historical news media are altering the historian’s heuristic process in fundamental ways.  The large digitization project the Dutch National Library currently runs can illustrate this. Until now, the KB has made publicly available over 80 million historical newspaper articles from the last four centuries. Researchers (as well as the wider public) are able to do full-text searches in the entire repository of articles through the KB’s own online search interface Delpher ( http://www.delpher.nl/kranten). Instead of manually skimming through a selected numbers of editions or volumes this functionality allows for the searching of particular (strings of) keywords within the entire corpus. As basic as it may seem, full-text searching completely overturns the way in which historians are used to approach newspapers. Instead of the successive top-down selections historians traditionally made in order to gradually isolate potentially interesting material, keyword searching treats the corpus as a singular bag of words and, therefore, enables researchers to immediately dive into the texts that meet their search criteria (Nicholson, 2013).  At the same time, keyword searching has some serious shortcomings for the use in (cultural) historical research. Historians commonly work with texts, but are rarely interested in language per se. Rather, they use written or spoken sources (be it correspondence, literature, diaries, or news media) to gain access to past cultures, ideas, or mentalities. The things that historians are mostly interested in, are often not made explicit (e.g. the Enlightenment attitude, generational conflicts) and difficult to abstract into singular keywords (modernity, secularization). Doing historical research with keyword searching is like painting a canvas using felt-tip pens: it loses every inch of subtlety.  We have successfully developed a technique of dictionary extraction and searching to address this problem. The use of dictionaries is able to bring greater subtlety and diversity into digital historical scholarship. The more elaborate these dictionaries are, the more they overcome the contingency that comes with the use of singular keywords in search strategies. Several research projects that have incorporated the use of highly domain- and time-specific word-lists ('dictionaries'), have already shown this. Text classification algorithms, for example, have helped find the most obvious indicator words for articles about strikes in the Dutch newspaper corpus (Van den Hoven et al., 2010). Implicit dictionaries based upon the MALLET ( http://mallet.cs.umass.edu) package's topic modeling functionality has assisted in finding Darwinian motives in Danish literature (Tangherlini and Leonard, 2013). Topic modeling was also used in building a neoliberalism dictionary to study Colin Crouch's post-democracy thesis in German historical newspapers (Wiedemann et al., 2013; Wiedemann and Niekler, 2014).  From the wide variety of techniques scholars have developed to build and use dictionaries, this project found most inspiration in the topic modeling-based method of the ePol Projekt (Wiedemann and Niekler, 2014). However, rather than aiming at building an optimal infrastructure for dictionary extraction of our own, based on existing techniques, our project centered around practical usability. We sought to develop a (set of) tool(s) for working with dictionaries tailored to the computational expertise to be expected from, but also the specific needs of professional historians (and humanities scholars in general). One of the aims of the KB's Researcher-in-residence program, in addition, is that resulting tools and techniques are usable by the wider public searching the National Library's databases of historical newspapers, periodicals, and books. Our code is fully open source and can be found on GitHub ( https://github.com/jlonij/keyword_generator). The ways in which we have tried to meet the specific demands this posed, can, in our view, be seen as exemplary for any Digital Humanities project aimed not at building highly specialized tools for individual projects, but at combining scholarly standards with the goal of generic usability.   There are a number of ways we have accounted for the targeted user groups in the development of our dictionary extraction and search techniques. On the one hand, for example, we aimed at agility and flexibility at the expense of the deployment of exhaustive computational means. Our algorithm is able to extract a dictionary of flexible length from a given source input of text files within minutes. Because the technique is intended for exploratory use, it is essential that iterations and experimentations are stimulated. Requiring too many preprocessing steps or demanding too much time would be counterproductive.  On the other hand, meeting the demands of tool criticism was crucial in every step of this project. Therefore, the risk of blackboxing was avoided wherever we could, while at the same time granting the user-expert as much control as possible. By varying the command, users decide over the segmentation of the source corpus, the number of topics to be generated, the number of words to be contained per topic, as well as the number of dictionary words required. Moreover, users may flexibly choose between Gensim's ( https://radimrehurek.com/gensim) and MALLET's implementation of LDA, as well as a straightforward tf-idf implementation. When making use of one of the topic modeling packages, users are, just before the process of dictionary generation, given the option of excluding any number of (irrelevant) topics of choice from the equation.   An evaluation in terms of generic precision and recall for any of the variables is, in our view, contradictory to the principle of flexibility. Instead, we evaluated and improved the dictionary extraction by comparing automatically generated dictionaries with ones that were built manually, based on domain knowledge. Comparing the results of searches with different dictionaries in the KB’s digitized newspaper archive was used as an additional evaluation method: dictionaries could be compared in terms of the ranking of some key articles about a particular topic, since the archive's Solr ( http://lucene.apache.org/solr) search engine scores the results of an OR-query (the search string, in which we expressed the dictionaries) on the basis of the number of times query words appear in an article, amongst other things. The case study that was used to test, evaluate, and apply the tools and techniques under development was the impact of American scientific management ideas in the Dutch public media before WWII.   On the basis of this case study we would in our presentation like to show how our implementation of dictionary extraction, search, and visualization can assist the scholarly historical study of digital corpora in general. By visualizing the search results from different dictionaries we are able to show shifting discourses in historical news media. Plotting the number of articles containing a user-specified number of words from any given dictionary, we can present trends in discourse-specific vocabulary usage over time. Whereas existing historiography, for example, suggests a continuing use of scientific management vocabulary in the Netherlands since its introduction in the 1910s, our project presents a more differentiated picture. Dictionary searches in the KB's newspaper corpus show how the use of words in public media connected to the sphere of scientific management (based on context- specific literature) waned after the WWII and how they made room for a new vocabulary belonging to a new era.  At the same time, this case study illustrates how digital techniques like ours bring about conceptual innovations in the study of history. After all, our case study shows that (combinations of) ordinary words (in this instance, for example, 'time', 'work', or 'supervision') are more distinguishing to trace discursive discontinuities than the 'big' words (like 'taylorism' or 'neoliberalism') that historians traditionally have focused on. ",
       "article_title":"From Keyword Search To Discourse Mining - The Meaning Of Scientific Management In Dutch Vocabulary, 1900-1940",
       "authors":[
          {
             "given":"Pim",
             "family":"Huijnen",
             "affiliation":[
                {
                   "original_name":"Utrecht University, Netherlands, The",
                   "normalized_name":"Utrecht University",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04pp8hn57",
                      "GRID":"grid.5477.1"
                   }
                }
             ]
          },
          {
             "given":"Juliette",
             "family":"Lonij",
             "affiliation":[
                {
                   "original_name":"Koninklijke Bibliotheek, The Hague, Netherlands, The",
                   "normalized_name":"National Library of the Netherlands",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/02w4jbg70",
                      "GRID":"grid.425631.7"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "project design, organization, management",
          "corpora and corpus activities",
          "data mining / text mining",
          "historical studies",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" New York City English (NYCE) has long been a stigmatized variety of English. In his seminal research on language use in the New York City dialect, the sociolinguist William Labov referred to New York City as “a great sink of negative prestige” (Labov, 1966)—a characterization that reflected the negative view of NYCE speech shared by non-New Yorkers and New Yorkers alike. Decades later, Preston (2003) elicited extremely low ratings of the New York City dialect on scales of both “correctness” and “pleasantness” by participants from across the US. While these studies present strong evidence of the prevalence of negative language attitudes toward NYCE speech, a more complete picture of linguistic ideology would include what speakers say about NYCE when they are not participating in an academic study. This project seeks to accomplish just that, by examining linguistic ideology with respect to NYCE as espoused by users of the social networking service, Twitter. Twitter has been recognized as an important resource for humanists and social scientists alike. Scholars have collected and analyzed Twitter messages (tweets) in order to investigate numerous textual and linguistic phenomena such as  lexical variation (differences in use of synonymous words and phrases, such as  pop vs.  soda vs.  coke). Russ (2012) in particular (see also Bamman, 2011) illustrates the utility of Twitter for examining regionally defined lexical variation through comparison of the geographic distribution of word choices in  geotagged tweets (with GPS coordinates from which they originated) to more traditionally collected dialectology data. All related research has focused on differences in production. However, I argue that Twitter represents an untapped resource for the investigation of  perceptions of language use, particularly language attitudes toward regional dialects and differences in their phonetic features (which can be identified by non-standard orthography). Using Twitter solves a primary quandary for language attitude researchers—how to acquire naturally occurring data given the fact that participation in research decreases naturalness.  Tweets containing attitudes and ideology were collected using a range of strategies, including text mining for words—and, crucially, spellings—that reference individual features. To do this, however, it is necessary to determine which features get noted and then which lexical items—and which spellings—are used to signal them. For instance,  cawfee (also,  cawffee) is a common orthographic representation of the word “coffee” as pronounced with a raised-THOUGHT vowel, one of the signature dialect features of NYCE. Widely used spellings that reflect  r-vocalization, another key feature of the NYCE dialect, include  New Yawk and  fuhgeddaboudit. In addition to collecting tweets containing orthographic representations of nonstandard features, Twitter search parameters included over 20 terms related to possible names for the dialect itself (e.g.,  New York accent, Manhattan dialect, Brooklynese). These were included in part to determine the extent to which the general public perceives a distinction among speakers from the five boroughs (a distinction which has not been borne out by linguistic analysis).  Repeated automated text mining of Twitter using a Python script to interact with the Twitter API yielded 6,384 tweets that match the aforementioned criteria. Elimination of retweets that did not introduce additional linguistic content and inspection to ensure the tweets reference NYCE produced a final corpus of 1,773 tweets. Relative frequencies of the borough-specific and pan-regional terms in the 1,315 tweets that explicitly reference NYCE by some name reveal that Twitter users most frequently refer to NYCE as the  New York accent (N=805; 61.2%), though  Brooklyn accent (N=359; 27.4%) accounts for more than a quarter, with  Bronx accent (N=54),  Queens accent (N=29, tied with  Brooklynese—the most frequent  -ese moniker), and  Staten Island accent (N=10) being used much less often. Whether  New York accents and  Brooklyn accents are perceived as linguistically or socially distinct, or two names for the same dialect region, will be explored in the paper.  All tweets were manually coded to determine their sentiment with respect to NYCE.  POSITIVE:  I swear girls from New York accent sound so sexy   NEUTRAL:  GAWGEOUS idea she said in her New Yawk accent   NEGATIVE:  If you have a Brooklyn accent I automatically want to punch you.    Almost half of these tweets are neutral in sentiment (N=584, 44.4%); 378 were positive (28.7%) and 200 negative (15.2%). However, 154 tweets were classified as UNCLEAR (8.7%)—many are ambiguous as to whether they evaluate an imitation of an accent or the accent itself, such as when describing an actor’s performance (which is common among these types of tweets):  UNCLEAR:  his New York accent is so bad /:    Examples such as these pose significant obstacles to automated sentiment analysis—which has been extended to Twitter data (see for instance Pak and Paroubek, 2010)—particularly of language attitudes. Automatic methods would simply code the tweet as negative without recognizing the need to differentiate its underlying meaning. It is noteworthy, however, that even if every UNCLEAR tweet is actually expressing negative sentiment, there would  still be a greater number of tweets with positive opinions of NYCE speech than negative ones. Furthermore, when Twitter users reference a specific NYCE feature, their evaluation of it is more likely to be positive, regardless of whether they use standard (N=79) or nonstandard (N=568) orthography to represent the feature.  These findings portray a broader range of reactions to NYCE than the language attitudes speakers have presented when engaged in academic research. The paper will include discussion of both negative and positive language attitudes that Twitter users espouse concerning the dialect features associated with NYCE. For instance, any tweets with positive sentiment will be examined to determine if they represent instances of “covert prestige” (Labov 1966), whereby speakers use stigmatized varieties for in-group identification and solidarity. Additional discussion will focus on which regional features evoke the most meta-commentary. Furthermore, I will explore the extent to which Twitter users draw (additional) attention to non-standard forms they employ through capitalization ( NEW YAWK), hashtags ( #newyawk), and other orthographic means.  ",
       "article_title":"Language Attitudes of Twitter Users Toward New York City English",
       "authors":[
          {
             "given":"Nathan",
             "family":"LaFave",
             "affiliation":[
                {
                   "original_name":"New York University, United States of America",
                   "normalized_name":"New York University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0190ak572",
                      "GRID":"grid.137628.9"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-07",
       "keywords":[
          "corpora and corpus activities",
          "data mining / text mining",
          "linguistics",
          "English",
          "social media",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Source criticism is a scholarly process fundamental to many disciplines of humanities, especially in historical studies. However, it is mainly designed for a traditional way of research, namely human scholars read a textual source without producing structured evidences for reuse. Our proposal is to extend the traditional methodology of source criticism to digital research infrastructure on which scholars records the reasoning process from evidences to facts, and share them with other scholars so that the detail of the reasoning process can be transparently reproduced. Our approach, digital criticism, aims at realizing this idea on digital criticism platform (DCP) toward evidence-based digital humanities with the support of Semantic Web technology.  An evidence-based approach is also used in quantitative humanities, but digital criticism is a fundamentally different approach. Digital criticism focuses on reading sources in a critical manner, while quantitative humanities focuses on deriving numerical values from the collection of sources. Generally speaking, quantitative humanities tries to make abstraction of corpus through quantitative aggregation, while digital criticism tries to make critique of sources through digital analysis with support for the management of a layered abstraction process.   Figure 1 Schematic diagram of evidence network    Core concepts of digital criticism The main contribution of the paper is evidence network on which digital criticism is performed. The network consists of four concepts, namely evidence, hypothesis, fact, and reliability. Those concepts have our own definition to organize the historical reasoning process into an explicit model.   Evidence is relationship between sources. If a photograph A takes the same scene with a photograph B, they are linked as an evidence with reproducible parameters of how photographs can be matched.  Hypothesis is relationship between historical concepts. If a ruin A and ruin B have different names but are believed to be the same ruins, they are linked as a hypothesis with supporting evidences and other descriptions on the reasoning process.  Fact is relationship between evidences and hypotheses to claim reusable knowledge for future research.  Reliability is an attribute of evidences and hypotheses to represent the degree of reliability estimated by the scholar on registration. Because the estimated reliability may be different for each scholar, evidences and hypotheses should always be linked to a user entity who made the action.   Figure 1 illustrates a schematic diagram of evidence network. A scholar can build up the network starting from each evidence and hypothesis. Another scholar who wants to reuse the knowledge can start from the fact, and track back to hypotheses and evidences to check the reliability of the reasoning process.    Figure 2 Three components of Digital Criticism Platform (DCP)    Digital Criticism Platform (DCP) Evidence network is a directed graph with semantic annotation, and the graph structure is built using RDF (Resource Description Framework). Hence a search over a graph can be implemented using SPARQL, which is a query language for a RDF graph. To construct and study the evidence network, we develop Digital Criticism Platform (DCP) with three components, namely data repository, evidence tool, and inquiry tool as illustrated in Figure 2.    Data repository archives digital resources with unique IDs and metadata. We developed data repository using DSpace as infrastructure for its reliability and extensibility to Semantic Web environment.    Evidence tool works on various types of media for collecting evidences. We developed three tools, Mappining, Photofit, and MemoryHunt, for maps, photographs, and field work, respectively.   Inquiry tool is to study and sophisticate the evidence network using Semantic Web technology such as SPARQL. This component is now under study, and has not reached the development phase.    In the following, we focus on three evidence tools to describe more detail of the tools. Those tools should be integrated into data repository so that every evidence is registered as a part of evidence network. Three tools are already in operation, but only Photofit is fully integrated into data repository at the time of writing.    Mappining Mappining (Kitamoto 2012) is a web-based tool for matching maps (Figure 3). This tool employs an idea of “interactive georeferencing” in contrast to geometric correction. Interactive georeferencing uses a pin to match two maps at a single point, in a similar way of pinning cloths. When we put a pin, two maps scroll together, while when we release a pin, only one map scrolls. Using this interface, any point on a map can be matched with another map for overlay-based comparison. Interactive georeferencing is advantageous for reading sources because geometric distortion is not introduced. A pin is an evidence to claim that each point on a map represent the same location on earth.   Figure 3 Matching two maps on Mappining    Photofit Photofit is a web-based tool for matching photographs (Figure 4). The target of the tool is two photographs taken from similar locations but different angles at different time. It allows planar shift and zooming transformation for two photographs to find the best match. On success, it is an evidence to claim that two photographs take the same scene or the same object. This evidence may lead to a new hypothesis on the identity of historical concepts when the captions of two photographs represent different historical concepts.    Figure 4 Matching two photographs on Photofits    MemoryHunt MemoryHunt (Kitamoto 2015) is a mobile app for matching a photograph with the real world (Figure 5). It shows a target photograph on the viewfinder of a smartphone camera with controllable transparency, and the task of a user is to find the same location and the same direction that the photograph was taken. On success, a mobile app can record the location and direction as metadata of the photograph. This may lead to an evidence between an object appearing in the photograph and one in the present world.    Figure 5 Interface of MeomoryHunt app    Evidence network The purpose of digital criticism is to build an evidence network in which historical sources are linked through evidence nodes, and historical concepts are linked through hypothesis nodes to derive fact nodes supported by evidence and hypothesis nodes. We start by a bottom-up process of registering evidences and hypotheses which are still fragmented. We then switch to a top-down process of viewing a graph structure as a whole to discover unknown relationships. In the following, we use our past results on Silk Road ruins as case studies to check the validity of our approach. Due to the incompleteness of DCP, the following diagrams were manually drawn. Figure 5 shows a simple evidence network. A photograph in a book is matched with another photograph in another book using Photofit, and an evidence node is added with transformation parameters. We also know that, through captions, each photograph represents a ruin known by different names. We then add a hypothesis node based on the above evidence to claim that two ruins known by different names are the same.  Figure 6 shows a complex evidence network. Multiple tools are used for matching multiple sources, such as Mappinning, Photofit, and Google Earth. The evidence network suggests relationship among a Buddhist ruin and a ruin known by the name Chikkan-kol and another ruin known by “七康湖遺跡. ” This relationship was our discovery previously reported in a paper (Nishimura and Kitamoto 2010), but digital publishing in the form of evidence network offers clearer representation of a compex reasoning process.    Figure 6 A simple evidence network    Figure 7 A complex evidence network    Conclusion We proposed digital criticism platform (DCP) as a model of source criticism on a digital platform. Digital criticism tries to simulate the traditional method of source criticism and extend it to take advantage of digital research infrastructure. Digital Criticism is the upgraded version of our past proposal on “data criticism” (Kitamoto and Nishimura 2014), after shifting research focus on the type of sources to the way of digital scholarship. The methodology of criticism is also investigated in different approaches, such as algorithmic criticism (Ramsay 2011).  Knowledge representation of historical evidences has a large number of related literature. In particular, Pasin proposed the usage of factoid model for accumulating evidences for prosopography in the linked data world (Pasin and Bradley 2013), and linked data is also used for places such as Pelagios (Isaken et al. 2014). We also use the same concept of linking entities and uses sources as evidences, but digital criticism focuses more on accumulating structured evidences and hypothetical links. Another important work for the refinement of evidence network is knowledge representation for argumentation, such as CRMinf argumentation model, an extension of CIDOC-CRM (Paveprime Ltd and collaborators 2015). Finally, we are yet to make choices on metadata standards or vocabularies, which are open questions. The long-term goal is to design a digital publishing platform for evidence citation. Historical facts can be tracked to evidences to clarify the reliability of evidences that support historical facts. This leads to increased transparency of research, and to realize better data management planning.   Acknowledgments This work is supported by JSPS KAKENHI Grant Number 26540178. Software codes for DCP was developed by Mr. Tomohiro Ikezaki.  ",
       "article_title":"Digital Criticism Platform for Evidence-based Digital Humanities with Applications to Historical Studies of Silk Road",
       "authors":[
          {
             "given":"Asanobu",
             "family":"Kitamoto",
             "affiliation":[
                {
                   "original_name":"National Institute of Informatics, Japan",
                   "normalized_name":"National Institute of Informatics",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/04ksd4g47",
                      "GRID":"grid.250343.3"
                   }
                }
             ]
          },
          {
             "given":"Yoko",
             "family":"Nishimura",
             "affiliation":[
                {
                   "original_name":"Hanazono University, Japan",
                   "normalized_name":"Hanazono University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/02wm03a77",
                      "GRID":"grid.444209.f"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016",
       "keywords":[
          "linking and annotation",
          "data modeling and architecture including hypothesis-driven modeling",
          "archaeology",
          "historical studies",
          "knowledge representation",
          "English",
          "publishing and delivery systems",
          "semantic web"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introducing  thresholds   handwritten sticky notes, highlighted document pages, and grainy photographs rub against one another, forming dense and shifting thickets. the blank spaces between once-distinct districts become cluttered and close. geographically distant realms ache to converge. the bookcase furiously semaphores toward the far corner of the room. thin lines of colored paper arrive to splay across sections. the wall bursts at every seam.  Whether it be real or virtual, every project has its own “wall”: the irrepressibly interdisciplinary network that inspires and propels the work. Populating this capharnaum are the ideas, images, sentences, scenes, and characters that “stick to us,” to use Lara Farina’s evocative phrase (Farina, 2014). They are the “encounters” that Deleuze describes as the impetus toward work, the things that “strike” us, as Benjamin puts it, like a hammer to unknown inner chords (Deleuze, 1988; Benjamin, 1999). This affective principle of collection (what strikes you) means that the wall is an intensely personal artifact. Its unique architecture springs from a thinker’s nomadic wanderings through and amidst a cultural and aesthetic landscape, whose dimensions are stretched beyond traditional disciplinary boundaries to include anything that clings to us, whether it be Werner Heisenberg’s letters or an episode of  Breaking Bad.  Although instrumental to every humanities project, the wall has a brutally short lifespan. The writer strives to reassert control over its borders and boundaries by whittling down its undisciplined excesses; indeed, training to be a scholar is in large part learning to compress and contain the wall’s licentious sprawl. We shorten our focus to a single period, place, or author; excise those fragments that fall outside the increasingly narrow range of our “expertise”; and briskly sever any loose ends that refuse to be tied. These regulatory measures help align our work with the temporal, geographic, and aesthetic boundaries of our disciplinary arbiters: the journals and university presses that publish our work, the departments that hire and tenure us. In an increasingly tight academic marketplace, where the qualified scholars, articles, and projects far outnumber the available positions, deviation from the standard model can seem like risky business indeed.  Even as entrenched structures dictate compression and containment in scholarly writing, the open networks of the web have enabled a publication model based on public sharing and collaboration, spurring a turn to process across the humanities. It has become normal for scholars of all fields to share their incipient, in-progress research on blogs and wikis, and look to the comments sections for peer review. On a larger scale, these moves toward a collaborative process of knowledge-making are visible in the editing policies of Wikipedia; in Femtechnet’s Distributed Open Content Course (DOCC), an open repository for course materials; and in new open access imprints like the Dead Letter Office of Punctum Books, which publishes abandoned scholarly projects (to name just a few examples among many). This turn to process has put pressure on the gatekeeping mechanisms described above, as many scholars yearn for a less rigid publishing model that foments the networked creativity of the wall. Advocating for the transformative effect of a process-oriented model of digital publication, this short paper asks: how can digital humanities not only embrace process rhetorically, but in fact accrete tangible value to the more piecemeal, contingent aspects of knowledge creation? How can we make it the wall’s scholarly sprawl “count” within systems that still rely on the trimmed and trussed-up products of research? How can we not only laud conceptually but help to build materially critical practices that eschew disciplinary (and disciplining) boundaries in favor of openings and traversals?  After a brief survey of existing digital journals and other publishing initiatives, including  Hyperrhiz, Scalar, and Electric Press, we turn to our own incipient venture, titled  thresholds.  thresholds is a web-based digital publishing platform for creative scholarship, stitched together from existing digital humanities tools. By sketching the primary design features of  thresholds – both their theoretical motivations and technical solutions, described in brief below – this short paper argues for a capacious digital publishing model that negotiates, without dissolving, the shifting edges between reading and writing, process and product, the fragment and the collective.    Design Features The primary design feature of  thresholds is the split screen. On the webpage’s virtual verso are short critical essays that exceed disciplinary boundaries, whether it be in content, style, or approach. We solicit work that a traditional academic journal may deem unfinished, unseemly, or otherwise unbound, but which discovers precisely in its unboundedness new and oblique critical perspectives. Along with her essay, the author submits the textual, visual, and audible fragments that provoked and surreptitiously steered her work. These are published on the right side of the screen and scroll in tandem with the corresponding essay. These scraps are not explicitly harnessed to the work’s main body, but instead lie beside it to create provocative juxtapositions; it is left to the reader to forge lines of connection between recto and verso.  Reinforcing its commitment to process and material form,  thresholds further provides a digital toolkit for readerly making. These tools assign names and haptic functions to those critical traversals that a reader makes through and against a text. As the author’s fragments scroll up the right-hand side of the screen, the reader can anchor a piece, holding it in place for future reference, or join one scrap to another to generate new patterns and co-movements. She can also import new material, either by copying text over from the essay on the verso or by composing additional fragments that leak new texts, artists, or ideas into the system. At the end of any given reading session, then, the reader will have generated her own “wall,” plucking, amassing, and recomposing the author’s fragments to create her own annotative assemblage.  At any time, the reader can capture and conserve the “constellation” that she has produced—that is, the current arrangement of the fragments that she has chosen to lock and join together. Although every user has access to the same firmament of texts that cycle through  thresholds, each constellation will be singular; their unique spatial architecture will attest to the creative and critical value in visualizing the relations between fragments and texts, readers and authors, and readers and texts. Readers who choose to publically share their work will be able to see how their own creation fits into a galaxy of all other users’ constellations, mapping their own choices against that of a collective readership. By enabling the reader to place herself in relation to both the author’s text  and all other readers of the site,  thresholds models criticism as an intimate yet communal activity that inheres in the delicate links we build in the spaces between each other, as much as between the texts themselves.  To ensure that this intervention is not only conceptually provocative but also formally useful,  thresholds endows each fragment with a flexible markup language. Readers can download their constellations, receiving a file listing all texts, objects, and art cited therein. This file can then be imported into citation software or shared with others. This underlying information architecture, not immediately present to visitors but baked into the structure of the site, plugs the swirl of scraps that make up any given constellation into the existing citational infrastructure of the humanities. In so doing, it allows  thresholds to negotiate the gap between that which is in-progress and incomplete within our reading practices—the stray underline, the forgotten marginal note—and more formalized and prescriptive methods for incorporating others’ work into our own. There is a place,  thresholds implicitly argues, for the fragmentary in our collecting and collective practices; for the wall’s sprawl within the more regimented systems that order our work.   ",
       "article_title":" Thresholds: Valuing the Creative Process in Digital Publishing  ",
       "authors":[
          {
             "given":"Whitney",
             "family":"Trettien",
             "affiliation":[
                {
                   "original_name":"UNC Chapel Hill",
                   "normalized_name":"University of North Carolina at Chapel Hill",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0130frc33",
                      "GRID":"grid.10698.36"
                   }
                }
             ]
          },
          {
             "given":"Frances",
             "family":"McDonald",
             "affiliation":[
                {
                   "original_name":"Duke University",
                   "normalized_name":"Duke University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00py81415",
                      "GRID":"grid.26009.3d"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "cultural studies",
          "creative and performing arts, including writing",
          "authorship attribution / authority",
          "knowledge representation",
          "English",
          "publishing and delivery systems",
          "audio, video, multimedia",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction In the context of literary studies, which are mainly concerned with the hermeneutic interpretation of literary texts, narratological annotation can be helpful in at least two ways. First, the identification of narrative structures can point to peculiarities of the individual texts that are in need of interpretation, thereby advancing the generation of interpretation hypotheses. Second, since narrative structures can often be detected on the surface level of texts and described intersubjectively, narratological analyses may provide a robust and concrete backing for more comprehensive and complex interpretations.  Against this backdrop, the project heureCLÉA aims at developing a “digital heuristic”: a functionality that automatically annotates specific narrative features in literary texts. To achieve this, a corpus of short stories is manually and collaboratively annotated based on a narratological tagset (cf. Gius, 2015; Gius/Jacke, 2015a). The automation is subsequently achieved in a combined approach of rule-based NLP methods and machine learning techniques (cf. Bögel et al., 2015a). However, the automation process is complicated by a specific interdisciplinary conflict: the textual phenomena literary scholars are interested in are often very complex and closely interconnected, which seems to significantly hinder the automation process. In this paper, we present our way of addressing this problem by way of example: we introduce the basic narratological concept of  temporal order and its theoretical prerequisites/application conditions; we show how the concept’s complexity causes technical issues in the context of automation and how converting  order to the stripped-down concept of  order switch significantly enhances the automation results; finally, we explain in which way the new concept is still suited for literary analysis.    Collaborative manual annotation of  temporal order  The discipline of narratology mainly deals with analyzing the (textual) features typical for narrative representation (cf. Meister, 2014). Narratological text analysis is based on often widely accepted narratological concepts or categories. The project heureCLÉA focuses on the operationalization of a subset of these categories: categories that describe temporal relations between a story and its representation. These categories are:  order (when does an event happen? – when is it told?),  frequency (how often does it happen? – how often is it told?), and  duration (how long does it take to happen? – how long does it take to tell about it?) (cf. Genette, 1980). While these categories are reckoned comparably simple and straightforward in narratology, collaborative manual annotation revealed that they are not. We would like to illustrate this using the example of  order (cf. fig. 1).    Fig. 1: Tagset order  Basically, the events of a story can either be presented in chronological order or the chronology can be interrupted by “flashbacks” ( analepses in narratological terms) or “flashforwards” ( prolepses). Each analepsis and prolepsis can further be qualified according to their reach and extent. Whenever anachronies occur, the whole text passage constituting the anachrony is annotated as either  analepsis or  prolepsis. Furthermore, anachronies may be nested: they can contain further anachronies (cf. fig. 2).    Fig. 2: Annotation of nested anachronies “Outside under the high gateway she stopped, breathing deeply. Her heart grew heavy; she had [just] pushed back the helping hand by which she had been guided since her youth; she knew none she could grasp now.” (Theodor Storm: Veronika).   The complexity of  order annotation is significantly increased by the fact that the analysis of  order showed to be dependent on a different narrative phenomenon: that of narrative levels. Narrative texts can contain embedded narrations, i.e., narrations within narrations. This occurs whenever a character in the story starts telling a story of their own (“new speaker”) or when counterfactual passages occur in a narration (“new world”) (cf. Ryan, 1991). As should be immediately plausible (at least for ontologically distinct narrative levels), it does not make sense to try and analyze the temporal relation between different narrative levels, i.e., between “actual” and counterfactual events in a story. It thus became necessary to establish an additional round of annotation preceding the annotation of  order: we first had to identify the embedded narrations in a story, so that temporal order could subsequently be analysed for each narrative level separately.    Automation: from  order to  order switch  From a computational linguistic perspective, modeling order phenomena imposes interesting challenges that can be grouped into two types: aspects inherent to the phenomenon and data-specific issues.  Phenomenon-inherent aspects Regarding characteristics of order phenomena, the aforementioned aspects of nestedness of order poses interesting challenges. As order phenomena are inherently nested, they yield a tree structure of annotations with multiple parent-child relationships. While there are models to formalize and predict tree structures (e.g., in the area of grammars), the prediction is orders of magnitude more complex than the prediction of linear or independent annotations, where complexity in this sense means the amount of training data required to sufficiently model the problem. In addition, the span of order annotations is highly heterogeneous comprising few tokens as well as multiple paragraphs. Finally, while a sequence classification approach would be suitable to annotate a sequence of tokens representing a specific order, additional aspects of the data at hand impede sequence classification. There is thus no clear annotation target that should be classified by a classifier.   Investigating the data To assess the annotation quality and thus feasibility of automation, we investigated the primary annotations of order phenomena. Investigating the number of different annotations for the entire training set (21 documents, see below) where two annotators agree with each other – which was the case for 90% of all annotations – revealed that there is an imbalance of annotations with seven times as many analepses (696) than prolepses (98). This imbalance poses three major problems for statistical modeling and machine learning:  sparsity,  noise and class- imbalance. Sparsity occurs for annotations that are not well reflected in the data set, such that a classifier cannot find enough evidence to integrate the annotations into its model. This issue is reinforced by noise, meaning inconsistencies in the annotations. One sequence of tokens could either represent a certain order phenomenon or just reflect a change of narrative levels, making it hard for the classifier to learn anything meaningful. Finally, class-imbalance imposes a bias on the classifier, resulting in the phenomenon that the minority class is rarely predicted or even not considered at all.    From order to order switches Investigating the annotations revealed that sub-sentences serve as boundaries for order switches. Thus, to solve the issues mentioned above, we do not attempt to classify order phenomena directly but instead predict for each sub-sentence whether it introduces a  switch of the order in the previous (sub-)sentence. While this is, of course, a simplification of the task, it allows us to model the task as a binary classification problem with a clear annotation target and alleviates the issue of sparsity because we do not distinguish between different types of order annotations. To generate training and test data from the original manual annotations of order, we determine all sub-sentences where the order annotation changes, and tag them as order switches.  The resulting annotation statistics are shown in table 1 and indicate that switching from order to order switch increases the number of positive instances in the training set to 1802, meaning that 1802 out of all sub-sentences introduce order changes. Note, however, that the issue of imbalanced data still exists.   annotation count percentage   order-switch 1802 12.3%   no switch 12871 87.7%   tab. 1: New training set based on order switches   Evaluation and experiments Our training set consists of 21 documents from various authors of the 20th century, comprising about 80,000 tokens in total (cf. Bögel et al., 2014). For evaluation, four additional documents were annotated. Overall, we use 21 features (presented in the appendix) to model order changes. We investigate different aspects of tense (e.g., whether a sub-sentence and the previous (sub-)sentence use the same tense), direct speech, temporal signals (cf. Bögel et al., 2015b), as well as structural features, e.g., paragraph boundaries. Finally, we add features to capture whether the sub-sentence represents a change of narrative levels rather than order. As mentioned above, the class-imbalance between positive and negative instances remains problematic. To reflect this during the evaluation, we perform randomized re-sampling (cf. Japkowicz/Shaju, 2002) with replacement on the training data which allows us to artificially adjust the spread between two classes. Table 2 contains the evaluation results for different spreads using Random Forests (Breiman, 2001) for classification. The more uniform the distribution of both classes and thus the lower the spread, the better the results. With the best setting (spread = 1:2), we are able to achieve a balanced result with a high F 1-score of 81.4%.    setting precision recall F 1    spread=1:6 20.4 14.7 17.1   spread=1:3 74.9 77.5 76.1   spread=1:2 79.1 83.9 81.4   spread=1:1 76.2 79.5 77.8   tab. 2: Evaluation results for different spreads Overall, the high performance confirms our hypothesis that breaking the complex task of predicting order phenomena into more manageable sub-steps yields promising results.  Nevertheless, we expect that even more complex narrative phenomena can be automatically annotated in the future. As simple narrative concepts have now been tackled successfully, their annotations could be exploited as features to predict more complex phenomena.     Conclusions Our aim to automate the annotation not only of basic and rather straightforward linguistically encoded temporal aspects like tense and temporal signals (cf. Bögel et al., 2014; Bögel et al., 2015b), but also of more complex phenomena like  order in heureCLÉA was a long shot. However, by cautiously reducing the concept’s complexity in active dialogue between computer scientists and literary scholars, we were not only able to yield good annotation results, but also to end up with a concept that is still of value for literary scholars: while deviations from the chronological presentation of a story cannot be automatically predicted in as much detail as in manual annotation, the automated functionality still serves as a robust heuristic pointing to temporally interesting passages upon which literary scholars can base their in-depth analyses and interpretations. By finding a way to include consideration of narrative levels in the automation, the original  order concept was not compromised with regards to its conceptual key features. The transformation from  order to  order switch is therefore yet another example of successful collaboration between literary scholars and computer scientists in heureCLÉA (cf. Gius/Jacke, 2015b): only a frequent exchange between the involved parties can yield results satisfactory to both sides. We are optimistic that this kind of collaboration has the potential to achieve a functional automated annotation of even more complex narrative phenomena – provided that the phenomena in question are of the kind that their analysis allows for a certain degree of inter-annotator agreement.   This precondition may be the critical factor in some automation attempts, e.g., the automated annotation of free indirect discourse that lacks a sufficient amount of reliable indicators (cf. Brunner, 2013). Its determination is rather interpretation-dependent and thus the phenomenon is barely qualified for high inter-annotator agreement.        Appendix: feature set for order switch prediction   tense  tense of target tense of target -1  same tense for target&target -1?  target contains imperative? target -1 contains imperative     direct speech  target starts direct speech? target -1 starts direct speech?  target within direct speech? target -1 within direct speech?     structural  target occurs after paragraph boundary? target is at beginning/end of sentence? length of target relative to entire sentence    temporal signals  target contains temporal signal? target starts with temporal signal? string of temporal signal first token of temporal signal preposition of temporal signal last token of temporal signal    narrative levels  target in conjunctive mood? target -1 contains utterance verb?      ",
       "article_title":"From Order to Order Switch. Mediating between Complexity and Reproducibility in the Context of Automated Literary Annotation",
       "authors":[
          {
             "given":"Bögel",
             "family":"Thomas",
             "affiliation":[
                {
                   "original_name":"University of Heidelberg, Germany",
                   "normalized_name":"Heidelberg University",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/038t36y30",
                      "GRID":"grid.7700.0"
                   }
                }
             ]
          },
          {
             "given":"Evelyn",
             "family":"Gius",
             "affiliation":[
                {
                   "original_name":"University of Hamburg, Germany",
                   "normalized_name":"Universität Hamburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00g30e956",
                      "GRID":"grid.9026.d"
                   }
                }
             ]
          },
          {
             "given":"Janina",
             "family":"Jacke",
             "affiliation":[
                {
                   "original_name":"University of Hamburg, Germany",
                   "normalized_name":"Universität Hamburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00g30e956",
                      "GRID":"grid.9026.d"
                   }
                }
             ]
          },
          {
             "given":"Jannik",
             "family":"Strötgen",
             "affiliation":[
                {
                   "original_name":"Max-Planck Institute for Informatics, Germany",
                   "normalized_name":"Max Planck Institute for Informatics",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01w19ak89",
                      "GRID":"grid.419528.3"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-08",
       "keywords":[
          "literary studies",
          "natural language processing",
          "interdisciplinary collaboration",
          "stylistics and stylometry",
          "data mining / text mining",
          "linguistics",
          "English",
          "crowdsourcing",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" When TEI P5 version 2.0 was published in 2011, scholarly editors who are interested in the writing process of literary works gained an important instrument for encoding their genetic Digital Scholarly Editions in TEI-conformant XML. After a long process of deliberation, this version of the TEI’s encoding schema incorporated a large number of modifications proposed by the TEI MS SIG’s Workgroup on Genetic Editions that aimed to re-evaluate the existing TEI tagset in order to facilitate the encoding of genetic phenomena (TEI Consortium, 2011). The Workgroup’s ‘Encoding Model for Genetic Editions’ (2010) reveals two major points of interest in this proposal: (1) the need for the ability to encode features of the document rather than those of the text; and (2) the need for the ability to encode time, sequentiality and writing stages in those documents’ transcriptions.  The main answer to the first point of interest was the introduction of the <sourceDoc> element (as well as its <surface> children), that was allowed to exist on the same hierarchical level as the <teiHeader>, <facsimile>, and <text> elements. Since the Text Encoding Initiative has (as its name implies) historically favoured ‘text’ over ‘document’, this can be regarded as a powerful statement to the TEI community that documents are as valuable as texts in textual scholarship, and that it should be possible to transcribe them as such. As a result, this encoding model has been gratefully adopted by editors who are taking a more document-oriented approach to the transcription of their materials – like those of the  Shelley Godwin Archive (S-GA) for instance (Shelley, 2013). The question remains, however, whether the use of this vocabulary is enough to classify a Digital Scholarly Edition as a ‘genetic’ edition. While the document will take up a central position in any genetic edition, the use of the ‘Genetic Editions’ document-oriented transcription model is not a distinctive feature of the genetic edition in itself.   The Workgroup’s second point of interest (the encoding of ‘time’) is much more central to genetic criticism. In ‘The Open Space of the Draft Page’, Daniel Ferrer makes a compelling argument that ‘the draft is not a text […], it is a protocol for making a text’, comparing it to a musical score that, though by itself inherently mute, can be interpreted as a set of instructions for a future performance (1998, 261). Likewise, a draft document leaves the writer with a set of instructions that help her transport the unfinished text from one writing stage to the next. The interpretation of these instructions, and of the distinction between different versions and writing stages, is one of the most important tasks of genetic criticism. This is what makes sequentiality such a key aspect of genetic editing: only by interpreting the draft materials as an interconnected sequence of writing acts can we expose the dynamics of the author’s writing process.  There are many ways of encoding this sequentiality in the transcriptions of draft materials, across varying levels of granularity. The Workgroup’s suggestion to use the <change> element to highlight distinct revision campaigns, for instance, effectively differentiates between individual versions of the same text when they are found within a single document. As Pierazzo and André’s ‘Proust Prototype’ demonstrates, this method can even be employed to sequence individual stages within a single version (2012). Going even further, projects like the CD-ROM edition of Willem Elsschot’s  Achter de Schermen (2007) and the  Melville Electronic Library’s TextLab software (2009-) analyze what John Bryant has called the internal ‘revision sequences’ of individual sentences (Bryant, 2008). The danger of analyzing the writing process on this small a level, however, is that the  mechanics of the writing process may start to interfere with the  dynamics of that writing process. From a genetic perspective, it is more important to expose the dynamic relation between the textual elements involved in a modification (e.g. ‘this is a substitution’) than the mechanical order in which that modification was made (e.g. ‘first this word was deleted, then this other word was added’). Since the exact writing sequence of such a modification is often impossible to reconstruct with any degree of certainty, consistently analyzing and sequencing all the work’s revision sites may introduce a number of hypotheses in the edition that the editor is not necessarily comfortable with committing to.   On the other side of the spectrum, analyzing larger macrogenetic processes across documents, the ‘Encoding Model for Genetic Editions’ refers to the TEI’s ‘Graphs, Networks, and Trees’ module, suggesting to encode the relations between documents as the <arc>s between <node>s in a <graph> element. Depending on the complexity of the writing process, this <graph> may result in an intricate data structure that can be used to visualize the chronology of the writing process on a highly abstract level. For writing processes that are less complex on the macrogenetic level, however, this model may be too much pain for too little gain, as a manually designed timetable could also do the trick. The Beckett Digital Manuscript Project’s approach to encoding sequentiality into its genetic Digital Scholarly Edition of Samuel Beckett’s works tries to seek a middle ground between these two extremes: rather than analyzing the way in which individual sentences were written, the BDMP’s encoding model allows the user to discover how those sentences were changed from version to version, across different documents. By linking related semantic clusters on the sentence level across versions, this model allows for the on the fly generation of a chronological overview of all the different versions of each sentence in the corpus. As such, this model combines the ability of comparing different versions of the same work of more macrogenetically oriented approaches with the higher granularity of more microgenetically oriented approaches.  After illustrating the challenges and opportunities of these different models of encoding sequentiality in genetic editions, this paper will demonstrate how the BDMP transcribes its genetic materials in view of visualizing their sequentiality in the edition’s ‘Synoptic Sentence View’ (see ‘Figure 1’). The paper will conclude by presenting an example of how this encoding model may also be used to interpret the macrogenetic writing sequence of individual documents by means of an animated visualization of the writing process of the first draft of Beckett’s  L’Innommable.    Figure 1: BDMP Synoptic Sentence View  ",
       "article_title":"Sequentiality in Genetic Digital Scholarly Editions. Models for Encoding the Dynamics of the Writing Process",
       "authors":[
          {
             "given":"Wout",
             "family":"Dillen",
             "affiliation":[
                {
                   "original_name":"University of Antwerp, Belgium",
                   "normalized_name":"University of Antwerp",
                   "country":"Belgium",
                   "identifiers":{
                      "ror":"https://ror.org/008x57b05",
                      "GRID":"grid.5284.b"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "encoding - theory and practice",
          "English",
          "xml"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" In this paper, we focus on media (and multimedia) production as a spatial practice of ni-Vanuatu youth. We explore the contexts and practices of youth media production in a contemporary postcolonial urban society and how organizational forces shape these practices. Articulation theory is used as a framework for thinking about the way that young urban ni-Vanuatu are negotiating community, institutional, and professional obligations as well as leveraging opportunities for producing media that demonstrate new modes of relating to place. The authors draw on long term coactivity and co-performance (Conquergood and Johnson, 93) via participant engagement with artists and producers from Vanuatu. Both authors have lived and worked in Vanuatu for over five years and much of our engagement with the young producers is through our roles with Further Arts – an NGO based in Port Vila. We employ a radical empiricism that blurs the boundaries between observer and observed – a form of dialogic performance that embraces and complicates diversity, difference, and pluralism (Conquergood and Johnson, 93). Madison describes it as living in “embodied engagement of radical empiricism, to honor the aural/oral sounds that incorporate rather than gaze over” (168, emphasis in original). A range of visual media productions is analyzed, principally video productions. We also draw on interviews conducted with members of Nesar Studio – a community access media production studio located at Further Arts – who constitute an emergent social category in Vanuatu, that is: young independent media producers or the ‘youth media crew’. The contemporary inflections of the precolonial ni-Vanuatu relationship to place reveal contingent openings for indigenous people to transform their world through ontologically liberating participation in media production (and consumption). From another perspective, multinational companies in the resource extraction (mining, forestry, fishing), agriculture, tourism, and creative industries, operate in ways that diminish Oceanic ontologies and subjectivities through processes that undermine local agency, knowledge, wisdom, value systems and biocultural diversity. But a dualistic framing such as this reinforces tired false binaries. It sets us a dangerous path to navigate through essentialism, exceptionalism and reductionism that, even if successfully negotiated ultimately leads to intellectual dead ends. There needs to be another way of understanding the: dynamic trajectories of visiting and returning, assembling and reassembling; the pluralism of Oceanian actuality, beyond the static divisions of rural villages and urban towns. In this paper we extend Clifford’s reading of Hall’s articulation theory (Hall; in Grossberg \"On Postmodernism and Articulation: An Interview with Stuart Hall\"), and demonstrate ways that “the partial entanglements of indigenous and local societies in global structures are not simply the world system’s unfinished business. They have their own roots and trajectories.” (Clifford, 475). We explore these roots and trajectories through – “a praxis of spatial articulation” (Tawa, 49). Dick has documented elsewhere forms of ni-Vanuatu cultural production and expression that reflect a chorographic engagement with place (Tawa; Olwig; Maxwell; Dick).  In this paper, we extend these ideas exploring the performative inflections of media production in a praxis of youth participation and spatial articulation. We will make visible the roots and trajectories of ni-Vanuatu youth and “the forces (the articulations) that create and maintain identities that have real concrete effects” (Slack, 126). Recognizing the joining and the un-joining, the assembling and the reassembling, the creating and the recreating, the articulating and re-articulating, imbues this approach with its efficacy (Slack; Clifford) and interdisciplinary approaches can converge to refine the way that we understand the contemporary media world (Horst, Hjorth and Tacchi). The paper begins with a description of the major historical and cultural forces and tensions that influence the locative identities of people in urban Vanuatu, integrated with a contextualization – a re-articulation – of the Vanuatu mediascape from the perspective of young urban ni-Vanuatu producers. We explore the specific case study of the establishment of Nesar Studio and how this studio is facilitating the emergence of young independent (and interdependent) producers, or the ‘youth media crew’ (henceforth YMC), as differentiated from the category of media consumers. The dissolution of observer and observed in the project, that is, our embedded-ness in the data, creates an opportunity for a deeply integrative analysis of contemporary indigenous engagements with media and place and multilateral strategies of articulation and de-articulation. Working in this radically empiricist way, it is often difficult to balance the requirements of the academy and the expectations of the community; it is difficult to “shape the data”, as it were, when one is both in and part of the data (Grossberg We Gotta Get out of This Place: Popular Conservatism and Postmodern Culture 55-56). Thus we have integrated discussion and analysis into the presentation of data to which it directly relates.  Despite major challenges, the young producers at Nesar Studio assert the importance, and indeed their ownership, of the structure and its organizational relationships – in particular with FA. This is evidenced in the extent to which they worked to maintain their momentum after massive damage from Cyclone Pam. ‘Nesar’ means ‘nasara’ in a local language – nasara is the Bislama term used throughout Vanuatu to talk about the ceremonial meeting place of a village for the intergenerational transmission of kastom knowledge and wisdom (through song, dance, art and other practices) is transmitted. Taking on this word and its connotations, Nesar Studio becomes a digital urban nasara in an age of increased use and access to telecommunications and media platforms as ways to transmit messages and knowledge. Providing the community with education on these tools is a powerful means to enact change through engaging people with their rights.  Developing the capacity of young media professionals in Vanuatu is not a simple process. Stakeholders must navigate divergent interests and compete for resources within an environment where the digital media and creative industries are poorly understood by state mechanisms and supported in an ad-hoc fashion by development partners at both national and regional levels. The bodies that do exist to advocate for and stimulate this sector, including the Vanuatu Cultural Centre, Secretariat of the Pacific Community, and the Pacific Arts Association (amongst others) have not been overly influential or consistently effective. This means that the role of shaping and nurturing the capacity of young media professionals rests in the hands of civil society actors, either in cooperation with, or struggling alongside, the dominant media and communications companies. Functions, processes, and forces historically familiar to ni-Vanuatu communities are deployed in the articulation of youth (id)entities through media production practices in urban settings. Only a small number of the more talented and adept media crew members of Nesar Studio have been able to penetrate the industry in an independent and professional capacity, acquiring employment in other cultural and media agencies or as contractors for national and international organizations. While these opportunities are few and far between, they require a certain level of application and assertiveness on the part of the media producer, something that Nesar Studio aspires to for its members, but perhaps does not emphasise enough since its media work is based foremost on the principles of collaboration and teamwork. Achieving this requirement for astuteness is furthermore hindered by social and cultural factors in Vanuatu that implies doing things communally and for the common good rather than outshining others for personal benefit. ",
       "article_title":"Urban Youth and Community Media: A Digital Place-Making Process in Vanuatu",
       "authors":[
          {
             "given":"Thomas",
             "family":"Dick",
             "affiliation":[
                {
                   "original_name":"Further Arts, Vanuatu",
                   "normalized_name":null,
                   "country":"Vanuatu",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Sarah",
             "family":"Doyle",
             "affiliation":[
                {
                   "original_name":"Further Arts, Vanuatu",
                   "normalized_name":null,
                   "country":"Vanuatu",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-25",
       "keywords":[
          "cultural studies",
          "English",
          "audio, video, multimedia",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The Single Interface for Music Score Searching and Analysis (SIMSSA) project is building tools and best-practices for performing large-scale document image recognition, analysis, and search on music documents. In this paper, we will describe a novel technique for providing cross-institutional music document image search, allowing for the creation of a search engine for the contents of the world’s music collections through a single search interface.  This paper will describe our methodology for building large-scale search systems that operate across institutions. We will describe the optical music recognition (OMR) process, which, like optical character recognition (OCR) for text, extracts symbolic representations from document images and places them in a structural representation for further processing. We will then describe our techniques for music analysis, extracting patterns for indexing the musical contents of these images into searchable representations. Finally, we present our efforts at building a system that will allow users to search musical documents from many institutions and retrieve the digitized document image.   Challenges Perhaps the most significant challenge to building a global document image search system is how to retrieve, store, process, and serve document page images. These images have been produced through mass digitization efforts by individual institutions. Aggregating document images to provide cross-institutional document search has traditionally been provided through centralized efforts, where a single organization collects digital images and performs document recognition (i.e., OCR) on them.  While this approach provides a central tool for users to search and retrieve document images, it has several disadvantages. It often requires significant storage capabilities, as the central organization must store and manage all the images from its partner institutions. There are logistical challenges, integrating cataloguing data from multiple document collections and maintaining up-to-date information and error-fixes from the partner organizations. There are also legal implications over the ownership and copyrights of document image surrogates, even on out-of-copyright documents (Allan, 2007). This typically requires negotiations and embargoes on who can access certain types of content which differ across partner institutions, and which must be applied at the central organization level (HathiTrust, 2015).  These technical, legal, and logistical challenges may be mitigated if the partner organizations were able to host and control access to their images directly. Until recently, however, direct access to the document images hosted by an institution was difficult as it required interacting with a wide variety of digital repository software, each with their own particular ways of storing and serving images. There were no standardized methods to specify how a document image could be accessed directly in these repository systems.   Interoperable Image Collections The International Image Interoperability Framework (IIIF) (Snydman et al., 2015) is a new initiative that attempts to standardize methods for retrieving digital images from an institution’s digital image collection. The IIIF specifies two mechanisms for this, the Image API and the Presentation API. The Image API sets out a standard URI-based request format to which IIIF-compatible systems must conform. Using this URI format one may specify the size, region, rotation, quality, and format of the requested image, as well as basic information about the image. The Presentation API is used to describe structural and presentation information about an image, or a sequence of images. The Presentation API is structured using JavaScript Object Notation (JSON), which may then be parsed by other software, and within which pointers to images using the Image API are stored.  To give an illustration, a digitized book may be represented as a IIIF Presentation API manifest file. Each page image within the book would be retrievable by a URI to the page image stored on a remote server. To view the book, the manifest would be loaded into a IIIF-compatible image viewer, which would then fetch and load each of the document images and present them in sequence.  The typical use case for a IIIF manifest is for the purposes of retrieving and viewing document page images. However, we are proporsing a novel application of IIIF as a standard interface to perform document image recognition tasks on digital collections from many different institutions.    Distributed Document Image Recognition We are building a web-based document recognition system, named Rodan, for performing optical music recognition (OMR) on large quantities of page images (Hankinson, 2014). Rodan is a workflow system, where different image processing, shape recognition, and document processing tools can be chained together to produce the sequence of discrete steps through which an image must proceed to extract the symbolic music representation of the content. Crucially, the exact cartesian positions of every musical symbol on the image are stored, providing a way to correlate the musical content with its physical position on the page image (Hankinson et al., 2012).  By providing Rodan with a IIIF Presentation API manifest, the document page images may be downloaded and the symbolic music notation extracted. However, rather than storing the image, we store just the IIIF Image API-formatted URI back to the original image. This allows us to discard the downloaded image file but point back to the image hosted by the originating institution. This approach eliminates the need to store and serve the images on our own systems, while still providing content-level access to document images hosted in different institutional repositories.   Music Analysis Within music notation there are several levels of representation. The most basic level is that of the symbol–the graphical element printed on the page. Structures such as melodies, phrases, and cadential patterns are built from these symbols, and exist in multiple overlapping hierarchies; a phrase might contain a number of cadential patterns. A music search system must understand the different levels and structures in a musical work, beyond simply understanding the individual notes, as these structures may form structural objects that a user may wish to retrieve. Within the SIMSSA project we are developing tools and techniques for extracting patterns from symbolic music representations using the Music Encoding Initiative (MEI) and other structured music representations (Schubert and Cumming, 2015; Sigler et al, 2015). The Vertical Interval Successions (VIS) (Antiilla and Cumming, 2014) tool we are developing provides a platform on which pattern analysis and extraction methods may be built. Like the document recognition process, VIS operates on the principle that computational music analysis is a sequence of tasks, where each task is responsible for extracting specific types of information that may then be passed on to subsequent tasks. In this way, the underlying symbolic representation of music notation may be used to build higher-level representations, which may then be sent to an indexing service for use in query and retrieval tasks.   Cross-Institution Indexing and Retrieval After analysis, the symbolic representations and the structures of the music documents are indexed for retrieval in a search engine. The IIIF Image URI associated with the page image, stored in the document recognition stage and carried along in the analysis stage, provides the mechanism through which the page image may be retrieved from host institutions in response to a query on the symbolic music contents. Through this system, musical full-text (or “full-music”) search can be performed on document images hosted and served from IIIF-compatible digital collections. Additionally, metadata and cataloguing data may be embedded in the IIIF Manifest, or linked to other machine-readable representations. This data may also be centrally indexed, allowing users to retrieve documents across institutions with useful textual searches such as titles, composers, or dates.    Impact and Future Work With cross-institutional music document image search, institutions may make their collections available to a broader audience without the need to host their images with a third-party service. With IIIF-compatible image and manifest services, the barriers to entry for these institutions to provide these capabilities is relatively low; the metadata and images are already part of their digital infrastructure. Furthermore, by serving the images and metadata directly from their own infrastructure, institutions can track collection usage patterns through their own server analytics. More general applications of this methodology will have significant impacts on libraries, archives, and other institutions’ document image collections. By providing machine-readable access to document images directly, third-party services for document analysis, including distributed optical character recognition (OCR) may be built and deployed. This will have implications on large-scale computational re-use of digital resources, and will open up document image collections to distributed analysis by a global audience.  We are currently in the process of building a prototype system that incorporates all elements of the process described in this paper. Our existing tools, Rodan and VIS, are currently being used in research and production, with a third system in development that will provide a platform for developing search and retrieval tools. One of our biggest unanswered questions concerns the human side of retrieval. With large quantities of recognized musical content, what sorts of tools and interfaces will people use to query the symbolic content of music documents? How will they conceptualize their symbolic music information needs, and what types of interfaces will they use to express these needs to a search system? What types of musical patterns will we need to extract from our musical documents to provide a useful symbolic search system? All of these questions we hope to investigate with a completed system.   ",
       "article_title":"Cross-Institutional Music Document Search",
       "authors":[
          {
             "given":"Andrew",
             "family":"Hankinson",
             "affiliation":[
                {
                   "original_name":"University of Oxford, United Kingdom",
                   "normalized_name":"University of Oxford",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/052gg0110",
                      "GRID":"grid.4991.5"
                   }
                }
             ]
          },
          {
             "given":"Reiner",
             "family":"Krämer",
             "affiliation":[
                {
                   "original_name":"McGill University, Montreal, Canada",
                   "normalized_name":"McGill University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01pxwe438",
                      "GRID":"grid.14709.3b"
                   }
                }
             ]
          },
          {
             "given":"Julie",
             "family":"Cumming",
             "affiliation":[
                {
                   "original_name":"McGill University, Montreal, Canada",
                   "normalized_name":"McGill University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01pxwe438",
                      "GRID":"grid.14709.3b"
                   }
                }
             ]
          },
          {
             "given":"Ichiro",
             "family":"Fujinaga",
             "affiliation":[
                {
                   "original_name":"McGill University, Montreal, Canada",
                   "normalized_name":"McGill University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01pxwe438",
                      "GRID":"grid.14709.3b"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "information architecture",
          "information retrieval",
          "music",
          "standards and interoperability",
          "GLAM: galleries, libraries, archives, museums",
          "content analysis",
          "archives, repositories, sustainability and preservation",
          "English",
          "image processing",
          "metadata"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Several decades ago, corpus linguists undertook systematic analyses of a lexical unit larger than the individual word but smaller than syntactical or phrasal units—multi-word sequences they called “lexical bundles” (Biber et al., 1999: 989; Biber and Conrad 1999: 58). Lexical bundles are extremely common collocations of three or more words, such as “I want to”, “it was a”, or “going to be a.” Unlike idioms or clichés, which are statistically very rare— and unlike word n-grams, which can occur with any frequency— lexical bundles are diffused throughout the language, occurring at least ten times per million words, and in many cases much more frequently (Biber et al., 1999: 989). In stylometry, there has been lively debate about the use of n-grams for authorship attribution. Our study differs in focusing on very common n-grams (i.e., on lexical bundles) rather than the rare n-grams studied by Vickers (2011), and it does so not for the purposes of authorship attribution, but rather to identify how fiction’s most common lexical bundles may have evolved over time, and may have done so differently in narration and in dialogue. Because of their frequency, lexical bundles are regarded as “discourse framing devices”: sequences of words that function as connective tissue in organizing discourse, expressing stance, or conveying referential status (Biber, 2006: 174).   While much of this ground-breaking research has focused on register—i.e., on differences in the use of multi-word sequences in various discourse contexts (like conversation vs. academic prose)—surprisingly little attention has been paid to the role of bundles in fiction. Biber et al. (1999) do not discuss results for fiction in their work, focusing instead on differences between academic prose and conversation (cf. also Conrad and Biber, 2004). Michaela Mahlberg’s corpus stylistics approach to Dickens’ fiction (2012) was pioneering in the use of lexical bundles to study fiction, but to date, no studies focusing on corpus- or register-wide trends in fiction have appeared. Jonathan Culpeper’s 2012 book is another recent study that uses bundles to study literary texts, but Culpeper’s object of inquiry is the early modern English dialogue, not the novel.  Yet the dramatic differences in the use of lexical bundles in other registers suggest that lexical bundles play an important role in building the discourse of fiction, and also that the bundles that occur in fiction will be significantly different than those occurring in other registers.  This paper analyzes the role of lexical bundles in a corpus of over 1000 novels published in Great Britain and America between 1800 and 1905. The two key contributions of the paper are: 1) to provide a taxonomy of the discourse functions of lexical bundles in nineteenth-century English and American fiction; and 2) to historicize that usage by tracking changes in our corpus over the course of the nineteenth century. We also provide some data about differences between narration and dialogue.  In expanding the unit of analysis beyond the level of the word, this paper also aims to intervene in recent methodological debates about digital humanities research on literary style. Much DH research on style in recent years—including some of the authors’ own work—has relied on a bag-of-words approach (Heuser and Le-Khac, 2012; Underwood and Sellers, 2012; for a discussion of the virtues of this approach, see Underwood, 2013). Since the terrain of higher-level lexical patterning in fiction remains under-explored, this paper contributes to the field both a methodological approach and a set of empirical results about the language of the nineteenth-century Anglo-American novel.    Methods Our corpus derives from two digital fiction collections (licensed to the Stanford University Libraries) by Proquest: “Early American Fiction” (805 American novels published from 1789 - 1875; and “Nineteenth-Century Fiction” (250 British novels published from 1782-1903). To extend the American corpus’ historical scope to match that of the British corpus, we added a collection of about 325 American novels published between 1875 and 1905. We selected these texts based on their inclusion in the  Annals of American Literature (Ludwig and Nault, 1989) and their availability in Project Gutenberg.  To identify bundles in a corpus with an uneven historical distribution of texts, we split each national corpus into four twenty-five year segments, creating eight sub-corpora. Each sub-corpus is derived from an identical number of words per author. Authors with fewer than 100,000 words in a particular period were not included; authors with more than this number were included by selecting 100 random slices of 1,000 words from all of their texts published in the period. Sub-corpora ranged in length from 900,000 words (U.S. publications, 1800-25) to 5.9 million words (U.S. publications, 1850-75), with a median length of 2.3 million words. Following Biber et al. (1999), we defined lexical bundles as the most commonly occurring tri- and quad-grams in our sub-corpora, with a threshold of at least ten occurrences per one million words (frequency per million [FPM]). After tokenizing each of the eight sub-corpora, we counted the number of occurrences of each unique tri-gram and quad-gram, normalizing by the length of the sub-corpus.   Departing from Biber et al., we decided to tokenize contractions as separate words (with “won’t” becoming “wo” + “n’t”), in order to place bundles involving contractions in direct comparison with their uncontracted equivalents (“i wo n’t” and “i will not” being both tri-gram bundles).  Any tri-gram or quad-gram with a frequency above 10 FPM in any of the eight sub-corpora was considered a potential lexical bundle. Biber et al. additionally required that bundles occur in at least five different texts in the corpus, in order to guard against the possibility of individual authorial or textual effects (991). Similarly, we excluded from our list of bundles those that occurred in fewer than three unique authors, so as to exclude idiosyncratic stylistic habits, as well as bundles containing character names or other novel-specific traits.   Biber and Conrad (1999) argue, and we agree, that lexical bundles often serve as mechanisms for bridging syntactic and semantic units. Accordingly, we allow lexical bundles to cross punctuation as well as clause and phrase boundaries.   We manually created an interpretive typology of the most frequent lexical bundles: the 150 most frequent tri-gram bundles (all with a median FPM across sub-corpora above 31), and the 240 most frequent quad-gram bundles (all with a median FPM across sub-corpora above 13). By looking at randomly-selected examples from the sub-corpora, each of these lexical bundles was annotated for its apparent function within fictional discourse. For example, “there was a” (5th most frequent tri-gram bundle, with a median FPM of 184) was annotated as “expletive”: grammatically, “expletives” are phrases of the form [“there” or “it” + to be], and within fiction, they provide a means by which the existence or effect of something can be easily introduced.   For example, in the 1835 novel by William Gilmore Simms, The Partisan: A Tale of the Revolution: “but there was a reckless audacity in his replies to the friendly suggestions of the landlord, which half-frightened the latter personage out of his wits.”    Finally, because corpus linguistics research has shown dramatic differences in lexical bundles across oral and written registers, we separately tracked their frequency in the narration and dialogue portions of our texts. To separate dialogue from narration, we used a tool developed by Grace Muzny at Stanford University. For this task we used a slightly reduced version of our corpus, which we curated by hand to ensure proper typographic markings of dialogue and thus a high precision and recall for the dialogue separation. We then replicated the corpus design described above, creating eight sub-corpora for each register, dialogue and narration. In this case, however, the periods are of twenty-year increments, from 1825 to 1905, due to the paucity of typographically well-formatted novels previous to 1825; also, each author contributes not 100,000 but 50,000 words to a particular sub-corpus, and authors with fewer than this number are not included in the sub-corpus.   Results Due to constraints of space, we can provide here only an overview of our findings in each area.  1. The function of lexical bundles in nineteenth-century fiction From our annotations (see “Methods” above), the most frequently-occurring bundles in fiction have one of the following functions:   Expletive (there was a, it was a, there was no, it was not, and there was)  Auxiliary forms (i do not, i did n’t, he did not, he had been)  Modal forms (i could not, would have been, i ca n’t, as if he)  Relative clause markers (that he had, that he was, which he was, that she was)  Temporal markers (for a moment, as soon as, the first time)  Partitive constructions (one of the, part of the, some of the)  Spatial markers (out of the, at the door, in the house)   Stance markers (do n’t know, i am sure, to be sure, seemed to be)   Of-genitive (the name of; the voice of; the heart of; the hands of)  Discourse organization markers (in spite of, as to the, in order to)     Figure 1: The most frequent 240 4-gram bundles, and most frequent 150 3-gram bundles, were annotated for their primary function in fiction; displayed here is the percentage of occurrences of all 390 annotated bundles within each unique functional type  The discursive mechanisms and requirements of fiction are immediately discernable in this ranked typology, especially when compared to prior work on lexical bundles in other discursive contexts. The prominence of expletives in fiction, for example, seems particularly significant, as they condense a fundamental gesture of storytelling into a phrase: once upon a time,  there was a... Expletives function in our corpus as a means of positing and sustaining a fictional ontology:  it was the best of times,  it was the worst of times. Similarly, the prevalence of temporal and spatial markers ( as soon as, in front of) help perform another fundamental task of fictional narration, that is, maintaining a complex and evolving network of persons, objects and events in their changing relationships to one another.  2. Historical behavior of lexical bundles On the whole, the historical trends we find in the use of lexical bundles suggest an increasing specialization of certain narrative functions over the course of the century, especially in the language of dialogue and its narrative orchestration. For example, the most frequent functional type of lexical bundle—the “expletive” bundles described above, like “ there was a”—are actually more frequent in dialogue than in narration earlier in the century, but are then increasingly adopted by fictional narrators as part of their machinery for coordinating the existence of fictional objects (see Figure 2).     Figure 2: The sum frequency per million of 21 lexical bundles annotated as expletives. Frequency is calculated in the dialogue and narrative portions, per period, of U.S. fiction  We also see evidence of historical changes that do not depend on internal register differences in fiction (narration vs. dialogue). Particularly striking is the general decline in the use of of-genitives in both narration and dialogue, in both British and American fiction (see Figure 3)—presumably replaced by the more concise Saxon genitive (’s) or by noun phrases (as in the center of the city vs. the city center). This may be an instance of a broader shift towards more informal, concise diction as the nineteenth century progresses, since the Saxon genitive and such noun phrases privilege efficiency of expression over the rhythmical advantages of the of-genitive.     Figure 3: The sum frequency per million for 35 lexical bundles annotated as genitive, per period, in the dialogue and narrative portions of U.S. and British fiction  Finally, we find some key differences in the use of bundles between dialogue and narration, largely having to do with national differences in colloquial expressions. In American dialogue, for example, over the course of the century we find an increased use of  going-to future forms (are going to; am going to; going to do; are you going; going to be), along with an increase in colloquial discourse markers (sort of thing; kind of a; all the time). In the British dialogue corpus, similarly, we find a decrease in formal and polite phrases over time (to be sure; i have no doubt; my dear sir; depend upon it; god bless you; as you please; by the bye; the honour of), and a concurrent rise in more informal modern phrases (in spite of; at any rate; here and there; now and again). Taken together, such trends suggest that this shift towards more informal bundles is particularly concentrated in fictional dialogue.   ",
       "article_title":"Building Blocks of Fiction: Lexical Bundles in Nineteenth-Century Novels",
       "authors":[
          {
             "given":"Marissa Lynn",
             "family":"Gemma",
             "affiliation":[
                {
                   "original_name":"Max Planck Institute for Empirical Aesthetics",
                   "normalized_name":"Max Planck Institute for Empirical Aesthetics",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/000rdbk18",
                      "GRID":"grid.461782.e"
                   }
                }
             ]
          },
          {
             "given":"Ryan James",
             "family":"Heuser",
             "affiliation":[
                {
                   "original_name":"Stanford University",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-02",
       "keywords":[
          "literary studies",
          "english studies",
          "data mining / text mining",
          "linguistics",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" This paper aims to explore the methodology and the potential of the “multi-text tool,” an advanced feature of the Tesserae Project. This tool is capable of identifying potentially interesting textual parallels between works, and collating all instances of these and similar phrases throughout the entire corpus of canonical Latin or Greek. As such it expands the researcher's ability to identify meaningful intertexts among Tesserae results, and makes it possible to quantify the literary influences which act upon a given work. Tesserae is an online tool that aims to automatically identify allusions and more general forms of intertext between ancient authors. The program can identify specific intertexts between two works, as in previous research which has brought to light new parallels between Lucan's  Bellum Civile and Vergil's  Aeneid. A standard Tesserae search identifies a possible intertext when the same words (regardless of inflection) appear within the same phrase in two different texts. This 'bigram' measurement of similarity has been demonstrated to capture roughly 67% of intertexts previously noted in scholarship. The multitext function begins with a standard two-text Tesserae comparison; all bigrams shared between the two texts are then compared to a corpus of additional works selected by the user.  Although the first aim of most Tesserae users is to discover previously unnoted allusions, not all bigram similarities discovered by a Tesserae search are allusions. The program's scoring algorithm attempts to sort the meaningful intertexts sought by the researcher from undesired, coincidental overlap by considering the rarity of the shared words and their proximity to one another. This method has shown to be at least partially effective, yet it does not fully predict the assesment of the expert researcher. Because Tesserae results can number in the tens of thousands, further means of identifying desirable intertexts is necessary, particularly when the number of results is expanded by new search features such as semantic matching, introduced in October 2015. Tesserae's multitext search can be used to trace the history of a phrase throughout the corpus, and thereby eliminate oft-repeated bigrams. This method assumes that phrases appearing in many previous works are less likely to represent allusions. In addition to identifying new allusions between two works, intertextual scholars often wish to measure the rate of connection between them. The level of active engagement between authors suggests the literary influence of a given work, and we propose a way to quanitfy this engagement. Using the multitext tool, the researcher can eliminate all widely-occuring textual parallels between potentially connected works and retain only unique results, then consider the rate of unique intertextuality against a baseline figure. Unlike the micro-level analysis of individual textual parallels, this macro-level analysis allows the reader to examine the intertextuality between works as a whole.  Our preliminary research shows that specific textual parallels with a large number of multitext results tend to indicate the use of generalized language rather than a meaningful communication between works. For example, Augustine’s language in  De Doctrina Christiana bears measureable resemblance to the language of Quintilian, Cicero, and Tacitus. Yet scholars have long argued that Augustine’s primary influence was Cicero. Our multitexts results show Augustine’s engagement with Cicero include a large percentage of “unique” intertexts, seldom picked up by other authors. His connections to other authors were mostly composed of “general” intertexts, which appear to consist of standard language used by almost all Latin writers engaged in the discussion of rhetoric. The same method has been previously used to examine Claudian's differential level of interaction with the various authors of Latin epic. The examples of Claudian and Augustine demonstrate the efficacy of unique intertextual connections as a measurement of relative literary engagement.  Although the elimination of oft-repeated language is effective for eliminating meaningless intertexts when considering works at the macro-scale, the scholar in pursuit of specific, meaningful intertexts should not eliminate these parallels. Sometimes a phrase with a large number of multi-texts results is actually more significant than one with fewer multi-text results. Certain phrases are quoted and alluded to so often that they become “viral.” These instances do tend to apply in cases where a Tesserae match is ranked particularly high, but it ultimately remains the responsibility of the reader to sort meaningful intertexts from coincidental ones.  The multi-text tool expands the potential big-data research in the field of Classics, allowing for quantitative stylistic analysis at a rate of speed and a level of specificity previously impossible. It is an essential addition to the basic matching of the Tesserae Project, and we hope to see it become an element of digital humanities programs in various academic curricula. From specific analyses of sets of texts to more comprehensive explorations of style, the multi-text tool should be a fundamental resource in the study of allusion in Latin literature. In addition to demonstrating the efficacy of this tool, in this paper we explain how to use the results generated by a multi-text search to build an empirical measurement of authorial engagement, beginning with how to run the module on a personal computer and concluding with how to meaningfully analyze its results and tailor its parameters for individual projects. ",
       "article_title":"Big Data and the Study of Allusion: an Exploration of Tesserae's Multitext Capability",
       "authors":[
          {
             "given":"James O'Brien",
             "family":"Gawley",
             "affiliation":[
                {
                   "original_name":"University at Buffalo, United States of America",
                   "normalized_name":null,
                   "country":"United States",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"A. Caitlin",
             "family":"Diddams",
             "affiliation":[
                {
                   "original_name":"University at Buffalo, United States of America",
                   "normalized_name":null,
                   "country":"United States",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "digital humanities - pedagogy and curriculum",
          "literary studies",
          "classical studies",
          "corpora and corpus activities",
          "data mining / text mining",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The purpose of this study is to determine the extent to which online roleplayers   Roleplay in videogames refers to users remaining “in-character” while playing the game. While roleplaying, users use performative communication, interacting in a manner suited to their character’s personalities, and remaining constrained within the established limits of the game’s backstory.  make use of language in the construction of narrative. Using computational approaches to text analysis, we compare in-game chatlogs of roleplayers with those of non-roleplayers. In doing so, we identify the particularities of the language of roleplay. Our findings are significant in that they macro-analytically demonstrate the differences between the narrative language of roleplayers and the objective-driven language of traditional players. While these differences have already been analyzed by other researchers, this study is the first to offer a thorough account of them using quantitative methods.  Roleplay is both a historic and present practice, dating as far back as ancient Greece (Corsini, Shaw and Blake, 1961). Throughout the 1970s and 80s, face-to-face roleplay surged in popularity with the advent of renaissance fairs and tabletop games such as  Dungeons and Dragons (Barton, 2008). Today, MMORPGs (Massive Multiplayer Online Roleplaying Games)   A Massively Multiplayer Online Role-Playing Game, or MMORPG, is a roleplaying videogame that takes place in a persistent Web-based world. This world is shared by a large player base interacting within a common environment for the purposes of socialising and playing the game in both an individual and collective manner.  are believed to enjoy over 47 million collective subscriptions. Research on the communication habits of online roleplayers provides insight into the contemporary refiguring of this long-running narrative and representational practice. The existing research on roleplay in video games relies on qualitative methods and focuses on games that have since waned in popularity. Our game of choice is  World of Warcraft, commercially one of the world’s most popular MMORPGs (Bainbridge, 2015), and thus a rich and legitimate source of data. We approach the analysis of this dataset using a range of computer-assisted methods.    Methodology and Results For the purposes of this study, we gathered two sets of data:  Chatlogs volunteered by  World of Warcraft roleplayers  Guild A donated strictly-roleplay chatlogs and non-roleplay chatlogs Guild B donated strictly-roleplay chatlogs    Chatlogs volunteered by  World of Warcraft non-roleplayers  Guild C donated non-roleplay chatlogs from a server without background roleplay Non-RP RP server consisted of the study authors’ chatlogs, from a server with background roleplay mixed with non-roleplay (task-oriented gameplay and casual chat)  this sample provided a middle ground between strictly-roleplay and strictly non-roleplay samples      We applied a variety of Digital Humanities methodologies to our dataset in an effort to extrapolate the differences in language of interest to this study. Our experiments included:  Most frequent (uncommon) word analyses to determine if any dominant themes differ across roleplayers and non-roleplayers.  As can be seen (see Fig. 1), the majority of words in the non-roleplay sets are related to objective-based gameplay, whereas in the roleplay sets, there is a dominance of emotive and descriptive words relating to interactions between characters.  Delta analysis to determine if chatlogs clustered by style depending on whether they were taken from roleplay or standard play.  We were able to obtain both roleplay and non-roleplay chatlogs from the same group of players (Guild A), allowing us to perform a valid stylometric analysis (see Fig. 2) to determine if there is a stylistic separation between the language used in roleplay and non-roleplay. We conduct the Delta analysis using R, which shows that the linguistic styles of these player groups are distinct.  Zeta analysis to establish words that were distinctive to each player group.  Our Zeta analysis reveals those words which are distinct to each group. Words particular to the non-roleplayers include objective-based terms such as “dps” (damage per second), and slang terms such as “nerf” (overkill). Meanwhile, the roleplayers tend to use descriptors of body language (e.g. “looks”, “nods”,“stares”, “glares” and “grins”) and otherwise emotive terms (e.g. “cheers”, “breath”, “growls”).  Topic modelling to determine any discursive trends across roleplayers and non-roleplayers.  Using non-negative matrix factorization, we ran topic models for a non-roleplay guild and two roleplay guilds (see Table 1). The topic models reinforce the previous findings, demonstrating that the focus of roleplayers is largely on narrative content, whereas non-roleplayers are predominantly concerned with game objectives.  Sentiment analysis to determine the extent to which roleplayers and non-roleplayers make use of emotive language.  Sentiment analyses of the non-roleplay chatlogs reveals that the language of non-roleplayers, as would be expected, remains, from the perspective of sentimentality, consistent throughout. The results of the analysis of the roleplayers (see Fig. 3) are interesting in that the sentiment oscillates to a significant degree, demonstrating a high degree of verbosity in the language that they use.   Significance While a difference between the play styles of roleplayers and non-roleplayers has often been assumed by researchers based on players’ self-identification, our study confirms a real distinction between roleplay and non-roleplay in terms of language. We have shown quantitatively that the language of roleplay is more emotive, narrative, and verbose than that of non-roleplay. MMORPGs can provide a platform not only for interactive play, but also for interactive storytelling or group narrative construction. Of particular significance is our finding that roleplayers frequently describe their avatars’ body language. Through use of descriptors such as “looks”, “nods”, “stares”, “glares”, “grins”, etc., roleplayers adapt to limitations in the control of their avatars by verbally recreating non-verbal cues. Analyzing the complex interplay of roleplayers' chat descriptions and the actual movements of their avatars can help us to better understand the challenges and potential of virtual forms of embodiment. It can also assist game designers in better accommodating the needs of a dedicated subpopulation of players. Methodologically, our choice to seek chatlogs from roleplay and non-roleplay guilds, rather than rely solely on our own in-game chatlogs, is a strategy that can be of use to other researchers. Guilds in  World of Warcraft frequently store—and, in the case of roleplay, even clean—their own chatlogs, which can provide a focused dataset.  Overall, this research demonstrates the potential for increased intersection between games studies and critical DH methods. Our study provides the statistical evidence necessary to further extrapolate how roleplayers use language to create their own storylines, invent character personalities, and develop meaning in the context of a game’s fictional world. In this paper, we will further detail the outcomes of our analyses, and offer a number of interpretations founded upon our results.    Figure 1    Figure 2    Figure 3  Table 1: Top NMF topics in non-roleplay Topic 0: kill mcs interrupt dps weapon switch power lol world corruption Topic 1: power world horde whirling corruption lol true yes dps flows Topic 2: rested feel longer learned blueprint watch frenzy goes thats just Topic 3: pst lf guild wod just heroic looking soo man raid Topic 4: loot corruption need help raid 100 tower arrogance free come Topic 5: honor kill warchief fallen blood garrosh iron drown crush hold Topic 6: lol like im yeah just ok think don good ll Topic 7: noodles river fish harmonious think fresh hour caught curse island Topic 8: roll weapons unfinished assembly begin line hey come gonna automated Topic 9: group role lol queued just ok members raid selected initiated Top NMF topics in roleplay (Guild A) Topic 0: looks look nods eyes smiles blinks head just grins right Topic 1: conversation 11 donnelly looks nods Sanctuary don Topic 2: says nods da looks ah nod liene Sanctuary ta horde Topic 3: ya like looks just know ta don oh drink good Topic 4: Sanctuary ze nods peace justice looks nod mercy oh Topic 5: looks smiles nods oh look ze good chuckles ve know Topic 6: looks hand eyes just don look like know head doesn Topic 7: looks look know just don nods like ve ll Topic 8: rhenold te looks look oh smiles like smile don tat Topic 9: nods Sanctuary looks like nod smiles look good don Top NMF topics in roleplay (Guild B) Topic 0: don right okay like ll think wolf oh um just Topic 1: alliance yay vote nay yes terms rose nods just debt Topic 2: portal makes gi strange gestures mog ha il team looks Topic 3: nods mallory accused looks rann tribunal crimes silvergear questions little Topic 4: nods ll need aye just guildB good time don know Topic 5: morgan hand smiles bowl ashford firestar child light looks nods Topic 6: horde iron time did looks nods know draenei speak foundry Topic 7: looks nods draconic form head asea blinks eyes moment takes Topic 8: alliance nods war GuildB looks king table aye order Topic 9: girl glitter oh okay like grins time took kneels maybe  ",
       "article_title":"Player-Driven Content: Analysing Textual Communications in Online Roleplay",
       "authors":[
          {
             "given":"James",
             "family":"O'Sullivan",
             "affiliation":[
                {
                   "original_name":"Pennsylvania State University",
                   "normalized_name":"Pennsylvania State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/04p491231",
                      "GRID":"grid.29857.31"
                   }
                }
             ]
          },
          {
             "given":"Michelle",
             "family":"Shade",
             "affiliation":[
                {
                   "original_name":"Pennsylvania State University",
                   "normalized_name":"Pennsylvania State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/04p491231",
                      "GRID":"grid.29857.31"
                   }
                }
             ]
          },
          {
             "given":"Ben",
             "family":"Rowles",
             "affiliation":[
                {
                   "original_name":"Pennsylvania State University",
                   "normalized_name":"Pennsylvania State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/04p491231",
                      "GRID":"grid.29857.31"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "English",
          "games and meaningful play"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Background and Motivation It has long been understood that Greek and Roman epic poems partake of a shared repertory of stock themes and typical scenes: the catalogue of heroes, the warrior arming for battle, the tempest, etc. These themes originally evolved under circumstances peculiar to oral-formulaic composition in archaic Greece, where they served the exigencies of real-time composition in performance by organizing poetic material into mnemonic chunks, each with a predictable internal structure (Rubin, 1995; Minchin, 2001). For later, literate Greeks, and still later for the Romans, who continued to develop the epic tradition, the use of these themes was no longer an aide to memory, but a complex intertextual gesture that formed one of the defining features of the genre (De Jong, 2014; Nünlist, 2009). The study of elemental, thematic building-blocks in epic and related genres has a long history, through the twentieth century and even earlier, including work by Claude Levi-Strauss, Milman Parry, and Vladimir Propp. While catalogues and typologies of epic themes exist ( e.g., Edwards, 1992), at the same time significant disagreement remains over their definition and delineation. Here we take first steps towards automated detection of theme in epic, defining features that target scene-sized samples of text under in bag-of-words model, and applying both unsupervised and supervised classification methods.  Our research is on intertextuality in Latin epics of the Flavian period, and in particular the ways in which an intertextual relationship at the thematic level can support or undermine specific verbal allusions at the sentence or phrase level. The ability to automatically detect text reuse in Latin epic at the scale of individual verse lines is provided by tools such as Tesserae ( http://tesserae.caset.buffalo.edu), Musisque Deoque’s co-occurrence search ( http://www.mqdq.it/mqdq/cooccorrenze.jsp), and eTRAP’s forthcoming TRACER framework ( http://etrap.gcdh.de), so that it is now conceivable to generate an exhaustive list of all such correspondences between any two texts. Yet the same efforts have shown that simple text-reuse search alone cannot capture all of the allusions noted by professional commentaries, and that sensitivity to scene-level thematic parallelism would improve both recall and precision over a model based on single verse lines or sentences (Coffee et al., 2012).  Tesserae has in fact produced a prototype thematic search using topic modelling to create scene-level features (Scheirer et al., forthcoming). At the same time, work by the Memorata Poetis project affiliated with Musisque Deoque is carrying out systematic manual tagging of themes in Latin vernacular poetry (Ciotti et al., 2015). Both approaches—the supervised and the unsupervised identification of themes—show promise, although neither has been fully integrated into its respective parent project’s verse-level, text-reuse search tool. In the ongoing work presented here, we attempt to combine elements of each approach, comparing unsupervised, bag-of-words classifiers at the scene-level with manual tagging for specific, selected themes. Our ultimate goal is to combine similarity scores for thematic parallelism with existing phrase-level search tools’ scores for text reuse in order to improve their accuracy.   Method We consider a corpus including the three more or less complete epics of the Flavian period—Valerius Flaccus’  Argonautica, Statius’  Thebaid, and Silius Italicus’  Punica—as well as three earlier poems to which our works of interest respond—Lucan’s  Civil War, Ovid’s  Metamorphoses, and Vergil’s  Aeneid. These works are initially subdivided into samples of 50 consecutive verse lines.  After lemmatization, tf-idf weighted feature vectors are calculated for each sample, dropping terms common to 50% or more of the samples, and the resulting feature set is reduced using principal components analysis to the most significant 500 components.  The samples still show a strong tendency to cluster by author despite the removal of very frequent words. Since our goal is to examine parallel variation across authors, we attempt to remove a characteristic ‘authorship signal’ from each author’s  œuvre before classification. A mean vector representing the author is thus subtracted from every one of his samples.  In the unsupervised approach, we then perform k-means clustering on the corpus as a whole. Because we do not have any  a priori set of themes to look for, we test multiple values of  k in an attempt to identify the most stable number of clusters. For each possible value of  k, we perform multiple repetitions and compare pairwise agreement among the resulting classifications using the adjusted Rand index. We also attempt to identify the most stable passages through comparison of error across repeated classifications using different offsets for our 50-line sampling window. For those passages most consistently classified across repetitions, we return to the text to investigate, through close reading, whether the passages indeed demonstrate meaningful similarity.  In the supervised approach, on the other hand, we begin by identifying scenes of interest using a set of  a priori thematic categories—for example, battle scenes and storms at sea. We then attempt to train a classifier that can distinguish these types. Earlier work on this project has shown that certain thematic contexts can be separated at a coarse scale (e.g, love versus war, generally) using principal components analysis alone; however here we will attempt to achieve finer precision using support vector machines, a popular approach in stylometric tasks and one that has previously been applied to intertextuality in Latin in particular (Forstall et al., 2011).    Results This work is ongoing, and one of our goals in presenting at DH 2016 is to elicit feedback that will help to shape the design of future experiments. At the same time, early results suggest that the supervised and unsupervised approaches may indeed meet in the middle. For example, many of the most stable passages in the unsupervised classification belong to a ‘nautical’ theme which includes as a subset the storm theme identified by human tagging. Across 100 different classifications of the six epics, using differently-offset sampling windows, the passages with the lowest rate of disagreement included characteristic descriptions of tempests in  Aeneid 1 and  Argonautica 1 (Figs. 1–2). The same class, and a high degree of stability, also marked nautical chase sequences in  Aeneid 5 and  Argonautica 8 as well as the recounting of the Argonauts’ story in  Thebaid 5.    Figure 1. Vergil’s Aeneid, book 1, after k-means classification with 8 classes. The line shows disagreement between repeated re-classifications. Shaded bands at bottom show classes in one randomly-selected trial.    Figure 2. Valerius Flaccus, Argonautica, book 1, after k-means classification with 8 classes. Line shows disagreement among repeated re-classifications; shaded bands show the same classes as in Fig. 1.    Implications The concrete goal of this project is to provide a thematic feature set for automated intertext-detection, compatible with the phrase-based results of existing tools, so that, for example, otherwise slight verbal correspondences can be promoted where they occur within thematically parallel passages and thus are likely more interesting to readers. More generally, we would like to understand how stock themes function in epic intertextuality, and whether they can indeed be sufficiently modeled using a bag-of-words approach. To the degree that they can, we can say that thematic features are in fact made of words and exist on a continuum with the word-level bigrams matched by Tesserae, for example, instead of representing ‘deeper’, semiotic structures distinct from the word forms that appear on the ‘surface’ of the text (as in, e.g., Levi-Strauss, 1955)  ",
       "article_title":"Approaches to Thematic Classification for Latin Epic",
       "authors":[
          {
             "given":"Christopher W.",
             "family":"Forstall",
             "affiliation":[
                {
                   "original_name":"Université de Genève, Switzerland",
                   "normalized_name":"University of Geneva",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/01swzsf04",
                      "GRID":"grid.8591.5"
                   }
                }
             ]
          },
          {
             "given":"Lavinia",
             "family":"Galli Milic",
             "affiliation":[
                {
                   "original_name":"Université de Genève, Switzerland",
                   "normalized_name":"University of Geneva",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/01swzsf04",
                      "GRID":"grid.8591.5"
                   }
                }
             ]
          },
          {
             "given":"Nelis",
             "family":"Damien",
             "affiliation":[
                {
                   "original_name":"Université de Genève, Switzerland",
                   "normalized_name":"University of Geneva",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/01swzsf04",
                      "GRID":"grid.8591.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-13",
       "keywords":[
          "classical studies",
          "stylistics and stylometry",
          "data mining / text mining",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The dynamics of language change over time are most evident in the lexicon component of natural languages. In particular, the gradual semantic changes words may undergo have a strong effect on the comprehension of historical texts by modern readers. Yet, efforts to automatically detect and trace this lexical evolution are scarce. Our study follows the work of Kim et al. (2014) who detected lexico-semantic changes in English texts over the 20 th century via a series of neural network language models. Our models were trained on the German part of the  Google Books Ngram   An  n-gram is a sequence of  n words plus information on their frequency/probability of occurrence for a given corpus. The available version of the corpus does not consist of running text, but of  n-grams instead.    corpus (Michel, et al., 2011; Lin et al., 2012), which covers over 657k German books. Such models have the particular advantage that they can be queried for the semantic similarity of arbitrary words. We tested this query option by sampling nouns from  Des Knaben Wunderhorn (Arnim and Brentano, 1806-1808), a collection of German folk poems and songs from the German Romantic period. The choice of this volume is merely motivated by our interest in the literary period it belongs to. We detected interesting semantic changes between 1798 (often taken as the starting point for the German Romantic period) and 2009 (last year in the Google corpus).     Methods Using the specific contexts in which words appear in order to determine the (distributional) meaning of words is an old idea from linguistic structuralism (Firth, 1957). For a long time, this appealing approach could not have been seriously investigated due to the lack of suitably large corpora and adequate computational power to deal with distributional patterns of words on a larger scale. Thus, only few studies on automatically detecting semantic change have been conducted up until now, with a clear focus on the high-volume data provided by Google Books. This collection is widely popular due to its immediate availability and enormous coverage despite well-known problems stemming from both the quality of optical character recognition (OCR) and the sampling strategies used to compile it (Pechenick  et al., 2015).   The  Deutsches Textarchiv (DTA) can be considered as a counter example, at least, as far as the quality of OCR is concerned. Yet, DTA suffers from tremendous size limitations in comparison with the (German portion of the) Google corpus, since this corpus for historic German texts contains only about 2.4k texts ( http://www.deutschestextarchiv.de/list).    Early approaches towards modeling lexico-semantic change patterns used frequency and bi-gram co-occurrence data (Gulordava and Baroni, 2011), as well as (context-based) classifiers (Mihalcea and Nastase, 2012). Riedl et al. (2014) built distributional thesauri to cluster similar word senses. All of these approaches detected lexico-semantic changes between multiple pre-determined periods. In contrast, neural network language models can be used to detect changes between arbitrary points in time, thus offering a longitudinal perspective (Kim et al., 2014; Kulkarni et al., 2015). In our experiments, we use a skip-gram model, a simplified neural network that is trained to predict plausible contexts for a given word, thereby generating (computationally less expensive) low-dimensional vector space representations of a lexicon (Mikolov et al., 2013). Despite their simplicity, neural network language models are a state-of-the-art approach, with details concerning ideal implementation solutions and training scenarios still being under dispute (Baroni et al., 2014; Schnabel et al., 2015).    Experiment We trained our models on 5-grams spanning the years 1748 to 2009, using a uniform sampling size of 1M 5-grams per year; the first 50 years were used for initialization only. Test words for high-lighting semantic change patterns were selected from  Des Knaben Wunderhorn by identifying the ten most frequent nouns, i.e.  Gott [‘god’],  Herr [‘lord, mister’],  Liebe [‘love’],  Tag [‘day’],  Frau [‘woman, miss’],  Mutter [‘mother’],  Herz [‘heart’],  Wein [‘wine’],  Nacht [‘night’] and  Mann [‘man’]. For each of these ten nouns we selected the three words most similar to them (according to the cosine of their respective vector representations) during 1799 and 1808 and between 2000 and 2009, tracking how the similarity of these words developed between 1798 and 2009. The programs used for our experiments and resulting data are publically available via GitHub.   https://github.com/hellrich/dh2016      Results The cosine similarity between the 1798 and the 2009 vector representation of the ten test words is rather high, ranging from 0.72 for  Mann to 0.84 for  Wein, thus showing only minor semantic changes. Manual interpretation of their most similar words revealed an interesting change for  Herz (see Fig. 1) that is nowadays more similar to other anatomical terms (such as  Gehirn [‘brain’],  Lunge [‘lung’], or  Ohr [‘ear’]) and less likely to be used metaphorically (such as indicated by  erschrecke [‘frighten’], or  Gemüth [archaic for ‘mind’]). As this change predates Google Books’ tendency to overrepresent scientific texts (at least for English, cf. Pechenick et al., 2015) this finding can be assumed to be an example of true lexico-semantic change. The example also demonstrates a need for a metric incorporating frequency information and normalization of input, since  Gemüth is an archaic form for  Gemüt non-conformant with modern German spelling conventions, although it is rated as currently similar to  Herz.  Fig. 1 Lexical semantics of  Herz [‘heart’] as expressed by its similarity with six other words; similarity-axis not depicting whole range of possible values (0–1)       Conclusion This research note has gathered preliminary evidence for the feasibility of corpus-driven studies into German diachronic semantics. We advocate a computational, neural network-based approach where the evolution of lexico-semantic changes is traced by similarities of distributional patterns in the context of words over time. Looking backwards for semantic changes is, however, constrained by the quality and quantity of linguistic data available. While the primary corpus we use for determining semantic evolution patterns, the Google Books Ngram corpus, is remarkably large, it suffers from a idiosyncratic sampling policy, as well as OCR shortcomings and even more advanced issues, such as the absent normalization of historic orthographic variants. Other historic corpora dealing with the latter quality issues (such as the  Deutsches Textarchiv) are plagued by their comparatively minuscule size.   Future research in Digital Humanities, besides dealing with these issues, will exploit the similarity data in order to make proper use of them under a humanities’ perspective and, thus, hopefully determine the added value of such computational results. This can be achieved by incorporating complementary types of data (e.g. historical, economic ones) to render additional evidences to change patterns. Since this is a huge and complex task, we plan to make our similarity data publically available on a website, together with an easy-to-use interface, as a humanities tool for comparative, diachronic lexico-semantic studies, with several user-adjustable parameters (e.g. different grain sizes of time intervals, alternative ranking metrics,  etc.). From a methodological perspective, we plan to focus our research on protocols for training models covering long timespans, a metric to measure the quality of historic language models (probably including the need for a manual evaluation) and a way to include frequency information–a word which is no longer used cannot be said to be unchanged in its semantics. Such a system would ideally be tested by an in-depth study of the semantics of carefully selected words, including a comparison with prior, hermeneutically guided work in the humanities as a rich, yet completely informal background theory.    Funding This work was supported by the DFG-founded Research Training Group \"The Romantic Model. Variation - Scope - Relevance\" [grant GRK 2041/1].  ",
       "article_title":"Measuring the Dynamics of Lexico-Semantic Change Since the German Romantic Period",
       "authors":[
          {
             "given":"Johannes",
             "family":"Hellrich",
             "affiliation":[
                {
                   "original_name":"Research Training Group \"The Romantic Model. Variation - Scope - Relevance\", Friedrich-Schiller-Universität Jena, Jena, Germany",
                   "normalized_name":"Friedrich Schiller University Jena",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/05qpz1x62",
                      "GRID":"grid.9613.d"
                   }
                }
             ]
          },
          {
             "given":"Udo",
             "family":"Hahn",
             "affiliation":[
                {
                   "original_name":"Jena University Language and Information Engineering (JULIE Lab), Friedrich-Schiller-Universität Jena, Jena, Germany",
                   "normalized_name":null,
                   "country":"Germany",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "semantic analysis",
          "natural language processing",
          "german studies",
          "data mining / text mining",
          "linguistics",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Visualizations have a central role in the Digital Humanities. The second most popular author-chosen topic word of DH2015 was visualization (cf. Figure 1 from Weingart, 2015). Yet, when one revisits all accepted abstracts at DH2015 with the keyword “vis(z)ualization”, one may notice that not many of these texts indicate which libraries were used for interactivity (only some mention D3), and even fewer had direct links to their websites for testing (many were prototypes).     Figure 1. Fragment from graphic with the topical coverage of DH2015 (Weingart, 2015)  One of the more common visualizations are those of relational data. In Katrien Verbert’s more thorough survey of interactive visuals in DH2014's presentations, relations-visualizations represent 50% of all prototypes (that is 29 out of 58). The most popular way to represent relationships are uni- or bi-directional graphs (23 out of 29) and only one of them used a matrix to display connections (see figure 2 from Verbert, 2015). To crack open the discussion on the pros and cons of this visualization technique, I will show how I answered some questions of cultural history, more specifically of Latin American Literary History, with a tailored interactive matrix (to this the visualizations visit:  http://www.sgutierrez.seewes.de/).     Figure 2. Visualization techniques used by work presented at DH14 (Verbert, 2015) Theoretical Framework The history of associations is a goldmine for the intellectual queries of scholars interested in literary and intellectual history. In Latin America, for instance, the appearance of Angel Rama's posthumous book  The lettered city (1984), lead to a series of studies concerned with the constitution of enlightened groups, especially in new nation-states. As the capital city of one of the most powerful ex-Spanish Colonies, the lettered network in Mexico City makes an interesting case study. However, despite the valuable monographic studies on this subject (most notably Perales Ojeda, 2000 and Sánchez, 1951), which register around 200 active literary societies during the 19 th century, no overview on the subject has been possible and not all questions have been resolved. How diverse or homogeneous were these groups? Who were their most recurrent actors? Were certain generations more likely to be part of groups from a certain literary movement? I will propose a way of using visual and interactive displays of literary societies’ membership data to answer these three questions.  Before me, others have sought to gain new insights by exploring the possibilities of data modeling to understand modern sociability. The  Berliner Klassik Gesseligkeit Datenbank (The Societies Database of the Berlin Classical Period, 2013) aimed to understand the cultural bloom of the early 19 th century and Stanford’s  Salons Project (2012) was designed to get an understanding of the social composition of the French Enlightenment network. However, to date, there are no online dynamic visualizations of either of these projects. Methodology a) data collection The network's information was obtained by scrapping each associations' entry in the Encyclopedia of Mexican Literature (ELEM). Since ELEM is the most complete source of biographical data for 19th century Mexican writers, it is very unlikely that information about these writers can be found elsewhere; thus, I only considered members with an entry in this source. This procedure leaves out many characters, but it is at least representative of the known characters of the lettered city. It contains information of 51 associations (founded between 1808 and 1894) and of 195 members born between 1781 and 1870. b) data model The database derived from these two nodes (members and associations) was modelled to answer my research questions, but its metadata is designed to be reusable: members were assigned standard identifiers using Jeff Chiu's VIAF reconciliation service for OpenRefine (Chiu, 2015), and neutral aspects about these nodes -- such as birth and death dates or founding and closing dates—were included. In addition to these neutral aspects, I added two categories that scholars have used to cluster literary characters and societies, namely, generations and literary movements. c) visualization My first attempt was to follow the most common visualization for networks in the digital humanities, the Gephi-spaguetti (see figure 3). I did everything I could to enhance readability. I set the societies-nodes’ size according to the number of connections they had with other associations and the thickness of the edges to vary depending on how many common members two groups had. Even more, in order to get a chronologically-ordered layout I used Spekkink’s useful plugin, the Event Graph Layout (Spekkink, 2014). From this display, I was able to confirm kinship-relationships between societies. That is, that although persistence was never their  forte, when one looks at the number of members that went from an extinct society to the next new one, one gets the impression that despite the ephemeral nature of these groups, there was still a type of continuity among them.      Figure 3. Network visualization where nodes are 19th century Mexican societies and edges represent the number of common members between them Yet, even when I created an interactive graph with Sigma.js it was very hard to read the quantitative differences between my nodes’ connections. On the one side, I was interested in creating a visual display that allowed interactivity, providing end-users with both additional information for each data-point and the possibility to select specific ranges of the network. On the other, I wanted to control the order of my data and the quantities’ color-coding for readability. The solution was provided by a Python-library, Bokeh. Results The first visual I created was a co-occurrence matrix where each literary association was compared against all others. This display allowed me to understand how many members each pair of associations had in common. In order to enhance the identification of meaningful co-occurrences, I followed the principles of sequential color schemes –where low data values are represented by light colors and high values by dark ones (Wyssen, 2014) – and I assigned different colors and alphas according to the quantities’ distribution of my data: associations’ pairs above or equal to five common members were coded in red, and below five, in blue (see Figure 4). Additionally, I set different and consecutive alpha values to each glyph according to their exact value (intersections of less density had lower alphas). This display was helpful to address the question on the diversity or homogeneity of literary societies: with this tailored visual I was able to identify the homogenous hub of ten literary associations around the  Liceo Hidalgo that had a considerable amount of common members, suggesting that although they had different approaches they were nonetheless constituted by recurring members (cf. Figure 5).    Figure 4. Co-occurrence matrix of literary societies ordered by the sum of common members with other associations    Figure 5. Selection of societies with the highest common-members’ density    Figure 6. Associations’ co-occurrence matrix by founding date Moreover, when I changed the order of the matrix (by founding date, see Figure 6) I was able to understand these connections in its temporal dimensions. For instance, when zooming on the glyphs for the  Liceo Hidalgo (see Figure 7) one can easily identify the previous societies with which this association had enough common members to consider them as predecessors, or which other later groups could be considered as successors for the same reasons.      Figure 7. Liceo Hidalgos’ co-occurrences, a box-selection of the associations’ matrix by founding date Finally, in another color coding of the glyphs (by the literary movement that was in vogue when these societies were established) I could identify which societies of the same period had more common members (see Figure 8).     Figure 8. Societies’ co-occurrence matrix with literary movements’ color-coding. Conversely, I created another matrix –this time comparing members— which was useful to understand which characters co-occurred more often in the same associations and thus address the second question, namely, which were the most recurrent actors in the network (see Figure 9). The result: thirteen characters formed the core of actors who were most involved (see Figure 10). This information, however, could have been obtained with a simple bar-chart. The difference in perspective that this matrix offers is that it allows the user to see that these characters were not only in many but also similar associations (which can be retrieved by hovering the glyphs), and, additionally, it makes evident how proportionally small this core is when compared to the whole matrix.    Figure 9. Members’ co-occurrence matrix ordered by maximum summed values.    Figure 10. Members’ co-occurrence snapshot done with the selection tool of Bokeh’s visuals. Finally, to address the second question –namely, the correlation between generations and literary movements–, I created a matrix where associations were ordered by founding date on the y-Axe, and members by birth date on the x-Axe, and where the colors were coded according to their correspondent literary movement (see Figure 11). The dark colors of the glyphs represent the literary movement of a given society (all the blue ones are from the neoclassic movement, for instance), and the light colors in the background represent the members’ generations (for example, in light orange -in a vertical division- are all the members of the  Renacimiento generation). Arranging them like this enabled me to take snapshots of different societies and observe the generations’ patterns of membership-adscription. For instance, I could note that although the group formed around the  Renacimiento magazine was heavily constituted by its homonym generation (see Figure 12), almost half of its members were born in the timeframe of the previous generation (coded with a light yellow background).     Figure 11. Generations versus literary movements: a co-occurrence matrix                   Figure 12. Active members in  Grupo de la Revista el Renacimiento  Conclusions In this paper I have shown how customized visualization of modeled data can enable new readings and lead to new understandings of how societies were formed in a key period of national history. Among other things, matrices help us “see” connections between previous categories of literary history (like generations and literary movements), between societies, but also between members, thus supporting new narratives of the lettered city were the alleged homogeneity of this “elite” group can be seen in a nuanced perspective that integrates complexity without sacrificing abstraction. ",
       "article_title":"New Maps for the Lettered City: a Data Visualization Exploration of 19th Century Salons in Mexico",
       "authors":[
          {
             "given":"Silvia Eunice",
             "family":"Gutiérrez De la Torre",
             "affiliation":[
                {
                   "original_name":"Würzburg Universität, Germany",
                   "normalized_name":null,
                   "country":"Germany",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-05",
       "keywords":[
          "visualisation",
          "spanish and spanish american studies",
          "English",
          "databases and dbms"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Humanists – inclusive of digital humanists – are preoccupied with telling stories. Some of our most interesting subjects, however, have left only the barest of marks on historical records. Their stories are among the most captivating, but also some of the most difficult to access. This paper knits together recent trends in digital humanities practices that have helped us to elevate unrepresented voices with a discussion of how to elevate the marginalized within the DH community. It showcases select projects that undermine archival silences.   These might include work like Ben Schmidt’s, elaboration upon late twentieth-century cliometrics and use of “big data” methods to explore historical sources ( http://benschmidt.org/projects/digital-humanities-research/); maps like Vincent Brown’s “Slave Revolt in Jamaica” which uses sources produced by slaveholders to argue for the agency and tactical prowess of enslaved people (http://revolt.axismaps.com/map/); and Michelle Moravec’s use of metadata to “unghost” lesbian women in the past (http://michellemoravec.com/).    It then argues that digital humanities practitioners should add these theories to the collection of tools currently used to forward social justice projects in DH spaces.    Elevating the Archivally Silenced Various methodologies have been adopted to address the problem of how to tell stories about people who left behind few records. In the 1970s and 1980s, practitioners of “history from below” worked to elevate narratives about “people with no history,” by chronicling the everyday lives of peasants and non-elites. At the same time, practitioners of the “new social history” turned to cliometrics – and adopted methods that would be familiar to those who work with “big data” today – to highlight trends about marginalized peoples from historical data like censuses, probate records and financial documents.  There have been various resurgences and developments in these methods in the intervening four decades. These include practices of reading archives “against the grain” to get at the unstated assumptions that historical actors made about those they held power over. They also include theoretical approaches that advocate the reading of silences to understand those whose voices were intentionally obscured by official recorders and gatekeepers.    Marginalizations Within DH Questions about whose voices are elevated and whose are silenced have also long been a theme in DH scholarship and discourse. These questions seek to unpack the ways in which DH as a field is exclusionary. This former is a much (though still not enough) referenced problem in panels at former DH conferences, which have asked how DH research can address (and remedy) social problems.  Digital humanities scholarship has also begun to address problems of access within the broader DH community, and the barriers erected to women and people of color in particular. For example, Adeline Koh has argued that we need to examine the ways in which DH publics are constituted, in order to better understand the creation of “limits of the discourse that defines the idea of a digital humanities ‘citizen.’” Similarly, Tara McPherson has argued that we must see the evolution of DH as a field shaped by structural inequalities – of race, class and gender – which accompanied the rise of computation technologies.    A Knitted View   These are much needed interventions, and help us to understand the evolution of our field as one in which certain groups have been marginalized and others have been centered. These conversations also mirror methodological debates within history about whose voices to elevate, and under what circumstances. This paper complements extant work by arguing that theoretical interventions concerning current structural inequalities must be brought to bear on the past, and that digital methodologies are ideally suited to elevating subsumed voices in the present. It further demonstrates that these projects, the theories that underlie them, and current work to make DH more equable should be read together to further the practice of digital history and humanities “from below.  ",
       "article_title":"Digital History “From Below”: a Call to Action",
       "authors":[
          {
             "given":"Anelise Hanson",
             "family":"Shrout",
             "affiliation":[
                {
                   "original_name":"Davidson College, United States of America",
                   "normalized_name":"Davidson College",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02f7k4z58",
                      "GRID":"grid.254902.8"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-01",
       "keywords":[
          "cultural studies",
          "digital humanities - diversity",
          "history of Humanities Computing/Digital Humanities",
          "historical studies",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Faceting and Mining Networks   The network graph is one of the best-known and overdetermined of all data visualizations. And it suffers, more than most such modes, from the problem of fetishization. When non-specialists see network graphs, which are information-heavy, aesthetically appealing, cognitively suggestive, and yet curiously hard to read, their first reactions are often along the lines of “Oh wow!”, “Cool!”, or “Neat!”. The corollary to admiration, for members of the academy, however, is often distrust.   The problem is two-fold. Critical thinkers are justly skeptical of enthusiasm, since enthusiasm can foreclose critical engagement. But the network graph’s prominence as a mode of data visualization, as featured in new outlets like the New York Times, or in prominent projects like Mapping the Republic of Letters  or  Six Degrees of Francis Bacon , makes it uniquely vulnerable to the hermeneutic of suspicion (see Elijah Meeks, “The Digital Humanities as Lightning Rod”). If the digital humanities are  still  distrusted in some more conservative corners of the academy — such as mine, eighteenth century english literary manuscript studies — then network graphs, because they have become synecdochic for digital humanities writ large, are doubly distrusted.    This short paper addresses three states of addressing this skepticism head-on, embedding network graph literacy in the context of a larger disciplinary argument. Those three stages correspond to three key perspectives practitioners of network analysis can assume when wanting to persuade skeptics of the probative value of network graphs, to show how condign those graphs are to traditional “analogue” analysis, and to build a persuasive argument within your own field.  Those three perspectives are: targeted data gathering; graph design, and argument design.  Much of my own work centers on networks of distribution of Jacobite manuscript poetry from 1688 — 1750. Once I knew that I would be using network graphs to illustrate the richness of the social and material facts I had uncovered, I started enriching my dataset with an eye to encoding metadata to reveal those richnesses as precisely as possible. Those metadata categories include size of manuscript, number of copies of each poem in circulation, names of manuscript collectors, languages contained in each manuscript, dates of manuscript, and so on. I entered this metadata on each future node in my two sets of network graphs (one of individual poems, one of manuscripts).  Graph design, the next stage, was just as important. Having settled on a layout in Gephi that I felt reliably showed relations among objects in a way that I knew I could explain simply (such as proximity as a function of similarity; distance as an index of relative dis-similarity) I then worked to combat the visual fetish character of the network graph by partitioning the nodes multiple times while keeping them within the same layout. This meant that one my audience had had time to accustom themselves to the layout, successive graphs with differing color schemes, node sizes, and edge weights would show them the facets, the richness, of the dataset. By showing the same graph layout faceted in multiple different ways, my audience could comprehend the layout as a function of multiple and overlapping functions.  Argument design, the last stage, is a way of making the procession of network graphs an intuitive progression of visual arguments that draws the audience into, and educates them about, the rhetoric of the network graph precisely by showing its malleability and, in some ways, its very partiality. By showing the protean capacities of the network graph, by showing the extent to which each network graph is itself a reading, skeptics more at home with paleography and stemmatology are able to grasp quite intuitively the resource that is faceted network graph: a flexible and powerful tool to show the richness and interrelatedness of large datasets based on archival discoveries very like their own.  This short paper will include slides of my own faceted network graphs to show how we can use this form of argumentation as a kind of nested pedagogy.  I enclose here a collection of faceted and mined network graphs made for a colleague’s project on Modernist journals. These bimodal graphs show the interrelation of different contributors to various roughly contemporary journals. By showing the same layout several times, but using node color, node size, edge weight and color, we are able to see: which nodes are journalists and which are journals; nodes clustered with a granularity of 1.0 (fewest communities of the largest size); nodes clustered with a granularity of 0.23 (many smaller communities); which journalists contributed the most (edge weight), which journals had the most contributors (node redness and size), and which journalists contributed to the most journals (by node size). These five network graphs taken together not only supply a much richer picture of the system of writing and publication than a single graph would; by showing the range of information that can be displayed on that graph with a series of simple adjustments, the reader is shown how the graph is a product not of vulgar scientism but of intentional humanism.    Illustration 1: Lilac nodes are journalists; blue are journals      Illustration 2: Modularity at granularity 1.0; the fewest communities of the largest size.      Illustration 3: Modularity class 0.23; more communities of a smaller size.      Illustration 4: Edge weight shows number of of contributions by a journalist; node redness and size indicates number of contributors.      Illustration 5: Node size indicates number of journals to which a journalist contributed.    ",
       "article_title":" Faceting and Mining Network Graphs  ",
       "authors":[
          {
             "given":"Claude",
             "family":"Willan",
             "affiliation":[
                {
                   "original_name":"Princeton, United States of America",
                   "normalized_name":null,
                   "country":"United States",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-08",
       "keywords":[
          "digital humanities - pedagogy and curriculum",
          "literary studies",
          "archives, repositories, sustainability and preservation",
          "English",
          "networks, relationships, graphs"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Chronological corpus – definition  The notion of chronological corpus has appeared so far only in publications of limited circulation (cf. Pawłowski, 2006), whereas it is practically absent from mainstream literature on corpus linguistics and digital humanities. This is somewhat surprising given that the research of so called “lexical series” has been conducted in the past (cf. Brunet, 1981; Salem, 1987) and that Google has been for some years developing the tools  Ngram Viewer and  Google Trends, which allow sequential analysis of Google Books resources and of users' Internet queries (cf. Michel et al. 2011).   In order to understand what chronological corpus is one must grasp the basic distinction between synchrony and diachrony at the heart of structural linguistics. Synchrony is a study of language at a certain moment in time, where \"moment\" can last even several decades, provided there are no apparent changes in grammar, vocabulary and pronunciation. As for diachrony, it exposes evolution of language taking place in the process of its development, usually over long periods, spanning even several centuries. However, corpus research supported by NLP tools enables a far more flexible approach to the time variable, since text samples consistent in terms of their typography, spelling and grammar can be annotated with their exact publication dates. Scientific description then focusses on the dynamics of the frequency change of specific lexemes (or other segments) in time, rather than on the evolution of word forms beginning with a hypothetical proto–language until the present state.  An important issue to be resolved is that of naming such an approach, whereby the chosen term would instate it in the domain of corpus linguistics. While social sciences and psychology use the term of \"longitudinal\" research, its equivalent in linguistics could be ‘microdiachrony’. Having said that, the term’s association with traditional diachrony and history of language could prove misleading. This is why it is more advisable to speak of  chronological analysis and  chronological corpora, where, similarly to diachrony, both terms contain a reference to time – gr.  chronos.  Consequently,  chronological corpus should be understood as a sequence of text samples consistent in terms of their spelling and grammar, corresponding to subsequent points on the axis of time (e.g. weeks, months etc.). Such a corpus would make it possible to explore the dynamics of the change of lexeme frequencies in successive periods using the method of time series analysis (cf. Gottman, 1981; Cryer, 1981; Pawłowski, 1998, 2001). A chronological corpus would differ from a diachronic one in that in the former texts are evenly spread in time and word forms remain unchanged, while in the latter it is the opposite: word forms must evolve to become object of interest and time spans between measurements may be of any length. It needs to be emphasised that the annotation of the time variable may be ignored (if such is the premise of the conducted research); a chronological corpus would then be treated as a synchronic one.    Chronological corpora  There exist corpora that are intrinsically arranged with regard to the time variable, e.g. literary outputs of some authors presented as Shakespearean Corpus, Corpus Thomisticum or Corpus Platonicum. If the date of creation of every piece of data in the corpus is known, the development of the text's stylostatistical properties over time can be established. If text chronology remains partly unknown, stylometric research can help to put undated works in the correct order (cf. Lutosławski, 1897).  But there is also another set of text corpora that seem to be naturally predestined to sequential analysis. Contrary to other genres or functional styles,  press and media texts necessarily incorporate the date or even the time of their creation. This is of course little surprising, since media must comment upon actual events. Publication dates in the headers or footers of printed newspapers may be thus regarded as “time markup” that is invaluable as a Gutenberg era contribution to the sequential corpus research in the digital universe.     ChronoPress – features and origin  At present ChronoPress corpus contains texts representing Polish press of the post-war totalitarian period (1945-1954). Future extensions are planned to cover the entire period of the country's sovereignty (1918–2018). As the flow of press information was abundant even in the early post-war period, representative method of sampling has been applied. It has also been assumed that the minimum time spans, visible from the user level, are limited to subsequent months (the volume of text corresponding to weeks was too small to guarantee a sufficient level of representativeness). Apart from that it should be pointed out that the pace of public discourse in the times of printed press was slower than it has been in the digital world, so that months as time units can be regarded as sufficient to trace relevant events or processes.  In order to obtain reliable values of lexeme frequencies the number of words per month has been established as ca 120.000-140.000. Since sample size has been fixed to ca 250-300 words of continuous text, every month is represented by 480 samples and every year by 5760 samples (ca 1,5 million words per year). Equal numbers of words per month are important to guarantee that time series and other parameters generated from ChronoPress remain statistically unbiased with regard to the sample size. Selected newspapers represent a relatively broad spectrum of public post-war discourse of the totalitarian state. Particular titles to be included in the corpus have been selected according to two criteria: newspaper circulation (only titles with the highest values and national range have been considered) and target groups, as defined by the ideology of the time (first of all the working class, peasantry, army, youth, and, to a lesser extent, \"intelligentsia\"). The corpus is freely available through an interactive user interface.  Data have been thoroughly sampled from Polish dailies and weeklies. Preparatory stages of analysis consist of: sample selection, scanning, OCR, markup with metainformation (XML scheme) and text curation. The corpus has been annotated morphosyntactically using Morfeusz tool which had been designed for the Polish National Corpus. Metadata (Fig.1) include newspapers’ titles, articles’ titles, authors’ names, exposition, and data support.  Fig. 1 Example of sample annotation with metadata     ChronoPress offers a variety of online linguistic and exploratory tools, such as concordances, frequency lists, word profiles and word maps. Its specific feature is, however, the possibility to generate time series. The analysis of lexeme frequency values displayed on an axis of time allows users to discover and explore the dynamics of events and phenomena represented in daily press over long periods of time.  Fig. 2 Moving average of the frequency of the lexeme  machine (maszyna). It  supports the view that despite the systemic inefficiency of communist economy technological progress was promoted in post-war Poland.       Goals of the presentation  ChronoPress allows to conduct a wide range of research in linguistics, contemporary history, cultural anthropology, media studies and communication science. The perspective adopted here is oriented towards the analysis of dynamic processes occurring in time rather than towards static states of language reflecting extralinguistic reality. The goal of the presentation is to provide an overview of the available exploratory techniques for chronological corpora, highlighting their explanatory power and limitations. In particular the following issues will be addressed and exemplified with empirical evidence from the ChronoPress corpus:   Manual and automated trend detection, modelling and interpretation  Long-term change in frequency of selected lexemes will be analysed as the expression of dominating topics of interest in public discourse, but also of social change imperceptible from the point of view on an average. Illustration material will include salient and informative examples generated from the ChronoPress database ( inter alia lexemes from the semantic fields  war,  health,  technology). Since trend analysis requires some conceptualisation of time, various approaches to time will be presented and discussed (linear or circular, anthropological, political and astronomical).    Detection of events  Events are understood here as sudden changes, such as natural or technological catastrophes, wars, accidents etc. They manifest themselves as sudden rises in the lexeme frequency. This task will be illustrated with some examples generated from the ChronoPress corpus and from other online sources.    Testing the efficiency of statistical tools  The utility of the autocorrelation function (for univariate series) and of the cross-correlation function (for multivariate series) in the analysis of chronological corpora will be verified.    Comparison of ChronoPress and online resources  Informational potential and functionalities of the ChronoPress corpus will be compared with other online research tools, such as Google Ngram Viewer, Hansard Corpus and Polish National Corpus.    Word profiles analyses  Collocations and semantic profiles of selected lexemes in subsequent years will be generated (using Log Likelihood Ratio) and compared.    Automatic detection and annotation of named entities  Proper names and other named entities in Polish can be detected and annotated using Liner 2. Spatial distribution of toponyms identified by Liner 2 will be automatically displayed on a map and further analysed. Both tools have been created in the framework of the Clarin-PL project (http://clarin-pl.eu/en/home-page/).   ",
       "article_title":"Chronological corpora: Challenges and opportunities of sequential analysis. The example of ChronoPress corpus of Polish",
       "authors":[
          {
             "given":"Adam Tomasz",
             "family":"Pawłowski",
             "affiliation":[
                {
                   "original_name":"University of Wrocław, Poland",
                   "normalized_name":"University of Wrocław",
                   "country":"Poland",
                   "identifiers":{
                      "ror":"https://ror.org/00yae6e25",
                      "GRID":"grid.8505.8"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-28",
       "keywords":[
          "concording and indexing",
          "corpora and corpus activities",
          "digitisation, resource creation, and discovery",
          "historical studies",
          "linguistics",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Increasingly, scholars have access to large, heterogeneous collections of information such as those provided by the Digital Public Library of America, Europeana, or Canadiana as well as more targeted thematic research collections like the Whitman Archive. In additions to purpose-built humanities projects, scholars commonly make use of general-purpose resources such as Flickr, Google Search, HathiTrust Digital Library and many other services.  Traditional search interfaces allow scholars to rapidly search for specific items and to explore a collection via facetted search interfaces and other techniques. Search, however, is only one part of a complex ecosystem of behaviors associated with information seeking (Toms and O’Brien, 2008; Buchanan et al., 2005). In practice, scholarly information seeking is not characterized by a single search event or interaction with a single system. Rather, it is a process that takes place over an extended period of time and involves searching different sources for potentially relevant material. As material is found, scholars organize, annotate and make notes about the retrieved material; activities that lead to more questions and more seeking. These tasks serve a dual purpose of cataloging material for re-discovery, use and interpretation as well as engaging the participant in a process of sensemaking and internalization. The fundamental purpose of this work is not merely the discovery, consumption or indexing of information or even the production of a concrete research output such as a paper. Instead, scholarly information seeking serves to hel p the seeker develop an internalized, systematic understanding of a body of knowledge and cultivate a distinctive interpretive voice (Audenaert and Furuta, 2010). We are developing the Visual Workspace for Information Seeking and Exploration (vWise   vWise builds on key concepts and findings of a previous project, CritSpace (Audenaert et al., 2010) and provides a new implementation and enhanced capabilities. ) to support these tasks. This paper introduces the key features and capabilities of the vWise platform and discusses how those features and capabilities reflect and build on a theoretical framework for designing systems to support open-ended information seeking and exploration.     System Overview vWise is an extensible Web-based application framework that allows scholars to search for content from different data providers and bring the resulting information together into a single display. Once resources have been added to the workspace, they can then organize and reframe that information, for example by culling unwanted results or juxtaposing digital surrogates for newly discovered relationships.  Panels are the primary way people interact with content in the vWise interface. They are used to display different types of information drawn from different sources in a single workspace. Users can organize this information by rearranging panels within the workspace and add visual annotations by setting properties such as background color, border style, font properties and drop shadows. The panels themselves support user interactions tailored to the types of content they display. For instance, users could read a book from HathiTrust, watch a video from YouTube, explore metadata from DPLA or manipulate manuscript page images to improve readability.  Users have several options for adding content panels to a workspace. A basic search service allows them to execute searches using pluggable content providers. Alternatively, custom panel implementations may provide advanced searching capabilities or allow users to drag composite content such as pages from a manuscript onto the workspace to create new panels.  Workspaces provide a site for both individual and collaborative work. To enable collaboration, a user can share a workspace with others. Modifications to this shared workspace are reflected on all users’ display simultaneously. This allows both synchronous and asynchronous collaboration.  As a framework, vWise is designed to be customized and configured by an application developer prior to being deployed. This customization allows the basic application to be extended in four major ways:  Integration with external data sources. vWise provides a pluggable architecture for defining interfaces to external data sources such as a digital library or search service. For instance, we provide data sources that allow users to load videos from YouTube videos, search Wikipedia and display web-pages.    Custom panels for working with data objects. Individual units of content are displayed using panels that provide basic support for moving, resizing and styling. Extensions support rich interactions tailored to specific types of content. For example, an image annotation tool could allow the creation and storage of annotations.   Ad hoc panels for interacting with underlying search services. The core use-case for panels is to display and organize individual content elements. Panels can also be used to implement other application services. For example, we envision implementing a search service panel that will display basic and advanced search options, display facets from the currently active search, maintain a history of recent search and display search results as panels in the main workspace.    Integration with server-side workspace persistence. vWise runs in a browser and stores workspaces and workspace configuration information via a well-defined RESTful interface. Application developers can use the default implementation that comes with vWise or provide a custom implementation. For instance, we developed a custom data storage implementation to connect the vWise interface with a proprietary system used to train emergency responders.     Supporting Information Seeking and Exploration Information seeking and exploration in scholarly research is an intensive, creative activity. Supporting these activities require tools that go beyond merely helping scholars find resources to provide environments that reflect and facilitate the creative process. vWise is based on ideas that have emerged from the literature in hypermedia systems, digital libraries and creativity research, including the following core concepts:  Information Triage: The process of information seeking requires people to rapidly assess the utility of various information resources, discard those that aren’t relevant and prioritize those that show promise (Marshall and Shipman, 1997). vWise goes beyond traditional search interfaces by allowing users to remove individual results, prioritize the remaining objects and combine the results from multiple searches in a single workspace.    Incremental Formalization: One aspect of open-ended information seeking tasks is sense-making—the process of gaining a broad understanding of the structure of ideas within a domain. During sense-making, the emerging organizational framework is partial, provisional and implicit (Shipman and Marshall, 1999). vWise facilitates incremental formalization of knowledge by allowing users to express an emerging organization framework implicitly via the spatial arrangement and visual properties of the panels displayed on the workspace. Within this environment, user can rapidly form and reform organizational structures by manipulating the visual characteristics of the display.   Representational Talkback: The visually expressed knowledge structure serves as an externalized representation of a user’s internal, evolving mental models. Creating and interacting with this content supports representational talkback (Schön, 1983; Nakakoji et al., 2000). Representational talkback occurs when material externalizations of an internal mental model initiate a “reflective conversation” in which they talk back to their creator to inform subsequent stages of the sense making process.    Heterogeneous Sources and Material: Complex information seeking tasks rarely involve one-stop shopping. vWise allows people to integrate, analyze and synthesize information from different sources, by bringing content into a unified information organization space. Manipulation of spatial and visual properties (border color, drop-shadows, backgrounds, etc.) provides a lightweight and open-ended interface manipulating this content. Domain-specific panel implementations allow users to engage in content-specific interactions.    Summary vWise provides support for the complex information seeking needs of scholars and other professionals that goes beyond the capability of traditional search systems. It allows people to gather information from multiple sources and to work either independently or collaboratively to organize and analyze that material using a workspace metaphor.  To date, we have demonstrated prototype implementations of vWise in two different settings. While initially conceived and designed to support the needs of scholars in the humanities, we have integrated a version of vWise into the Emergency Management*Exercise System (EM*ES). EM*ES is a simulation tool used by Texas A&M Engineering Extension Service (TEEX) to train incident managers, supervisors, and jurisdiction officials in the management of a large-scale crisis (TEEX, 2016). We have added vWise as a component to support the training of communication and coordination between emergency managers and cyber response teams. vWise is currently in use in a series of training exercises starting in January of 2016 and running through September 2017. During this period we will investigate options for more widespread use within the EM*ES application. Our second deployment of vWise was a demonstration prototype for use in wall-sized display of the Humanities Visualization Space (HVS) at Texas A&M University’s Initiative for Digital Humanities, Media and Culture (IDHMC). While vWise is envisioned primarily for desktop-oriented use, when working on knowledge organization tasks, bigger is often better. The HVS provides just such a space and opens intriguing possibilities for scholars to work with this class of interfaces both as individuals and in collaborative settings.   Figure 1: vWise demonstration at the IDHMC Humanities Visualization Space  Moving forward, we continue to develop the core implementation of vWise framework and to add additional content provider connections and panel implementations for displays. We are specifically interested in developing integrations with major content providers for humanities and cultural heritage scholars and are working to pursue such collaborations.  ",
       "article_title":"vWise: Visual Workspace for Information Seeking and Exploration",
       "authors":[
          {
             "given":"Michael Neal",
             "family":"Audenaert",
             "affiliation":[
                {
                   "original_name":"Texas Center for Applied Technology, United States of America",
                   "normalized_name":null,
                   "country":"United States",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Matthew",
             "family":"Barry",
             "affiliation":[
                {
                   "original_name":"Texas Center for Applied Technology, United States of America",
                   "normalized_name":null,
                   "country":"United States",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Paul",
             "family":"Bilnoski",
             "affiliation":[
                {
                   "original_name":"Texas Center for Applied Technology, United States of America",
                   "normalized_name":null,
                   "country":"United States",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-13",
       "keywords":[
          "English",
          "social media",
          "visualisation"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The implementation of geo-historical gazetteers increasingly depends upon the development of Natural Language Processing (NLP) and Corpus Linguistics as well as geographical analysis in disciplines such as History, Archaeology and Literary Studies. The application of these methods usually relies on the appropriate modelling of databases for performing the semantic enrichment of documents including geoparsing tasks. At the same time, even when performing a manual enrichment and referencing of place mentions in texts or in library or museum catalogues (e.g. when applying the CIDOC CRM model and its spatio-temporal extension), an adequate source of external information is crucial. Today, geo-historical data are more and more often published following the Linked Data (LD) principles: i.e. using URIs and data format standards (RDF) and linking to other data sets to enable information discovery. Moreover, an implicit driving principle of LD, widespread in the Semantic Web community, is the reuse of vocabularies and ontologies already defined by others to avoid duplication. Keeping track of provenance is crucial. Pleiades is one of the best examples these days, but other generalistic sources such as DBpedia, Wikidata or GeoNames also provide interesting - albeit partial – geo-historical information and have proved to be useful in Digital Humanities (DH) projects. Linking texts to external sources using URIs enables the retrieval of additional information about the referenced places. Once this has been achieved, the information in the sources can be easily used to produce different views and aggregated analysis of corpora: i.e. visualizations (Jessop, 2008); this in turn is meant to help scholars to capture place perceptions and to analyse spatio-temporal phenomena described in corpora. The choice of geo-historical datasets which are used as gazetteers depends on the domain of the texts under consideration. Pleiades is specifically suited to places in Mediterranean Ancient History texts. However, tasks such as referencing places from historical periods other than Antiquity, or identifying geographically vague or imaginary places in literary texts, if ever possible, might need the development of a different methodological approach, which would include the construction of conceptual mapping models and the creation of a completely different kind of gazetteer. In any case, the choice will have an important influence on the results of such visualizations as well as on the pertinence of the interpretation. Existing gazetteers vary widely in how they abstract the world. Important aspects – such as scale, the representation of time (and change over time), complex geometries, uncertainty and vagueness as to location and/or date, multiple points-of-view, representation of hierarchies of political-administrative units, their boundaries and their change over time, alternative names (rejected and standard forms, vernacular and multilingual), representation of fantastic places – are modelled in different ways, or are missing altogether. This limits their applicability in the Humanities. Moreover, interlinking between corresponding entities in different gazetteers is often lacking, although progress has been made in this regard, through community initiatives or by using GeoNames, or Wikidata as backbones (Simon et al., 2015). Finally, the ontologies used to link toponyms in texts to spatial references need to be further developed, especially when it comes to deal with fuzziness and uncertainty in mentions (Reuschel and Hurni, 2011). Clearly, new models should conform to LD principles, and they should privilege the reuse of existing and consolidated ontologies, vocabularies and datasets whenever possible. Long term preservation and maintenance are also crucial problems in this sense because texts enriched with references to sources that have become obsolete or unavailable may have results that are unusable for the task for which they were tagged (Janowicz et al., 2012). In this sense, specialisation of efforts on the one hand (for pooling efforts) and coordination on the other, are crucial for such projects. Finally, geo-historical projects should also promote harmonization of their data with standards and practices of the broader DH community, and of the current research trends, in particular for what concerns the interoperability of resources within the framework of larger research infrastructures such as CLARIN or DARIAH. In the proposed full-day workshop, we will focus on geo-historical gazetteers, and we will discuss their limits in supporting the needs of the Spatial Humanities (SH) community. The proposed workshop will be composed of nine presentations (abstracts are listed below), each of which concerns the production of geo-historical gazetteers as LD as well as the annotation, the recognition and the geoparsing of place names referenced in texts, library and museum catalogs, digitized maps, etc.  Christopher Donaldson (University of Birmingham), Extracting and visualizing the geographies in historical travel writing: This presentation will introduce a procedure for the automated extraction and resolution of geographical information from a corpus of historical writings about the English Lake District. The research on which the presentation is based is using the spatial analysis of geo-historical LD sets to achieve a more comprehensive and refined understanding of how the landscape of the Lake District was perceived, represented, and experienced in the past.   Karl Grossner (Stanford University), Joining Place and Period in Historical Gazetteers: Places referred to in historical documents and gazetteers have temporal as well as spatial extents. Likewise, historical periods have spatial extents. However existing data models and format standards and the mapping and timeline software that use them do not reflect this. I will discuss recent work on Topotime, an extension to the GeoJSON format adding temporal expressions, and allowing for some types of uncertainty encountered in historical data.   Katherine Hart Weimer (Rice University): A wealth of geographic information is included in library catalogs, with existing structures for name disambiguation, cross-referencing and inclusion of geographic coordinates. Recently, efforts are underway in libraries to convert this data into LD allowing for cross platform applications. The presentation describes an experiment in this sense.   Maurizio Lana (Università del Piemonte Orientale): Annotation of place mentions in Latin Literature. The annotation pipeline uses parsing+NER but later mentions are manually checked and referenced to external gazetteers such as Pleiades. The novelty of the project is the GeoLat GO! ontology that allows for a more complex annotation.   Bruno Martins (University of Lisbon): NLP and IR methods for handling geospatial information in textual documents. In my talk, I will present a brief survey of techniques for handling geospatial information within textual documents, including work at our team in the University of Lisbon, and other methods proposed within the Computational Linguistics and IR communities. I will discuss methods to address the problems of (i) document geocoding, (ii) toponym resolution, and (iii) selecting geographically relevant key-phrases. Applications within the broad field of DH, and SH in particular, will also be outlined.   Patricia Murrieta-Flores (University of Chester): So far, research in the SH has been mainly concerned with geographically precise information or what could be considered as ‘real’ places in historical and literary sources. Nevertheless, non-locational places play an important role in narratives of all sorts of sources from the fantastic, to geographically vague travel accounts. This is an important limitation in the analysis of place in the DH. Using Medieval Romances as an example, this presentation will discuss the challenges posed by literary narratives of place in terms not only of disambiguation, but also reference to fantastic and non-locational places.   Michael Page (Emory University): Atlanta Explorer: Historical Geocoding & the City: Atlanta Explorer focuses on building datasets and geospatial tools to explore the history of the city. Completed is a geodatabase and geocoder for circa 1930 and the pilot 3D virtual environment. The next phase includes producing geocoders for the remaining years (1868–1930) and therefore strategies and methods for developing historical geocoding datasets and tools for place discovery will be discussed. Our goal is to also share the underlying data with the community CityGML as how we would likely share and archive the model.   Rainer Simon (Austrian Institute of Technology), Pelagios project: an international community initiative concerned with the development of Linked Open Data methods, tools and services to better interconnect geo-historical datasets. In its most recent project phase (“Pelagios 3 - Early Geospatial Documents”), Pelagios has developed Recogito, a semi-automatic geo-annotation tool; Peripleo, a geotemporal search engine. Furthermore, Pelagios has annotated more than 300 historical sources from different cartographic traditions, collecting more than 120,000 place references in literary texts and early maps.   Humphrey Southall (University of Portsmouth): Engaging the wider public with historical gazetteers. Gazetteers are a powerful tool for humanities researchers, but are also of great fascination and utility for the general public. That interest enables academic projects to achieve wider “impact”, enables popular web sites to be sustained by advertising income, and enables expansion through crowd-sourcing. This presentation covers three experiences: the established Vision of Britain site; PastPlace, our new LD gazetteer which uses Wikidata as a spine to which we are adding historical toponyms; and GB1900, a crowd-sourced gazetteer building project developed in collaboration with National Libraries in Great Britain.  The proposed workshop targets an audience of scholars, data designers, and software developers, and will also comprise a speed presenting session for participants, topic-based breakout discussions between experts and attendees, a panel to highlight research priorities and summarize the main contributions of the workshop and research directions. ",
       "article_title":" A Place for Places: Current Trends and Challenges in the Development and Use of Geo-Historical Gazetteers  ",
       "authors":[
          {
             "given":"Carmen",
             "family":"Brando",
             "affiliation":[
                {
                   "original_name":"Institut National de l'information géographique et forestière (IGN), France",
                   "normalized_name":"Institut Géographique National",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/05jxfge78",
                      "GRID":"grid.424645.5"
                   }
                }
             ]
          },
          {
             "given":"Francesca",
             "family":"Frontini",
             "affiliation":[
                {
                   "original_name":"Istituto di Linguistica Computazionale \"A.Zampolli\", Italy",
                   "normalized_name":"Institute for Computational Linguistics “A. Zampolli”",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/028g3pe33",
                      "GRID":"grid.503055.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2015-10-16",
       "keywords":[
          "spatio-temporal modeling, analysis and visualisation",
          "natural language processing",
          "linking and annotation",
          "maps and mapping",
          "ontologies",
          "English",
          "semantic web",
          "geospatial analysis, interfaces and technology"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Does your library or digital humanities center employ graduate students? Are you considering employing graduate students in digital scholarship work at your university? This full-day workshop will bring together institutions that do to discuss the variety of institutional arrangements for employing graduate students in digital scholarship labor. If graduate students can help build capacity for digital work on your campus, what are some best practices for structuring their employment? What are some current models in place, and what are the benefits and challenges of fellowships vs. part time employment or RAships? This workshop will present helpful practical advice on this topic, but also serve as a starting point for a broader discussion about the place of student labor in DH work.   Graduate students represent valuable members of digital humanities teams in a variety of institutional and library settings. They collaborate with scholars in labs, as members of project teams, as fellows, interns, instructors, research assistants, principal investigators, and everything in between. In her 2015 Office of Digital Humanities keynote presentation entitled “on capacity and care,” Nowviskie argued that, as we continue to build individual, institutional, and even national capacity for digital scholarship in higher education, we should make an “ethic of care” the foundation upon which we work. This workshop will address how libraries and digital humanities organizations can make an ethic of care the foundation upon which their varied graduate student labor arrangements are built as they look to expand capacity within their institutions and beyond. Workshop participants will discuss the benefits, challenges, and best practices for the wide variety of institutional arrangements that result in graduate students doing DH work in libraries and DH organizations today.  ",
       "article_title":"Building Capacity with Care: Graduate Students and DH work in the Library",
       "authors":[
          {
             "given":"Alan Gilchrist",
             "family":"Pike",
             "affiliation":[
                {
                   "original_name":"Emory University, United States of America",
                   "normalized_name":"Emory University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03czfpz43",
                      "GRID":"grid.189967.8"
                   }
                }
             ]
          },
          {
             "given":"Dawn",
             "family":"Childress",
             "affiliation":[
                {
                   "original_name":"University of California at Los Angeles (UCLA), United States of America",
                   "normalized_name":"University of California, Los Angeles",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/046rm7j60",
                      "GRID":"grid.19006.3e"
                   }
                }
             ]
          },
          {
             "given":"Smiljana",
             "family":"Antonijević",
             "affiliation":[
                {
                   "original_name":"Pennsylvania State University, United States of America",
                   "normalized_name":"Pennsylvania State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/04p491231",
                      "GRID":"grid.29857.31"
                   }
                }
             ]
          },
          {
             "given":"Jim",
             "family":"McGrath",
             "affiliation":[
                {
                   "original_name":"Brown University, United States of America",
                   "normalized_name":"Brown University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05gq02987",
                      "GRID":"grid.40263.33"
                   }
                }
             ]
          },
          {
             "given":"Alex",
             "family":"Gil",
             "affiliation":[
                {
                   "original_name":"Columbia University, United States of America",
                   "normalized_name":"Columbia University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00hj8s172",
                      "GRID":"grid.21729.3f"
                   }
                }
             ]
          },
          {
             "given":"Brennan",
             "family":"Collins",
             "affiliation":[
                {
                   "original_name":"Georgia State University, United States of America",
                   "normalized_name":"Georgia State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03qt6ba18",
                      "GRID":"grid.256304.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "project design, organization, management",
          "digital humanities - diversity",
          "digital humanities - institutional support",
          "GLAM: galleries, libraries, archives, museums",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The global project of data conversion of the notable Italian ‘Zeri Photo Archive’ into a Linked and Open Dataset   The project is supported by the Zeri Foundation with the University of Bologna and started in 2014  primarily regarded the analysis of the  description model of the available records, so as to define a collection of suitable ontologies to describe such a complex domain.  Indeed, the uniqueness of the Zeri collection   Fondazione Zeri, Photo Archive Catalog,  http://catalogo.fondazionezeri.unibo.it  , which includes about 290,000 photographs of works of art and monuments, lies in the rich documentation of the described artefacts, mostly related to provenance, attributions, restoration events and their connections to the collection of 46,000 books and 37,000 auction catalogues (Mambelli, 2014).   The full project entails, together with the ontological modeling, the production of the RDF dataset, the creation of proper links to the LOD cloud, and the definition of the user interface for browsing the dataset.   F Entry Ontology The first activity of the project had been the  formalization of the  Scheda F (‘Fotografia’) (Mibact, 1999) , – the metadata standard of the Italian Istituto Centrale per il Catalogo e la Documentazione (ICCD) – by mapping the schema onto CIDOC-CRM model (Crofts et al., 2011). In the conversion we initially considered the specific flavor of the Scheda F used by the Zeri Foundation, i.e. its subset of 113 fields (based on the experimental 1.04 version of the ICCD standard) and an handful of custom extensions to it. A deep analysis of the schema of the Scheda F showed that it is organized in semantically independent sections (called “paragrafi”, or  paragraphs), each one belonging to a specific FRBR concept (Work, Manifestation, Item); this allows the mapping to proceed by logical sections affecting only a limited number of entities and relating these entities to the data documented by the fields of the schema.  Before accomplishing the mapping, we proceeded with the creation of a  new ontology called FEO (F Entry Ontology   http://www.essepuntato.it/2014/03/fentry  ). Since our final goal was to make available Scheda F data in a triple store, the target language we chose was OWL 2 DL. The current version of FEO introduces the classes and properties that characterize three specific concepts: the photograph, the work of art that is the subject of the photograph, and the Scheda F itself describing the photograph and its subject.  So, through the use of well-known ontologies – i.e. CIDOC-CRM, but also PROV-O (Lebo et al., 2013), and FaBiO (Peroni and Shotton, 2012), as part of the SPAR Ontologies   http://www.sparontologies.net     (Peroni, 2014) – plus the FEO ontology developed  ad hoc, most of the content expressed as descriptive entries in the Scheda F have been already formally represented (Gonano et al., 2014).    OA Entry Ontology In this presentation we propose an extension of our work on the Zeri Photo Archive by introducing a  new ontology for representing works of art and their related information, namely, the  OA Entry Ontology   http://purl.org/emmedi/oaentry  . In particular, this ontology is based on the  Scheda OA (‘Opera d’Arte’) – another ICCD metadata standard   See the ICCD cataloging standards at:  http://www.iccd.beniculturali.it/index.php?it/473/standard-catalografici   – and proposes a mapping between the content standard and, again, the CIDOC-CRM, in order to create shareable descriptions of metadata   We are planning to publish both the F and the OA mapping to CIDOC-CRM in the next few months, according with the ICCD activities . In addition, other kinds of information that are not easily representable through the aforementioned standards, such as certain peculiar relations between works of art, are modelled by means of new classes and properties created in the OA Entry Ontology. This last allows to describe, in particular, the work of art and the related items, by focusing on some classes (e.g. copy, derivation, fake, drawing, model, replica, sinopia) and by using properties as necessary connection typization (e.g. conceived or former).    HICO and authorship attribution Moreover, in this paper we further investigate a way for providing a clear and shareable representation of questionable information, i.e., the  authorship attribution of works of art. In the Zeri Photo Archive there are particular authorship attribution created by either the Zeri Foundation cataloguers and/or by Federico Zeri (collector of photographs) himself, and such attributions (that could be accepted or discarded at certain point) are accompanied with the criteria that corroborate the cataloguers’ choice.   In order to provide a precise characterisation of all these aspects, we discuss the adoption of  HiCO,  Historical Context Ontology (Daquino and Tomasi, 2015) as a way for enabling a definition of authoritative attributions based on Zeri cataloguers’ own criteria. HiCO   http://purl.org/emmedi/hico   is an OWL 2 DL ontology we created for describing contents of data (e.g., an authorship attribution), in particular cultural heritage data, and data creation itself (e.g., RDF statements representing above mentioned authorship attribution) as part of an interpretative process. Cultural heritage object is a wide concept: it includes any sort of representation of culture heritage embodied in a tangible form like artifacts (books, documents, and, as in this case, works of art), but also any concept, assertion and interpretation somehow bounded to cultural objects.  With the hico:InterpretationAct class it’s possible to  describe the interpretation act as a process:   the conceptualization of the interpretation and its classification, for enabling further relations among different kind of interpretations;  the embodiment of the interpretation as RDF statements, for representing information extracted from the content of the object of interest.  Two fundamental object properties complete the process: the hico:hasInterpretationType property and the hico:hasInterpretationCriterion property. The former states an arbitrary classification of the interpretation, which can be simply defined as philological, historical, semiotic, linguistic etc. The latter is a briefly explanation of the criterion used to state information extracted from a source, e.g. a literally transcription, a hypothesis, or the adoption of the literature about a specific argument. A crucial aspect of the project is the  correct formalization of statements so as to allow the ontologically-consistent coexistence of data created by different actors that express contradictory statements about the same subject (e.g., authorship attribution data of a work of art), in order to guarantee the data integration with Pharos (Reist et al., 2015) project partners, of which the Zeri Foundation is a member. By the use of SWRL rules applied to relations between sources, criteria and context information used by an agent to explain his interpretation,  we could formally infer when an interpretation can be considered authoritative.    An example of authoritative assertion We could give an  example. We state aside the interpretation (i.e. the assertion “An artist X is author of a specific work of art Y in a specific time Z”) which sort of interpretation we are dealing with (i.e. authorship attribution), and which criteria have been used by the cataloguer to assert such proposition. A provenance statement ensures both author of assertion (i.e. interpretation) and author of data conversion are fully described, ensuring that no contradictory statements will affect data validity.   When necessary conditions for stating authorship are fulfilled, an attribution may be inferred as authoritative. In the example (Fig. 1), we have an attribution which respects minimal requirements for being considered an  authoritative assertion:   it has been stated by a well-known author (i.e. the cataloguer of Zeri Foundation); it considers as criterium an authoritative source (i.e. the photo depicting the work of art); it agrees with another interpretation, i.e. Federico Zeri classification.  This obviously doesn’t entail that the attribution is surely correct, but it can be a useful tool for historians when searching for different attributions and related criteria adopted in interpretative process.   Fig. 1 Sample of multiple interpretation of the same object and Zeri authoritativeness    Conclusions To conclude, as said before it will be the natural completion of the Zeri Project the RDF triple store set up, the creation of links to other datasets, and the definition of the user interface for browsing the linked open dataset. All these activities are now object of our research group industry and the final publication of the project is expected for the middle of 2016. Data had already been transformed in a RDF/XML dataset, according to above mentioned ontologies. Next steps of the project involve thus data publication, ensuring access to them through a SPARQL endpoint. When published, data will be enriched with further RDF links to major datasets and authority files online (e.g. VIAF for people and works, Getty thesauri, GeoNames for places   A first check on  https://datahub.io/dataset  ). This ensures our data will really be part of the LOD Cloud, avoiding creation of another data silo. So enriched data will then be exploited in a new smart application, which will enable users to search data about both photos and works of art. Through this modelization data and new relations will be easier discovered, enhancing user experience.   ",
       "article_title":"The Project Zeri Photo Archive: Towards a Model for Defining Authoritative Authorship Attributions",
       "authors":[
          {
             "given":"Marilena",
             "family":"Daquino",
             "affiliation":[
                {
                   "original_name":"University of Bologna, Italy",
                   "normalized_name":"University of Bologna",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/01111rn36",
                      "GRID":"grid.6292.f"
                   }
                }
             ]
          },
          {
             "given":"Silvio",
             "family":"Peroni",
             "affiliation":[
                {
                   "original_name":"University of Bologna, Italy",
                   "normalized_name":"University of Bologna",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/01111rn36",
                      "GRID":"grid.6292.f"
                   }
                }
             ]
          },
          {
             "given":"Francesca",
             "family":"Tomasi",
             "affiliation":[
                {
                   "original_name":"University of Bologna, Italy",
                   "normalized_name":"University of Bologna",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/01111rn36",
                      "GRID":"grid.6292.f"
                   }
                }
             ]
          },
          {
             "given":"Fabio",
             "family":"Vitali",
             "affiliation":[
                {
                   "original_name":"University of Bologna, Italy",
                   "normalized_name":"University of Bologna",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/01111rn36",
                      "GRID":"grid.6292.f"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-01",
       "keywords":[
          "linking and annotation",
          "standards and interoperability",
          "ontologies",
          "authorship attribution / authority",
          "knowledge representation",
          "English",
          "semantic web"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" In many online platforms and websites, audio-visual data is gradually playing an equal or greater role than text. Similarly, in multiple disciplines such as anthropology, ethnomusicology, folklore, media studies, film studies, history, and English, scholars are relying more and more on audio-visual data for richer analysis of their research and for accessing information not available through textual analysis. Developing aural and visual literacy has therefore become increasingly essential for 21st century digital scholarship. While audiovisual data allows for research to be disseminated and displayed linearly, within one modality (e.g., reading a book from first to last page), it also allows for non-linear discovery and analysis, within multiple modalities (e.g., reading a webpage, browsing to a link with a sound clip, from there to a clip with film). This workshop will address both the challenges of analyzing audiovisual data in digital humanities scholarship, as well as the challenges of educating contemporary digital humanists on how to access, analyze, and disseminate an entire century of information generated with audiovisual media. Other challenges to be addressed at this workshop concern issues surrounding copyright and sustainability and their impact on the dissemination and long-term access of audiovisual resources.  One area of heated debate is whether certain copyright laws, which were originally instated to protect the development of new inventions, are in fact hampering the dissemination of new knowledge due to the restrictions they place on ways information may be displayed or disseminated.  Some scholars contend that copyright laws will become obsolete over the next decades, while others argue that these laws will progressively restrict how scholars use audiovisual media in digital humanities research. Because of the complicated ways these copyright restrictions relate to audiovisual media, they affect films, television and sound more profoundly than digitized books with text and still images. Given these particular challenges faced by digital humanists working with audiovisual materials, a number of questions arise regarding how we might navigate these complex issues concerning copyright, sustainability and long-term storage and access. Can infrastructures essential for shepherding these digital transitions be made available for individual audiovisual digital humanities projects? Can digital humanists look to the university library as the place to support and sustain the websites, datasets and tools created by audiovisual DH research? How can digital humanists secure the massive server space needed to sustain the large-scale storage needs inherent in audiovisual DH projects? Who should oversee the recurring process of inevitable file migration and quality assurance needed for film, photo and sound formats?  Are ‘business-models’ and their potential commercial benefit the way to go or are such arrangements overly optimistic? Are there encouraging examples of successfully sustained audiovisual DH projects that have effectively dealt with copyright issues? Will audiovisual DH scholars become increasingly dependent on philanthropic monopolistic corporations such as Google, Facebook, IBM and Microsoft to sustain their projects? What role should universities play as custodians and advocates of the knowledge produced by our audiovisual DH projects?  Workshop Overview  This full-day workshop will start with a keynote address on multimodal literacy by  Dr. Claire Clivaz, Head of Digital Enhanced Learning at the Swiss Institute of Bioinformatics of Lausanne and active in  #dariahTeach for which she is Head of dissemination and developer of the module Multimodal Literacies. This keynote will be followed by three sessions of paper presentations based around three themes:   Models for training digital humanists in accessing and analyzing audiovisual collections Analysis and discovery models for audiovisual materials Copyright and sustainability  During the fourth session, workshop participants can give very short lightning talks/project pitches of max 5 minutes of ongoing work, projects or plans. Registration for this session will take place during the workshop so no submission is needed for part of the workshop. The workshop will be closed with a plenary & interactive session. All papers will be selected by members of the Programme Committee, following a Call for Abstract which was published at  https://avindhsig.wordpress.com/workshop-2016-krakowcfp/   Programme Committee  Prof. Franciska de Jong, Erasmus University Rotterdam / CLARIN ERIC (chair) Dr. Jakob Kreuzfeld – University of Copenhagen Prof. dr. Julia Noordegraaf – University of Amsterdam Dr. Cord Pagenstecher – Freie Universität Berlin Dr. Marianne Ping Huang – University of Aarhus Dr. Willemien Sanders – Universiteit Utrecht Dr. Khiet Truong – University of Twente Dr. Lars Wieneke – University of Luxembourg Organisers This workshop is organised by the following members of the  ADHO Special Interest Group AudioVisual Material in Digital Humanities (AVinDH):  Dr. Clara Henderson, Indiana University, Bloomington, USA Dr. Martijn Kleppe, National Library of the Netherlands (KB) Dr. Stef Scagliola, Erasmus University Rotterdam Johan Oomen, Netherlands Institute for Sound and Vision ",
       "article_title":"Audiovisual Data And Digital Scholarship: Towards Multimodal Literacy",
       "authors":[
          {
             "given":"Martijn",
             "family":"Kleppe",
             "affiliation":[
                {
                   "original_name":"National Library of the Netherlands, The Hague",
                   "normalized_name":"National Library of the Netherlands",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/02w4jbg70",
                      "GRID":"grid.425631.7"
                   }
                }
             ]
          },
          {
             "given":"Scagliola",
             "family":"Stef",
             "affiliation":[
                {
                   "original_name":"Erasmus University Rotterdam",
                   "normalized_name":"Erasmus University Rotterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/057w15z03",
                      "GRID":"grid.6906.9"
                   }
                }
             ]
          },
          {
             "given":"Henderson",
             "family":"Clara",
             "affiliation":[
                {
                   "original_name":"Indiana University, Bloomington",
                   "normalized_name":"Indiana University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01kg8sb98",
                      "GRID":"grid.257410.5"
                   }
                }
             ]
          },
          {
             "given":"Oomen",
             "family":"Johan",
             "affiliation":[
                {
                   "original_name":"Netherlands Institute for Sound and Vision",
                   "normalized_name":"Netherlands Institute for Sound and Vision",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/025ae8628",
                      "GRID":"grid.425952.d"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-16",
       "keywords":[
          "folklore and oral history",
          "information retrieval",
          "music",
          "teaching and pedagogy",
          "cultural studies",
          "english studies",
          "media studies",
          "art history",
          "film and cinema studies",
          "GLAM: galleries, libraries, archives, museums",
          "historical studies",
          "anthropology",
          "English",
          "audio, video, multimedia"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Although there have been a handful of transmedial explorations by some novelists in the past, narrative in novels has mainly used linguistic elements in conventional print fiction. When the first generation of hypertext novelists remediated print novels in their works they only used language too. However, images, sound, movies and other non-linguistic elements in works of the second generation digital novels—called web-fiction here—have mainly challenged the hierarchy of language and have posed serious threats to the autonomy of words in novels. This challenge has worked both ways, on one hand, works of web-fiction have tried to distance themselves from works which have been merely linguistic (for instance  Afternoon (1990)), on the other hand experimental print novelists have become media-conscious and created novels which have incorporated images, colors, innovative page design and music in their physical body (for instance  House of Leaves (200)). Consequently these media-conscious novels, especially in digital media, have posed serious questions for novel. Some of these questions are: How much narrative in novels is dependent upon words and in what ways can it take advantage of the narrative potentials of the non-linguistic elements? In other words how can non-linguistic elements contribute to the narrative of novels in the media in which they are rendered and in what ways does this new synthesis of words and non-linguistic elements can change our understanding of the narrative in novels? Finally how these novels should be analyzed? In order to find the answers of these questions, two works of web-fiction,  Reconstructing Mayakovsky (2011) by Illya Szilak and  Dreamaphage (2006) by Jason Nelson, and one work of media-conscious print  S (2013) by Doug Dorst and J.J. Abrams have been chosen and the mutual relationship between their linguistic and non-linguistic elements have been explored. All of these works have tried to break down the hierarchy of language in the narrative of the novel and in doing that have highlighted the role of non-linguistic features and have highlighted the ways these features can contribute to narrative in a novel especially in a digital medium.   Analyzing these works will help us to find the answer to a bigger question. How do these mixtures of several media justify their existences as novels? This is where pushing Bakhtin's ideas a little bit further to include works of digital fiction can become extremely useful. Bakhtin’s ideas have been used as the theoretical base of my discussion because out of the three other theories of novel presented in the  Routledge Encyclopaedia of Narrative (2005) , it is the only one which takes the materialities of the production of the text into consideration and according to Howard Mancing “seems most justified by an informed understanding of literary history and theory” (ibid, 399). According to Bakhtin “There is no specific form, technique, theme, or approach to character that makes a text a novel; rather, the distinguishing characteristics of the novel are its heteroglossia and its dialogism” (ibid, 400). Since Bakhtin does not limit dialogism to literature only and believes all language (and all thought) is dialogical, in this paper it has been tried to extend these two concepts into “modes” in both digital and media-conscious print novels. In these novels multiplicity of modes can result in novels in which the words’ hold over the narrative is not as strong as it has been in conventional novels. Such novels can potentially provide a dialogical engagement between linguistic and non-linguistic elements which can eventually lead to a different understanding of what novel is (or it can be). In this way, the current proposal aims at providing a theoretical background and justification for these kinds of narratives which claim to be novels and offers a practical method to read and analyze these novels.   Historical Background The introduction of digital media to the literary scene encouraged a number of experimental novelists—some of whom like Joyce had published novels in print—to try their hand at this medium to create works which were both written on the computer and necessarily had to be read on the screen as well. These writers who were later known as hypertext novelists experimented with the materiality of this new medium, and made its materiality an explicit part of the conception of their novels. Although some conventions of the print medium were discernible in the works of these writers, the arrival of the Internet and the developments in digital media provided a significant opportunity for these writers and the new generation of writers to experiment with new conventions for novel writing in digital media. Moreover, these experimentations with the materiality of the medium encouraged the experimental print novelists of the digital era to experiment more extensively with print as a medium. N. Katherine Hayles is a prominent scholar who has consistently written on the “materiality of the medium” and this paper has heavily borrowed from her theoretical discussions and coinages. Hayles advocates a method of reading called Media-Specific Analysis (MSA) which involves paying particular attention to the materiality of the medium in which the work of fiction is presented. The importance of Hayles’ analytical method is that it provides a practical method for thinking about text as a linguistic object, and provides a new perspective to think and write about texts. Another advantage of Hayles’ approach is that it brings the medium to the foreground from the very beginning and can be applied for the analysis of both print and digital novels. Since the novel as a specific genre and media form developed its defining characteristics and conventions in association with the evolution of print technology, the question of how narrative in novels is transformed through works of hypertext or web-fiction is a significant one. H. Porter Abbott has a useful definition of narrative and his definition will be used as a guide in the controversial subject of narrative and how it should be thought of in the works of hypertext and web-fiction. In Abbott’s definition, “narrative is the representation of events, consisting of  story and  narrative discourse.” Story “is an event or sequence of events (the action), and narrative discourse is those events as represented” (16). The main reason that Abbott’s definition has been chosen here is that it can be applied to the study of narrative in an almost any medium.  The non-linguistic elements which have been used in media-conscious novels are referred to as modes here, therefore a novel which has used several modes in its narrative is a multimodal novel. This usage of the term mode is more in line with the way Alison Gibbons has defined this term as “a system of choices used to communicate meaning.” Looking at novel from the modality perspective, provides us a better understanding of how each work is created out of the different combinations of modalities of three different but related categories. A fictional text uses a specific modality of the text which is the narrative genre. It uses the modality of the medium either print or digital. The last modality which comes into play here is the modality of verbal/visual which is part of the modalities of representation. These modalities can work in different combinations, but segregating them in this way, makes them more visible and shows how each writer can create texts, by manipulating either of these so that the reading process would be affected by the way either of these modalities is brought into play. Thus, in order to study the he fictional works discussed here, three different but interrelated dimensions of the fictional text have been identified. (1) Physical Organization and Design, (2) Narrative Strategy, and (3) Reading Process. This tripartite model can be used by other scholars for analyzing novels which incorporate linguistic and non-linguistic elements in their narrative(s). In the first dimension, Physical Organization and Design a text is analyzed from the perspective of the use of its physical resources its authorship, and design. Narrative Strategy, the second dimension, is the angle through which a text from the perspective of the use of its physical resources and signifying strategies to create a narrative is analyzed. In the third dimension, the Reading Process, the way a text shapes the experience of the reader is explored.   Novels have always been media forms which lead the reader through them to a world which is the real world or like the real world in its spatial/visual form. These worlds exist beyond the page and the language and materiality of the novel are expected to be effaced during this process. However, the digital novels and media-conscious print novels show resistance toward this self-effacement and by mixing different modes in their narrative and bringing their own materiality into the foreground. The narrative and how it is created through the interplay of different modes in a single work is analyzed in these three novels:    Reconstructing Mayakovsky by Illya Szilak. This novel which claims to be the “novel of future” has been published in electronic format in the second volume of ELO in 2011. Later in 2012, a print version of the same novel was published by Revolution Nostalgia Disco Theater. Mayakovsky’s believed that poetry is a mode which can disrupt a fixed narrative and this is exactly what Szilak tries to accomplish in her novel. The narrative in this novel is broken into several modes—or “mechanisms,” as the writer calls them—including text, audio podcasts, video, a live Google image search, etc.   Dreamaphage by Jason Nelson. The interfaces of Dreamaphage have been described as “innovative, surprising, alternately whimsical and unnerving” by ELO website and these words can be used to describe the whole narrative of novel as well. Narrative in this novel is presented through different layers of 3D text with several links to the books which the reader has to simulate the act of turning the page (which means clicking the mouse on the bottom right side of the screen where the edge of the book is and move it to the top left, so that the next page appears on screen), and move through pages filled with squares and circle to read some files which their loose connection together forms a narrative.   S by Doug Dorst and J.J. Abrams. This print novel comes in a slip cover which has a book named “Ship of Theseus” which is full of handwritten comments, as well as postcards, newspaper clippings, black and white photos, even a hand-drawn map written on a napkin from a coffee shop, which together create a mysterious narrative. These non-linguistic materials play such a crucial role in the narrative, that at times it becomes impossible to follow the narrative and solve the mystery of the novel without them. S is described a “love letter to the written word,” on its slipcover, however it is through the interaction of the linguistic and non-linguistic elements that this love letter finds its true meaning.                 ",
       "article_title":"The Mutual Relationship of Linguistic and Non-linguistic Elements in Breaking Down the Hierarchy of Language in Digital Narrative",
       "authors":[
          {
             "given":"Mehdy",
             "family":"Sedaghat Payam",
             "affiliation":[
                {
                   "original_name":"SAMT Center for Research and Development in Humanities",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "hypertext",
          "literary studies",
          "english studies",
          "media studies",
          "content analysis",
          "English",
          "bibliographic methods / textual studies",
          "interface and user experience design"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" For many at this conference, stylometry and authorship attribution need little introduction; the determination of who wrote a document by looking at the writing style is an important problem that has received much research attention. Research has begun to converge on standard methods and procedures (Juola, 2015) and the results are increasingly acceptable in courts of law (Juola, 2013).  The most standard experiment looks something like this: collect a training set (aka \"known documents,\" KD) representative of the documents to be analyzed (the testing set, aka \"questioned documents,\" QD) and extract features from these documents such as word choice (Burrows, 1989; Binongo, 2003) or character n-grams (Stamatatos, 2013). On the basis of these features, the QD can be classified -- for example, if Hamilton uses the word \"while\" and Madison uses the word \"whilst\" (Mosteller and Wallace, 1963) a QD that doesn't use \"while\" is probably Madisonian.  ... unless it's not in English at all, in which case, neither word is likely to appear. The need for the KD to represent the QD fairly closely is one of the major limitations on the use of this experimental methodology. By contrast, the authorial mind remains the same irrespective of the language of writing. In this paper, we report on new methods based on cross-linguistic cognitive traits that enables documents in Spanish to be attributed based on the English writings of the authors and vice versa. Specifically, using a custom corpus scraped from Twitter, we identify a number of features related to the complexity of language and expression, and a number of features related to participation to Twitter-specific social conventions.  We first identified (by manual inspection) a set of 14 user names that could be confirmed to have published tweets in both English and Spanish. Once our user list had been collected, we scraped the Twitter history of each user to collect between 90 and 1800 messages (\"tweets\") from each user and used the detectlanguage.com server to identify automatically the language of each tweet. A key problem is feature identification, as most features (e.g. function words or character n-grams) are not cross-linguistic. For this work, we have identified some potentially universal features. One of the most long-standing (de Morgan, 1851) features proposed for authorship analysis is complexity of expression, as measured variously by word length, distribution of words, type/token ratio, and so forth. We used thirteen different measures of complexity that have been proposed (largely in the quantitative linguistics literature) to create a multivariate measure of complexity that persists across languages. Similarly, we identified three specific social conventions (the use of @mentions, #hashtags, and embedded hyperlinks, all measured as percentage of occurrence) that people may or may not participate in. Our working hypothesis is that people will use language in a way that they feel comfortable with, irrespective of the actual language. Hence, people who use @mentions in English will also do so in Spanish. Similarly, people who send long tweets in English also do so in Spanish, people who use big words in English also do so in Spanish, people who use a varied vocabulary in English also do so in Spanish, and of course vice versa.  We were able to show, first, that the proposed regularities do, in fact, hold across languages, as measured by cross-linguistic inter-writer correlations. (Thus, we also showed that our working hypothesis is confirmed, at least for these traits). Second, we showed via cluster analysis that these measures are partially independent from each other, and thus they afford a basis for a stylistic vector space. (Juola and Mikros, under review). This potentially enables ordinary classification methods to apply. The results reported here show that, in fact, they do.  To do this, we apply normal classification technology (support vector machines using a polynomial kernel) to the vector space thus constructed. We first broke each individual collection into 200 word sections (thus conjoining multiple tweets). Each section was measured using each complexity feature and then raw values were normalized using z-scores [thus a completely average score would be zero, while a score at the 97th percentile would be approximately 2.0; this is similar to Burrows' Delta (Burrows, 1989)]. For our first experiment, the English sections were used to create a stylometric vector space, then the Spanish sections were (individually) embedded in this space and classified via SVMs. For our second experiment, the languages were reversed, classifying English sections based on Spanish stylometric space. Since SVM with polynomial kernel is a three parameter model, we optimized the classifier's performance using a grid-search parameter tuning and comparing 3 different values for each of the three parameters (totaling 3^3 models). The classifier's performance was evaluated using a 10-fold cross-validation scheme and the best single language model was used for predicting the authorship of the texts written in the other language from the same authors. This resulted in 2652 attempts to predict authorship of individual 200 word sections in Spanish, and another 1922 attempts in English, classified across fourteen potential authors. Baseline (chance) accuracy is therefore 1/14 or 0.0714 [7.14%]. Using the English data to establish the stylometric space and the Spanish samples to be attributed yielded an accuracy of 0.095, a result above baseline but not significantly so. By contrast, embedding English data into a Spanish space yielded an accuracy of 0.1603, more than double the baseline. This result clearly establishes the feasibility of cross-linguistic authorship attribution, at least at the proof of concept level. Experiments are continuing, both to establish clearer statistical results, and also to evaluate the additional effectiveness of the Twitter-specific social conventions as features. We believe this result to be the first recorded instance of using training data from one language to attribute test data from another language using a formal, statistical attribution procedure. This is a very difficult dataset using an extremely small set of predictive variables, and the samples (200 words) are very small (Eder, 2013). In light of these issues, the relatively low (in absolute terms) accuracy may still represent a major step forward. Like many research projects, these results pose as many questions as they answer. Why is English->Spanish easier than Spanish->English? What other types of language-independent feature sets could be developed, and how would performance compare? Do these results generalize to different language pairs, or to different genres than social media and Twitter in particular? What additional work will be necessary to turn this into a practical and useful tool? Can this generalize to other authorial analysis applications such as profiling (of personality or other attributes)?  Further research will obviously be required to address these and other issues. In particular, this study is obviously only a preliminary study. More language pairs are necessary (but finding active bilinguals on Twitter is difficult). Studies of other genres than tweets would be informative, but again corpus collection is problematic. We acknowledge that the current accuracy is not high enough to be useful. For the present, however, the simple fact that cross-linguistic authorship attribution can be done and has been done, remains an important new development in the digital humanities.  ",
       "article_title":"Authorship Attribution Using Different Languages",
       "authors":[
          {
             "given":"Patrick",
             "family":"Juola",
             "affiliation":[
                {
                   "original_name":"Duquesne University, United States of America",
                   "normalized_name":"Duquesne University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02336z538",
                      "GRID":"grid.255272.5"
                   }
                }
             ]
          },
          {
             "given":"George",
             "family":"Mikros",
             "affiliation":[
                {
                   "original_name":"National and Kapodistrian University of Athens, Athens, Greece",
                   "normalized_name":"National and Kapodistrian University of Athens",
                   "country":"Greece",
                   "identifiers":{
                      "ror":"https://ror.org/04gnjpq42",
                      "GRID":"grid.5216.0"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "natural language processing",
          "data mining / text mining",
          "content analysis",
          "authorship attribution / authority",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Starting from Manovich’s (2007) statement that “a prototype is a theory”, Galey and Ruecker (2010: 406-07) argue that design can become “a process of critical enquiry itself”, a “thinking through making” pursuit allowing the combination of digital prototyping with critical analysis. Furthermore, Pierazzo (2011: 466) brings into discussion the theoretical assumptions that may underpin the decision making process in building an edition as an “interpretative scholarly product” based on the “selection of features transcribed from a specific primary source”.  The present proposal focuses on the construction of a visualisation framework allowing transformation and visualisation in the browser of XML-TEI encoded documents on European integration history. The tool developed for this purpose, the Transviewer, uses a combination of XML, HTML, XSLT, CSS and JavaScript technologies. The addressed research questions are related to the analysis of this prototyping case, viewed as a dynamic and iterative process of evaluation, adjustment, decision making, adaptation and in-house development. Our standpoint, inspired by the above mentioned studies, is that such type of analysis can shed light on the theoretical and practical questions, at the crossroad of tradition and new ground, involved in the creation of scholarly digital tools.   The Transviewer prototype The Transviewer concept consists of building a framework for the publication of European history documents on the CVCE’s Website, from treaties, official declarations and meeting reports to letters and interview transcriptions. In a first phase, a pilot testing set for the prototype (Figure 1) has been encoded in XML-TEI P5, including a selection of 55 documents on armament issues within Western European Union (WEU), from 1950’s to 1980’s.    Figure 1. Transviewer. Side-by-side view digital facsimile (left) and transcription (right) (WEU sample)  In line with Booth et al. (2008), Galey and Ruecker (2010: 412-13) consider that a prototype can be the embodiment of an argument (or more), with all the key components of a “good thesis topic”: to be “contestable”, “defensible”, and “substantive”. This refers to how a prototype includes old affordances in a new way or proposes something new, to its potential of convincing people to accept it, or finally, to its intellectual and practical value.  Our idea in designing the Transviewer has been based on the following “arguments”:  The historians or researchers in European integration studies (the CVCE’s main category of readers) are always interested in comparing a transcription with the original (when available). The architecture of the visualisation framework should be multi-project-oriented and support multiple types of historical documents (primary/secondary sources - text/image/audio/video).   A number of features to be encoded and rendered via the interface have been considered after consultations with the users (CVCE's researchers):   Table 1. “Grid of features” (Pierazzo, 2011: 467) encoded in the transcription (WEU sample)   Transcription   Feature type Encoded Ignored   Documentary Ink colour of stamps (red, black) Ink colour of handwritten text   Topology Document layout (position and alignment of headers/footers/ headings)    Writing Capitalisation and punctuation Empty lines and exact vertical spacing on the page   Handwriting Handwritten elements from header/headings Handwritten fragments, sometimes not legible, from the body of the text   Textuality Paragraphs and structural divisions    Semantics Named entities (e.g. names of persons/ organisations/places/functions/events/ products, dates)    The current version of the prototype supports functionalities such as side-by-side view of digital facsimile and transcription, page-by-page navigation, zoom-in/out, vertical scrolling, search (by names of persons/places/organisations, dates).   Decision points The initial Transviewer concept was inspired by the EVT (Edition Visualization Technology) model which proposes a “client-only architecture” based on XSLT transformation, HTML, CSS and JavaScript, and allows side-by-side view of manuscript images and related text (Rosselli Del Turco et al., 2014-2015).    Figure 2. EVT. Fragment (Vercelli Book)  Although the direct adaptation of EVT was considered from the beginning, different requirements for EVT and Transviewer project have been identified.   Table 2. EVT/Transviewer differences  Characteristics EVT Transviewer   General architecture Project-oriented (Vercelli Book manuscript) Multi-project-oriented (documents/collections in European integration history)   XML-HTML transformation granularity Page-oriented (one HTML file per page/manuscript image) Document-oriented (one HTML file per XML document)   XML-HTML transformation process HTML generation XML transformation on the fly, in the browser, and XHTML generation    Transcription Line-oriented Structure-oriented (divisions, paragraphs) with semantic annotations (named entities).   Navigation Page-by-page (supported by a single HTML file per page implementation) Page-by-page and vertical scrolling (applied to a whole document)   Image management/loading One by one Images for a whole document   Therefore, the EVT concept of side-by-side view of digital facsimile/transcription has been combined with the integration of third-party libraries (BookReader, Saxon-CE) and in-house development. The first one has been chosen for enabling on the fly loading of images, the second for supporting XSLT 2.0 transformations in the browser.     Figure 3. BookReader. Fragment (Don Quixote de la Mancha)  The in-house development mainly comprised modules for the implementation of a core/project specific architecture including elements of configuration, frames and buttons layout/actions, XSLT transformation and transcription rendering. A simplified diagram of the decisions points in the first development cycle of the Transviewer is shown in Figure 4.    Figure 4. Transviewer. Decision points (diamonds) in the first prototyping cycle  As illustrated, the prototyping process can imply multiple iterations. Once a functional prototype is built, the decisions on further development may reiterate similar phases of conceptualisation, search for solutions, implementation, evaluation and decision making. In this respect, our theoretical approach could be framed at the crossroad of “iterative prototyping” (Buxton and Sniderman, 1980; Buchenau and Suri, 2000; Lucena and Astua, 2012), “user-centered design” (Shneiderman and Plaisant, 2009; Warwick et al., 2009; Gibbs and Owens, 2012), and scholarly digital editions (Pierazzo, 2011; Rosselli Del Turco, 2011).   Evaluation Although partial evaluation had been carried out throughout the prototyping cycle, a more formal testing and evaluation phase has been conducted on the first functional version of the Transviewer. Several facets of assessment have been taken into account:  Table 3. Transviewer evaluation facets  Facet Short description User group/stakeholders   Technology Technical issues have been identified and worked upon (e.g. non-uniform support for Saxon-CE in different browsers and the use in BookReader of an older version of the jQuery library). Another evaluation aspect, related to argument 2, consisted of proving the scalability of the framework by testing it with different projects samples. CVCE's development team   User The prototype functionalities and argument 1 have been evaluated via usability tests (Nielsen, 2000; Lund, 2001) mainly enquiring on the ease of use, ease of learning, usefulness and user satisfaction (on a scale from 1 to 5), and on suggestions for potential improvement. Internal/external researchers as end-users of the Transviewer    Impact Beside the impact on the final product (e.g. how many/in what proportion the functionalities/features/arguments of the prototype are reflected in the final product), another point of interest concerns its impact on other products, in particular, the initial models having inspired it. Cooperation partners, research infrastructures     Discussion As an outcome of the analysis on the first prototype cycle, a new iteration and development reassessment is currently ongoing. The analysis helped us to: (1) identify and amend technical (see Table 3) and design issues (e.g. functionalities not quickly accessible or requiring extra-effort, unclear terminology or functionalities hierarchy, simplification); (2) adapt the encoding to support a larger variety of objects to be visualised (e.g. transcription only, facsimile only, transcription and audio/video); (3) better understand the needs of the users while dealing with historical documents (e.g. “trust” and “contextualisation” seem to play an important role).  Moreover, the development of new technical tools poses not only challenges to the process itself and the methods it introduces or refines but it also raises the question of sustainability: project based funding stimulates the development of new tools between different entities but far too often, this development stops once funding runs out. In the context of the informal and not project-funded cooperation between the CVCE and the EVT team, we therefore want to explore the trials and tribulations of building and maintaining a shared codebase that would be beneficial for both entities even though our specific use cases differ. As a result of the experiences made in this ongoing process of building the new version of the EVT framework, we will take the opportunity to discuss the practical challenges, opportunities and limitations of open-source development models for digital humanities projects as an approach to achieve sustainability.  In other words, we are not just testing an application but a development framework which implies considering complex theoretical-practical elements: Human-Computer Interaction (HCI) aspects, Information Technology (IT) methods and tools, specific types of documents to be published, projected audience, collaborative strategies and sustainability, etc.   Conclusion The article proposes an analytical approach to the prototyping of a visualisation framework for historical documents. Its main assumption is that a critical perspective can be applied not only to a finished digital artefact but also to the process of its creation. In line with more traditional methods of criticism from the Humanities, a \"thinking through making\" viewpoint may bring into light theoretical and practical aspects related to the construction of digital tools and to the mechanisms of the \"laboratory\". A prospect that, by its nature, positions itself at the crossroad of the old and the new, of the past and the future.  ",
       "article_title":"Prototypes as Thinking through Making. Decision Points and Evaluation in Prototyping a Visualisation Framework for Historical Documents",
       "authors":[
          {
             "given":"Florentina",
             "family":"Armaselu",
             "affiliation":[
                {
                   "original_name":"Centre Virtuel de la Connaissance sur l'Europe (CVCE), Luxembourg",
                   "normalized_name":null,
                   "country":"Luxembourg",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Roberto",
             "family":"Rosselli Del Turco",
             "affiliation":[
                {
                   "original_name":"Università di Torino",
                   "normalized_name":null,
                   "country":"Italy",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Catherine",
             "family":"Jones",
             "affiliation":[
                {
                   "original_name":"Centre Virtuel de la Connaissance sur l'Europe (CVCE), Luxembourg",
                   "normalized_name":null,
                   "country":"Luxembourg",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Lars",
             "family":"Wieneke",
             "affiliation":[
                {
                   "original_name":"Centre Virtuel de la Connaissance sur l'Europe (CVCE), Luxembourg",
                   "normalized_name":null,
                   "country":"Luxembourg",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Chiara",
             "family":"Alzetta",
             "affiliation":[
                {
                   "original_name":"Università di Pisa",
                   "normalized_name":"University of Pisa",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/03ad39j10",
                      "GRID":"grid.5395.a"
                   }
                }
             ]
          },
          {
             "given":"Chiara",
             "family":"Di Pietro",
             "affiliation":[
                {
                   "original_name":"Università di Pisa",
                   "normalized_name":"University of Pisa",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/03ad39j10",
                      "GRID":"grid.5395.a"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "visualisation",
          "xml",
          "digital humanities - nature and significance",
          "encoding - theory and practice",
          "user studies / user needs",
          "historical studies",
          "scholarly editing",
          "English",
          "interface and user experience design"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction This paper presents the results of an effort that our research team has done in order to develop an OWL 2 ontology to formally define the semantics of the Text Encoding Initiative markup language. The preliminary steps of this research project have already been presented at the TEI Conference in 2014 and 2015 (Ciotti and Tomasi 2014, Ciotti et al., 2015). We believe that our work has reached a satisfactory level of development, both on the theoretical side and in the practical implementation.   Why an ontology for TEI The reasons to have a formal and machine-readable semantics for TEI are manifold. In the first place we can set forth a list of pragmatic and technical benefits that have been already pointed out in many previous works dedicate to this topic, that dates back to the mid-90s (Di Iorio, Peroni and Vitali, 2009; Ciotti and Tomasi, 2014). Here is a brief summary of those arguments:  enabling parsers to perform both syntactic and semantic validation of document markup; inferring facts from documents automatically by means of inference systems and reasoners;  simplifying the federation, conversion and translation of documents marked up with different markup vocabularies; allowing users to query upon the structure of the document considering its semantics.  The advantages envisioned in this list are not specific to the TEI or aim to facilitate the relationships between different markup languages; but some of the issues have special relevance for TEI and for the usage of TEI inside its reference community. Take for instance the query issue: we all know that there are many ways of expressing one and the same textual feature in TEI markup, so that it is very difficult to query heterogeneous TEI corpora and text archives. Having a set of ontological definitions of the conceptual level behind markup, that is, a set of shared formal definitions of the textual features to which any single encoding project could bind idiosyncratic markup usage, could help solve this problem. The same argument could be made for a far more adequate management of interoperability of TEI text collections between different repositories or applications. But we believe there is also a deeper theoretical and foundational advantage in the idea of an ontological semantic model for TEI. It is a commonly acknowledged notion that the very core of digital methods application in humanities research is the notion of model/modeling. The pair of terms “model/modeling” is deplorably understood in many different ways in the community. We think that, as far as we are using Turing machine like device for computation, the only workable notion of modeling is a formal one: model we should be interested in are formal models. Where formalization is to be understood as a series of semiotic processes that generates an algorithmically computable representation of one (or more) phenomenon/object. It is widely recognized that the TEI is not only a markup facility but first and foremost a conceptual model of textuality. In fact, the Guidelines (TEI Consortium, 2015, chap. 23) explicitly introduce the notion of a TEI Abstract Model. The fact is that the notion of an abstract model is used in many formal procedures but this very notion is not formally defined. This ends up in a lot of problems and circularities. We think that we need to have a formalized account of the quasi-formal notion of TEI abstract model, if it has to be of any use other than a sort of regulatory principle. We do not advocate going back to a monist theory of textuality. Our suggestion to adopt contemporary Semantic Web formalisms to build this abstract conceptual model give us the possibility to have a “foundation” of TEI in a well-defined data model that is not dependent on the notion of a single hierarchical “ordered hierarchy of content object” (OHCO, DeRose et al., 1997), and that can accommodate, at least to some extent, the “pluralities” of textuality.    Structure of the ontology TEI as a whole is very complex, and its usage is governed by pragmatics and contextual requirements. We acknowledge that it is impossible to reduce to a unique formal semantic definition this fuzzy cloud. Though, we can identify a subset of shared assumptions, a common ground of notions about the meaning of TEI markup and the nature of documents like object: we think that this subset can be the object of an ontological formalization. For various reasons we have adopted the TEI Simple customization (Cummings et al., 2014) as an acceptable approximation of this common ontology. This is not an opportunistic ad hoc choice, as it may seem. TEI Simple in fact has been defined by a group of domain expert that have analyzed the actual usage of TEI markup in some big textual repositories and have selected and organized a set of one hundred or so elements that can describe all the textual features represented by the markup in those documents. This fits perfectly in the definition of a formal ontology development process. The main design requirements for building our ontology have been the following:  the ontology must express at the same time an abstract characterization of TEI Simple elements' semantics and an ontological definition of their structural role; the ontology must define a precise semantics of the elements having a clear characterization in the official TEI documentation (e.g., the element <p>), while it should relax the semantic constraints if the elements in consideration can be used with different semantic connotations depending on the context (e.g., the element <seg>); it must be possible to extend the ontology, reuse it and define alternative characterizations of elements semantics without compromising the consistency of the ontology itself; where possible existing ontologies or meta-ontologies must be reused  In accordance with these overall principles we have decided to implement a complex architecture using some pre-existing meta-ontology frameworks to express the meaning of TEI element set by the way of the classes and properties they define. In particular we have adopted: 1) LA-Earmark (Di Iorio, Peroni, Poggi, Vitali, 2011; Peroni, Gangemi, Vitali, 2011), a markup metalanguage, that can express both the syntax and the semantics of markup as OWL assertions, and an ontology of markup that make explicit the implicit assumptions of markup languages. LA-EARMARK is an extension of EARMARK with the Linguistic Act module of the Linguistic Meta-Model that allows one to express and assess facts, constraints and rules about the markup structure as well as about the inherent semantics of the markup elements themselves.  2) Structural Pattern Ontology (Di Iorio, Peroni, Poggi, Vitali, 2014), whose goal is to identify a small number of patterns that are sufficient to express how the structure of digital documents can be segmented into atomic components. The specification of markup semantics for the various TEI Simple elements is done by means of LA-EARMARK class and properties. The general Earmark class for any markup element is earmark:Element. The <abbr> element is defined as follows: Prefix earmark: <http://www.essepuntato.it/2008/12/earmark#> Prefix co: <http://purl.org/co/> Prefix tei: <http://www.tei-c.org/ns/1.0/> Class: tei:abbr a   earmark:Element that  earmark:hasGeneralIdentifier \"abbr\" and  earmark:hasNamespace \"http://www.tei-c.org/ns/1.0\" LA-EARMARK allows us to link particular class of elements with the actual semantics they express. From our point of view there are at least two semantic levels that we explicitly define:  one concerning the structural behavior of markup that is described by means of the Pattern Ontology (PO); the other regarding the intended semantics of an element (e.g., the fact that an element is a paragraph rather than a section, a personal name reference rather than a geographical reference), that is described by TEI Semantics Ontology or by a combination of already existing ontologies.  TEI Semantics Ontology is the core component that gives the actual semantics of TEI elements. Its definition is based on a categorization of the elements of the TEI Simple, based on a refactoring of the TEI model Classes.  The link between the class describing kinds of elements and their related semantic characterization is possible by means of the property “semiotics:expresses”. The associations of semantics to markup elements can be contextualized according to a particular agent's point of view, in order to provide provenance data pointing to the entity that was responsible for such specification. This is possible by means of the Linguistic Act Ontology included in LA-EARMARK that allows one to consider all these markup-to-semantics links as proper linguistic acts done by someone.   Conclusions The work we have done so far is limited to the Simple subset of TEI. We envision some further development:  Refine the TEI Semantics Ontology component. Extend to some other areas of TEI that are suitable for formalization.  We think that in the long term this ontological formalization could become the primary formalization of the TEI encoding schema, independently of any serialization format. Today XML is still the better strategy to encode digital texts in real word projects for many practical reasons. But there is no reason for the TEI to be strictly based on it, as it is  de facto now. Technical issues should not determine the choice of a formalization language. In the end, we believe that our effort can give a substantial contribution to the TEI to envision the shape of its own future.   ",
       "article_title":"An OWL 2 Formal Ontology for the Text Encoding Initiative",
       "authors":[
          {
             "given":"Fabio",
             "family":"Ciotti",
             "affiliation":[
                {
                   "original_name":"Università di Roma Tor Vergata, Italy",
                   "normalized_name":null,
                   "country":"Italy",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Peroni",
             "family":"Silvio",
             "affiliation":[
                {
                   "original_name":"Università di Bologna, Italy",
                   "normalized_name":"University of Bologna",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/01111rn36",
                      "GRID":"grid.6292.f"
                   }
                }
             ]
          },
          {
             "given":"Tomasi",
             "family":"Francesca",
             "affiliation":[
                {
                   "original_name":"Università di Bologna, Italy",
                   "normalized_name":"University of Bologna",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/01111rn36",
                      "GRID":"grid.6292.f"
                   }
                }
             ]
          },
          {
             "given":"Vitali",
             "family":"Fabio",
             "affiliation":[
                {
                   "original_name":"Università di Bologna, Italy",
                   "normalized_name":"University of Bologna",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/01111rn36",
                      "GRID":"grid.6292.f"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016",
       "keywords":[
          "xml",
          "encoding - theory and practice",
          "ontologies",
          "knowledge representation",
          "English",
          "semantic web"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" James Joyce is one of the most admired, emulated and mythologised masters of twentieth-century prose. His  Ulysses (1922) is among the cardinal texts of literary modernism produced between the world wars. The text creates its numinous effect through the repetition of short phrases over the course of a quarter of a million words. As early as 1929, critics invoked the musical device of leitmotif to explain this form of literary repetition (Curtius, 1929). Leitmotif describes a signature phrase or cue that accompanies and signals the presence of a character, locale or theme in a work. The device achieved a new importance in the nineteenth century through the opera of Richard Wagner, and it quickly migrated from music to literature in the writings of Édouard Dujardin, Thomas Mann and Marcel Proust. In Joyce’s hands, the pervasive use of the device amounts to “a sort of linguistic magic-realism” (O’Callaghan, 2011). But whereas musical leitmotif is conveyed through auditory recall – listeners recognise a brief musical phrase as an instance of leitmotif – written language cannot always offer this immediacy. The problem is a singular one: how do readers and how do computational tools recognise and unify the discrete instances of leitmotif that are distributed across a text?   Methodology The proposed paper will report on a series of experiments that combine methods of corpus and computational linguistics with close reading to gauge a fuller extent of the repetition in the novel and to assess its worth as leitmotif. From the perspective of text analysis, leitmotif is the purposeful repetition of textual fragments (or n-grams) in a text or over a collection of textual documents. Our computer assisted retrieval of leitmotifs is based on the following set of assumptions: First, for a sequence of words to be a leitmotif, it must capture a (human) reader’s attention through distinctive word choice or unsual collocation. This distinctiveness ensures a reader can recall earlier occurrences. Second, a given sequence of words functions as a leitmotif only if it occurs in a limited number of chapters or episodes of  Ulysses. The sequence must occur in more than one episode but not in all eighteen. Key to distinguishing a given sequence is a limitation in the number of its occurences. Finally, previous scholarship has observed that Joyce not only repeated leitmotifs verbatim but also paraphrased, transposed, abbreviated or otherwise altered their individual elements (see Büchler et al., 2014 in this context). A computational approach needs to take stock of this state of affairs.   Work to Date At the current stage of our research, we focus on bigrams. By treating each episode of the novel as a single document, we have constructed a document collection to measure the inverse document frequency (idf) of bigrams in the collection. The use of idf metrics enables us to retrieve those bigrams occur in a given number of episodes. Initially we set a threshold of occurrence at two, three and four episodes; in the light of the results produced we are continually revising this threshold. As a second step, the uniqueness or distinctiveness of bigrams was investigated by measuring the probability of their occurrence in a contemporary corpus of English texts put together from documents in Project Gutenberg and Archive.org. This helped to filter out very common constructions that enjoy a high idf. Named entities were also eliminated from the list of bigrams. We have also examined bigrams for “fuzziness” – gauging whether the constituent elements reoccur but with words inserted between them or in a reversed or otherwise transposed order. We have produced a list of around two hundred bigrams that are candidates for consideration as leitmotifs. Finally, a striking feature of Joyce’s style in  Ulysses is the frequent use of compound coinages. Not only did he insert such neologisms into the text, but he also split them, reusing the constituent units of compounds in close proximity. This stylistic device also contributes to the pervasive sense of repetition in  Ulysses, functioning as an alternative to and special case of leitmotif. To identify this type of repetition, we have retrieved and split all compound coinages in the novel, and examined whether their constituents reoccur within a given word distance.    Challenges The main obstacle to identification is the “protean nature” of leitmotif itself (Bribitzer-Stull, 2015). Whereas a highly distinctive phrase like Joyce’s “Agenbite of inwit,” which occurs in  Ulysses seven times (Joyce, 1922), can readily be identified as a leitmotif, the associative potential inherent in even very short units of language means it is not always clear how one is to distinguish leitmotif from mere linguistic flukes or from instances of more general intertextuality (Kristeva, 1967). Examples of the latter include lexical priming (Hoey, 2005) and natural language collocation – for example, if an author writes “he opened the,” a reader of English would reasonably expect the next word to be “door.” At the moment, our list of candidate leitmotifs is checked by close reading bigrams within the context of the sentences in which they occur in the novel. Those bigrams that are accepted or judged as valid instances of leitmotif are added to a purpose-built database.    Outcomes In addition to extending the inventory of leitmotifs identified in  Ulysses (see, for example, Schutte, 1982), the experiments and the subsequent analysis have another key goal. They are meant to test the hypothesis that leitmotif not only offers a way to flag the presence of a character, locale or theme in the novel, but also creates a series of “primitive hyperlinks” or “analogue hyperlinks” (Cope and Phillips, 2006) threaded throughout  Ulysses. Critics have often invoked hypertext to explain the myriad connections established within Joyce’s work (e.g. James, 1999), but our research will lead to an online hypertext edition of  Ulysses that allows the reader to explore the non-linear reading paths created by leitmotif.   ",
       "article_title":"Mining Leitmotif in James Joyce’s ‘Ulysses’",
       "authors":[
          {
             "given":"Ronan",
             "family":"Crowley",
             "affiliation":[
                {
                   "original_name":"Universität Passau, Germany",
                   "normalized_name":"University of Passau",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/05ydjnb78",
                      "GRID":"grid.11046.32"
                   }
                }
             ]
          },
          {
             "given":"Gábor",
             "family":"Mihály Tóth",
             "affiliation":[
                {
                   "original_name":"Universität Passau, Germany",
                   "normalized_name":"University of Passau",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/05ydjnb78",
                      "GRID":"grid.11046.32"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016",
       "keywords":[
          "literary studies",
          "english studies",
          "data mining / text mining",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" This study examines a “pulp” science fiction corpus (1930–1965) through corpus linguistic analysis in order to digitally reconstruct the gendered occupational identities created by those authors, and the culture they represent, which perpetuated a stereotype of “the scientist” and how they characterized women in professional scientific roles. I created “occupational archetypes” based on the linguistic analysis of collocates, clusters, and textual examples of science, technology, engineering, and math (STEM) career keywords in order to investigate the culturally informed gender roles demonstrated in modern stereotypes of the scientist. I chose to study pulp science fiction, a sub-genre of science fiction literature that enjoyed a wide readership during the formative decades of the creation of the scientific industrial complex in pre and post-war America. One way to get at the culture that created and then maintained our national scientific industrial complex is through examining the stories about science that people of that time produced. For indeed, stories, and even more simply, language are a transmitter of social and cultural values, especially when it comes to gender roles (Rey, 2001). Pulp science fiction existed as a sub-genre of science fiction from roughly 1930 to 1965, characterized by its wide audience and affordability. The accessibility and engaging style of this literary genre gained it a wide readership, and “the pulps” as they came to be called quickly became a feature of American life during the pre and post-war eras. These stories, and indeed the genre at large, represent popular conceptions of “appropriate” gender identities and reinforce those occupational stereotypes that play such a key role in the lives of women scientists.  The aim of corpus linguistics is to study patterns of language at their most fundamental level of words and phrases, thereby revealing patterns of meaning, making the implicit explicit (Biber, 1998; Biber, 2009; Stubbs, 2001; McEnery, 2001; Lakoff, 2008; Kennedy, 2014; Hettel, 2013). Since patterns of meaning are precisely what I wished to investigate with respect to gendered occupational stereotypes, this method served as the basis for my study. Corpus linguistics is able to harness the power of Moretti's distant reading approach (Moretti, 2013) in uncovering the scope and and nature of the literature, while also providing clues as to which specific pieces within a corpus merit a close reading. The corpus I constructed for this project consists of 560 full text copies of pulp science fiction stories from 1930 to 1965 (totaling just over 6 million words), published in magazines like Astounding Stories, Amazing Stories, Analog Science Fact and Fiction, Planet Stories, and If Worlds of Science Fiction. These full texts were obtained from public repositories, principally Project Gutenberg (https://www.gutenberg.org/) and The Internet Archive's Pulp Magazine Archive (https://archive.org/details/pulpmagazinearchive). While some of these stories were already conveniently in plain text files, others were scanned copies of the original pulp magazine pages stored as image files. The latter I converted into plain text through the application of Tesseract, an open source optical character recognition (OCR) program. I then organized these stories according to their date of publication in the magazines, stratifying according to five year periods: 1930-34, 1935-39, etc. Each of these five year periods contain 80 stories, coming to 560 in total. This stratification allows for representativeness through ensuring that all five year periods were weighted proportionally over time (Sinclair, 2004).  When I finished constructing the corpus, I used the software suite WordSmith Tools to generate keyword lists for the corpus in its entirety, in addition to each five year period respectively. When I generated the keyword lists for each five year period, I used the rest of the pulp science fiction corpus as my reference, in order to track how these words were being used over time (Bondi, 2010). From these general keyword lists, I chose the keywords which represented careers or occupations that constitute or interact with STEM disciplines: scientist, engineer, mathematician, doctor, nurse, and professor. I then used WordSmith Tools to analyze measures of association for the above science, technology, and engineering occupational words. By focusing on the language used to describe occupations related to the sciences, I was able to get a picture of the characterization of these professions at the time the stories were published. I also did a collocation analysis in order to uncover the words that most frequently co-occurred with these keywords, limited to five words to the right and left of the key word in question (the node). The character of the collocates reflects the nature of the node, and the distinctions offered by collocations are subtle, yet crucial to the creation of a linguistic profile (Hettel, 2013). In order to determine which collocates were statistically significant, I evaluated the association by its t-score, a statistic which works well with smaller corpora (such as mine) because it also takes frequencies into account, as opposed to mutual information (MI). Using the STEM occupation keywords, collocates, clusters, and qualitative examination of specific examples of the node words in context, I then created “lexical profiles” of each of these science, technology, engineering, and related occupations, which I'm terming “occupational archetypes.” The development of these archetypes is based largely on the work done by Hettel on the construction of lexical profiles from collocations, clusters, and context in the language of US nuclear plants and regulatory entities. Specifically, the archetype I constructed of “the scientist” revealed a middle aged white male, defined by his adherence to “true” or “good” science, and often called upon by other characters to provide scientific or technical insight. Though he spends a good deal of his time talking, others struggle to grasp his meaning and find him difficult to deal with. This occupational archetype is a mirror image of the American stereotype of scientific professionals, one which leaves no room for diversity in race or gender. Furthermore, through this analysis, I discovered that out of the hundreds of scientists in my corpus, only three were women. A linguistic analysis of these women in particular (a chemist, a physicist, and a mathematician) revealed American cultural assumptions about the intersection of femininity and science: a female scientist could either be beautiful or accomplished. And even then, the chemist's beauty came with exploitation (and the physicist's ugliness with prestige), and scientific genius in these women necessitated qualification (i.e. genius “in her own way”) while that of their male colleagues did not. The mathematician, the one woman in the corpus with beauty and brains, so to speak, appeared very late on the scene, and perhaps signals a shift in the cultural conception of who a scientist could be and what they could look like. Making these entrenched cultural stereotypes of women in science explicit through linguistic analysis is the first step in creating a STEM workforce strong through its diversity and acceptance.  ",
       "article_title":" Pulp Science Fiction's Legacy to Women in Science  ",
       "authors":[
          {
             "given":"Elizabeth Winfree",
             "family":"Garbee",
             "affiliation":[
                {
                   "original_name":"Arizona State University, United States of America",
                   "normalized_name":"Arizona State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03efmqc40",
                      "GRID":"grid.215654.1"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-22",
       "keywords":[
          "digital humanities - pedagogy and curriculum",
          "digital humanities - diversity",
          "gender studies",
          "corpora and corpus activities",
          "archives, repositories, sustainability and preservation",
          "linguistics",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The results of stylometric authorship attribution studies are strongly influenced by four choices:   Candidate Authors – The choice of candidate authors should be based on the historical context of the texts to be attributed.   Representative Texts – Representative texts should be chosen that are similar in genre, topic and time frame as the texts to be attributed (Argamon et al, 2003; Stamatatos, 2009).   Analytical Method – Many analytical methods are available. Burrows’ Delta is often considered to be the ‘gold standard’ to compare other methods (Burrows, 2002).   Distinguishing Features – One list of features to distinguish among candidate authors can provide greater distinguishing power than another list of features. This paper is about identifying the most distinguishing list.   Grammatical function words are used by all authors, but authors do not use function words in the same way or with the same frequencies. Therefore, different usage frequencies for function words are useful in characterizing an author’s writing style. Although the specific function words that are distinguishing among authors vary from study to study, their effectiveness as features to set apart an author’s writing style is well established (Mosteller and Wallace, 2007; Holmes, 1998). Discriminant analysis is a statistical technique to classify objects into known categories based on a set of features about those objects. The technique was developed by Sir Ronald Fisher, a botanist. He illustrated the technique by classifying iris flowers into three species using four features – the length and width of sepals and petals (Fisher, 1936). The approach is to compute linear combinations of the features that best separate the categories from each other. The most distinguishing combination of features is called the first linear discriminant function (LD1). Additional combinations (LD2, LD3 and so on) are computed that are orthogonal to each other to maximize the separation among categories. After computing the discriminant functions using a training set of data for objects with known classification, the discriminant functions can be used to classify objects of unknown classification into the categories to which they most likely belong. The discriminant analysis concept is illustrated in figure 1 for a two-category problem and two dimensions. Each ellipse in the graph encircles the items within a category. LD1 shows the direction of greatest separation between the two categories. Discriminant analysis can be extended to classification problems with any number of categories and dimensions.    Figure 1. Graphical illustration of discriminant analysis for two categories with two discriminant functions  Discriminant analysis can be used in authorship attribution since the problem is similar to that of classifying plants into species based on their physical features. In attribution, the process is to use a set of texts of known authorship to determine the discriminant functions using non-contextual words as the features, and then classify texts of unknown authorship into the set of authors using the discriminant functions.  A variation of discriminant analysis called stepwise discriminant analysis (SDA) first determines a subset of the most distinguishing features from a comprehensive list of features and then formulates the discriminant functions (Goldstein and Dillon, 1977). The most distinguishing features are the best predictors for classifying objects into the proper categories. n our stylometric work we have observed the utility of SDA to choose the words to use as distinguishing features in authorship attribution studies. This observation agrees with work done by other researchers (Smith and Aldridge, 2011). From a comprehensive list of non-contextual words, SDA identifies the most discriminating word first and subsequent words in descending order of discriminating ability. It stops when none of the remaining words add to the discriminating ability of the set of words. Thus, we end up with a subset of words that are the best predictors of authorship.  Another approach often used to select distinguishing features for authorship attribution is to use a list of the most frequent words listed in descending order of frequency in a set of representative texts of the candidate authors’ works. Consequently, we considered this research question: For a given set of authors and representative texts, and using Burrows’ Delta as the analytical method, will the most distinguishing words (MDWs) identified by SDA give more distinguishing power in the analysis than using the most frequent words (MFWs) approach? The corresponding null and alternative hypotheses are: H 0: Using non-contextual MDWs selected by SDA is not more distinguishing among candidate authors than using a set of MFWs.  H a: Using non-contextual MDWs selected by SDA is more distinguishing among candidate authors than using a set of MFWs.    Method To answer our research question, the metric we used for a set of words’ distinguishing power was the difference in Burrows’ Deltas for the two authors with the smallest Deltas to that text. If the null hypothesis is true, the differences between Deltas should be about the same whether using MDWs or using MFWs. If using MDWs produces larger Delta differences than using MFWs, that evidence would support the alternative hypothesis.  We used the difference in Deltas between the nearest authors because it is an indication of statistical power. Analogous to the power of a microscope, statistical power is a statistical technique’s ability to distinguish between things that are close together. The greater the distance between Deltas, the greater the power of the technique used to calculate the Deltas. To conduct the study we used  The Federalist Papers, commonly used for testing the usefulness of authorship attribution methods.  The Federalist Papers are well suited to the problem as there were a total of 85 published papers written by Alexander Hamilton, James Madison and John Jay. Fifty-one were known to have been written by Hamilton, fourteen by Madison, five by Jay, and three written jointly by Hamilton and Madison. Twelve had disputed authorship, but have subsequently been studied extensively and are commonly attributed to Madison.  Because the attribution of the disputed papers is relatively non-controversial,  The Federalist Papers provide a useful basis for comparing attribution methods. Since our research objective was not to answer the attribution question, but rather to compare methods of answering the question, using  The Federalist Paper removed the question of correct attribution for a more direct comparison of the distinguishing ability of SDA-selected MDWs compared to MFWs.  Using only the 70 papers of known authorship as the representative texts, we applied SDA and selected the MDWs from a large list of non-contextual words, and then calculated Burrows’ Delta distances for each paper to each of the three candidate authors. We compared these results to the results of using sets of MFWs ranging from 50 to 500 words in increments of 50 words. Results The SDA procedure select 29 words as the most distinguishing words for  The Federalist Papers. Those 29 MDWs produced 100% correct classification of the 70 representative texts and provided greater distinguishing power than MFWs for the 12 disputed texts. As shown in figure 2, for  The Federalist Papers, MDWs have from 1.5 to 4 times the discriminating power of MFWs.     Figure 2. Comparison of the discriminating power of MDWs vs. MWFs  To understand why this occurs, examine table 1 and notice where each of the 29 MDWs appears on the MFW list.   Table 1. Comparative ranking of MDWs and MWFs for The Federalist Papers  Words in a list of MDWs often are not included in typical MFW lists. For example, note that the word,  whilst, is the third most discriminating word selected by SDA and yet it is not even in the top 1000 MFWs. Even though Mosteller and Wallace identified  whilst as a key indicator of authorship for the disputed papers, MFW lists of less than 1231 words would miss this highly distinguishing word. Notice further that 12 of the 29 MDWs are not even in the top 200 MFWs. So using MFWs will miss many highly distinguishing words.    Discussion Thus, we reject the null hypothesis and assert that MDWs provide more distinguishing power between the Deltas for the two closest authors to the texts to be attributed as compared to MFWs. Our results show that MDWs can provide greater sensitivity than MFWs in discovering stylistic word-choice differences among candidate authors. The finding that it only takes 29 words selected by SDA to correctly classify all of the disputed Federalist Papers is a striking example of the power of using SDA-selected MDWs, since over 350 MFW words – more than ten times as many words – were required to achieve the same results. Although some research has shown that variations of Delta may perform better than Burrows’ original formulation (Evert et al., 2015; Hoover, 2004), we have found that using modifications of Delta does not improve the performance of MFWs relative to MDWs.  Conclusion We conclude greater discriminating power can be achieved with a small set of MDWs chosen by SDA than with even large sets of MFWs. Using SDA-selected MDWs a researcher is more likely to make correct attributions and may be able to do it with fewer representative texts and for smaller texts. As a result, a researcher will have a greater likelihood of discovering new insights about the possible authorship of unattributed or disputed texts.  ",
       "article_title":" Choosing Words for Stylometric Authorship Attribution Evaluating Most Distinguishing Words (MDWs) vs. Most Frequent Words (MFWs) ",
       "authors":[
          {
             "given":"Paul J.",
             "family":"Fields",
             "affiliation":[
                {
                   "original_name":"Brigham Young University, United States of America",
                   "normalized_name":"Brigham Young University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047rhhm47",
                      "GRID":"grid.253294.b"
                   }
                }
             ]
          },
          {
             "given":"Larry W.",
             "family":"Bassist",
             "affiliation":[
                {
                   "original_name":"Brigham Young University, United States of America",
                   "normalized_name":"Brigham Young University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047rhhm47",
                      "GRID":"grid.253294.b"
                   }
                }
             ]
          },
          {
             "given":"Matthew R.",
             "family":"Roper",
             "affiliation":[
                {
                   "original_name":"Brigham Young University, United States of America",
                   "normalized_name":"Brigham Young University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047rhhm47",
                      "GRID":"grid.253294.b"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-05",
       "keywords":[
          "English",
          "authorship attribution / authority",
          "stylistics and stylometry"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Abstract Over the past several years, we have worked to develop a human-interpretable computational method for quantifying “style” in literary texts. In projects focused on modernist texts, we have demonstrated the usefulness of this approach for studying dialogism (or multi-voicedness) in literature. Now we propose to extend our method to the “big data” scale by using a tool we have created, GutenTag (http://www.projectgutentag.org/). Our research promises insights into the historical evolution of dialogism in English-language fiction.   Aims and Approach Dialogism — the literary practice of allowing characters to speak in their own distinctive manners, without altering their speech to suit the particular linguistic practices and prejudices of the author — has been recognized as an ethically and politically significant aspect of fiction since the early twentieth century. Russian critic Mikhail Bakhtin, who coined the term “dialogism,” has been particularly influential in arguing that the dialogic novel could support pluralistic modes of thought that model democratic social systems. Yet despite the widely recognized importance of dialogism as an analytic category in literary studies, it is one that has proven difficult to study computationally, particularly at the “big data” scale. While style has proven a tractable aspect for computational literary study, and while excellent work has been produced on distinguishing character voices within literary texts using style-based methods (Burrows, 1987; McKenna and Antonia, 1996; Rybicki, 2006), existing approaches present two significant drawbacks. First, their reliance on Burrow’s PCA-based methodology means that while this work often produces reliable and insightful results, its computational outputs are generally not human-interpretable; they may be able to show  that an author distinguishes characters based on linguistic style, but not to tell us  how they are differentiated. Further, these methods tend not to be suitable to expansion to the “big data” level, since they require significant manual annotation of character speech in the texts under investigation. Our method — a human-interpretable quantitative method for analyzing literary style — and our tool — which performs automatic structural tagging of plain text — make such research possible, and thus open the way for the first large-scale investigation of dialogism in literary fiction.    Background Since 2011, we have been laying the foundations for a computational history of dialogism in English-language fiction. The first step was developing and refining our six-dimensional approach to quantifying literary style. Our method is based on six discrete aspects of style: objectivity (words that project a sense of disinterested authority); abstractness (words denoting concepts that cannot be described in purely physical terms); literariness; colloquialness; concreteness (words referring to events, objects, or properties in the physical word); and subjectivity (words that are strongly personal or reflect a personal opinion). To build our stylistic lexicons, we produce a relatively small set of words carefully selected for their stylistic diversity, which human annotators evaluate in terms of the six stylistic aspects listed above. Next, we use an automatic procedure to collect information on how these words are employed in all English texts in the 2010 image of  Project Gutenberg (Brooke et. al., 2016). Using this information, we are able to derive stylistic information for any word in our target text, and to build stylistic profiles for any character or speaking voice within a text.   We have demonstrated the usefulness of our six-style approach to literary problems in two projects. One project focused on free indirect discourse (FID) in Woolf’s  To the Lighthouse and Joyce’s “The Dead” (Brooke et. al., 2016). Our intention was to employ our method to test the long-held hypothesis that FID represents a stylistic middle ground between the neutral style of an objective narrator and the more extreme styles of personalized characters as rendered in direct discourse. Our method confirmed this assumption and, because it produces human-interpretable results, shed some new light on Woolf’s text in particular, finding that while Woolf’s upper-class characters exhibit a conventional power dynamic (they are more authoritative, more literary, more concrete, less objective, and less colloquial than characters of other classes) her female characters reverse these conventional dynamics: they are more objective, more abstract, less colloquial, and less subjective. The other project undertook a quantitative investigation of the problem of voice in T. S. Eliot’s  The Waste Land (Brooke et. al., 2015b). While it is generally agreed that  The Waste Land is composed of many speaking voices, these voices are not explicitly identified, nor are their points of transition provided. Our work explored methods of automatically segmenting and clustering voices in the text; we used the human-interpretable results of our six-style approach to evaluate the performance of various approaches and arrive at a blended human/machine interpretation.  The other key foundation for our work is GutenTag, an open-source software tool released in October 2015 (Brooke et al., 2015a). Two aspects of GutenTag are particularly relevant to the present project. First, it allows users to quickly create large, customized literary corpora. Working from the 2010 image of Project Gutenberg (PG), metadata provided by PG and derived automatically from other sources, and our automatic decision-tree genre classifier, one can, for example, easily assemble a large corpus of nineteenth-century novels in English. Second, it uses our sophisticated rule-based method to automatically generate reliable, genre-specific structural tags in standard TEI XML. Crucially, our tagging system is able to distinguish character speech from narration and identify individual characters in novels; and to separate character speech from stage directions and setting descriptions in plays, associating each speech with a character in the dramatis personae.    Developing a Metric for Dialogism With the major pieces in place to conduct our research into the history of dialogism, our main task is to develop a reliable quantitative metric for the dialogism of a given literary text. We will proceed by calculating stylistic profiles for each character using methods already established in previous work. We will include a minimum word cutoff to exclude characters for which the stylistic profile is likely to be too noisy due to lack of data. Next, assuming multiple characters, for each style we will treat each character as a datapoint and calculate a weighted variance, where weights are applied based on the relative proportion of speech by each character, to produce a number that indicates overall stylistic variation across characters for that stylistic dimension in the given text. We can average across styles to produce a single metric.  We will experiment with calculating dialogism based on stylistic variance between (a) individual characters, (b) groupings of characters (based on gender, social background, age, etc.), and (c) social networks. This will allow us to track (a) texts in which the speech of individual characters is highly varied, (b) texts in which, for example, male characters speak in a highly distinct manner from female characters, and (c) texts in which members of distant branches of a derived social network speak in their own differentiated styles. To carry out this analysis, certain modifications will be necessary to the GutenTag system; namely, methods to detect social networks in texts and to automatically identify character groupings (a method for identifying character gender is already in place; methods for identifying other groups will be more difficult, but we will explore them).    Research Questions With these technical foundations in place, we will be ready to present our answers to some of the following questions. Our aim is not to cover all of them, but to offer detailed investigations of those that yield the most interesting results.  Which texts in PG are the most stylistically diverse, according to our quantitative definition? Are they texts by authors traditionally celebrated as dialogic (Austen, Woolf, Joyce, translations of Dostoevsky) or are they non-canonical texts? If the latter, do these texts register qualitatively as “dialogic” to a human reader? What does a close reading of these texts reveal about the viability of our automatic method? How does stylistic diversity map onto historical time? Do periods of political turmoil (wars, revolutions, natural disasters, strikes, etc.) correspond to changes in the average stylistic diversity of fiction? What can we learn about the social role of fiction by studying this relationship? How is stylistic diversity distributed geographically? Which regions produce the most stylistically varied writing?  How is stylistic diversity distributed among genres? Does our method support or refute Bakhtin’s claim that the novel is the most dialogic of the genres, and poetry the least dialogic? Can dialogism be meaningfully compared across genres?  Are authors of different ages, classes, and genders more or less likely to produce dialogic fiction? How does the stylistic diversity of fiction track against that of non-fiction? Does non-fiction become more or less dialogic over time, and does it follow a similar curve to that of fiction? Do changes in the dialogism of fiction anticipate changes in non-fiction, or are the two unrelated?   ",
       "article_title":"Project Dialogism: Toward a Computational History of Vocal Diversity in English-Language Literature",
       "authors":[
          {
             "given":"Adam",
             "family":"Hammond",
             "affiliation":[
                {
                   "original_name":"San Diego State University, United States of America",
                   "normalized_name":"San Diego State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0264fdx42",
                      "GRID":"grid.263081.e"
                   }
                }
             ]
          },
          {
             "given":"Julian",
             "family":"Brooke",
             "affiliation":[
                {
                   "original_name":"University of Melbourne, Australia",
                   "normalized_name":"University of Melbourne",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/01ej9dk98",
                      "GRID":"grid.1008.9"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-06",
       "keywords":[
          "genre-specific studies: prose, poetry, drama",
          "literary studies",
          "natural language processing",
          "english studies",
          "linking and annotation",
          "interdisciplinary collaboration",
          "corpora and corpus activities",
          "stylistics and stylometry",
          "data mining / text mining",
          "linguistics",
          "English",
          "networks, relationships, graphs",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Many text-classification techniques have been proposed and used for authorship attribution (Holmes, 1994; Grieve, 2007; Juola, 2008; Koppel et al., 2011), genre categorization (Biber, 1988; Argamon et al., 2003), stylochronometry (Forsyth, 1999) and other tasks within computational stylistics. However, until quite recently, it has been extremely difficult to assess novel and existing techniques on comparable benchmark problems within a common framework using statistically robust methods. Toccata is a resource for computational stylometry which aims to address that lack, freely available at  http://www.richardsandesforsyth.net/software.html  under the GNU public licence. The main program is a test harness in which a variety of text-classification algorithms can be evaluated on unproblematic cases and, if required, applied to disputed cases. The package supplies four pre-existing classification methods as modules (including Delta (Burrows, 2002), widely regarded as a standard in this area) as well as five sample corpora (including the famous  Federalist Papers) so that users who don't wish to write Python code can use it simply as an off-the-shelf classifier and those who do can familiarize themselves with the system before implementing their own algorithms.  Noteworthy features of the system include:  sample corpora provided for familiarization; test phase using random subsampling to give robust error-rate estimation; ability to plug in new techniques or to employ existing standards; option of post-hoc phase applying trained model(s) to unseen holdout data; empirically grounded computation of post-hoc confidence weights to deal with 'open' problems where the unseen cases may not belong to any of the training-set categories; accompanying export file readable by R or similar statistical packages for optional further processing.    Sketch of the System's Operation Toccata performs three main functions, in sequence: (a) testmode: leave-n-out random resampling test of the classifier on the training corpus to provide statistics by which the classifier can be evaluated; (b) holdout: application of the classifier to an unseen holdout sample of texts, if given; (c) posthoc: re-application to the holdout sample of texts (if given) using the results from phase (a) to estimate empirical probabilities. Steps (b) and (c) are optional.   Sample corpora Toccata is a document-oriented system. Thus a training corpus consists of a number of text files, in UTF8 encoding, without markup such as HTML tags. Each file is treated as an individual document, belonging to a particular category. Example corpora are supplied to enable users to start using the system, prior to collecting or reformatting their own corpora.  ajps: ninety poems by 2 eminent 19th-century Hungarian poets, Arany József and Petőfi Sándor. Arany was godfather to Petőfi's child, so we might expect their writing styles to be relatively similar.   cics: Latin texts relevant to the authorship of the  Consolatio which Cicero wrote in 45 BC. This was thought to have been lost until in 1583 AD when Sigonio claimed to have rediscovered it. Background information can be found in Forsyth et al. (1999).   feds: writings by Alexander Hamilton and James Madison, as well as some contemporaries of theirs. This corpus is related to another notable authorship dispute, concerning the  Federalist Papers, which were published in New York in 1788. See Holmes and Forsyth (1995).   mags: 144 texts from 2 different learned journals, namely  Literary and Linguistic Computing and  Machine Learning. Each text is an excerpt consisting of the Abstract plus initial paragraph of an article in one of those journals, written during the period 1987-1995.   sonnets: 196 English sonnets, 14 each by 14 different authors, with an additional holdout sample of 24 texts, half of which are by authors absent from the main sample.    Validation by Random Subsampling A major objective of the system is to assess the effectiveness of text-classification methods by a form of cross validation. For this purpose the training corpus of undisputed texts is repeatedly divided into two portions, one used to form a classification model and the other used to test the accuracy of this model. After this cycle a number of quality statistics are computed and printed, along with a confusion matrix. This helps to establish a relatively honest estimate of the likely future error rate of the classifier. After subsampling, the program will construct a model on the full training set. This may then be applied to a genuine holdout sample, if provided.   Classifier Modules A classifier module is expected to develop trained models of each text category and deliver matching scores of a text to each model, with more positive scores indicating stronger matching. The category with the highest match-score relative to the average of all scores for the text, is the assigned class. Four library modules are supplied \"off the shelf\". Module  docalib_deltoid.py is an implementation of Burrows's delta (Burrows, 2002) which has become a standard technique in authorship attribution studies. Module  docalib_keytoks.py works by first finding the 1024 most common word tokens in the corpus, then keeping from these the most distinctive. For classification, relative word frequencies in the text being classified are correlated with relative frequencies in each class. Module  docalib_maws.py is a version of what Mosteller and Wallace in their classic work (1964/1984) on the  Federalist Papers call their \"robust Bayesian analysis\", as implemented by Forsyth (1995). Module  docalib_topvocs.py implements another classifier inspired by the approach of Burrows (1992), which uses the most frequent tokens in the training corpus as features.    The Holdout and Posthoc Phases The subsampling test phase (above) is primarily concerned with assessing the quality of a classification method. The holdout and posthoc phases are when that method is applied in earnest. If a holdout sample is given, the model developed on the training set is applied to that sample. The holdout texts may belong to categories that were not present in the training set, so each decision is categorized as correct (+), incorrect (-) or undetermined (?) and the success rate statistics computed accordingly. This is illustrated in Table 1, below, from an application of the MAWS (Mosteller and Wallace) method to a collection of sonnets. Here the training set consists of 196 short English poems -- 14 sonnets by 14 different authors. This is a challenging problem firstly because the median length of each text in the training corpus is 116 words, secondly because 14 is a relatively large number of candidates. Table 1 shows the ranking produced on a holdout sample of 24 texts, absent from the training set. Note that 12 of these 24 items are 'distractors', i.e. texts by authors not present in the training set. The program assigns these a question mark (?) in assessing its own decision. The listing ranks the program's decisions from most to least credible. The upper third include 6 correct assignments, 1 clear mistake and a distractor. The middle third contains 1 correct classification, 3 mistakes and 4 distractors. The last third contains no correct answers, 1 mistake and 7 distractors. (Incidentally, the distractor poem by the Earl of Oxford, ranked twentieth, is more congruent with Wordsworth than any other author, including Shakespeare, and not confidently assigned to any of the training categories.) This output addresses the very real problem of documents from outside the known training categories. The listing is ordered by a quantity labelled 'credit'. This is the geometric mean of the last two numbers in each line, labelled 'confidence' and 'congruity'. Confidence is derived from the preceding subsampling phase. It is computed from the differential matching score of the text under consideration as W / (W+L), where W is the number of correct answers which received a lower differential score during the subsampling phase and L is the number of wrong answers with a higher score. Congruity is simply the proportion of matching scores of the chosen category that were lower, in the subsampling phase, than the score for the case in question. It is an empirically based index of compatibility between the assigned category of the text and the training examples of that category. In all kinds of classification, the problem of never-before-seen categories can loom large. (See, for instance, Eder, 2013.) Like most trainable classifiers, Toccata always picks the most likely category from those it has encountered in training, but the most likely may not be very likely. The confidence and congruity scores give useful information in this regard. For example, if we only consider the classifications which obtain a score of at least 0.5 on both confidence and congruity, we find 6 correct decisions, 1 incorrect and 1 distractor. Treating the distractor (assigning a sonnet by Dylan Thomas to Edna Millay) as incorrect still represents a 75% success rate in an \"open\" authorship problem on texts only slightly more than a hundred word tokens in length, where the training sample for each known category consists of approximately 1600 words, with a chance expectation of 7% success. In other words, three crucial parameters -- training corpus size, text length and number of categories -- are all well \"outside the envelope\" of most previously reported authorship studies. Table 1 -- Posthoc ranking of 24 decisions on unseen texts, including 12 'distractors'   rank credit filename pred:true conf. congruity    1 0.9163  ChrRoss_WinterSecret.t  ChrRoss + ChrRoss  0.9530  0.8810    2 0.8768  WilShak_6.txt   WilShak + WilShak  0.9425  0.8158    3 0.8142  DylThom_Altar09.txt   EdnMill ? DylThom  0.8838  0.7500    4 0.7664  MicDray_Idea000.txt   MicDray + MicDray  0.6378  0.9211    5 0.7595  WilShak_137.txt   WilShak + WilShak  0.8118  0.7105    6 0.6950  JohDonn_Nativity.txt   JohDonn + JohDonn  0.6720  0.7188    7 0.6247  MicDray_Idea048.txt   JohDonn - MicDray  0.5430  0.7188    8 0.5356  WilShak_109.txt   WilShak + WilShak  0.5737  0.5000    9 0.5225  DylThom_Altar05.txt   RupBroo ? DylThom  0.4150  0.6579    10 0.4684  TomWyat_THEY_FLEE_FROM  EdmSpen ? ThoWyat  0.4596  0.4773    11 0.4226  PerShel_Ozymandias.txt  EliBrow ? PerShel  0.2217  0.8056    12 0.4027  EliBrow_SP23.txt   DanRoss - EliBrow  0.2237  0.7250    13 0.3061  WilShak_RomeoJuliet.tx  WilShak + WilShak  0.2094  0.4474    14 0.2739  PhiSidn_astel108.txt   EliBrow - PhiSidn  0.1080  0.6944    15 0.2625  DylThom_Altar06.txt   EliBrow ? DylThom  0.0992  0.6944    16 0.2283  JohDonn_Temple.txt   EdnMill - JohDonn  0.1179  0.4423    17 0.2014  Lincoln1863Gettysburg.  SamDani ? AbeLinc  0.0649  0.6250    18 0.1894  RicFors_LaBocca.txt   RupBroo ? RicFors  0.0649  0.5526    19 0.1352  HelFors_1958.txt   EliBrow ? HelFors  0.0263  0.6944    20 0.1089  oxford_13.txt   WilWord ? Oxford   0.0265  0.4474    21 0.0977  RicFors_Underworld.txt  EdnMill ? RicFors  0.0261  0.3654    22 0.0755  HelFors_1982.txt   DanRoss ? HelFors  0.0109  0.5250    23 0.0690  DylThom_Altar03.txt   RupBroo ? DylThom  0.0106  0.4474    24 0.0411  PhiSidn_astel030.txt   EdmSpen - PhiSidn  0.0106  0.1591   ++?+++-+???-+-?-???????-  ",
       "article_title":"Toccata : Text-Oriented Computational Classifier Applicable To Authorship",
       "authors":[
          {
             "given":"Richard Sandes",
             "family":"Forsyth",
             "affiliation":[
                {
                   "original_name":"independent, United Kingdom",
                   "normalized_name":null,
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-03",
       "keywords":[
          "natural language processing",
          "stylistics and stylometry",
          "programming",
          "authorship attribution / authority",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Approach Decades ago, alongside more traditional structuralist paradigms that were largely based on linguistic theorems (Lotman 1972, Titzmann 1977), literary studies began to undertake structural analyses based on empirical sociology, in particular the social network analysis. Structure was no longer solely defined by semantic relations (such as opposition or equivalence), but by social interactions, too (Marcus 1973; Stiller, Nettle and Dunbar 2003; de Nooy 2005; Stiller and Hudson 2005; Elson, Dames and McKeown 2010; Agarwal et al., 2012). In the context of the Digital Humanities, this kind of approaches has gained a new dynamic in shape of a dedicated literary network analysis (Moretti 2011; Rydberg-Cox 2011; Park, Kim and Cho 2013; Trilcke 2013). This method is based on the analysis of bigger literary corpora (i.e., quantitative data) and promises insights into the history of literature as well as generic characteristics of literary texts. In our project, \"dlina. Digital LIterary Network Analysis\", we already developed a workflow for the extraction, analysis and visualisation of network data from dramatic texts built on basic TEI markup (Fischer, Kampkaspar, Göbel, Trilcke, 2015). This paper will present results of our analysis of the network data gathered so far and discuss them in the light of current theories in the field of social network analysis.   Data Collection and Analysis Our current corpus comprises 465 German-language dramas (from 1730 to 1930), the better part of the Digitale Bibliothek corpus contained in the TextGrid repository ( ). The structural data crucial for the network analysis of these dramas (segmentation, character identification, etc.) was revised manually in a rule-based process to straighten out issues with the OCR and TEI tagging. We also had to level out philological peculiarities that would otherwise distort our results (such as different names for identical figures or groups of characters like 'Both' or 'All'). All the structural data is stored in an XML format we especially developed for that purpose (DLINA format). Network visualisation and network-value calculation has been automated (via Python and, alternatively, JavaScript to facilitate a direct embedding of our results into webpages). The scripts are fed with the data stored in DLINA files. In addition to graphs and simple network values that globally describe networks (like network size, density, average degree, average path length, clustering coefficent), we also calculate centrality values for the characters of each play (like degree, average distance, closeness centrality, betweenness centrality). In addition, we most recently implemented the calculation of random graphs based on the observed drama networks. All data and visualisations are freely available online on the project website (  and  ).    Evaluation, Part I: History of Drama The diachronic extension of our corpus over 200 years of German literary history allows the observation of larger developments in the structural composition of dramatic texts (we outlined some reflections on this in a blog post:  ). Values referring to networks as a whole will be broached (incl. network size, density, average degree; as an example, we put average-path-length values by decades in Fig. 1), as will be character-related values for each character of each play (centrality measures, primarily) providing information on the distribution of the personae dramatis or their division into 'central' and 'less central' characters. These values will lay the groundwork for the discussion of some global hypotheses of literary history. We will discuss, firstly, the extent to which we can observe a differentiation of the structural composition of drama at the end of the 18th century on the basis of network analysis values: Such a differentiation is to be expected given the coexistence of 'closed' drama (following the doctrines of French classicism) and 'open' drama (mostly influenced by Shakespeare). Secondly, we will discuss some common literary periodising hypotheses (originating from structuralism, social history, or other directions). We will have a closer look at correlations between our network data and well-established traditional periodisations.     Fig. 1: Average path length by decades (mean)     Evaluation, Part II: Types of Drama The data raised so far shows how very differently theatre plays were structured in the focal period. Traditional literary studies have developed various typologies for such different types, the most popular in German studies being Volker Klotz's subdivision into 'open' and 'closed' drama. We want to build on this kind of typological impulse and propose a method as to how certain types of structural composition can be distinguished by means of network analysis (and also placed in their historic context). With this proposal we want to take up reflections from research on so-called small-world networks. This branch of research assumes that the values of empirically collected networks often differ significantly from those of corresponding random networks (e.g., graphs generated with the Erdős–Rényi model). Following the approach of Stiller, Nettle and Dunbar 2003, but relying on a much larger set of texts, we investigate the plays in our corpus with regard to their small-world properties (clustering coefficient, average path length, node degree distribution). The results show that there are just a few plays that meet all the criteria (a total of five plays, i.e., just about one percent of the corpus) – see figs. 2.1 to 2.5. These findings will give us a deeper understanding of different types of structural composition. We shall first direct our attention to forms of networks that – unlike dramas with small-world properties – occur much more frequently in our corpus. Eventually, we will discuss structural characteristics of drama networks exhibiting properties exactly opposite to the properties of small-world dramas (e.g., reverse power-law form in the node degree distribution). It will also be discussed in this context whether we can contrast the strong hierarchical type of small-world drama with an anti-hierarchical type.    Fig. 2.1: Goethe, “Götz von Berlichingen” (1773): Spring Embedder Layout, Circular Layout, Node Degree Distribution      Fig. 2.2: Arnim, “Jerusalem” (1811): Spring Embedder Layout, Circular Layout, Node Degree Distribution      Fig. 2.3: Soden, “Doktor Faust” (1797): Spring Embedder Layout, Circular Layout, Node Degree Distribution      Fig. 2.4: Nestroy, “Der böse Geist” (1833): Spring Embedder Layout, Circular Layout, Node Degree Distribution      Fig. 2.5: Raimund, “Der Barometermacher” (1823): Spring Embedder Layout, Circular Layout, Node Degree Distribution    ",
       "article_title":" Theatre Plays as 'Small Worlds'? Network Data on the History and Typology of German Drama, 1730–1930  ",
       "authors":[
          {
             "given":"Peer",
             "family":"Trilcke",
             "affiliation":[
                {
                   "original_name":"University of Göttingen, Germany",
                   "normalized_name":"University of Göttingen",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01y9bpm73",
                      "GRID":"grid.7450.6"
                   }
                }
             ]
          },
          {
             "given":"Frank",
             "family":"Fischer",
             "affiliation":[
                {
                   "original_name":"Göttingen State and University Library, Germany",
                   "normalized_name":null,
                   "country":"Germany",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Mathias",
             "family":"Göbel",
             "affiliation":[
                {
                   "original_name":"Göttingen State and University Library, Germany",
                   "normalized_name":null,
                   "country":"Germany",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Dario",
             "family":"Kampkaspar",
             "affiliation":[
                {
                   "original_name":"Herzog August Library Wolfenbüttel, Germany",
                   "normalized_name":null,
                   "country":"Germany",
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-03-04",
       "keywords":[
          "literary studies",
          "corpora and corpus activities",
          "german studies",
          "English",
          "networks, relationships, graphs"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" In this short paper presentation a participatory ethnographic GIS-mapping approach and an example for a digital platform for the visualization and digitalization of local environmental knowledge will be introduced. The term “Indigenous” is used to refer to those who “have a historical continuity with pre-invasion and pre-colonial societies that developed on their territories and consider themselves distinct from other sectors of the societies now prevailing in those territories” (UNESCO, 2004). Based on examples and research results of the research project ANIK Alpine risks in times of climate change, funded (2012-2015) by the German Ministry of Education and Research (BMBF), this paper will question how local environmental knowledge (Pottier, 2003) and local perceptions and handling strategies of climate-related risks may be gathered through participatory GIS mapping (PGIS), (Reichel, Frömming 2015, 2014). Based on applied visual anthropological methods, PGIS is a relatively new cartographical digital approach which includes local perceptions and strategies of action gathered during interviews and participant observation. This approach fosters active participation of local people (Wood, 2012), not only during the research process in the field, but also during the presentation process of the research data as digital spaces of memory. ",
       "article_title":"Indigenous digital humanities. Participatory geo-referenced-mapping and visualization for digital data management platforms in digital anthropology",
       "authors":[
          {
             "given":"Urte Undine",
             "family":"Froemming",
             "affiliation":[
                {
                   "original_name":"Freie Universitaet Berlin, Germany",
                   "normalized_name":"Freie Universität Berlin",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/046ak2485",
                      "GRID":"grid.14095.39"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-18",
       "keywords":[
          "folklore and oral history",
          "digital humanities - multilinguality",
          "digital humanities - pedagogy and curriculum",
          "teaching and pedagogy",
          "cultural studies",
          "anthropology",
          "digital humanities - diversity",
          "digitisation - theory and practice",
          "knowledge representation",
          "archives, repositories, sustainability and preservation",
          "English",
          "audio, video, multimedia"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction  This presentation will focus on the results of the newest phase of the NEH-funded Livingstone Spectral Imaging Project (2010-present). This project seeks to apply spectral imaging and processing techniques to the study of some of the most damaged manuscripts produced by David Livingstone (1813-73), the famous Victorian traveler, abolitionist, geographer, and missionary. Our project asks whether spectral imaging can indeed restore erased or otherwise invisible portions of Livingstone’s writing. More recently, we also explore whether the technology can illuminate the specific circumstances of the production and preservation of Livingstone’s manuscripts, thereby better revealing the links between these manuscripts and the unique historical events that shaped the material dimensions of these manuscripts.   Methodology Spectral imaging, a technology beginning to make a significant impact in humanities research (see bibliography), relies on imaging an object, such as a manuscript, under multiple wavelengths of light, ranging from ultraviolet (UV) through the visible color spectrum to the near-infrared. A high-resolution monochromatic digital camera automatically photographs each illumination. Imaging scientists then manipulate this raw image data with computers by applying various processing algorithms with the goal of enhancing features of interest. Often these features are made more visible by creating pseudocolor (false-color) representations of the object that foreground or suppress other object elements. Our previous research (Wisnicki 2011) applied spectral imaging to recover the text of a diary that Livingstone had written over newspaper pages and that had become illegible because, first, the considerable fading of Livingstone’s ink (which he had concocted out of a local African clothing dye) and, second, the continuing prominence of the black newsprint over which Livingstone wrote his text. Using a combination of new and established spectral image processing techniques, our team successfully suppressed the newsprint so that it no longer interfered with Livingstone’s text. By enhancing his writing so that it could be read easily, we were able to recover some 99% of Livingstone’s words (up from about 40% before) and, in turn, use this technique to reveal important new information about Livingstone’s strategies for representing his experiences in interacting with local populations in Central Africa. Our current project takes our previous research in a new direction to suggest that spectral imaging can illuminate the production and preservation history of Livingstone’s fragmentary 1870 Field Diary. With spectral imaging, we have enhanced or revealed material features of this diary, such as overwriting, staining, and page topography, that are often not visible or difficult to discern with the naked eye. Our results represent an important advance in the study of manuscripts with spectral imaging, particularly because previous related research has generally focused on using of spectral imaging for the recovery of faded texts or illegible palimpsests such as the Dead Sea Scrolls (Shor 2012) and the Archimedes Palimpsest (Netz and Noel 2007).   Figure 1. Page XXXV of Livingstone’s 1870 Field Diary in natural light. Copyright National Library of Scotland. CC BY-NC 3.0    Historical and Scholarly Implications  Livingstone wrote the 1870 Field Diary in central Africa and carried it with him to his death in 1873 in what is now Zambia. Livingstone’s supporters then transported his manuscript (and his corpse) from the interior to the coast of Africa and back to Britain, where different repositories have since preserved the manuscript. The pages of the diary, therefore, bear the remnants of this complex history, from the changes and deletions that Livingstone made over time, to the marks left on the manuscript pages by the diverse local African environments through which the diary traveled, to the traces of modern curatorial methods used to preserve the diary’s pages.  Our efforts to study the material history of Livingstone’s diary with spectral imaging represent an important intervention in the field. Our work demonstrates new methods and frontiers for the use of spectral imaging in the humanities because it underscores that spectral imaging can do more than recover faded or invisible text. Spectral Imaging can provide crucial insights into the passage of a manuscript through time and into the relationship between a manuscript and the specific historical circumstances from which it emerged. By applying this technology to the 1870 Field Diary, we have gained key insights into the strategies by which Livingstone shaped his experiences and identity for public consumption and the impact of specific moments of handling in determining material features of the manuscript. These insights promise to enhance our understanding of Livingstone’s biography and the history of the many African cultures in which he worked.    Figure 2. Page XXXV of Livingstone’s 1870 Field Diary with topography illuminated by spectral image processing. Copyright National Library of Scotland. CC BY-NC 3.0  Moreover, the spectral image processing techniques we have developed for the 1870 Field Diary can now be applied to study other manuscripts with notable textual and material features or, indeed, other cultural objects such as paintings and sculptures whose surfaces retain their material history and bear the marks of the many people and events that have come to shape the objects we encounter today. Our project is helping us distinguish such marks, but, in our case, the project also sets the stage for future research in which we might use the spectral signatures of specific marks as a portal to defining tangible aspects of nineteenth-century African environments from which Livingstone’s manuscripts emerged and through which these manuscript circulated.  ",
       "article_title":"The Manuscripts of David Livingstone and New Frontiers for Spectral Imaging",
       "authors":[
          {
             "given":"Adrian S.",
             "family":"Wisnicki",
             "affiliation":[
                {
                   "original_name":"University of Nebraska-Lincoln, United States of America",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          },
          {
             "given":"Ashanka",
             "family":"Kumari",
             "affiliation":[
                {
                   "original_name":"University of Louisville, United States of America",
                   "normalized_name":"University of Louisville",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01ckdn478",
                      "GRID":"grid.266623.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016-02-23",
       "keywords":[
          "bibliographic methods / textual studies",
          "literary studies",
          "cultural studies",
          "interdisciplinary collaboration",
          "standards and interoperability",
          "digitisation, resource creation, and discovery",
          "historical studies",
          "audio, video, multimedia",
          "scholarly editing",
          "knowledge representation",
          "archives, repositories, sustainability and preservation",
          "authorship attribution / authority",
          "content analysis",
          "image processing",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Big Data is reshaping the historical profession in ways we are only now beginning to grasp. The growth of digital sources since the advent of the World Wide Web in 1990-91 presents new opportunities for social and cultural historians. Large web archives contain billions of webpages, from personal homepages to professional or academic websites, and now make it possible for us to develop large-scale reconstructions of the recent web. Yet the sheer number of these sources presents significant challenges: if the norm until the digital era was to have human information vanish, “now expectations have inverted. Everything may be recorded and preserved, at least potentially” (Gleick, 2012). While the Internet Archive makes archived web content available to the general public and mainstream scholarly community through its “Wayback Machine,” (at http://archive.org/web) which allows visitors to enter a Uniform Resource Locator (URL) to visit archived web versions of a particular page, this system is limited: not only do visitors need to know the URL in the first place, but they are limited to individual readings of single webpages. By unlocking the Wayback Machine’s underlying system of specialized files, primarily ISO-standardized WebARChive (WARC) files, we can develop new ways to systematically track, visualize, and analyze change occurring over time within web archives. Warcbase, an open-source platform for managing web archives built on Hadoop and HBase, provides a flexible data model for storing and managing raw content as well as metadata and extracted knowledge. Tight integration with Hadoop provides powerful tools for analytics and data processing. Using a case study of one collection, this paper introduces the work that we have been doing to facilitate web archive access with warcbase. We have growing documentation at http://docs.warcbase.org.   Project Rationale and Case Study In 1996, the Internet Archive launched a complementary research services company, Archive-It, which offers subscription-based web archiving to collecting institutions.  The University of Toronto Library (UTL) began collecting a quarterly crawl in 2005 of Canadian political parties and political interest groups (the collections were separate in 2005, merging in 2006) (University of Toronto, 2015). The collection itself has a murky history: UTL had been part of a broader project that would have collected political websites. It fell through, but UTL opted to carry out their crawl on their own and the librarian was responsible for selecting the seed list herself (faculty and other librarians did not respond for calls for engagement). While formal political parties are robustly covered, the “political interest groups” collection was a bit more nebulous: sites were discovered through keyword searches, and some were excluded due to robots.txt exclusion requests. Beyond this brief sketch, we have little information about the decisions made in 2005 to create this collection. This lack of documentation is a shortcoming of this collection model, as if a historian was to use this material in a peer-reviewed paper, questions would be raised about its representativeness. If a user wants to use the Canadian Political Parties and Interest Groups Collection (CPP) through Archive-It today, they visit the collection page at https://archive-it.org/collections/227 and enter full-text search queries. In August 2015, our group also launched http://webarchives.ca, based on the British Library’s SHINE front end for web archives; this was a way to facilitate a different form of more casual user access, aimed at the general public (we discuss this in a separate paper). The Archive-It portal is limited. There are no readily-available metrics of how many pages have been collected, how they break down by domain and date, and the portal undoubtedly provides skewed results unless the search phrase is dramatically narrowed down. Consider the search for “Stephen Harper,” Canada’s Prime Minister between 2006 and 2015 in Figure 1.   Figure 1: Archive-It Search Portal  The results are decent: Harper’s Facebook page from 2009, a Twitter snapshot from 2010, and some long-form journalism articles and opposition press releases. But amidst the 1,178,351 results, there is no indication as to how the ranking took place, what facets are available, and how things may have changed over the last ten years of the crawl. The data is there, but the problem is access.   Warcbase: A Platform for Web Archive Analysis Warcbase is a web archive platform, not a single program. Its capabilities comprise two main categories:  Analysis of web archives using the Pig or Spark programming languages, and assorted helper scripts and utilities Web archive database management, with support for the HBase distributed data store, and OpenWayback integration providing a friendly web interface to view stored websites  One can take advantage of the analysis tools (1) without bothering with the database management aspect of Warcbase – in fact, most digital humanities researchers will probably find the former more useful. This paper focuses on the former capabilities, showing how we can use the warcbase platform to carry out text and network analyses.   Using Warcbase on Web Archival Collections: Text Analysis We have begun to document all warcbase commands on a GitHub wiki, found at  https://github.com/lintool/warcbase/wiki. We begin with installation instructions, and then provide simple scripts written in Apache Spark to run the commands. While possible to generate a plain text version of the entire collection, a more fruitful approach has been to generate date-ordered text for particular domains. If a researcher is interested in say, the Green Party of Canada’s evolution between 2005 and 2015, they can extract the plain text for greenparty.ca by running the following script:    All they would need to change would be the path/to/input to the directory with their web archive files, the path/to/output for where they want to save the resulting plain-text files, and the greenparty.ca value to whatever domain they might be interested in researching. They then receive a date-ordered output of all plain text for that domain (as per the extractCrawldateDomainUrlBody command). It can then be sorted and used in other research avenues. For example, this plain text could be loaded into a text analysis suite such as http://voyant-tools.org/ or other digital humanities environments. We have also been experimenting with other visualizations based on the extracted plain text. Computationally intensive textual analysis can be carried out using warcbase itself. Using the Stanford NER package in parallel, we have a script that extracts entities, counts them, and then visualizes them using D3.js to help see overall changes in a web archival collection. Figure 2 below shows the output of the NER visualizer.   Figure 2: Named Entity Visualization within Warcbase  Finally, another text approach is topic modelling (Blei et al., 2003). LDA works by finding topics in unstructured text. To visualize topic models, we elected to use the Termite Data Server, which is a visual analysis tool for exploring the output of statistical topic models (“uwdata/termite-data-server,” n.d.). As Figure 3 demonstrates, the visualization allows you to get a top-down view at the topics found in a web archive.   Figure 3: Termite Topic Model  Warcbase presents versatile opportunities to extract plain text and move it into other environments for analysis. Unlike the keyword-based Archive-It portal, we now have data that can be inquired in many fruitful ways.   Using Warcbase on Web Archival Collections: Hyperlink Analysis Warcbase can also extract hyperlinks. While text can be very important, these sorts of metadata can often be more important: allowing us to see changes in how groups link to each other, what articles and issues were important, and how relationships changed over time. Consider Figure 4, which visualizes the links stemming from and between the websites of Canada’s three main political parties.   Figure 4: Three major political parties in Canada  Above, we can see which pages only link to the left-leaning New Democratic Party (ndp.ca), those that link only to the centrist Liberals (liberal.ca) in the top, and those that only connect to and from the right-wing Conservative Party at right. We can use it to find further information, such as in Figure 5.   Figure 5: NDP attack  The above links are from the 2006 Canadian federal election. The Liberal Party was then in power and was under attack by both the opposition parties. In particular, the left-leaning NDP linked hundreds of times to their ideologically close cousins, the centrist Liberals, as part of their electoral attacks, ignoring the right-leaning Conservative Party in the process. Link metadata illuminates more than a close reading of an individual website would. It contextualizes and tells stories itself. While we have traditionally used Gephi to do analysis, importing material into Gephi from warcbase required many manual steps as documented at https://github.com/lintool/warcbase/wiki/Gephi:-Converting-Site-Link-Structure-into-Dynamic-Visualization. We have been prototyping a link analysis visualization in D3.js, which can run in browser (Figure 6).   Figure 6: Link Visualization    Conclusions With the increasingly widespread availability of large web archives, historians and Internet scholars are now in a position to find new ways to track, explore, and visualize changes that have taken place within the first two decades of the Web. Warcbase will allow them to do so. This project is among the first attempts to harness data in ways that will enable present and future historians to usefully access, interpret, and curate the masses of born-digital primary sources that document our recent past.  ",
       "article_title":"Exploring and Discovering Archive-It Collections with Warcbase",
       "authors":[
          {
             "given":"Ian",
             "family":"Milligan",
             "affiliation":[
                {
                   "original_name":"Department of History, University of Waterloo, Canada",
                   "normalized_name":"University of Waterloo",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01aff2v68",
                      "GRID":"grid.46078.3d"
                   }
                }
             ]
          },
          {
             "given":"Jimmy",
             "family":"Lin",
             "affiliation":[
                {
                   "original_name":"David R. Cheriton School of Computer Science, University of Waterloo, Canada",
                   "normalized_name":"University of Waterloo",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01aff2v68",
                      "GRID":"grid.46078.3d"
                   }
                }
             ]
          },
          {
             "given":"Jeremy",
             "family":"Wiebe",
             "affiliation":[
                {
                   "original_name":"Department of History, University of Waterloo, Canada",
                   "normalized_name":"University of Waterloo",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01aff2v68",
                      "GRID":"grid.46078.3d"
                   }
                }
             ]
          },
          {
             "given":"Alice",
             "family":"Zhou",
             "affiliation":[
                {
                   "original_name":"David R. Cheriton School of Computer Science, University of Waterloo, Canada",
                   "normalized_name":"University of Waterloo",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01aff2v68",
                      "GRID":"grid.46078.3d"
                   }
                }
             ]
          }
       ],
       "publisher":"Maciej Eder, Pedagogical University in Krakow",
       "date":"2016",
       "keywords":[
          "information retrieval",
          "internet / world wide web",
          "data mining / text mining",
          "historical studies",
          "archives, repositories, sustainability and preservation",
          "English",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    }
 ]