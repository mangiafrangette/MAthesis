[
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Introduction This proposal reports on a research project, funded by a British Academy small grant to investigate the history, development and institutional context of a range of digital resources and computing projects in the humanities. This history is very recent, but rich and realtively neglected, except for Hockey, (forthcoming). The most long lived of such projects have existed only since the early 1970s. However, the pace of technological and institutional change has caused rapid, dramatic changes in this period. This research therefore aims to provide an analytical history of the field and its developments, placed within the context of institutional infrastructure and that of government policy and funding structures. Context The work began with a pilot project undertaken at the Department of Information Studies, University of Sheffield. (Warwick and Carty, 2001) Research was based on a small sample of projects on the History of Science: the Hartlib project, (University of Sheffield); the Darwin Correspondence Project, (University of Cambridge); the Robert Boyle project (Birkbeck College, London); the Mueller Correspondence Project and the Newton Project (Imperial College, London). Interviews conducted during the pilot project established the need for a of history of such projects which would place them in the context of the broader field as it has developed from the 1970s to the present. Humanities computing (HC) projects may collaborate with a University Library, Computing Services, or Research Centre. However, in many cases they comprise small teams of academics, working on a limited budget. Although the teams make every effort to document technical decisions that they make, they were concerned about a lack of time to construct a history of the project, whether oral or written. Researchers feared that they were working in isolation from other teams, since, especially in the early days of development, many scholars working in the area were unaware of each others' existence. As a result valuable knowledge may be not be shared. Similar difficulties might be encountered, and solutions developed without the benefit of others' hard won experience, costing valuable time and resources. (Hunter, 1995, Flanders, 2000) These problems have been ameliorated by advice from bodies such as the Text Encoding Initiative (TEI). Yet, given the diversity of material in the Humanities, many projects worried about whether they had applied the TEI dtd 'correctly' in the mistaken belief that others found this easy. Research teams also shared a corpus of knowledge and experience about broader factors affecting the academic progress of each project, and how it was related to the broader scholarly, human and institutional context. Such knowledge was being lost, and many thought that it ought to be preserved, not only as a record of early work, but also as a resource that future researchers might draw upon when planning new work. It became apparent therefore that there was a need for research into this area. This should not only be technical in nature, since what documentation exists tend to be in he area of technical decisions. (For example Leslie, 1990, Lucas and Short, 1999, Pearn, 1999) .The research described in this proposal therefore concentrates on less tangible factors such as the reasons for beginning the project, problems encountered, changes and developments that took place, the response of more traditional researchers in the humanities and how the result of the work were integrated into libraries and scholarship in the humanities. In the pilot project, those interviewed thought that such research should be done by someone from outside individual projects, libraries or e-text centres, so that a comparative view could be developed. This research therefore aims to analyse trends and problems, and to chart how the history of the sector has developed. Methods In order to acquire rich qualitative data which can be examined in depth, (Patton, 1990) representatives from a sample of projects were interviewed. Documentation provided, or articles written about the project have also been consulted. Sample choice The sample was chosen to represent the diversity of the field and to provide a comparison between work in the USA and the UK. Because of the small scale of funding it has not been possible to carry out research in continental Europe, but this is anticipated for a future grant application. The small size of the pilot study allowed us to come to very tentative conclusions. A wider range of projects has therefore been consulted. Respondent in the pilot study had stressed the important influence of HC centres and government bodies, so research was carried out not only on individual projects, but also on larger HC centres and with representatives of bodies such as the Arts and Humanities Data Service (AHDS) and the Resource Discovery Network (RDN). These have been chosen to reflect the following criteria. Size Interviews undertaken reflected the difference in scale between small individual and digital libraries such as the E-text Centre at the University of Virginia. HC centres tend to provide an overarching structure for a variety of projects. It was therefore important to examine how such centres have developed, and how smaller projects relate to them. Location: Our original research concentrated on projects in the UK. However, some of the most important developments in HC have happened in the USA. It is vital to gain a comparative perspective on this area, and the way that transatlantic collaboration has begun to take place. Research was therefore carried out at Brown University Scholarly Technology Group, the Institute for Advanced Technology in the Humanities (IATH) and the Center for Digital History (CDH) at University of Virginia, the Perseus Project at Tufts University and the Center for Electronic Texts in the Humanities (CETH) at Rutgers University. These have been compared to UK centres such as the Humanities Advanced Technology and Information Institute (HATII) at the University of Glasgow, Oxford University Learning and Research Technologies Groups (formerly the Humanities Computing Unit), the Centre for Computing in the Humanities at Kings College London, as well as individual projects such as The Perdita Project at Nottingham Trent University. Age The projects that began in the 1970s, such as the Oxford Text Archive, often developed on an adhoc basis with little funding. This is different from recent initiatives, funded by research grants and systematically managed, such as the LEADERS project at UCL. Comparison of such projects was therefore made to determine what effect the age of the project has on its procedures, and how the framework under which it operated might affect its research. Purpose Digital resource collections have been designed to serve different purposes. Some, like the Oxford Text Archive (OTA), historically had little sense of collection policy, aiming to collect as much heterogeneous material as possible. Others concentrated on one particular writer, or type of resource. Some were created for teaching, or to further a particular kind of research. This research therefore aims to investigate what impact these different purposes had on the development and life of the resource. Findings At the time of writing the proposal the interviews are being completed. Should the paper be accepted, findings can be discussed more fully, and will be presented under the headings discussed above. However, prelimary results suggest that the issue of funding and administrative structures has been vital to the development of projects and HC centres. They also indicate a marked difference between UK and USA examples. USA projects are typically more developed before seeking external research grant, due to the availability of funding from their own university. The vision of the university in which HC is undertaken was also vital and when this changed so did that of projects in HC. This is so noticeable that the title of this paper is a direct quotation from an interviewee reflecting on such a change in policy. It is clear that HC centres play an important part in shaping individual projects, and where local support was absent, despite government funding and central advice, problems of the non-use of standards and a lack of awareness of good practice are still noticeable. Unresolved issues included how to communicate with and gain respect for HC research from the more traditional scholarly community, and how researchers and academics should be rewarded for such work within traditional promotion and remuneration structures. Relevance to conference themes Although all those interviewed in this study speak English, the paper nevertheless addresses cultural differences between the USA and the UK and the manner in which HC work is carried out, funded and supported by institutions. The research also applies empirical methods from information studies, and social science (ie the analysis of qualitative interview data) and aims to consider the institutional role of HC within the contemporary academy. ",
        "article_title": "\"No such thing as Humanities Computing?\" An analytical history of digital resource creation and computing in the humanities",
        "authors": [
            {
                "given": "Claire",
                "family": "Warwick",
                "affiliation": [
                    "SLAIS, UCL"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Our corpus project is building a digital collection of both written and spoken texts. The corpus is a publicly available resource, mounted on and searchable via the Web. This paper will describe the corpus management and workflow administration methods that the project has developed and the technologies used. We believe that the structures we have created to manage the different parts of the administration of the project are the basis for a re-usable, generic package for scholars building an online corpus from new linguistic materials. While every existing corpus has, of course, developed its own methods of ensuring the correct procedures are followed, this knowledge is usually not available to other projects in their early development or is too specific to a particular area of language research. We have been approached by several projects who have seen or heard of our system and are planning to build multilingual and multi-media corpora. The paper will describe two case studies involving disparate data in various media and our proposed pilot studies on methods of creating an application suitable for their use, and extension to a generic package. Our corpus is synchronic, a 'snapshot' of the languages used in here in recent years, and a monitor corpus which will be continually updated. It includes written texts, sound recordings, video recordings, and transcriptions of sound from the latter two. It also contains extensive sociolinguistic metadata. We are processing readily available materials (c.1m words) and will identify the gaps and find or commission new materials to fill them. We intend that the corpus will contain upwards of 4,000,000 words, at least 20% spoken. This will form a valuable language research tool in its own right, and will offer a structure flexible enough to expand and accommodate future sub-corpora. Sociolinguistic metadata are held in the following categories: resource type, text type, setting, medium, audience, text details, author/speaker details and copyright information. For example, the author and speaker categories contain information on gender, age, geographic region, education, occupation, languages spoken etc. The author/speaker's parents' information is also recorded. Standards described by the Text Encoding Initiative Guidelines and Dublin Core have been applied and the metadata categories have been decided upon after extensive research into the requirements both for useful language research and the legal necessities for publication of the data. In creating the corpus structure we have included database functions to control the workflow. Administrative functions range from contact management through to document entry and associated metadata manipulation. These are functions that any project creating a corpus must perform. Having these functions built into the system means that, for example, copyright law compliance is enforced. The system generates reports to facilitate the administration and management of each key stage. The data storage method is more advanced than simple flat files and multiple concurrent users are fully supported, aiding large-scale data entry. Document contents are accessed via the unified interface, alleviating the requirement for file naming and directory organisation. The corpus system is divided in two sections: administration and online search. With such valuable data being held in the system it was important for us to choose open, standards-based solutions where possible. Although more advanced and proprietary software is available, we chose to configure a Linux server with MySQL RDBMS, Apache and PHP, as this configurations give the greatest degree of flexibility and portability. To reduce training of our team MS Access was chosen to provide the front-end to the administrative functions. This allows interaction with the whole Office suite to provide mail merge and data interoperability. The search system uses a subset of the administrative data. Only those documents that are complete and have all their relevant permissions are allowed into the public data set. As an added security measure this consists of a separate database, but this is not a necessity. Access to documents is provided via a two tier search mechanism. Commonly used criteria are the basis for a basic search interface, those users who wish to explore the dataset in more depth can choose an advanced query tool giving access to the whole range of metadata. Word/phrase search can be combined with a metadata search, matching documents provide word highlighting to show context of match. Following approaches from scholars proposing to start building digital corpora it became apparent that a subset of our corpus management system (concentrating on administration and workflow) would be very useful to others. Two such projects are detailed below. As our data structure is tailored and grew up with the particular requirements for our corpus, it will be necessary to develop an abstracted model of the corpus management system. This will allow standard data objects and types to be used freely inside the established model. As open source software is used there are many different software and hardware platforms available as host to any system developed. First steps to integration would involve identifying each document type and their metadata and contents. In addition the interrelationship of different objects (e.g. author to document) must be established before the data can enter our framework. Sample data would be identified to test the maximal set of possibilities available to verify expected operation. Any search front-end will necessitate a higher degree of customisation to match the specific project. The first corpus in our pilot study is a corpus of transitional dialects. 198 individuals were recorded for approximately 1 hour each (c. 120 gb of data). We know from our discussions with the scholars who have created this data that the abstracted model referred to above would be of use to them in creating an online corpus. The abstracted model would not deal with all the issues particular to this corpus and we have identified some areas requiring investigation. For example, the dialects concerned contain speech sounds which are not represented by characters in current Unicode sets. Transcription of the data will require the creation of new Unicode code points and glyphs to allow reproduction of the particular dialects in use. Until this is resolved methods for searching this data will have to be investigated. Storage and encoding are easily solved for the researchers local use but these methods may not be available to end users online. The second corpus in our pilot is based on recent Parliamentary elections. The data collected are a rich mix of media (text, images, sound and video recordings, and mixed media documents such as web pages). An investigation into which metadata are generic for all media types must be conducted as any search must be capable of scanning all media types and present a unified result. Integrating the various media types into the unified front-end will present different challenges for each media type, as would display of these media types. Upon search completion quick access to related documents would be very useful e.g. identify other documents by the same author / same location. We believe our corpus management software can give new projects an easy to use framework on which to base their system. Collaboration with other projects will enhance the system as we identify new data types and modules to include in the framework. As these issues are solved this will feed back to the abstracted model and provide ready made solutions for future projects. As any collaborative effort would have the same base structure it would make integration of data from many different corpora straightforward, this may be of particular use to researchers. Most literature on corpus projects concentrates on either the content, the encoding of the content or the research results of the use of the corpus. We have found little on the management and administration of a corpus project. Researchers often do not recognise initially the amount of time needed to develop good management procedures and the complexity required to control the process from contact management through document entry, metadata manipulation to publication. We believe this discussion will be use to us, to the scholars planning the corpora mentioned above and to others in the ALLC/ACH community.",
        "article_title": "A Generic Application for Corpus Management and Administration",
        "authors": [
            {
                "given": "David",
                "family": "Beavan",
                "affiliation": [
                    "University of Glagow"
                ]
            },
            {
                "given": "Jean",
                "family": "Anderson",
                "affiliation": [
                    "University of Glagow"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Introduction In this paper we describe the production model and dissemination models of a newsletters service. We will first describe the context of the problem, one of the biggest Web-based digital libraries of Spanish works, and the spirit and objectives of its interactive and dissemination services. Amongst these, the newsletters service is an information dissemination service that provides useful information to readers. We will talk about the production model of the newsletters based on XML-TEI (2) and XSLT, and about the personalizable dissemination model we are now implementing. The dissemination model combines adaptive with adaptable personalization techniques, being capable of ranking news according to navigation-inferred preferences and then filter them according to a user-given profile. Communication services of the digital library One of the goals of the MCDL is to act as a communication channel for the academic community. In this sense we have implemented a number of communication services, and we try to maintain a permanent communication with our readers. Electronic publishing makes it possible to reach every corner of the world and opens up new research and communication paths. The Miguel de Cervantes Digital Library's Web based news-service comprises five different digital library newsletters, and one monthly journal all managed and produced using XML-TEI and XSLT technology. News and articles may appear in different newsletters and/or the journal, and they also have different periodicity (some are published quarterly and some monthly). The news and articles come from different sources, which generally coincide with departments or units of our digital library. A general editor reviews the articles or news, decides where they must appear, and also manages the distribution lists. A simple solution for production Each of the five different newsletters anf the journal are delivered in several optional output formats, and all managed and produced based on XML-TEI and XSLT technology (see figure 1). First, we considered the possibility of developing a management system based on a database. Then, while reviewing the requirements, we realized that a much simpler solution using an XML editor, XML-TEI encoding and XSLT transformations was possible. A database based system would be better for editing and maintenance of news, but news have no maintenance. Once the are published, there is no more editing of the news. However, they can be searched, but this service can be provided by an XML searcher without the need of an DBMS (database management system). With this simple solution, no system programming was needed, only the XSLT transforms needed to be built. In addition, it takes advantage of the same TEI XML markup scheme and the same processing technology we use to produce our digital books. The differences are mainly the overlapping of news and the interrelated times of publication. We used XSL-TEI to markup each piece of news. In the case of the newsletters and the journal we use a subset of the tagset we currently use for books, so we developed a small DTD for this purpose. We had to develop several XSL transformation scripts to produce the different output formats required. Newsletters are generated both in plain text (for traditional mail readers) and in HTML for those who prefer a richer format. How it works Once the monthly news file is complete and supervised, we enter the automatic phase of generation of different output formats. The final output in HTML format is obtained from a double transformation of the XML-TEI file (see figure 3). First an XSL transformation processes the XML monthly file to generate a single HTML file (with special formatting marks embedded). Then a parsing program of our own design called MakeBook transforms that file into a digital book, generating a file for the table of contents, and a file for each section of the journal. Notes are extracted from the main file and placed in small external files, leaving hyperlinks to these files in their place. MakeBook was also thought to add page headers and footers for each section, that include buttons to implement a navigation pattern called \"indexed guided tour\" [2]. This navigation pattern allows both the presence of a central index or table of contents and also buttons to move back and forth the different sections of the journal (hence the metaphor \"guided tour\"). Buttons to jump up and down article headings with a mouse click are also provided. The result is an electronic journal that is nothing but a set of web pages interconnected with a ring-star topology: a bidirectional ring of connections to navigate the sections, plus a central table-of-contents page with bidirectional connections to each section (see figure 2). A set of templates is used for formatting and providing navigation functionality to the generated HTML files. The purpose of using templates is to give a uniform appearance to all the monthly journals, as well as to facilitate the maintenance, so that format changes can be applied easily and evenly to the whole set (e.g., section headers and footers, background colors and textures and navigation buttons can be changed through these templates). The proposed solution for dissemination The Web Engineering Group of the department of Computer Languages and Information Systems at the University of Alicante has developed a method, OOH [1], and an accompanying software tool (VisualWADE) to assist the design of language-independent Web Applications. This software, based on standards for information system's object-oriented analysis and design like UML, OCL and XML, supplies an environment for modelling personalized and device-independent user interfaces. In this project, VisualWADE was used to model the navigation and personalization aspects of the application. Personalization: ranking and filtering of news The user model we have implemented for newsletters automatically and transparently incorporates information gathered from user navigation (adaptive part). In addition, the user can set-up some filtering restrictions and customization preferences when registering for this service (adaptable part). The final model is based both on implicit interests on certain digital library sections (information gathered during navigation) and on explicit preferences compose the user profile. News are classified by category and subject-matter. Categories are: new publications (new digital resources), future publications, new sections, chat announcements, call for papers, suggestions from our departments, letters from readers, visits of important people, contests, and the remainder are classified as general news. Subjects or matters are derived from the actual thematic structure of the DL. Each theme section or subcollection generates a subject-matter, as for instance: Latin-American literature, humanities research, history, children's literature, theatre, interactive services of the DL, computers and humanities, critical studies, tribute to hispanists, Argentine Academy of Letters, PhD theses, movies, magazines/journals, recently printed books, law, and many more. This allows for a very fine granularity. An algorithm for ranking news preferences Every time the user clicks on an entry of the newsletter table of contents (see figure 4), the Web page jumps to a single piece of news, and the server increments in one the corresponding category and subject-matter counters. Only one category but multiple subjects can be assigned to a piece of news. Relative access frequencies can be computed for categories and for subjects. Then news can be given a ranking value for a given user for a given news-reading session, which is calculated as the sum of subject frequencies of the subjects corresponding to a given piece of news, multiplied by the frequency of its category. For instance, if a user has an access frequency of 0.3 for the \"new-publications\" category, and, 0.1 for the \"history\" and 0.2 for the \"PhD-theses\" subjects, then a piece of news announcing the publication of a PhD thesis on history will weight (0.1 + 0.2) 0.3 = 0.09, and will be ranked accordingly. A profile for filtering news On registration, the users can specify Boolean constraints for categories and subjects, saying which ones should to be sorted out and which should be displayed. This profile can be modified by the user. Personalization at work Newsletters are accessed through a monthly index, where news are ordered first by category and then by subject, according to the dynamically computed ranking. But not all the ranked news appear, they are filtered according to the user explicit profile. The first N (3) ranked entries that pass the filter are shown openly, and the rest appear as a collapsed \"more news\" button. OO-HMethod Overview Personalization properties are captured at navigation/presentation level and are reflected in their corresponding conceptual models by means of a set of association rules. The design and generation of the navigation logic is specified in two parts: a stable part, independent from the personalization properties, and a variable part, that supports the treatment of these rules. Finally, a rules engine provides the context to interpret the generated rules at execution time. (see figure 5). Conclusions This solution to the production of newsletters and a journal for a digital library saves an important amount of time: now we can produce the newsletters and the journal in less time and with less effort than what was previously required to produce only the newsletters by hand. The output formats are uniform and regular, and less error prone. Previous newsletters showed les uniformity and some rendering errors. Compared to the other production models of our digital library (for digital text books and for digital facsimiles books), this model is different in three ways: The production does not begin with scanning. This model deals with the production of digitally born material. This model includes many more different output formats. This model is tied to a given fixed periodicity proper of this kind of publication. The general newsletter, which is the most demanded one, currently has 14,000 subscribers worldwide. Concerning the dissemination model for these newsletters, we have enhanced the granularity by offering more detailed personalization options, which allow us to rank the news based on preferences gathered from user navigation (observation model). The design of this solution was performed according to the OO-H Model and using the VisualWADE tool. This technology can significantly increase the productivity at the time of developing Web applications. The MCDL, with this effort, struggles to fulfill its objective of spreading research knowledge to the global academic community through the Web. Our aim is to be able to offer a better service by optimizing searches of digital resources, by reducing waiting times for digital publication, by promoting dynamic scientific research communication and by developing efficient preservation strategies. Daily experience results in the continuous integration of new ideas. Our goal is not only the mere publication of research work, but to build a rich and open communication channel for the global scientific community. The newsletter service described here plays a key role in this communication channel. ",
        "article_title": "A Newsletters Service based on XML-TEI",
        "authors": [
            {
                "given": "Alejandro",
                "family": "Bia",
                "affiliation": [
                    "University of Alicante"
                ]
            },
            {
                "given": "Jaime",
                "family": "GÃ³mez",
                "affiliation": [
                    "University of Alicante"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The wide array of XML data specifications and the recent deployment of basic XML processing tools provides an important opportunity for the collaborative development of higher-level, interoperable tools for Humanities Computing applications. The sophistication and power of the TEI-XML encoding specification supports the development of extremely rich textual data representations that encourage, if not require, development of sets of tools to exploit features of encoded text to perform particular tasks. It may be the case that one general tool will never fit all possible uses for encoded documents, but that a set of more specialized, interoperable tools for end-user applications will provide mechanism for cost-effective deployment of end-user applications. As the ARTFL Project's contribution to the collaborative development of these tools, this paper will outline recent work on PhiloLogic 2 to support a wide variety of TEI-Lite (XML and SGML) encoded documents optionally using the Unicode character specification. We will present a general design overview, indicating the current features and limitations, of this implementation, which the ARTFL Project is releasing under the GNU General Public License. 3 We feel that Humanities Computing applications are particularly well suited to open source development by a community with wide ranging technical abilities that is not well supported by the commercial sector. PhiloLogic is the primary full-text search, retrieval and analysis tool developed by the ARTFL Project and Digital Library Development Center (DLDC) at the University of Chicago. Originally implemented to support large databases of French literature, PhiloLogic has been extended to support a wide variety of textual and hypermedia databases in collaboration with numerous academic institutions and, more recently, commercial organizations. 4 PhiloLogic is a modular system, in which a textbase is treated as a set of coordinated or related databases, typically including an object (units of text such as a letter, scene, document, etc) database, a word forms database, a word concordance index mapped to textual objects, and an object manager mapping text objects to byte offsets in data files. Each of these databases is stored and managed using its own subsystem. Textual metadata, for example, is extracted from the database and loaded into a subsystem that handles delimited field data using appropriate functionality, including arithmetic, boolean, and regular expression searching and may be implemented using variety of systems including standard open-source packages such as MYSQL or PostgreSQL. This model also supports extensive use of standoff markup 5, since objects in the document tree may be linked to extensive records in relational databases describing these objects. As with most full text search and retrieval systems, the amount of text processing actually involved in a user search is very limited, typically being only that required to extract an element from a document and format it on output. Similarly, building a PhiloLogic database typically requires a data pass to extract, in a consistent way, structured data from the text. The addition of TEI-Lite support for PhiloLogic required creation of new loaders and output formatters. We are using various approaches to both tasks and will discuss the strengths and weaknesses using SGML/XML-aware tools as opposed to more general programming techniques. It appears that certain tasks, particularly those involving data extraction from heavily nested elements, are better implemented using a tree based approach, while others, such as calculating the position of words and byte offsets in an abstract document object hierarchy are better approached as streams. Output formatting of objects also appear to present similar options, depending on the complexity of the encoding and display requirements. The current development version of PhiloLogic is also able to process Unicode in the form of UTF-8. 6 The internal word and object indexing system of PhiloLogic has been UTF-8 capable for some time, since it is independent of language or character specifications. Word searches are performed independently of look-ups in the word occurrence indexes. We are using several different approaches to Unicode support based on a multifield word management subsystem. While index entries are stored in UTF-8, search fields can be configured for various languages and combined with slightly modified regular expression matching which allows for searching on Unicode representations or various simplified representations. Romanizations, for our current experiments, are performed by Perl module (Obliterator) developed by ARTFL and the DLDC for transliterating and transcoding among ISCII, Unicode, and a host of romanizations of Indic scripts. 7 This model may be extended to handle many more writing systems. We are basing this paper on the 2t/2e series of the PhiloLogic engine. While fast, robust, and well proven, it is based on a fixed object depth word indexing architecture. This has two distinct limitations: it flattens object depths and it does not provide for a word \"attribute\" field to indicate that a word belongs to particular types of objects that users may want to include or exclude from searches (such as notes or stage directions). We will describe current work on the 3t generation of engine and hope to be able to include a development version of this variant in 2004. Even then, the possible variations of document encoding possible within the TEI-Lite specification may not be treated in ways that original encoders had in mind. We are planning for a base release of PhiloLogic with examples of how to implement a wide variety of options required for different document types and collections. Based on the long history of PhiloLogic use and development at ARTFL and among various collaborators, we are planning that the base release will include as many features as possible while not requiring significant administrative or development work to use effectively. Humanities Computing needs to foster collaborative tool development. It is our belief that these tools will be specialized, focusing on the theoretical orientations and practical experience of various humanities computing organizations. PhiloLogic is a result of ARTFL's need for tools to handle large amounts of relatively lightly encoded text with a significant orientation to the manipulation of large amounts of descriptive and analytical metadata. Encouraging interoperability and collaborative development, particularly the use of standard processing tools and encoding systems, will provide for a way to leverage development work being done at various institutions. Some sample prototypes of PhiloLogic for TEI (XML/SGML) are available at: ",
        "article_title": "An open source implementation of PhiloLogic for large TEI-Lite document collections",
        "authors": [
            {
                "given": "Mark",
                "family": "Olsen",
                "affiliation": [
                    "University of Chicago"
                ]
            },
            {
                "given": "Robert",
                "family": "Voyer",
                "affiliation": [
                    "University of Chicago"
                ]
            },
            {
                "given": "Orion",
                "family": "Montoya",
                "affiliation": [
                    "University of Chicago"
                ]
            },
            {
                "given": "Leonid",
                "family": "Andreev",
                "affiliation": [
                    "Harvard University"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "INTRODUCTION Models based on the frequency of word-types assume that tokens in a text occur randomly. Even with this constraint, many different strategies can be followed in the process of selecting words randomly. The best models proved to be those that assume that word-types are binomially distributed. Based on this binomial distribution a model was created first by Baayen (Baayen, 1996a; Baayen, 1996b; Baayen et al., 1996; Tweedie and Baayen, 1998) than used by Hoover (2003). In these models a constant was calculated for each type which occurred m times in the original text. Summing up these constants for each type a predicted number of word-types can be calculated. The model created this way is static, since it always provides a constant for a selected M (M = N, where N is the number of tokens in the text). In the era of the new generation of personal computers dynamic models can also be built based on the same assumptions. The ultimate goal of our studies was to build such a dynamic model. The question was if this new model could help us to explain the regularities of the introduction of word-types in a text, to support the hypotheses that by significantly decreasing the size of blocks in a text we can get a more sophisticated insight into its overall structure. Namely, by doing so, we can trace important narrative events (the introduction of a new scene, the embedding in the flow of narration of a scene, setting or event only distantly related to the general flow of the text) and thus pointing to possible individual characteristics of the given narration. This hypothesis is tested by matching statistical data of the actual text against a model which generates the hypothetical distribution of the introduction of new word types in a text. Any differences between real text data and data from the model are expected to show both the potential and the limitations of the approach. METHODS To conduct the analysis of a given text a program was developed which provides a model and creates an artificial text based on the frequency of the word-types in the original text. The essence of the model is that the relative frequency of the word-types are counted, and based on these frequencies a distribution function is generated. Then, to a randomly picked number a word is mapped with the distribution function. To provide comparable results to previous models and to analyze Hungarian literary works, the program is designed for English and Hungarian texts, but the users have the opportunity to create their own character sets for further investigations. Unlike models described in previous reports (Baayen, 1996a; Baayen, 1996b; Hoover, 2003) this program, due to the distribution function-based random selection, provides a dynamic model. The other difference as compared to previous methods is that the program divides the texts into short, constant-length intervals with usually 100 words. This method was found to be more accurate in two aspects. First, the length of the intervals does not depend on the length of the text, and this way, texts of different lengths can be compared on a more reliable way. The other advantage comes from the short intervals, since subtle changes in the course of the story can also be traced back. MATERIALS The analyzed texts were chosen by the following strategy: different works of an author, different volumes of a series from the same author, works of the same genre, concatenated short stories from the same, or different authors, both in English and in Hungarian. The main source of the e-texts was the Internet. The works not found on the Internet were manually digitalized. Due to the many different sources of the texts, first these texts had to be standardized: delete those paragraphs which are added to the e-text, but not part of the original texts; delete footnotes and endnotes; deal with the different typographic rules; concatenate the chapters; correct the mistakes which occurred at the process of digitalization, and finally the different file-formats had to be converted into text files (with .txt extension). To analyze a text first the introduction of the types in the original work, then the model, the artificial text was plotted. To each 100-word long block an integer was mapped, the number of the newly introduced types in the block. RESULTS The intervals into which the texts were divided were short enough to show subtle changes in the discourse. Such events occurred when a longish description was inserted into the text, when a new character with a new style (different from the style of the other characters) was introduced, and when foreign expressions, sentences popped up. The results, of course, show not only those changes that were coming from the logical flow of the story but also those, where the text contains parts which are not related to the events. In both cases the monotonic decay of the introduction of types was somewhat reversed. The above listed reasons provided, in most, cases more significant changes than the introduction of a new chapter. The model was able to follow those changes which were due to the flow of the story by providing small heaps in the otherwise decaying graph. However, the trace of those surprising, unrelated events, understandably, never occurred in the model. Comparing the original text and the related model we can pinpoint parts which are only loosely related to the story. It was also found that the length of the texts has significant importance, which is in accordance with previously mentioned results (Holmes, 1994). This can explain the fact that concatenated short stories of the same genre did not provide huge jumps at the beginning of a new story. The concatenated short stories behaved like a novel in this sense. The difference between concatenated stories and a novel was that the number of the newly introduced types was higher in the concatenated text. The concatenated short stories also showed that the introduction of types is not a characteristic of an author since concatenating stories from different authors sometimes showed higher repeating rate than stories from one author. Similar results were presented with another method in earlier works (Baayen, 1996b) where it was found that the differences in register [genre] may override differences in authorship. Not only the analysis of literary works was carried out but also the analysis of textbooks with second language teaching purposes. The recently accepted strategy to choose the vocabulary of a textbook is that to teach at least 1000 new words in each stage of a general course (120-140 hours' work), and over this suggested minimum as many words as possible. (Cunningsworth, 1995), To provide this amount of new words the textbooks should also provide a sufficient amount of text to make the courses effective enough. The result of the analysis of the selected series shows that these books fail to fulfill this requirement. The introduction of new types and the high number of hapax legomena show that the vocabulary of a textbook in these senses is not different from the vocabulary of concatenated short stories. SUMMARY Hungarian and English literary works and English textbooks were analyzed the find regularities in the introduction of word-types in these works. To carry out the study the texts were divided into short, constant-length intervals with a usual length of 100 words. Based on the frequency of the word-types in the original text a model, an artificial text was created. Comparing the original and the artificial text we were able to find intervals in the original text that corresponds to unpredicted, sometimes illogical events in the discourse of the text. Analyzing the textbooks, we learned that the introduction of word-types in these books showed resemblance to randomly chosen and then concatenated short stories. It seemed that the authors of the textbooks ignored that not only the number of word-types should be increased, but the words should be repeated a certain times in these books. The analysis of the given works showed that the introduction of new types in a text mainly depends on the length and genre of the work, and not a significant characteristic of the author. In further works I would like to analyze series of novels from different authors, also to see how they differ from those whose author is the same. The analysis of other monolingual textbook series is also in progress. The results of this analysis can lead to a change in planning textbooks where the designing of the vocabulary will not only mean the setting of the number of words, but also the effectiveness of vocabulary teaching will be improved.",
        "article_title": "Another Method to Analyze the Introduction of Word-Types in Literary Works and Textbooks",
        "authors": [
            {
                "given": "Maria",
                "family": "Csernoch",
                "affiliation": [
                    "University of Debrecen"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The TEI has grown and matured greatly in recent years, both in the number and breadth of its applications, and in their sophistication. It can be taken as a sign of the success and state of health of TEI to see persistent efforts to push its boundaries. One area that is repeatedly cited as one where the TEI \"should\" provide a competitive alternative, but apparently does not, is the realm of authoring or original composition by scholars and writers. 1 A closely related one is the interchange between authors and editors: the famous \"submission format\" issue. Thankfully, the ACH has already stated its support, in principle, for accepting conference papers in TEI format, and the supporting pieces in the form of initial cuts at stylesheets for given TEI subsets are fast getting put together. The work of a few pioneers amply demonstrates that in the hands of an expert at XML systems and processing, TEI can indeed be a serviceable authoring format: 2 it can clearly be made to fit the chore. Indeed, the widely used (and sometimes controversial) TEI Lite DTD is in many respects a balance of tags apparently chosen for their utility in authoring. The question is whether a set of declarations as provided by any P3- or P4-conformant DTD is a very strong approach to the need, as opposed to other thinkable alternatives. And at this point, there are several thinkable alternatives: there remains HTML or (for those who can assume the discipline) XHTML; there remain proprietary word-processor formats, which for all their shortcomings are still ubiquitous and well-understood; there is PDF, Postscript or TeX (still liked in some communities in the sciences). And now, there are alternative XML-based formats such as DocBook [DocBook 2003], the NCBI journal publishing DTD [NCBI 2003], and other \"standard\" (openly specified) XML technologies, designed either for authoring or for handling some problem close to authoring such as web site design and linking. Any technology, in order to gain acceptance, must be demonstrably better than all these, at least in some critical respects. The Next Question Down: Are We on the Same Layer? On the assumption that a solution, given present-day technologies, should be XML (and therefore be able to take full advantage of the wide range of available XML tools), it should be recognized at the outset that it is the element content models and element/attribute semantics in practice (at least insofar as they are captured by their containment or referential relations) that will ultimately make the difference for processing not the particular names themselves (of elements or attributes). What the names are is certainly important, that is, but it is not the most important, insofar as an \"alpha-renaming\" transform is a trivial operation; it is where element-to-element mappings do not work due to incommensurable semantics (whether operational or implicit), that we run into problems. A simple example is the way div elements work in HTML. Unlike TEI div elements, which enforce a deliberately strict containment hierarchy, div elements in HTML can be used for arbitrary formatting of what TEI would consider either structural divisions (true TEI divs) or block-level elements such as paragraphs, quotes, line groups, epigraphs, closers and what not. As an authoring format, therefore, HTML is both simpler and more flexible, and relatively more impoverished (which is to say, perhaps more difficult for some processing step down the line), than would be the TEI equivalent. The flexibility in this case is a false gift, effectively making HTML (unless some imposes some super-ordinate layer of modeling and validation over the top of it) useless for anything but display (to which these div elements are, after all, addressed). A strong authoring format, therefore, would make the distinction between structural divisions and ad hoc block elements even if that line is confessedly blurry, as it is at times, for the most part it reflects a distinction of enough practical use and importance, that modeling should be stricter than the bland HTML-style div. We run into this problem because in TEI, div does not \"mean\" what it does in HTML; and it proves on examination in this case that the tighter TEI meaning may be more useful for an authoring schema. The fact that these \"div\" elements share a name does not, that is, make these elements alike in any important respect. And even given that when we examine them closely, the question of which div an authoring format should prefer is easy enough to determine, it does not solve the general problem of \"TEI or not TEI\" even given our conclusion in this case that the preference should be for TEI. Yet looked at from another angle (maybe looking at it sideways), in fact our answer is here. In fact, simply by stipulating that our putative tag set would be where possible TEI, but might in some cases need to bend to local exigencies we have in effect already sequestered it, assigned to it a special position in our processing architecture different from TEI proper. In other words, having recognized that in most practical cases, an authoring markup language will work at another layer from display formats, we should acknowledge that it might stand apart from other instances of TEI, where the tag set is \"tuned\" to an editorial or archival application. That is, we should take authoring in stride as another layer, requiring of us that we support transformations between this format and others that we might want (indeed, having recognized that some formats, such as HTML or a print-oriented format such as RTF or Quark, will inevitably be on separate layers in any case), but rewarding us for that extra investment of concern with a complementary flexibility not only renaming of elements is possible, but the mapping of element relations as well. Designing to the TEI Architecture In fact the solution of implementing an authoring tag set \"at a remove\" in this way has already been endorsed, both in theory and in practice, by early implementors such as OUCS or anyone who has written a stylesheet for TEI Lite. As early as four years ago (1999), Architectural Forms were already being proposed as a more consistent and more stable form of maintenance of TEI elements and their relations [Simons 1999] than literal declarations; likewise, Lou Burnard's examinations of the authoring question have identified TEI's usefulness not as a solution \"out of the box\", but rather as an architecture [Burnard 2001, slide 4]. By this mechanism, specific allowance can be made for altering tag structures to suit the needs of authors or web-site maintainers (for example), as opposed to editors, while likewise bringing from TEI that which is best about it, its \"bones\", one might say, along with the notion that if we can define, document and implement a consistent set of element structures within this framework, they can be made sufficiently close to TEI in spirit to satisfy our community's wish for a TEI orientation. One way to approach the design problem is simply to formulate content models including useful element types straightaway, from the TEI architecture. This is the approach being taken by the strictly TEI efforts such as the OUCS web-site authoring format (see [Rahtz 2001]). On the other hand, it is exactly here that it proves helpful to have a prototype of an authoring format ready to hand that is not TEI. A tag set, in other words, that is designed specifically with scholarly authoring requirements in mind, but which does not in itself draw from the TEI design, can be a very useful foil, helping us to abstract solutions to design problems away even from TEI itself. Fortunately, this approach, while it may at first blush seem radical, is quite in keeping with the application of TEI as an architecture. Moreover, the stresses here can be identified easily enough by assessing the simplicity and accuracy of whatever transformation logic is necessary to get us from one model to the other. Since I have already developed such an authoring tag set for my own uses, this is the direction I am currently working in towards full TEI authoring. While I have not yet built this particular transformation, my expectation is that it will not be difficult but that the more important findings of this five-year experiment in making a tag set from scratch, to order, may be not where its design can be readily recast as TEI, but rather where it cannot. My findings on this issue will be part of the paper I propose for ALLC/ACH 2004.",
        "article_title": "Authoring Scholarly Articles: TEI or Not TEI?",
        "authors": [
            {
                "given": "Wendell",
                "family": "Piez",
                "affiliation": [
                    "Mulberry Technologies, Inc."
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "When Rudyard Kipling's Lahore scrapbooks became available to scholars in 1976, the clippings they contained confirmed his authorship of numerous unsigned pieces published in Anglo-Indian newspapers in the 1880s. Less commented upon were the three scrapbooks in the Kipling collection that had been filled by his father, John Lockwood Kipling, with clippings of his own writing for some of the same papers. One of Lockwood's clippings, the 4 June 1887 Civil and Military Gazette (CMG) leader \"A Word on Indian Progress,\" provides important reason to doubt recent arguments about Rudyard Kipling's multiculturalism - partly because significant circumstantial evidence suggests that Rudyard had a hand in its composition. The question of this paper is whether stylometric analysis supports the hypothesis of collaboration. For many years the standard narrative explained Rudyard Kipling's early success by imagining him peering into Indian and Anglo-Indian by-ways, achieving unmediated contact with British and Indian and Eurasian people in Lahore and Simla. But there is no evidence of such investigations during Kipling's first year in India. He joined his parents (he lived with them until 1887) in Lahore in October, 1882, and he began work as sub-editor of the CMG in November. While his 1884 letters document early extramural assignments, the 1883 letters suggest a hard routine that kept him at the office ten hours a day and otherwise at home. He was seventeen years old in 1883; it seems unlikely that he would have wandered the old city of Lahore. Instead, the young Rudyard Kipling discovered British India in the newspapers he read as a major part of his job and from his parents at home. One may find in 1883 issues of his paper many of the ideas that appear in his best known stories, and one may find his politics in the writing of his mother and father, who had lived in India since 1865. This conclusion leads us to question the view of revisionist critics like Sara Suleri, Zorah Sullivan, and Don Randall, who argue that one can see through Kipling's sympathetic understanding of the Indian people a nearly postcolonial ambivalence. But Lockwood's clippings provide a colonialist view that features no ambivalence. Particularly straightforward is \"A Word on Indian Progress,\" which offers a \"scientific\" justification of empire that both is congruent with the Kiplings' other writing on the subject and offers deep sympathy for the Indian people - but on the grounds of their racial inferiority. Rudyard probably edited his father's essay. He was in Lahore in June, so he would have read it before it was printed. In fact, the habit in the Kipling family was a kind of composition that invited collaboration. Lockwood notes in one scrapbook that Alice wrote one of his columns while he was ill; Rudyard and his sister collaborated on the 1885 Echoes by Two Writers; all four family members produced Quartette, the 1885 Christmas number of the CMG. Rudyard publicly thanks his father repeatedly for a deep knowledge of India. \"A Word\" justifies imperialism as a humane salvation for a people whose sordid lives cry out for the liberation by European civilization. The ultimate scientific causes of their condition are India's climate and the consequent \"limits set by Oriental prescription to the feminine mind.\" The heat in India \"enervates the mind and will\" and \"stimulates the passions.\" The effects of the climate have been magnified by Indian treatment of women. \"Hindu and Muhammadan women are literally imprisoned all their lives in cells which would be condemned as unfit for human habitation by the most barbarous prison-administration in existence. . . . The very talk of women thus confined is a clipped jargon, apt enough [only] for the expression of affection and the narrow round of housewifely duty . . . .\" Since \"these are the mothers of the people, . . . by the iron law of heredity, the peculiarities of their character and the limitations of their mental powers are faithfully transmitted to their children.\" That Lockwood's son read these \"facts\" in draft is nearly certain. That he shared in their presentation is likely. This likelihood, however, is based on historical evidence; the question is whether stylometrics can be used to support or reject the inference. Certainly, stylometrics has been used to examine collaboration, as in Brian Vickers' Shakespeare, Co-Author. However, previous studies focus on cases in which individual authors may have written discrete parts of a text. Rudyard and Lockwood Kipling are likely to have collaborated in a way more akin to thorough editing, revision, and re-writing. We begin by asking what stylometric techniques are appropriate in determining whether Rudyard did, indeed, have this kind of role in \"A Word,\" and we take a cue from John Burrows' distinction between \"open\" and \"closed games\" in \"Delta: a Measure of Stylistic Difference and a Guide to Likely Authorship.\" Open games are those \"where we are faced with an anonymous text but have little or no outside evidence to identify the most likely candidates\" whereas closed games are those in which \"only two or three writers are eligible candidates for the authorship of a particular text\" or the \"question is whether or not a particular writer (and no other) is the author\" (267). At least one element of the question of collaboration is clearly a closed game situation: is Lockwood (and no other) the author of \"A Word\"? About such closed games, Burrows suggests that we are relatively \"well equipped to form strong inferences\" utilizing methods \"currently employed in computational stylistics\" (267). One of those methods is the Burrows technique of using the relative frequencies of the forty most common function words as variables, performing a principal components analysis, and displaying the results on a scatter graph. We have run such an analysis, using six essays known to have been written by Lockwood and six known to have been written by Rudyard, as well as \"A Word.\" (The number of texts is limited because of the need to have texts of a sufficient length.) In the analysis, the first two principal components account for 64.57% of the variation in the data. The results are displayed in Figure 1. The results show discrete clusters that seem to differentiate the style of Rudyard on the one hand and Lockwood on the other, this being the case even when samples of Rudyard's short stories are included. Most important in attempting to determine authorship is that \"A Word\" does not cluster with other Lockwood essays in either this analysis or one that includes only essays by Lockwood. Possibly, \"A Word\" is an outlier, but, from this evidence, it would be difficult to argue that \"A Word\" is by Lockwood \"and no other.\" Given the historical evidence for Lockwood's authorship, it seems that collaboration could be a reasonable explanation for the difference in style between \"A Word\" and Lockwood's other texts. More compelling evidence for this explanation may be found in the results of a hierarchical cluster analysis using average linkage, of the same forty function words. (See Fig. 2) The dendrogram shows the close relationship among the known texts by Lockwood and also among the known texts by Rudyard. As well, it shows that \"A Word\" is more closely linked to Lockwood's texts than to Rudyard's but that the texts of the two Kiplings are more closely linked to each other than to a sample text from a t hird writer from the Civil and Military Gazette. In a series of analyses (in the full paper) using other sample texts, the results remain consistent. \"A Word\" shows slightly closer links to Lockwood's texts but is relatively closely linked to Rudyard's. The results do not prove collaboration, but they are what we would expect from a text in which Lockwood was the primary author but in which Rudyard had a role. In the conference, we would discuss the words that account for the variation in the data and attempt to suggest how they may offer evidence for collaboration insofar as the frequency of particular words is more consistent with Rudyard's practice than with Lockwood's. For instance, Lockwood uses the term \"has\" at a frequency about twice that found in any of Rudyard's texts, yet \"A Word\" uses the term at about the same frequency as found in Rudyard's texts and substantially less often than found in any of Lockwood's other texts. Looking at particular sentences allows us to see how revision and editing might account for these differences. It seems to us that stylometric evidence, when combined with historical circumstantial evidence, does allow for inference of collaboration in this case, an inference that has important consequences for our understanding of Rudyard Kipling's multiculturalism. ",
        "article_title": "Before Multiculturalism: Stylometric Analysis of a Collaboration by Lockwood and Rudyard Kipling",
        "authors": [
            {
                "given": "Peter",
                "family": "Havholm",
                "affiliation": [
                    "The College of Wooster"
                ]
            },
            {
                "given": "Larry",
                "family": "Stewart",
                "affiliation": [
                    "The College of Wooster"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "While computational stylistics and text analysis seems to be in constant quest for the just right set of criteria (see e.g. David Hoover's presentation at the 2002 ALLC/ACH Conference in TÃ¼bingen), this presentation will try to apply what has already become a standard in statistical stylistic analysis to a (relatively) novel material. Taking for granted very unoriginally the usefulness of John Burrows's method that has been around since his 1987 Computation into Criticism, in discerning stylistic differences between individual characters in works by the same author, I will try to see if the same or similar patterns of similarity and difference travel across linguistic boundaries; if differences between characters' \"idiolects\" are preserved in translation. In a typically Polish approach to the matter, I have chosen as my material the trilogy of historical romances by Poland's first literary Nobel Prize winner, Henryk Sienkiewicz, written between 1882 and 1888, and its two English (or, more precisely, American) translations by Jeremiah Curtin (completed between 1890 and 1893) and W.S. Kuniczak (1991). The reasons for this choice have been manifold. First, Sienkiewicz's three novels, With Fire and Sword, The Deluge, and Pan Michael, although set in the turmoil of 17th-century Poland, remain to this day a major classic of Polish literature and the country's most popular reading. Second, a trilogy, the subsequent parts of which share some of their characters, seems ideal for Burrowsian analysis (a fact confirmed by the interesting coincidence of John Burrows's undertaking of a study of Beckett's bilingual trilogy in a much later paper). The final reason was the difference between the two translations, evident both in their being separated by an entire century and, what follows, in the entirely different approaches and results obtained by the two translators: the largely word-by-word transcoding by Sienkiewicz's contemporary and the highly adaptative and \"free\" method of the modern Polish-American writer. Faithfully maintaining the original Burrows model, the study of distances between the \"idiolects\" of the major characters has been based on relative frequencies of the 30 most frequent words in the dialogue of each version of the trilogy. The resulting correlation matrices were then used to produce two-dimensional multidimensional scaling charts of distances between such \"idiolects.\" This procedure has yielded, in Sienkiewicz's original, a very consistent influence of the personae's social status and ethnic background, especially in the first part of the series. It is particularly visible in idiolects of 'enemy' (non-Polish) collective characters, usually plotted at some distance one from the other. Curtin's translation is notable for 'de-clustering' idiolects of various Polish gentry characters, making their idiolects much less alike. There is also a visible tendency in Curtin to limit the distances between rival collective characters and making them markedly similar rather than divergent as in the original. Idiolects in Kuniczak are even more evenly distributed, with a general trend towards greater distances and less clustering observable in the graphs. The very high resemblance between the idiolects of two major characters, ZagÅoba and WoÅodyjowski, in all three versions of The Deluge (almost identical in Sienkiewicz) is one of the most consistent traits of this portion of the analysis an interesting illustration of the fact that the two personae's function of keeping the three volumes together becomes evident in the second part of the series. Sienkiewicz's social/ethnic idiosyncrasy has been confirmed in a plot for idiolects of characters involved in the Polish-Ukrainian conflict in With Fire and Sword, a feature slightly visible in Kuniczak and almost not at all in Curtin. As perhaps the most consistent effect of all, the peripheral situation of female idiolects is a constant element in almost any configuration. The study of more detailed and thematic configurations of characters is also the source of interesting insight. Plots for female characters exhibit a tendency to group together young (and marriageable) Polish women; Helena's Ukrainian provenience is highly visible in Sienkiewicz, while the ethnic element is indiscernible in both translations. Among characters involved in each novel's eternal triangles, both of the above aspects are clearly visible in all three versions; differences between characters in the same triangle are quite considerable. In an examination of characters that recur throughout the series, a good consistence has been observed between the three idiolects of the Polish Falstaff, ZagÅoba, the most inveterate talker of the series, in each version separately: best in Sienkiewicz, worst in Curtin. Another character, WoÅodyjowski, is much more of a developing character, which agrees well with the evolution of the persona in the course of the series, from a humble officer to the hero and spearhead of Sienkiewicz's ideology. This has been confirmed in a separate plotting of idiolects of those two characters. A number of joint plots for Curtin's and Kuniczak's translations (based on frequent words common in both versions) have been made to investigate if there is a constant pattern in their respective differences. In agreement with some \"intuitive\" assessments as to the decreasing differences between Curtin's and Kuniczak's versions (mainly due to Kuniczak's gradual abandonment of adaptative procedures, especially on the microstructural level), the patterns become more consistent with time: fairly chaotic movement for the first part of the trilogy has become more ordered in the second and almost uniform in the third. The \"stylistic drift\" observed between idiolects in Curtin and in Kuniczak divided, apart from their contrasting approaches to translation, by an even more significant difference of a whole century is a vindication of Burrows's 'tiptoeing towards the infinite:' that visible and uniform shift in the configuration of the most frequent words in English texts with time. ",
        "article_title": "Burrowing into Translation: Character Idiolects in Henryk Sienkiewicz's Trilogy and its Two English Translations",
        "authors": [
            {
                "given": "Jan",
                "family": "Rybicki",
                "affiliation": [
                    "Krakow Pedagogical University"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "This paper supplies the results of a study of explicitating cohesive shifts in translations of English fiction texts into Swedish. On the basis of the results, the paper discusses different potential explanations for the phenomenon of translational explicitation. The subject of the paper relates specifically to the theme of \"Computing and Multilingual, Multicultural Heritage\" in that it focuses on cross-cultural and cross-linguistic mediation as a specific type of text production (cf. Baker 1995). This aspect of text production is particularly relevant to a small language such as Swedish, since a great deal of text published in Swedish is translated text. Moreover, very many translated texts are translations from English (cf. Wollin 1998). Explicitation is a generic term for the different ways in which implicit source text content is being made explicit in the target language text through translation. Examples of such explicitation are cases in which a surface item is added in the target text, or cases where an item is made more semantically specific or more prominent in the target text (cf. SÃ©guinot 1988). Explicitation is often claimed to be one of the more common typical features of translated texts (e.g. Chesterman 1997:71; Marco 2000:13). Further, explicitation has been listed among other proposed translation universals such as normalization and simplification (Baker 1996). However, although the phenomenon of explicitation is often pointed to, and although a small number of studies of varying scope and size have been carried out for some languages, it has generally not been empirically verified on a large scale in corpus studies. As for English and Swedish, no corpus research has been carried out which is specifically directed towards explicitation in Swedish translations of English texts. A class of items which has been proposed to be likely to be explicitated is cohesive markers (Blum-Kulka 1986). This hypothesis has been confirmed by studies on explicitation in translation between different language pairs (cf. e.g. Weissbrod 1992; Klaudy 1993; Englund Dimitrova 1993; ÃverÃ¥s 1998). The explicitation of cohesive markers is the specific area of explicitation with which the study accounted for in the paper is concerned. In the investigation, the perspective is limited to grammatical cohesion. (Grammatical cohesion is described in terms of the frameworks in Halliday and Hasan 1976, Halliday 1994 and NystrÃ¶m 2001.) The investigation focuses on explicitation of cohesive relations on the clause and sentence levels. Further, the area under study is restricted to explicitation through addition and semantic specification, i.e. not explicitation through changes in focus or items being given greater prominence by other means. The corpus used for the investigation is The English-Swedish Parallel Corpus (ESPC), a combined comparable and parallel aligned corpus of English and Swedish original and translated fiction and non-fiction texts (cf. Altenberg, Aijmer and Svensson 2001). The structure of the corpus allows the comparison of non-translated and translated texts in the target language as well as the comparison of target language translated texts and their source language counterparts. The direction of translation studied is translation from English into Swedish, and the material used for the investigation is the fiction part of the ESPC. The method employed is computer-aided manual coding of corpus material. The further purpose of this type of coding is to establish a basis for formulating adequate search techniques for studying explicitation in corpora through automatic retrieval of surface patterns. In addition to describing explicitating cohesive shifts of translation, a further aim of the paper is to tentatively discuss the distinction between systemically and pragmatically conditioned explicitation. The purpose of this discussion is to distinguish between such explicitating shifts that result from systemic linguistic contrast between the two languages involved, and shifts that have occurred in the absence of such typological restrictions, and that can thus be supposed to be more related to other factors. Examples of such factors are target community linguistic and translation norms, register differences, properties of specific source texts and the stylistic preferences of individual translators. The findings of the study are expected to be of some relevance for translation research as well as translation teaching. The relevance for research is first of all the empirical description of explicitating shifts in translation from English into Swedish. Secondly, the findings are expected to contribute to the development of adequate tools and methodologies for retrieving material indicating explicitation from corpora. Thirdly, the findings are expected to contribute to theoretical development in the area of explicitation. Finally, the relevance for teaching is expected to be that a better description of explicitation will supply a better basis for discussions about explicitation and its role in cross-linguistic and cross-cultural communication.",
        "article_title": "Cohesive Explicitation in Translations",
        "authors": [
            {
                "given": "P-O",
                "family": "Nilsson",
                "affiliation": [
                    "GÃ¶teborg University"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "This proposed paper is concerned with approaches to describe the meaning of markup in (textual) documents, i.e. \"markup semantics\", and approaches to describe abstract, conceptual models which are sometimes applied to markup, i.e. \"semantic markup\". A motivation to combine both approaches is discussed, and the respective methodology is explained. Markup semantics In the field of humanities computing, textual documents with markup play a crucial role. A sophisticated document grammar like the TEI is a key component for the creation, editing, analysis and interchange of electronic documents. Document grammars describe the syntax of documents. In recent years, a new area of research called \"markup semantics\" has emerged, which is concerned with the (formal) description of the meaning of these constructs. For example, a <para> element can be used to mark-up paragraphs, but its interpretation as a paragraph is not part of the formal declaration of the element in the document grammar. The BECHAMEL project (Renear et al., 2002) is probably the most prominent approach in this area of research. Other approaches have been developed (e.g. Simons, 1999; Welty and Ide, 1999) and are discussed in detail elsewhere (Sperberg-McQueen et al., 2000). These approaches share the aim to describe the meaning of markup \"bottom-up\": Various mechanisms are developed to enhance given document grammars and marked-up documents with additional, semantic information. Semantic markup In contrast to markup semantics, semantic markup (1), developed for example within the \"semantic web\" initiative (Berners-Lee et al., 2001), offers a \"top-down\" approach, from a given semantic, abstract description of \"resources\" (a \"conceptual level\"), to concrete resources (for example to markup). The abstract notion of a \"resource\" plays a crucial role: the goal is to allow for a meaningful access to every imaginable kind of \"resources\" on the web. The universality of this approach might become a drawback if it is applied to a specific kind of resource like markup. For example, the \"RDF Vocabulary Description Language RDF Schema\" (Brickley and Guha, 2003), a central specification in the development of the semantic web, allows to assign properties to classes of resources in the form of triples, e.g `paragraph is-part-of section'. But no mechanism is supplied to verify whether instances of classes really exist and whether they have the properties being assigned, e.g. whether a <para> element is nested inside a <sect> element. Current research tries to fill this gap, again moving \"top-down\". Klein et al. , 2001, describe the transformation of abstract, \"ontological\" descriptions to document grammars. Erdmannn and Studer, 1999, offer a similar approach, focusing on the aspect of querying documents at the conceptual level. Markup semantics and semantic markup: A unified approach This proposed paper suggests a combination of the ongoing efforts in the two areas of research described above. Markup is interpreted as a concrete instance of concepts, which are defined at the abstract, conceptual level supplied by the semantic markup approach. A <para> element might be an instance of a concept `paragraph' or of a concept `thematicUnit'. On the other hand, a concept `paragraph' might be instantiated as a <para>, <segment type=\"p\"> or <absatz> element. A unified approach to markup semantics and semantic markup offers new prospects for scholars in the humanities: The integration of text encoding and abstract, conceptual resources. Currently, an enormous amount of such resources is being created, for example Metadata relying on the Dublin Core standard, ontological descriptions of linguistic categories (Lewis et al., 2001), or lexical resources like WordNet. These resources can contribute to the solution of some research questions in the field of markup semantics. An example considering multilingual documentation of document grammars and instance documents will be given in the next section. A \"secret marriage\": Mapping between markup and the conceptual level The methodology is visualized in fig. 1. The upper part contains three lexical concepts from the WordNet database, `paragraph', `section' and `book'. They have semantic relations like being a meronym (`paragraph is-meronym-of book'), i.e. a part of book, etc. The lower part of fig. 1. contains markup, i.e. document grammar constructs like the element declaration `<xsd:element name=\"book\"> ...' or instance documents. The key part of the methodology is the mapping between the conceptual level and the markup, which is visualized in the middle of fig. 1. Currently, this mapping is realized within a format called \"Context Specification Document\" (CSD, Sasaki and PÃ¶nninghaus, 2003) (2), which allows for the sub-classification of markup according to its structural properties. The sub-classification is necessary because document grammars do not allow to specify all contextual, structural properties of markup which are necessary for the mapping to the conceptual level. For example in an instance document there might be <para> elements which are nested within a <book> element, or <para> elements which are nested within an <article> element. For the first, a mapping called `part-of-book' is declared. The structural properties of markup are described via path expressions (3) like `up* book', which are matched by the respective <para> elements in instance documents. In addition to this specification of structural properties, a CSD contains pointers to concepts or interconceptual, semantic relations. For example the mapping called `part-of-book' has a pointer to the interconceptual relation `paragraph is-meronym-of book'. That is, the meaning of the path expression `up* book' is specified as `paragraph is-meronym-of book'. Using counterparts of WordNet in other languages, e.g. EuroWordNet, a multilingual documentation of markup can be generated, e.g. for German `Absatz ist-Meronym-von Buch'. Unlike the approaches described above, this mapping can be created \"bottom-up\" AND \"top-down\" in a declarative manner: From the conceptual level, a description of meaning can be added to the markup, i.e. as a markup semantics. This allows for the semantic validation of documents during the authoring process. In addition, markup in documents can be retrieved as instances of concepts, i.e. as a semantic markup. For the creation of the mapping and the generation of markup semantics or semantic markup, both markup and the conceptual level do not have to be changed, so this approach is called \"a secret marriage\". A prototype of a processor for CSDs, implemented in the programming language Python, creates information about the structural properties found in documents. In addition, XSLT-Stylesheets are being created, to integrate this information into document grammars, into instance documents or into the conceptual level, represented within an XML-serialization of RDF Schema.",
        "article_title": "Combining Markup Semantics and Semantic Markup: A secret marriage",
        "authors": [
            {
                "given": "Felix",
                "family": "Sasaki",
                "affiliation": [
                    "University of Bielefeld, Department of Computational Linguistics and Text-technology"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "We, at the Wired Humanities Project at the University of Oregon, are creating a model Distance Research Environment (DRE) for fostering international collaboration in manuscript studies. We are developing the DRE for initial use on a group of Mesoamerican pictorial manuscripts (or mapas ) and then, later, will test the model's broader applicability on a group of medieval European manuscripts. It is our intention to take online manuscript analysis well beyond what is currently available. Mesoamerican pictorial manuscripts capture the as yet incomplete story of the ramifications of the Spanish conquest and colonization of the Americas. They also illustrate internal cultural evolution both prior to European contact and after. It is only relatively recently that scholars have directed some of their energy into the study of the native languages that were employed in the original composition of many Mesoamerican manuscripts. The decipherment of their pictorial elements has also made significant gains particularly in the last few decades. Part of what has held up the advancement of this field of scholarship is the fact that it has been costly for scholars to travel to distant archives and spend extended periods consulting them. The distance research environment will provide a net that integrates both dispersed documents and dispersed scholars, facilitating communication and exchange from multiple locations. Through digitization we will make the initial group of targeted manuscripts more accessible on a \"Mapas Project\" web site. Through the site, we will provide full views and details, enhancing the whole and its parts with \"digital wash,\" magnification, and rotation to provide for the clearest possible online reading, using Flash or Shockwave. We will build \"hot spots\" over all graphic elements, glosses, and text sections, offering windows for the various scholars to submit transcriptions, translations, and commentaries in association with the most minute portions. To facilitate the paleography, we will have the facsimiles of the texts side by side with the transcription windows. To facilitate translation, we will have the transcription windows side by side with the translation windows. To aid with the analysis of images, facsimiles of all pictorial details and groups of related images will be offered next to windows for the submission of comments. We will archive these submissions in a database that is searchable on line by the other collaborating scholars and that allows for further, asynchronic commentaries. Scholars, for instance, will be able to pull up an image or cluster of images and see all associated analysis and discussion. They will be able to do the same with text, viewing all contributions surrounding, for example, a particularly difficult passage of text and the various transcriptions it has spawned or the potentially conflicting translations. We will also periodically synthesize and publish to the Mapas Project web site the more notable scholarly dialogues that emerge. We believe that the collaboration of multiple scholars around the globe will produce high-quality interpretations, the exchange will benefit the understanding and knowledge of the participating scholars, and the process will inform anyone in the humanities computing community interested in replicating this research environment. Managing multiple languages is one of the primary challenges of the DRE. The Mesoamerican manuscripts that we have selected contain texts in Spanish, Nahuatl, and Zapotec. The scholars we have chosen to work with these manuscripts are native speakers of English, Dutch, Spanish, and Nahuatl. It is our primary aim to have transcriptions for all the original texts, with translations, where necessary, into both English and Spanish. The database, however, will record comments made in additional languages, and users accessing the database will be able to request all languages or only selected ones. Users will also have the option of having search results displayed in various formats, choosing, for example, to have the original Nahuatl version next to the Spanish translation. Another one of the challenges we face in the construction of our database will be to clear some of the roadblocks we have encountered when trying to pursue advanced philology. Searching to find the use of specific terms in their original context in Nahuatl and colonial Spanish, for instance, can be complicated by the inconsistent use of diacritics. Sometimes accent marks, glottal stops, and vowel length indicators appear, and sometimes they do not. We will experiment with having multiple versions of such texts, one with all the diacritics as they appear in the original manuscript, and one that has been simplified. We will also develop guidelines for our users, suggesting various approaches they might try when searching. Unstable orthography is another potential pitfall. For example, where one might hope to find the Nahuatl term for woman, \"cihuatl,\" in a text, one might have to try \"cihvatl,\" \"ciuatl,\" \"cihuat,\" and so on. We will experiment with hypothetical searches to try to anticipate these obstacles and help our users avoid them. Our inclusion of a Nahua scholar on our team is symbolic of our hope to encourage the integration of indigenous people with native-language ability into the regular interpretation of manuscripts that once did -- or still do -- reside in their community archives. The diaspora of indigenous peoples' material culture, whether museum objects or archival manuscripts, is an issue that touches on the United Nations' concern for Indigenous Peoples' Intellectual and Cultural Property Rights, as does the presentation of such materials before a global audience by way of the Internet. We believe that indigenous people should not only participate in the study of the manuscripts in question, but should also enter into decisions about materials that might be deemed too sensitive to make public. We therefore have an advisory board that is both culturally and linguistically diverse. We are also very concerned to obtain the input of professional librarians with expertise in metadata. We will build into our databases searching vocabularies that are recognized globally and will make retrieval of discrete elements of larger documents feasible. We have recruited several librarians at our home institution to work with us on this development. The Mapas DRE will create a sustainable online, collaborative venue for scholars, involving a broad spectrum of institutions, individuals, and perspectives, free from geographical or temporal limitations, for the distribution of high-quality digital versions of historical material. Once this has been accomplished (and we anticipate it will take two years to study the six initial manuscripts), we will test the tools and standards of the DRE for its wider usability by applying it to a group of medieval European manuscripts. For the ACH/ALLC conference, we would like to demonstrate how we envision the Distance Research Environment will function, explore some of the complexities associated with the multi-lingual dimensions of the data, and solicit comments and suggestions from the audience on all of these elements. By June 2004, we anticipate having some of DRE features already functioning. But it will also be early enough in our two-year plan that we would greatly benefit from the input of our ACH/ALLC colleagues.",
        "article_title": "Creating a Distance Research Environment (DRE) for the Mapas Project",
        "authors": [
            {
                "given": "Judith",
                "family": "Musick",
                "affiliation": [
                    "Center for the Study of Women in Society, University of Oregon"
                ]
            },
            {
                "given": "Stephanie",
                "family": "Wood",
                "affiliation": [
                    "Center for the Study of Women in Society, University of Oregon"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "This paper will discuss the encoding of Medieval characters in XML documents, based on the recent publication of a character recommendation by the Medieval Unicode Font Initiative (MUFI), . Although an impressive number of characters have been included in the Unicode standard, many characters or variant letter forms are still missing, and it is an open question whether these will be accepted by the standard. Furthermore, precomposed characters -- i.e. characters with various diacritics -- will no longer be accepted by Unicode. These characters can be encoded and displayed with smart font technology, but support for this technology is not wide-spread and there are cross-platform problems. Unicode has set aside a Private Use Area (PUA) for the encoding of characters by individual projects or user groups. The PUA will not be used for any official characters, but it is as well supported as any other part of the standard. Font applications like FontLab allow users to define fonts with characters in the PUA, and these fonts will by and large be displayed correctly in applications with Unicode support. Documents containing characters in the PUA can be interchanged across applications and platforms, but they will only display the correct PUA characters if the same font is installed or there is a local standard for the PUA in a specific user group. In spite of the obvious compatibility problems, the PUA of Unicode can prove to be a good solution in the short or even medium-term perspective. There are in fact three separate PUAs in the Unicode standard. The first of these is the one in the Basic Multilingual Plane covering 6,400 code points, ranging from E000 to F8FF. Recently two supplementary planes, nos. 15 and 16, have been set aside, each containing 65,534 code points. The supplementary planes are not well supported yet, so the PUA of the Basic Multilingual Plane has received most attention. Commercial companies have been using some code points in this area, especially towards the end, but otherwise it is being used by various interest groups, many within the academic community. One example is the Titus project, which has allocated several thousand characters for linguistic usage to this area. Other font projects are following, such as the Junicode font for Old and Medieval English, and Alphabetum, a multi-purpose font with many characters for the classical languages. Version 1.0 of the MUFI recommendation has focused on Medieval Nordic characters, but it is expected that the coming versions will cover additional national or regional characters. It should be noted, though, that many characters and letter forms were used across large areas of Europe, so it is often misleading to locate characters to specific areas. For Medieval Nordic primary sources, the recommendation lists approx. 800 characters, of which only approx. 400 are in the Unicode standard. The remaining characters have been divided into 20-odd subranges in the PUA, in three main categories: base characters, precomposed characters and variant letter forms. At the moment, several fonts are being developed or extended to include the PUA of the MUFI recommendation, and they will in due course be made available on the MUFI site. Fortunately, TrueType fonts are now supported by all major platforms (Linux, Mac, Windows) so the whole set of MUFI characters can be encoded in a single font. Although a Unicode font can be used as easily as any ordinary 8-bit font in a word processor, many academic projects would like to encode texts in a more interchangeable format than the one offered by word processors. Of the SGML derivatives, Extensible Markup Language (XML) has proven to be a robust and versatile encoding language, in spite of the well-known limitations of this standard, notably the encoding of concurring or discontinuous structures. Basically, an XML document is divided into a header with general information about the text and the nature of the encoding, and the body, with the text proper. The header refers to a Document Type Definition (DTD) with further specifications of the encoding. An XML document is by default encoded according to the Unicode standard; this is specified in the first line of an XML document, which typically reads <?xml version=\"1.0\" encoding=\"UTF-8\"?>. However, the use of characters from the full Unicode standard may cause problems when files are interchanged or processed by older software without Unicode support. For this reason, it may be advisable to encode all characters outside Basic Latin (a--z / A--Z) with entities. The DTD will be needed for the definition of the entities, and by linking each entity to a corresponding code point in the Unicode standard -- including the PUA -- this can be done in a simple and efficient manner. With an appropriate font, texts encoded in this manner will be shown correctly. This paper will discuss how this can be effected for the encoding of Medieval Nordic primary sources, so that the usage of the PUA is sufficiently well documented. Of particular interest here is the question of decomposition. Many Latin characters with diacritics, such as 'Ã¡', 'Ã¨' and 'Ã´' have been encoded in precomposed form in Unicode (retaining old ISO standards), but new combinations must be encoded as a sequence of one or more characters; that applies for example to a commonly used character in Old Norse poetry, the 'o' with ogonek and acute accent. A robust solution for the display of decomposed characters is thus strongly needed, and this solution should by analogy also be tried and tested for the already encoded precomposed characters. As a consequence, simplified keyboard layouts can be designed for medium-size character inventories such as the one needed for Medieval Nordic characters. The PUA is at best a medium-term solution. The long-term solution is obviously to propose characters for the Unicode standard, with the aim of reducing the need for PUA encoding. The paper will conclude by discussing procedures for decommissioning characters in the PUA that have been accepted by the official Unicode standard.",
        "article_title": "Dealing with difficult characters: XML encoding and the Private Use Area of Unicode",
        "authors": [
            {
                "given": "Odd Einar",
                "family": "Haugen",
                "affiliation": [
                    "University of Bergen"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The Humanities Computing (HC) community has had a long and fruitful association with SGML and XML through the work of TEI. Indeed, the TEI has greatly deepened our understanding of the significance of markup of digital materials. With certain qualifications, the OHCO model that XML and TEI embodies (DeRose et al, 1990, and some useful follow-up discussion in DeRose 1997) has largely been shown to meet the needs document-oriented markup tasks. The HC's relationship with the relational database, on the other hand, does not appear to have been so positive. Part of the reason for this arises from the nature of many HC projects, which are focused on preparation of digital editions of source texts and are much better served by SGML/XML and the TEI. However, the dissatisfaction with the relational technology seems to go beyond this. We find it described as a \"matrix straightjacket\" in (Townsend 1999) -- although more generously described and understood in Greenstein's A Historian's Guide to Computing (Greenstein 1994). The Orlando project reports that they have deliberately rejected the relational model as a way of structuring their materials, asserting that they do not represent \"a 'database'; [because] the tagged prose must say subtle, complex things\" (Orlando 1998). At almost every ACH/ALLC meeting this writer hears the Relational model disparaged as inappropriate for some material or other. Our experience at the Centre for Computing in the Humanities at King's has been different from what one might imply from this seemingly widely held negative view of the relational model. It is based on the experience produced from long term (multi-year) and intimate associations with a broad range of humanities oriented projects (more than 30, and growing) in a large number of different disciplines, and dealing with source materials including images as well as text. For a somewhat different perspective on similar issues, see also Alvarado 1999 â framed there in the context of multimedia and metadata and digital libraries rather than the more specifically textual-oriented view we are taking in this paper. Of course, the relational model is not appropriate for representing the structure of textual materials, and the OHCO model, with the possibility of mixed content within elements, provides a perhaps not perfect but often good-enough model. In a number of our projects, however, project materials that primarily look like documents fit for textual markup often turn out to harbour materials that are not well handled in that way alone. Take, for example, a brief excerpt from the Relics and Selves project, which puts online a number of articles analysing how 19th century museums contributed to the development of national identities in several South American countries. TEI-like markup for a small snippet of this text might look like this (here rather simplified for the purposes of the present argument) (Andermann 2001) By tagging references to persons in the text (using the TEI's \"rs\" â \"referencing string\" tag) the access system can locate references to any individual referenced in the collection. The \"key\" attribute (defined in TEI P3 as \"provid[ing] an alternative identifier for the object being named, such as a database record key\") is needed because we are not so much interested in identifying names in the text as names, but as references to persons. If there were two Jorge Luis Borges's referenced across the entire project article collection then the rs elements for one might have the key attributes as shown here (let001), and a different key (say, sci192) for the other. The biographical information about the person (say, the person's birth and death dates, and/or some brief biographical prose sketch) is not appropriate to store here in the reference within the article, but belongs in another separate structure altogether â something structured more like a glossary, encyclopaedia entry, or an index. In fact, a similar argument could be made for the person types recorded in the rs tag. A glossary entry to define for the reader what was meant by a letrado (or scientist, for that matter) would be in a structure separate from the article text itself, and the type code here perhaps should be viewed as a reference to that entry. Furthermore, it could be argued that this classification belongs with the person data, rather than here with the name reference. Graphically, we could see these relationships as: Note the different character of the information in the \"person\" or \"role\" structure from the narrative-like article. If Relics and Selves was published as a book the Person data would likely appear at the back of the book as an index, and would be separated from the article text. This book index would, in turn, have an actually semantically arbitrary ordering based on the spelling of the person name so that references could be readily looked up. Unlike the article text which is meant to be read in \"document order\", the person index is meant to be consulted, and the order of consultation will almost certainly vary for different readers. The \"document order\" would have to be fixed on paper, but would, in fact, be determined solely to allow the user to jump into the middle it to locate an individual of interest. Furthermore, the index is, in fact, an editorial object, created by the editors from the original material and more useful when it is, as far as possible, made consistent. While preparing a person index the editor tries to include data for all agreed components entered for persons (even if, perhaps, the data was sometimes provisional). Thus, for each person a standard form for the name is given, even if, perhaps, that exact form does not appear in the body of the text at all. Possibly the person's birth and death dates, etc would be provided as accurately as possibly, as consistently as possible (accuracy and consistency sometimes conflicting, of course, but both essential aims for usability!) and this data would be provided for as many entries as was possible. The virtues of consistency and completeness, and the structured feel of the materials (\"there shall be provision for the recording of birth and death dates for all persons in the collection\") is characteristic of a table in a relational database. Figure I is, in fact, the beginnings of an \"entity-relationship\" diagram which is often used to display the structure of a relational database itself. By structuring this data in a database-like fashion one can then ask the computer to make use of it by not only allowing access through an alphabetical list of names, but also perhaps by ordering persons by birth decades, or as person type/role as an alternative way to access the data. In fact, the significance of the \"relational\" part of the name \"relational database\" is not captured by the \"matrix straightjacket\" description applied by Townsend et al. By \"relational\" we draw attention to the ability of the model to capture and then exploit connections between different objects that it describes. Figure II, for example, shows a small subset of the tables defined for our Prosopography of the Byzantine World project, and shows some of the kinds of data stored there for about persons, \"factoids\", (primary) sources, and (geographic) locations. PBW records statements made by different primary sources as \"factoids\" (see Bradley, Short 2003 for considerably more detail about factoids and the implication of the factoid approach on this and other similar projects), and each factoid is explicitly linked to the spot in the source from which it was derived. The lines drawn between boxes show the connections between the data types that the system knows about. For example (because of the intermediate table between Factoid and Person) it is possible to associate one or more persons with each recorded Factoid. The diagram shows that the database records which primary source contains each factoid, and that it is possible to associate more than one location with each factoid as well. These relationships allows the system to equally readily select or order factoids by the persons they are associated with, by the sources in which they appear, or the locations in which they occurred. Now, XML based models of representing this data can manage a similar flexibility of expression, but the primary way of representing associations between elements in XML (containment â the nesting of elements within each other) is not sufficient. One might group factoids by Source, for example, and order them in source order as well â having, say, a set of <SOURCE> elements, and inside each source elements the <FACTOID> elements that belong to the containing source. However, having used containment to express the relationship of factoids to source, obviously one cannot simultaneously use containment to express relationships between these same factoids and persons or factoids and location. For material like this, in fact, the tree-model that element containment implies, is, by itself, insufficient to represent this data. There is no \"document node\" in this data and no natural ordering of the material that corresponds to a document order. SGML and XML allow one to use IDs and IDREFs to assert links between different element instances, and can be pressed into service to represent the associations we show naturally in the relational database model. One could have, for example, a list of XML elements containing Person information, a separate list for factoid information, a Source list, and a Location list. IDs and IDREFs would express the links between the particular persons, factoids, locations and sources. One would uniquely identify each factoid, location, person, source object by assigning each occurrence an ID in the corresponding <FACTOID>, <PERSON>, <LOCATION> element. Then, the link between a person and factoid could be provided by providing a list of IDREFs in each person element that identifies the set of factoids associated with him/her. Similar techniques would be used to link the other data together. Using ID/IDREF and containment in this way is, indeed, a fully adequate representation of both the basic data and the relationships between them in XML. Similar approaches linking highly structured material with narrative text is indeed demonstrated in one of the Feature Structure examples shown in TEI P3 and P4. The downside to making use of ID/IDREFs to indicate linkage is that, although XML supports the explicit use of linking in this way, the use of ID/IDREFs is definitely a \"second class\" association technique compared to element containment. Furthermore, it is much less central to the design process of an XML DTD than it is in the relational model. This is shown in a number of ways. The DTD language allows IDs and IDREFs to be specified but provides no way to further limit the kind of links they might be asked to represent. XML editing tools, although they may be able to enforce the uniqueness requirement for XML ID attributes are very poor in making use of this linking information in other ways. The XPATH query language does support element selection using ID/IDREF links, but does it in ways that are significantly more awkward than selection based on containment alone. Because element containment is the first-class way to represent relationships between elements there has been recent work to explore of the limits of the tree-oriented structure for non tree-like data (Shanthi and Venkatesan 2003). Others have worked to extend tree-oriented XPATH to handle the more general situation of navigating graph-like data (see Cassidy 2003). There is evidence in the design of XML Query that the importance of efficiently supporting links between different hierarchies is being recognised -- not surprising since XML Query's design team contained several people with a relational database background. Very recent work in the development of XML-databases â perhaps inspired by XML Query â shows that there too developers are beginning to understand that it is important to ensure that links between different element hierarchies â expressed using IDREFs or in other ways â need to be recognised as \"first-class\" information in the system in the same way that element containment already is. Hopefully, in time XML databases will be designed in such a way that processing involving links between elements will work as efficiently with potentially large amounts of data as that expressed by containment. The XML development world, then, seems to be on the way to recognising that the modelling of material in XML needs more than the OHCO model and containment. We believe, however, that people working on text-based projects would benefit from designing XML-based projects with more awareness of this issue as well, even if, in the end, a relational database is not needed. We has seen examples of situations where, by the designing data representation by DTD alone, the project principals missed important relationships between materials that would have been revealed if some 'entity-oriented' design of the kind undertaken in the building of databases had also been done. In our presentation we will expand on the issues mentioned above, and discuss some design strategies we have developed to handle the design of mixed document/data projects.",
        "article_title": "Documents and data: modelling materials for Humanities research in XML and relational databases",
        "authors": [
            {
                "given": "John",
                "family": "Bradley",
                "affiliation": [
                    "King's College London"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Our proposal introduces a methodological approach for an enhanced link between architectural objects at various scales and what documents them: uncertain raw documentary sources. It provides an operational framework for the capitalisation of the interpretation phase during which experts establish meanings and credibility of raw data, and derive from the reading of sources possible scenarios of morphological evolutions. The issue we address is an interdisciplinary one: it stands where scientific analysis of raw documentary sources meets the needs for representation and visualisation. 2D or 3D representations have historically, since the renaissance, been at the heart of the way edifices or sites are described, visualised, documented and understood, and in close connection with textual documentation analysis efforts. It seems that with the development of computer techniques architectural modelling has mainly focused on issues connected with realism, in the so-called computer graphics discipline. In parallel, scientific analysis of raw documentary sources has benefited from the development of various data management techniques, but often without even a concern for graphical visualisation. Our field of experimentation is the preservation of the architectural and urban heritage. This includes a concern for the edifice itself when it is still standing, but it also includes a concern for the edifice's documentation helping to try and state for instance how the edifice evolved through time or how the edifice was when nothing is left of it today. In this research area, the meaning of the word visualisation is often narrowed to this of virtual reconstruction. But an undocumented virtual reconstruction can hardly be considered as something more than as a dead-end realistic 3D representation (see [Kantner 2000]). Communication through realistic renderings is all cases an abusive simplification since the morphology is not the only element that should be visualised, (see for instance [Huber 2000] or [Kodym 1999]). We favour an opposite approach in which what is \"beyond\" the image is more important that the image itself, in line with contribution like [Alkhoven 1993]. What we try to visualise are not the ocular effect of elements in the real world, but a momentary state of knowledge on the edifice and its evolution. In our experiments, we give to the word visualisation another meaning: this of an interpretative graphical interface to the documentation (see [Dudek and Blaise, 2001] and [Dudek and Blaise, 2003]). In the new experiment we report here, carried out on the remains of the Roman Theatre in Arles (southern France), we addressed the issue of how to better exploit results of a survey (based on image modelling techniques) in terms of readability for architects. Our theoretical knowledge about such edifices is formalised (see its theoretical representation in figure 1) and acts as support for the various stages of investigation on the edifice, from 3D surveying to representation in a real-time 3D model for the web. The SIA3D Experiment Architectural surveying has benefited from numerous technological advances in a recent past (see [Marbs 2002] [Dekeyser et al. 2003] [Chen et al. 2001]), both in terms of data acquisition disposals (laser scanning, image-based modelling, etc..) and in terms of performance. But how can one really exploit the results of such surveys remains a key research question, as demonstrated by Ramondino [2001]. Architectural surveying and architectural documentation seem to relate to tally independent research areas whereas they by nature have one thing in common: they circle the set of information and knowledge that can be related to an edifice. As an answer, and in line with principles established in [Dudek and Blaise 2003], we consider it is necessary to identify and organise non-ambiguous elements of morphology to which we will attach various pieces of information, including raw results of surveying campaigns. Those results can be contextualised, i.e.: localised in the space of a reconstruction featuring both elements surveyed and theoretical elements, positioned with regards to the area of knowledge considered (although techniques may be the same, surveying an edifice is not equivalent to surveying a tea pot, it does not correspond to the same area of knowledge) Let us take an example: the remain of a cornice once surveyed will be displayed in a 3D model of the whole edifice with indications concerning: Its belonging to a typology of objects (stylistic references, role in the edifice, etc..) Its hypothetical position (marked as such) in the virtual reconstruction of the edifice, itself marked as hypothetical. Its links to a variety of documents Remains that were surveyed are localised in the space of a theoretical Roman Theatre (see figure 2) that we define through an analysis of the relevant architectural vocabulary. This analysis aims at identifying non-ambiguous concepts that on one hand correspond to physical objects present in the edifice (base, capital, etc..) and on the other hand have a significant role in the edifice's composition. Each such element of vocabulary can be represented by one or several architectural primitives (see figure 3). The vocabulary is used to rebuild, level after level, the theoretical model of a Roman Theatre in the Augustinian period. One should here read level after level in the sense of rebuilding starting by individual elements such as a capital to whole groups such as colonnade, i.e an understanding of the word level that matches the idea of architectural scales. The edifice is described by a hierarchical structure that derives form our analysis of the vocabulary, rooted in [FormigÃ©, 1914] and [PÃ©rouse De Montclos, 1988]. The hierarchical structure formalises part-of relations between elements in a five level hierarchy (example: capital/part-of/column/part-of/colonnade/part-of/ etc.) Once the elements are classified as shown in figure 4, and once the compositional rules are expressed, the understanding of the edifice is within reach. The hierarchical structure allows us to establish bilateral relations between pieces of information and the 3D model. To each of the five hierarchical levels correspond specific pieces of information . Each element in the hierarchy acts as a filter in the exploitation of the 3D model since it is represented by its own geometry or by sub-elements. Fragments can then be attached to elements of level 5. The hierarchical structure monitors the switching between levels, either inside the 3D scenes where geometry are grouped depending on the level observed, or inside the information sheets. These information sheets gather pieces of data on theoretical elements, but also descriptions of the remains that were surveyed (actual localisation, state of conservation, materials, etc.. ). The interface (see figure 5) is accessible on a standard web browser with the Virtools plug-in for reading the 3D scene. Scene/windows interactions are written in JavaScript, enabling easy updating of the various links implemented. Information sheets as we call them range from results of survey for remains to purely bibliographical information. What this experiment has clearly shown is that even though we have credible pieces of information (results of a survey) we still need, when the time has come to exploit these pieces of information, graphics that interpret them. In other words, this experiment further underlines the necessity in the field of the architectural heritage to deliver graphics that demonstrate doubts.",
        "article_title": "Exploiting 3D graphics for non-ambiguous anchoring of documentary sources",
        "authors": [
            {
                "given": "Iwona",
                "family": "Dudek",
                "affiliation": [
                    "UMR CNRS/MCC 694 MAP"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Buzzetti (2002) believes strongly-embedded SGML/XML markup produces inadequate digital representations of texts. He considers the text model such markup assumes--the \"OHCO\" model[1]--confused about its object and incapable of appropriately representing textual content. Renear et al ( 2003) respond to one part of Buzzetti's critique by asserting the value of OHCO-type markup as a data model in its own right. This paper, in contrast, examines the case Buzzetti makes for a \"deep\" data model that reflects the *form of the content* and from which must flow any truly adequate digital representations of a text. In his argument I find occlusions, silences, and ambiguities that, I conclude, fatally undermine his project. II. Buzzetti specifies two criteria for an adequate digital representation of a text. These are exhaustivity--\"[it] should in no way impoverish the informative content of the text\" (61)--and exploitability, that is, its \"liability ... to automatic processing and its functionality with respect to the critical operations of reconstructing or interpreting the text.\" (62) He argues that descriptive markup of the kind exemplified by the Text Encoding Initiative's scheme does not meet the two criteria because despite the foundational claim of OHCO-type markup to be addressing essential content instead of superficial form (DeRose et al 1990), it still confuses the form of the text's expression with the form of the text's content. In mistaking surface for depth and reproducing the form of expression, the underlying text model cripples itself with respect to exploitability. In effect Buzzetti gives a radical turn of the screw to one of SGML's fundamental justifications: the notion of creating an Ur-representation of content from which many different forms can be generated. We should see the entire text as a generated expression whose form we must not take to be the form of the thing it was generated from; the model that adequately represents the one cannot adequately represent the other. The originary 'thing' is a structure of informative content: abstract, non-linear, possessing its own properties and behaviours which an adequate data model (and thus an adequate text model) should represent through an appropriate formalism. III. In humanities computing at least, OHCO-type markup has come to dominate digital representations of full texts for scholarly use, which means that the majority of the products of more than a decade of encoding effort don't meet Buzzetti's criteria. The easy pragmatic observation would be that the very prevalence of OHCO-type markup suggests many people have found it quite adequate for their purposes to date,[2] but for a scholarly response this obviously will not do: the case should be properly judged on its own merits. We might assert that Buzzetti's critique is bound to have no lasting impact because a number of the arguments have been made before--albeit not within such a sophisticated theoretical framework--and if OHCO-type markup survived them then it will survive them again. Survival, though, is not the same as triumph. Responding to Buzzetti, Renear et al acknowledge that OHCO proponents have in the past let these issues lie unresolved and that he is right to foreground them again. They also acknowledge some history of terminological/conceptual confusion but argue--I believe correctly--that Buzzetti overstates the case and that his examples \"are far from convincing evidence for systematic conflation [of expression form with content form].\" However, because their main concern is with defending against a particular charge that Buzzetti takes up from Raymond, Tompa, and Wood (1992, 1996) regarding SGML's supposed lack of a \"standard semantics,\" they do not address the main thrust of Buzzetti's argument. IV. The text encoding community has well rehearsed the strengths and limitations of OHCO-type markup.[3] The real interest of Buzzeti's critique lies in his attempt to conceptualise what *should* underpin digital representations. It is clear that Buzzetti has in mind not a digital representation of \"a text,\"[4] but a comprehensive digital edition, an agglomeration of data that is the raw material from which any desired view can be generated and upon which any scholarly analysis can be based. He cites as congenial to his argument ideas on treating textual materials from Theodore Nelson and from Manfred Thaller, and much of what he says implies a classically data-centric approach. The database has always served not just as a store, but also as a base for algorithmically (re)constructing views of the data. Equally clear, though, is that Buzzettis notion of generation has been strongly influenced by the two linguistic models he co-opts: Chomsky's Transformational Grammar and Hjelmslev's Glossematics. Codds relational data model in itself implies absolutely no stratigraphical or temporal relation between any of its components. Such relations can be modeled within a relational database, but only as a function of the users choice of domains and attributes; so such relations are created by the model, and are not inherent to it. Buzzettis adequate digital representation, however, clearly involves working backwards or downwards from the manifest data. It looks not to simply collect and label what is, but to treat what is as a result or surface effect and to store instead what what is came from. With this comes a notion of reduction, of constraint: the multiplicity of what is comes from different operations on a limited set of essential data objects. Hence the appeal for Buzzetti of linguistic models that reverse engineer data as it appears to us. Johansen (1993), for example, speaks of Hjelmslevs desire to arrive at a calculus of language, a set of axioms and functions from which all possible acceptable sentences can be derived; similarly in a Transformational Grammar approach a set of transformation rules and a set of basic sentence structures can generate all the legal syntactic variations. However, because these models are specific to the abstractions of language Buzzetti cannot bring them in as actual candidates for the data model he thinks is necessary; instead they serve as analogies. The problem is that while each models general character conveys nearly enough what Buzzetti wants, its specifics do not. For example, Buzzetti's argument depends heavily upon Hjelmslev's four-layered model of semiosis which \"allows us,\" he says \"to clearly distinguish between the form of the representation and the form of the content represented.\" (64) However, to leave it at that (that is, at a distinction between representation and content) is really to say no more than the association between signifier and signified, though binding, is arbitrary, which is to say no more than Saussure's model does. The whole point of what makes Hjelmslev's model different from Saussure's is the further distinction between form and substance, on both content and expression planes, one that involves an extremely subtle interrelationship between the two layers on each plane. Yet this distinction--so visible in the diagrammatic representation of Hjelmslev's model that Buzzetti includes--disappears from the discussion itself along with any complications it introduces to the simple mapping Buzzetti wants to establish between the form of the expression and traditional markup.[5] V. Such moments in Buzzetti's text are symptomatic of an illusory quest. His critique assumes an ultimate textual essence that we can capture in a supermodel that preserves all informative content and serves all scholarly needs. We just have to push further in from the surface form than OHCO to find it. On close reading of Buzzetti's argument, however, the rhetorical paths which promise to lead there instead peter out, or bring us back to the 'surface.' Buzzetti quite legitimately desires to move beyond the limitations of OHCO to digital texts unconstrained by a single representational form, towards a comprehensive representation. Seduced by two powerful linguistic models that seem to promise a path from effect to cause, surface to depth, incidental to essential, he assumes a comprehensive data model: a grand unified abstraction of a text that accounts for all textual phenomena and enables all textual analysis. The subtlety and scope of the case he puts forward make his, as Renear et al have already acknowledged, a landmark contribution to markup theory, but one which I find, in the end, unconvincing.",
        "article_title": "Form, Content, and the Philosopher's Stone",
        "authors": [
            {
                "given": "Paul",
                "family": "Caton",
                "affiliation": [
                    "Brown University"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Increased IT literacy amongst students in combination with cuts in library funding means higher demand for texts in electronic form. Or does it? Is the use of electronic texts really an option to more traditional material? What are the potential benefits and drawbacks? What issues face someone wanting to use electronic texts for teaching or learning? These are all well-known questions that have been discussed before. We suggest that they are questions that merit further discussion in the light of the increased availability of free e-books. We will touch on those in our presentation, but focus on less frequently discussed questions relating to quality and availability of free e-books, attitudes towards free e-books, and issues relating to technical aspects of free e-books: formats, tools, and scope for repurposing. This paper builds on the results of an investigation into free e-books that the Oxford Text Archive performed in 2003. The study was commissioned by the Joint Information Systems Committee (JISC), a body which provides strategic guidance, advice and opportunities to use Information and Communications Technology (ICT) to support teaching, learning, research and administration in higher and further education in the UK. The main aim of the study was to produce a comprehensive report for the JISC E-Book Working Group. A number of related studies were performed simultaneously addressing other aspects of e-books. The Free E-books study focussed on the following key issues: Availability: The study concentrated on e-books which are available at no cost with the minimum of intellectual property rights constraints. User needs: The study investigated the possible users, uses, and usability of free e-books, paying particular attention to customisation. Repurposing: The study explored the extent to which existing freely available e-books can be repurposed, converted to other delivery formats, and assimilated into other activities or collections. Within these categories, the following activities were undertaken: Availability The study first clarified the notion of \"free e-books\" and focussed on those resources which meet this definition and that are available with the minimum of IPR constraints. An initial survey of sources of free e-books was undertaken and provides an indication of subject coverage within the Arts and Humanities, the variety of formats available (e.g. (X)HTML, XML, Microsoft Reader etc.), and any factors which serve as indicators of quality and provenance. User needs The study concentrated on the needs and concerns of students, teachers, and research staff working in the Arts and Humanities within the UK's HE/post-16 sector. Through a combination of survey instruments, online publications, discussion lists, and workshop events, we identified current and potential users of free e-books, and defined their requirements (for example with regard to possible customisations such as book-marking, marginalia, highlighting, building personal collections, and citation). Repurposing Throughout the lifetime of the study as well as after the conclusion of it, we have conducted research to establish the issues involved when existing freely available e-books are repurposed, converted to other delivery formats, or assimilated into other activities or collections. We have investigated the actual and potential procedures used to both create and repurpose free e-books, in an attempt to define a set of recommendations for either the creators or users of free e-books (e.g. simple advice to creators which will help ensure that users can make the most of their e-books, and advice to users which may help them decide if a particular type of free e-book is likely to meet their needs). The following outputs and deliverables have been produced: A study-specific website containing: Information about the study and guidance on how to participate. Regular progress reports on the study, accompanied by the project plan and a clear statement of aims and deliverables. Publicly available documents and instruments developed and used during the course of the study. Links to sources, resources, tools, and software identified or discussed as part of the study. An initial survey of sources of free e-books (to include an indication of subject coverage within the Arts and Humanities, the variety of formats available, and any indicators of quality and provenance) mounted on the website. An investigation into the actual and potential procedures used to both create and repurpose free e-books, the findings of which will be summarised on the website and form a significant component of the final report. The production of appropriate questionnaires and survey instruments to gather information about the requirements and expertise of the UK HE/post-16 community with regard to their use of free e-books. As appropriate, summaries of the findings of such questionnaires and surveys mounted on the website A one-day workshop to promote the findings of the study and inform the community about the possibilities and limitations of free e-books. A draft final report on the study for consideration by the Steering Committee (and E-Book Working Group if necessary). The production and promotion of the final report of the study. The final study report represents the findings and outputs of a variety of activities. The work is of two distinct but necessarily related types: desk-based research into free e-books themselves, and survey instruments combined with activity-led workshops etc. designed to gather user feedback and establish user requirements. The survey instruments are a questionnaire to key proponents of e-learning in post-16 institutions, a dissemination workshop highlighting available resources, focus groups exploring potential uses and barriers to use of free e-books and case studies of existing good practice.",
        "article_title": "Free E-books - a real alternative?",
        "authors": [
            {
                "given": "Ylva",
                "family": "Berglund",
                "affiliation": [
                    "University of Oxford"
                ]
            },
            {
                "given": "Martin",
                "family": "Wynne",
                "affiliation": [
                    "University of Oxford"
                ]
            },
            {
                "given": "Rowan",
                "family": "Wilson",
                "affiliation": [
                    "University of Oxford"
                ]
            },
            {
                "given": "Alan",
                "family": "Morrison",
                "affiliation": [
                    "University of Oxford"
                ]
            },
            {
                "given": "Greg",
                "family": "Simpson",
                "affiliation": [
                    "University of Oxford"
                ]
            },
            {
                "given": "James",
                "family": "Cummings",
                "affiliation": [
                    "University of Oxford"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The main consequence of contacts between languages is borrowing, ie. \"the incorporation of foreign features into a group's native language by speakers of that language\" (Thomason and Kaufman 1988: 37). Borrowing begins, and is at its strongest, in the lexicon; with prolonged intensive contacts, morphosyntactic and phonological features will also begin to be borrowed. Language contacts are typically found in border regions and multilingual, multicultural communities. However, the present dominance of English as a world language, combined with the spreading of Anglo-Saxon popular culture into the daily lives of many peoples across the world, has brought English in particular into contact with most European languages. This paper examines some of the consequences of such contacts. Among the manifestations of the ever-present, powerful influence of English on other languages are the names of businesses, especially, it seems, of those catering for younger generations or dealing in freetime activities. Both of these criteria are met by restaurant businesses. In the study at hand, we investigated the extent and geographical distribution of the influence of English and other foreign languages on restaurant names in Finland. Material and methods Our data was collected in a truly Labovian fashion: from the white and yellow pages of all of Finland's 1999 telephone catalogues, we gathered the names of all restaurants (including bars, cafÃ©s, grills and pubs), comprising a total of 4041 businesses. A database was created comprising information on source language (English, Finnish, Swedish, Russian, French, Italian, Spanish, or other), type of loan (unadapted loanword, adapted loanword, loan shift, loan translation, or hybrid; the classification is essentially that of Lehiste 1988), type of word (place name, person name, noun [including simple, modified and compound nouns], or phrase), type of establishment (restaurant, bar, pub, cafÃ©, or grill), region (represented by area codes), name of community, and type of community (ie. whether it was a largish city or part of an outlying area). In the determining of the community there was a slight problem: while the white pages indicated the community of each establishment, the yellow pages typically indicated the region (=area code) only. To assign the latter establishments, too, to a particular community, the following procedure was used: the percentage of each community of all the restaurants in the region was computed from the white page listings, after which the unspecified yellow page listings were allocated to that region's communities in the same proportions (a standard statistical procedure). For example, if a region had 10 English-named restaurants in one community and 5 in another, plus 6 that were not specified for community, we assigned four of the six to the first community and two to the other. While this procedure led to fractional figures for some communities, we preferred that to losing the information provided by the yellow-page listings. In the processing of our data, we used a variety of methods for dealing with categorical data, such as chi-squared tests for differences in multinomial populations and tests for 3 and 4-dimensional contingency tables. In addition, to look at the data more closely, we digitised a map of Finland marking the boundaries of some 450 cities and municipalities. This map was then used to portray the acquired information in a variety of ways. Results The results show that approximately half (52%) of the restaurants in Finland have Finnish names and approximately one quarter (21%) have English names. Names derived from other languages thus make up the last quarter (27%). There is also a significant difference in the distribution of English names across the area codes. If the country is divided into four areas, ie north, south, east and west, using area codes, the proportions of English names are fairly evenly distributed across the country. However, if we look at the data by community, a digitised map produces some very interesting results. There are five main areas where the proportion of restaurants with English names exceeds 30%. Two of these show quite clearly the importance of tourism in the respective area, ie. Lapland and the lake district. In two areas the large porportion of English names might be due to the fact that in a bilingual country these areas are almost monolingual and lie close to areas which are almost monolingual with respect to the other national language (i.e. Finnish or Swedish, as the case may be). Perhaps therefore it is better for your restaurant to have a foreign name than one in the other native language. The last area where English names have a proportion greater than 30% is an area dominated by a religious community that does not approve of alcohol. If you are setting up a restaurant in this area it seems to be better to give it an English name than a Finnish one, which is more likely to mark it as an establishment selling liquor. The map also reveals the interesting fact that there are two main areas that have no restaurants with English names, ir the easternmost communities and central Finland. The reasons for this are not quite so clear. The eastern communities are sparsely populated and have been influenced by Russian culture. Central Finland seems to have a disproportionate amount of restaurants with names derived from other foreign languages than English and may also be an area which has taken in a number of refugees, who speak diverse languages. The difference in the distribution of loan types is also significant. Most (70%) English restaurant names are unadapted loans, e.g. Buggy Burger. These may or may not have an adapted pronounciation in the speech of the locals, which it is impossible to determine, however. The distribution of word type is also significant and is dependent on the type of restaurant. Establishments with place names are likely to be restaurants, whereas pubs are somewhat more likely to be named after people than other establishments are, e.g. O'Malley's. It seems then that the influence of English is greater than that of any other single language on Finnish restaurants and is at its greatest in the names of pubs and bars, which tend to be frequented by the younger gerations. The English establishment names are mainly nouns in an unadapted form. If your establishment is called Hudson Bay, Hollivoot or Nelson, you are associating it with the place or the person. If, however, you have called it The Watering Hole or Jet Set Bar, the influence is linguistic rather than cultural. It is another matter whether the restaurants that have foreign names actually are referred to by these names, or whether they have a finnicised name that is used by their patrons, e.g. Restaurant Sea Horse, which is called Sikala (the Pig Sty) by its patrons, and Old Dog Pub, which is called Koira (the Dog) by its patrons. A questionnaire is being developed to help answer this question.",
        "article_title": "Hudson Bay and Hollivoot: The influence of English and other foreign languages on Finnish restaurant names",
        "authors": [
            {
                "given": "Lisa Lena",
                "family": "Opas-HÃ¤nninen",
                "affiliation": [
                    "University of Oulu"
                ]
            },
            {
                "given": "Pekka",
                "family": "Hirvonen",
                "affiliation": [
                    "University of Joensuu"
                ]
            },
            {
                "given": "Fiona",
                "family": "Tweedie",
                "affiliation": [
                    "University of Edinburgh"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The development of information technology over the last couple of decades has carried with it a shift from largely textual representation to increasingly detailed graphical and multimodal representations. The narratives of graphics and gaming industries often reflect a wish to recreate reality or realistic alternative realities. This wish is also very obvious in many high-end humanities projects in for instance history and archeology. In this paper a range of virtual environments will be analyzed along a continuum ranging from iconic to symbolic representation. It will be argued that in many cases a lower level of realism is functionally and esthetically advantageous. Evidence will be drawn from visual art research, medical use of information technology, reception studies, cartoons, the simulation industry, and from research on video conferencing. A theoretical discussion of realism, iconicity and symbolism will provide a foundation for analyzing two case studies. The first case study is a contrastive analysis of two computer games; one old-type text adventure game and one modern \"realistic\" game. The second case study is concerned with an educational project where third-term students of English construct a graphical virtual world instead of writing a traditional textual degree paper. Finally, a more general discussion of the role of non-textual representation in the humanities will be undertaken. The theoretical discussion will draw on research in several relevant fields: a) cognitive science and cognitive linguistics (Deacon 1997 and Lakoff and Johnson 1999), b) reception studies and literacy research (Langer 1995, Kress 2003), c) philosophy and cultural studies (Baudrillard 1994, Eco 1996, Peirce 1958), and d) new media studies (Laurel 1991, McCloud 1994, Montford 2003, Schmitz 2001). From this multidisciplinary platform and from practical experience (e.g. MacEachren et. al 1999) a model for describing virtual environments in terms of iconic-symbolic will be suggested. An important basis is Peirce's discussion of iconic, symbolic and indexical signs. The model will partly be presented in the form of a continuum ranging from iconic to symbolic representation. The continuum will be populated with a number of virtual environments (text adventure games, MUDs, graphical virtual worlds, modern computer games, video conferencing etc.). These will be briefly discussed and related to concepts such as interpretation, presence and level of detail. The model will be further tested in two case studies. In case study number 1, we will look at two computer games from different eras: Spellbreaker (a text adventure game from 1980s) and The Black Mirror (a graphic adventure game from 2003). These will be analyzed in terms of symbolism-iconicity, interpretative freedom, human-computer interaction and presence. In the second case study, we will consider an educational project where students populate a graphical environment and create their own representations of themes to do with linguistics, literature and cultural studies. Examples of themes used include the city, monstrosity and recreating reality. The environment used (ActiveWorlds) allow for a graphical representation which is not photo-realistic but very useful for creative work where recreating reality is not the ultimate goal. This ties in with Brenda Laurel's idea about the importance of ambiguity. She says that \"ambiguity and sensory incompleteness are key elements in the kind of deep participation we desire with a work of art.\" In conclusion, a recent prototype project on the use of computer-supported visualization in the humanities will be briefly described. Here the literature and some relevant projects have been reviewed. In a study of more than 100 recent humanities Ph.D. theses, the use of visualization has been investigated and classified. It will be argued that it is important that the humanities bring in more non-textual representation into their work and that the symbolic-iconic continuum might be a good starting point for such a development, or at least for an engaging discussion.",
        "article_title": "Iconic and symbolic representation in virtual worlds",
        "authors": [
            {
                "given": "Patrik",
                "family": "Svensson",
                "affiliation": [
                    "HUMlab, UmeÃ¥ University"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Intertextuality in its broader sense considers all culture and cultural rendering as a form of interacting text, where text must be considered as a mental construct or representation rather than a physical appearance, e.g. the characters printed on a certain medium [Graham, 2000]. Although intellectually appealing, such a definition of intertextuality does not have much practical use in the literary study of medieval Dutch romance, one reason being that contemporary cognitive conceptions of intertextuality can of course no longer be \"measured\" and are very hard, if at all possible, to infer or assess. But a rather more limited definition of intertextuality as concrete references from one text to another may have practical use as a means to further the examination and understanding of interaction between various forms of literature and literary works in the medieval context. For certain Middle Dutch Arthurian romances (in particular Roman van Walewein, Moriaen and Ridder metter mouwen) such an approach has been used to qualitatively assess aspects of intertextuality [Besamusca, 1993]. It would advance such \"traditional\" research considerably if it were possible to identify with computational means text episodes within two or more Middle Dutch texts that show strong semantic or conceptual similarities. Provided that computer assisted methods for pinpointing possible intertextual references would be sufficiently reliable, such technical instrumentation would greatly reduce the resources and time needed to assess large corpora for intertextual references. Moreover such a solution would benefit from explicit inference rules and semantics, enabling philological researchers to approach the assessment of intertextuality in a more quantitative modelled way.  It is the purpose of the proposed paper to describe an as proof of concept implemented solution for the identification of text places in medieval Dutch Arthurian romance that show high likelihood of containing intertextual references to other medieval Dutch texts. Regarding software applications for the computer assisted analysis of intertextual references in Middle Dutch texts, little research and implementation has been done. Furthermore the research conducted up till now is for the most part theoretical in nature. Greco and Shoemaker argue in favour of an a priori rather sophisticated model of intertextuality [Greco, 1993]. In the proposed paper it will be argued that such a model, though theoretically sound, is very hard to implement given the current state of computer assisted text analysis techniques. Consequently, a more elementary model for intertextuality will be applied within the software solution, as to provide a bottom up approach to the problem of modelling intertextuality within the domain of medieval Dutch text. For this purpose, intertextuality will be regarded in its limited form of intertextual reference on a more literal level.  The software solution that will be demonstrated in the paper is based on identifying episodes in different medieval Dutch romances that show considerable semantic similarity, assuming that semantic similarity indicates high probability of referential intertextuality. It will be argued that a vector space modelling approach [Salton, 1975; Manning, 2000] is best suited for this task, because the textual data in question may be considered extremely sparse data from a statistical point of view. Therefore a probabilistic approach would be far less suitable. Before any vector space analysis may be applied though, two other problems arising from the specific nature of the text material will have to be considered and solved. Firstly the orthography of analogous concepts within and among different Middle Dutch texts tends to vary considerably. Because a vector space approach relies on isomorphic input for analogous concepts, this variance has to be adjusted for. Several solutions to this means have been proposed [Braun, 2002]. Both an n-gram and stemming solution and a solution based on the longest common sequence (LCS) will be discussed as an approach to the automated temporarily rewriting of texts in a morphological congruous form suitable for further vector space analysis. Secondly, to enable the comparison of episodes from different texts, all texts to be analysed will have to be broken down into smaller fragments corresponding to these episodes. Such a segmentation must of course be based on well defined structural or narratological heuristics. It will be argued that it's feasible to assume that intertextual references will not expand beyond certain narratological or discourse boundaries. But although such boundaries may be readily identified by a medievalist researcher, the form and structure of medieval Dutch texts hold no or only very few computer interpretable indicators for automated identification of such \"natural\" narratological or discourse boundaries. Text structuring initials present in the manuscripts may provide one of the very few computer interpretable indicators of these boundaries. The usefulness of these initials as boundary indicators of narratological episodes will therefore be evaluated as part of the solution proposed.  The latter part of the proposed paper will report on the results from an application of the implemented solution to two medieval Dutch Arthurian romances, the Roman van Walewein and Walewein ende Keye. It's been argued that the latter text holds echoes of the story told in the former. The proposed paper will report on the fragments identified by the application as being possible episodes containing intertextual references. The computer assisted analysis of the two texts will be evaluated by a comparison with a manual assessment of intertextual references in Walewein ende Keye. This evaluation will form the basis of a short discussion that will propose further refinements of the methodology for identifying episodes containing intertextual references in Middle Dutch texts. One of the points that will be argued is the necessity to expand heuristics of the method with a form of conceptual modelling, as intertextual references among two texts will always be only in part similar on a more literal level.",
        "article_title": "Identifying intertext susceptible episodes in Middle Dutch Arthurian romance",
        "authors": [
            {
                "given": "Joris",
                "family": "van Zundert",
                "affiliation": [
                    "Dutch Institute for Scientific Information Services (NIWI - KNAW)"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The paper will describe an experiment to apply a suite of tools for Natural Language Processing (NLP) of Italian to literary texts. The work will show that state-of-the-art computational linguistics technology can be profitably used to support text linguistic analysis and to gain deeper insights into authors' language. The corpus chosen for the reported experiment was formed by some Calvino's works, selected on diachronic line to cover all his production (~ 500.000 words). The corpus was parsed through a \"pipeline\" of NLP tools performing a \"shallow\" syntactic analysis of input texts. The output analyses have been used to collect data about the distribution of a set of relevant features in Calvino's texts, in order to assess the impact of spoken language upon the narrative, by means of syntactic adjustment as mention of the subject, dislocations, hanging topic, ellipsis and repetition. Italian NLP is a suite of linguistic processing tools based on the paradigm of \"shallow NLP\". Traditional full-parsing techniques seek to associate to each sentence a fully specified recursive structure, in order to identify the proper syntagmatic composition, as well as the relations of functional dependency among the identified constituents. The drawback of full parsing is that it is an extremely costly task for most of existing systems since it needs huge amounts of linguistic knowledge to work properly. Conversely, the main philosophy of \"shallow processing\" is that NLP systems can resort to a shallower level of syntactic description, which, although underspecified under various respects, still provides enough syntactic information as the basis for higher-level analyses. After a short outline of shallow parsing, the paper will describe the main features of the processing tools forming the Italian NLP suite. The text processing chain starts out with the morphological analyser (MAGIC), which assigns to each word form in the tokenised input all its possible lemmas, together with the morpho-syntactic features describing them. On the morphologically analysed text a shallow syntactic analysis is then carried out, which includes chunking, a process of non-recursive text segmentation, and dependency analysis, aimed at identifying the full range of functional relations (e.g. subject, object, modifier, complement, etc.) within each sentence. Text chunking is carried out through a battery of finite state automata (CHUNK-IT, Federici et al., 1998), which takes as input a morphologically analysed and lemmatised text and segments it into an unstructured sequence of syntactically organized text units called chunks. Chunking requires a minimum of linguistic knowledge that is through recourse to an 'empty' syntactic lexicon, which contains no other information than the entry's lemma, part of speech and morpho-syntactic features. The resulting analyses are flat: all chunks are represented at the same structural level, as daughters of the same top node. At this stage, a chunked sentence does not give information about the nature and scope of inter-chunk dependencies. These dependencies are identified during the last phase of dependency analysis, carried out by IDEAL (Italian DEpendency AnaLyzer, Lenci et al. 2001). IDEAL includes two main components: (i.) a core grammar; (ii.) a syntactic lexicon of ~26,400 subcategorization frames for nouns, verbs and adjectives derived from the Italian LE-PAROLE syntactic lexicon (Ruimy et al. 1998). The IDEAL core grammar is formed by ~100 rules covering the major syntactic phenomena. The grammar rules are regular expressions (implemented as finite state automata) defined over chunk sequences, augmented with tests on chunk and lexical attributes. The rules are organized into two major modules: 1) structurally-based rules and 2) lexically-based rules. It is worth stressing the fact that the Italian NLP tools have been developed for purposes and domains other than literary text analysis, and mainly concerning human language technology and language engineering applications. A challenging aspect of the experiment we carried out was exactly the customisation of computational linguistics technology at the needs of literary texts investigation. The target linguistic analysis to be performed on the corpus concerned the identification of features of spoken language into the narrative, with special regard to syntactic adjustment and sentences with marked order of elements, in order to evaluate the impact of spoken Italian over the written production. In the spoken language, phenomena of markedness pertain the mise en relief of certain information in the utterance. The same mechanism can be applied in the written language whenever placing emphasis provokes the reiteration of an information already provided (this is the case of the subject, being Italian a pro-drop language) or a repetition (anaphoric or cataphoric use of unnecessary pronouns) along with a shift of the focused element to a marked position in the sentence (hanging topic). The nature of the investigation, i.e. the utilization of a computational tool for the specific domain of the narrative, lead us to plan and develop essential changes within the framework of IDEAL, not just to fulfil the particular objectives on Calvino but to serve the narrative domain as a whole. In order to gain more precise results, some existing rules were implemented and new ones compiled. Adaptations and new rules moved from the subcategorization of various VPs and endeavour to achieve a better definition of their arguments. The rules were designed and arranged follow the degree of embedment of the clauses, moving from the most embedded one to the main clause. The paper will report the first experiments to assess the contribution of the new rules to derive more accurate syntactic analyses. Preliminary evaluations are encouraging and interesting patterns of evolution in Calvino's language and style are emerging out of the corpus processed with Italian NLP.",
        "article_title": "Italian NLP: Computational Linguistics Meets Literary Text Analysis",
        "authors": [
            {
                "given": "Valentina",
                "family": "Di Giovanni",
                "affiliation": [
                    "Royal Holloway University of London, Dept of Italian"
                ]
            },
            {
                "given": "Roberto",
                "family": "Bartolini",
                "affiliation": [
                    "Istituto di Linguistica Computazionale CNR"
                ]
            },
            {
                "given": "Alessandro",
                "family": "Lenci",
                "affiliation": [
                    "UniversitÃ  di Pisa - Dipartimento di Linguistica"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "In this paper we will describe a pilot study based on two different subsets of electronic resources to be used in the Virtual Corpus system developed at the Oxford Text Archive (OTA). The Virtual Corpus system is designed to make the OTA more useful for researchers by enabling the selection of texts for a corpus on basis of metadata categories in the TEI header resource description. Currently these categories include such fields as language, date, genre, author etc. (Berglund & Wynne, 2003). In order to make the Virtual Corpus system (VC) even more useful the texts would benefit from data enhancement. This pilot study's aim is to evaluate the necessary procedures for enhancing the metadata available in the TEI header with further categories and also to explore the possibilities for migration of legacy data in a wide range of formats into a TEI-conformant XML format. Background Founded in 1976 by Lou Burnard, the OTA has over twenty years experience of serving the research and teaching needs of electronic text users within the scholarly community. We have witnessed and encouraged the widespread acceptance of digital resources within academia. Formerly the preserve of small research-oriented groups of specialists, electronic text is now the common currency of academics. More recently the OTA has become the hosting subject centre for the UK Arts and Humanities Data Service: Literature, Languages, Linguistics. The OTA currently holds several thousand electronic texts and linguistic corpora, in a variety of languages. Its holdings include electronic editions of works by individual authors, standard reference works such as the Bible and mono-/bilingual dictionaries, and a range of language corpora. The OTA does not produce digital resources, and instead relies upon deposits from the wider community as the primary source of high-quality materials. While the deposited resources of the OTA may be of a very good quality, they were often originally deposited in any number of highly individualistic markup schemes. In this case study we are evaluating the proposed process for conversion of legacy data and viability of enhancement. There are many issues at hand, for example how do we categorise formats? How much time and effort is involved when evaluating the formats and any markers or markup used in the texts? Two subsets of the OTA's holdings were chosen in order to give a smaller fixed amount of material to evaluate. One subset contains biographies and shorter works while the other has sixteenth century English drama. These subsets were determined by the examination of the Library of Congress subject headings, which the OTA adds to the TEI Headers for each of its resources. Rationale When evaluating legacy data one has to consider a number of issues, related to viability and usability of the texts. Besides, in any endeavour of metadata enhancement and format conversion one has to consider a range of issues, such as evaluating the amount of work involved or finding appropriate methods for a conversion from a number of formats and unconventional markup schemes into XML format. Our aim is to evaluate if individual texts should be considered for ehancement and migration to XML. Part of this decision relies on accurate information concerning their format, the quality of markup if any, the uniqueness or significance of the text, and what features have been encoded.[1] However, before considering any format conversion we also need to check whether the there is a better version of text is freely available elsewhere.[2] Whether a currently available text is able to be considered 'better' than the version originally deposited with the OTA is in itself problematic. The availability of a significantly encoded XML text does not mean that the quality of the textual edition itself is of higher quality. An increase in functionality should not be coupled with a downgrading of textual integrity or academic merit. The pilot study On the selected subsets of texts we performed a number of checkpoints in our evaluation and enhancement process: we checked the retrieval restrictions, the text format and for the existence of a text elsewhere that could be considered a better version. If a text not is freely available elsewhere in a better form, the viability enhancing the existing resource will be evaluated. If it is unfeasible to enhance the text, it will be flagged as having limited usability. When the text is in XML/SGML format and evaluated as suitable for enhancement, the new metadata is added, and it will be flagged as \"completed\" in the VC. However, if the text is in an unknown or un-encoded format, it has to be carefully analysed in terms of the quality of any embedded markers or markup. Thus, the evaluation will also consider workload and time spent on a number of aspects concerning analysing the texts. The document analysis focuses on whether the embedded markup/markers are sufficiently consistent and distinct for conversion into XML format. One of the tasks in the analysis is to identify a format for the text or to check whether the format is the same as stated in the TEI Header and examine any variance from that format.[3] Electronic resources in the OTA are prepared for computer analysis and/or retrieval in a number of ways. The level of encoding varies and while some texts are in well-formed XML or SGML formats, many are in plain text/ASCII with little or no markup embedded in the text. Some plain text resources are not encoded at all and some of the older texts are in upper case and others are sparsely marked up, through either conventional schemes or individual markup schemes. These may use the same markup to denote significantly different aspects of the texts, for example italic words, stage directions or numbered lines. A significant number of the earlier electronic texts are prepared in COCOA (named after an early general-purpose concordance programs from the 1960's). The Oxford Concordance Program (OCP) used COCOA as the referencing scheme and an expanded version of COCOA was also used by the text retrieval program called TACT (Oxford University Computing Service (1988); Bradley, 1996). The COCOA method uses angle brackets for enclosing references in texts. In addition, extra characters are sometimes used to mark certain features, such as italic words, proper names, diacritics, editorial marks, grammatical categories or foreign words (Oxford University Computing Service, 1988 pp. 11-21). However, the additional characters used for marking these aspects of the text frequently vary between different resource creators. Many of these texts are able to be converted through a variety of electronic means, but because of their lack of consistency, they would still need to be individually proofread. In the case of the numerous Shakespeare resources held by the OTA, it is unlikely that the majority of them will be deemed of significant academic merit over the many versions freely available elsewhere. Thus, little effort should be expended in the enhancing such texts, unless the OTA resource is a uniquely interesting version without any distribution restrictions. While the texts are being examined so closely, the addition of new metadata has been undertaken. The categories of metadata added are those of the gender of the author of the original text, the birth country of this author, and the original publishing/printing date of the text that the electronic edition is based on. Closing It is intended that this evaluation will lead to the conversion and enhancement of a significant proportion of the OTA's holdings. The benefits implicit in the conversion to XML include the ability for more sophisticated virtual corpus manipulation as well as more detailed search and retrieval options. The increase in the amount of metadata applied in a consistent and coherent manner within the TEI Header also will enable and increase the functionality for user manipulation with these texts. The paper will conclude with an examination of the possibilities and pitfalls for the future.",
        "article_title": "Legacy Data Migration: A pilot study on the methodological feasibility of conversion and enhancement of electronic resources.",
        "authors": [
            {
                "given": "James",
                "family": "Cummings",
                "affiliation": [
                    "Oxford Text Archive, University of Oxford"
                ]
            },
            {
                "given": "Monica",
                "family": "Langerth Zetterman",
                "affiliation": [
                    "Digital Literature, Uppsala University"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "In a paper presented at Extreme Markup Languages 2003, Allen Renear et al. suggest that XML documents enjoy--or suffer--a notably ambiguous status with respect to the Functional Requirements for Bibliographic Records (FRBR). That specification identifies a hierarchy of textual modalities, wherein the term \"work\" designates an authorial creation, \"expression\" designates a particular form of that creation instantiated in a particular language or medium, \"manifestation\" designates a particular physical embodiment of the expression (specifying type face, size, materials, etc.) and \"item\" designates a particular physical object with the properties specified in the expression (e.g. a particular copy of a book). Renear et al. argue that XML documents possess a \"double aspect\" within the FRBR hierarchy, by which they can be seen either as manifestations or expressions, depending on how their markup is understood. This doubleness or uncertainty might be taken to indicate that the XML document is interestingly at odds with the FRBR universe. This would seem plausible in light of the resolutely documentary nature of that universe; the FRBR specification does not mention electronic documents and seems very much geared towards organizing the world of conventional libraries and their conventional contents. Renear et al.'s analysis is self-avowedly preliminary, \"intended to convene rather than advance discussion\", and the first goal of the paper now being proposed is to probe that analysis in greater detail. For instance, while Renear et al. locate the XML document's dual aspect between the expression and the manifestation, there are grounds as well for considering XML documents as works. Discussions of FRBR assume that a play and a movie thereof are different works, even though their dialogue, actors, and even their sets may be identical, and in a similar way we can imagine that two XML documents which contain the same text but whose markup models that text in different genres might well be considered different works. And while Renear et al. consider the XML document not to be a FRBR item on the grounds that it is not \"a concrete physical object\", this line of argument seems like an oversimplification, implying that no electronic object can ever be an item, a claim we would like to probe more carefully. But although it is clearly difficult to assign an XML document unambiguously to a single FRBR category, in a larger sense the XML document is--or can be--very much at home in the FRBR universe, which draws its intellectual inheritance overwhelmingly from the editorial traditions of G. Thomas Tanselle and Anglo-American textual editing. If our understanding of the nature of XML markup derives from the influential OHCO model, which insists on the separability of form from content and identifies the latter as the preservable informational essence of a document, then the encoded XML document represents the textual essence plus the formal structuring needed to reembody the text in what for FRBR would be an infinitude of potential specific \"manifestations\". Its \"double aspect\" thus registers the conundrum of its being both a deliberately disembodied textual mode (its physical configurations having been deliberately discarded or held in abeyance) and a form that is capable of generating contingent embodiments as needed. But this doubleness does not run across the grain of the FRBR scheme--on the contrary, it reaffirms its essential distinctions. If we look at documentary and editorial modes which resist this kind of idealization, however, we find a different kind of problem. Within the rare book and manuscript communities, critics of FRBR have identified a number of areas of ambiguity affecting the classification of these specialized document types, particularly where the FRBR categories of expression and manifestation are concerned--in other words, where questions of the relationship between physical form and texual meaning are concerned. These concerns are also reflected in important segments of the digital text community. Researchers such as Johanna Drucker, Morris Eaves, Jerome McGann, and others have been investigating the special status of images and what Drucker calls \"configured information\". From this perspective, the familiar problem of markup is that it too readily discards physical configuration, as being either unnecessary, inapposite, or too difficult to capture. To use the terms of Renear et al.'s analysis, markup that editorially describes textual structure is apt to discard physical configuration once it has served its own purpose as markup (i.e. as the clues through which we intuit the structure of the source document), since it assumes that such configurations are unimportant in themselves. And markup that authorially, performatively \"instantiates\" textual structure is logically anterior to any physical incarnation. Markup that aims to preserve details of a source text (a category Renear et al. do not discuss, but whose existence they would surely acknowledge) takes theoretical responsibility for capturing physical configurations, but faces limitations and challenges which are usually taken to be fundamental and nearly insurmountable: the basic disparity between the formalism and granularity of markup and the unruly realities of the physical world. This disparity is the chief concern of the remainder of this paper, and will be explored in detail in the finished version. Can we envision a way of representing the physical object in markup that does both analytical and representational justice to the \"configured information\" of texts? And from an analytical standpoint, what are the thresholds at which it is useful to identify these configurations? Can we identify meaningful groupings of phenomena that reflect actual analytical categories, or does the physical realm resist such categorizations? Finally, might such an analysis provide a new viewpoint from which to undertake a reassessment of the OHCO model of markup?",
        "article_title": "Markup, Idealism, and the Physical Text",
        "authors": [
            {
                "given": "Julia",
                "family": "Flanders",
                "affiliation": [
                    "Brown University"
                ]
            },
            {
                "given": "Syd",
                "family": "Bauman",
                "affiliation": [
                    "Brown University"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Literary critical discussions of Daniel Defoe's The Fortunes and Misfortunes of the Famous Moll Flanders have long focused on the protagonist's, Moll's, narrative voice. Dorothy Van Ghent (1953) notes the \"relatively great frequency\" of Moll's \"use of words naming that kind of object which constitutes material wealth\" (49) and indicates that these words connote \"the counting, measuring, pricing, weighing, and evaluating of...things in terms of the wealth they represent and the social status they imply...' (50). More recent criticism has problematized the identity of Moll's voice or, at least, the voice that purports to be Moll's. These studies frequently call attention to the fictional editor of the novel, an editor who reports on the difficulty of putting Moll's words \"into a Dress fit to be seen\" and attempts to frame Moll's narrative as a story of repentance for a life of sin. For example, Thomas Grant Olsen argues that the editor's claim to have re-written Moll's prose \"obliterates the notion that the narrative is Moll's at all\" (468). Thus, Moll's narrative voice is alternately viewed as one that uses specific kinds of words and has a specific identity or one that cannot be discovered, possibly an absence rather than a presence or, in the view of some, a female voice preempted by that of a male.  The question posed by this paper is whether quantitative analysis can provide a useful way of way of thinking about this problem and supplement more traditional critical approaches. More generally, this is part of an on-going series of explorations and test-cases to determine whether the kind of stylometric techniques used in studies of authorship attribution can distinguish among different narrative voices of an individual writer and, thus, provide useful insights concerning literary-critical issues focused on narrative voice. (See Stewart, 2003.)  The study began by utilizing the Burrows' technique to compare the voice of the fictional editor of Moll Flanders to the voice found in the rest of the novel. The relative frequencies of the forty most common words were used as variables in a principal components analysis of the novel's preface and the rest of the novel divided into seven approximately equal parts. The assumption was that if the voice of the preface is statistically similar to that of the other parts of the novel there is at least some evidence to support recent theories that Moll's voice is not separable from that of the editor. In fact, however, the principal components analysis displayed on a scatter graph shows a wide discrepancy between the editor's voice and the voice in the remainder of the novel.  Of course, this discrepancy does not necessarily show the voices to belong to different characters. Baayen and others have convincingly argued that \"differences in genre override differences in education level and authorship\"; one would assume the same to be true in trying to differentiate the voices of different characters by an individual writer. The genre of the preface may simply differ from the genre of narration. Clearly, there needed to be a different method of determining whether Moll's voice is \"her own.\"  It seemed worthwhile then to compare the voice in Moll Flanders to the voices of narrators in Defoe's other novels as well as to narrative voices in several other eighteenth-century narratives. (The other novels are Susanna Rowson's Charlotte Temple, Sarah Fielding's The Governess, Clara Reeve's The Old English Baron, Henry Fielding's Jonathan Wild, Samuel Richardson's Pamela, Jonathan Swift's Gulliver's Travels, Charlotte Lennox's The Female Quixote, and Eliza Haywood's Miss Betsy Thoughtless.) Again, the Burrows' technique was used. Several different analyses were made: one with the whole of each of Defoe's novels, one with each of Defoe's novels, several of his essays, and the group of sample novels from the same time period, and one with each of Defoe's novels divided into sections. As well, a cluster analysis was done using each of those sets of texts. In every case, the results are remarkably similar to those shown in Figures 2 and 3.  All the analyses show a clear distinction between Defoe's five novels narrated by male characters (Robinson Crusoe, The Further Adventures of Robinson Crusoe, Captain Singleton, Col. Jacque, and A Journal of the Plague Year) and those narrated by female characters (Roxana and Moll Flanders). As well, the analysis using sections of each novel shows overlapping among the sections of Moll Flanders and Roxana but no overlapping between sections of those novels and sections of Defoe's other novels. At least in quantitative terms, Defoe's two female narrators seem to speak in nearly the same voice.  Obviously, such evidence does not indicate that Moll has a unique voice. However, insofar as her voice is statistically indistinguishable from the voice of Defoe's other female narrator, there seems no reason to believe her voice has been preempted by the voice of the editor. She speaks as Roxana speaks, and no one has suggested Roxana's voice to be other than her own. (At the conference, a much fuller analysis would be given of the differences in voice between Defoe's female and male narrators.)  In attempting to understand Moll's voice, another set of tests was used. Taking a cue from Dorothy VanGhent's comment that Moll's vocabulary is characterized by words of measuring and evaluating and from other critics who remark on the emphasis on morality and religion, I attempted a content analysis by inductively generating two lists. One list is comprised of the language of calculation of which VanGhent speaks and the other of what I call the language of religion and morality. The procedure was to create a single alphabetical list of all the words used in Defoe's novels as well as in the other eighteenth century novels and then, with no knowledge of the specific novels from which they came, produce a list of words that seemed to connote in the one case calculation and evaluation (words indicating measurement, finances, and ranking) and, in the other, religion and morality (words referring specifically to religion such as deity, pastor, lord, etc. and those referring more generally to morality such as chastity, innocence, guilt, repentance, etc.). A list of 200 lemmata resulted and would be shown at the conference. Clearly, such a procedure is open to subjective factors and results need to be used with extreme caution. As Burrows has stated, \"it has become customary, in recent years, to allow variables to 'declare themselves,' thus obviating, as far as possible, the danger of a predetermined outcome\" (268). The words used in these analyses are not self-declared.  However, the results do show that Moll Flanders uses these words of calculation at a rate significantly higher than that of other novels in the study. Among Defoe's narrators, Moll and Roxana have narrative voices in which such words occur most frequently. More importantly, these words occur significantly more frequently than they do in the language of the editor. On the other hand, the language of religion and morality is used much more frequently by the fictional editor than by Moll herself. (See Figure 4) The editor, in fact, uses words connoting religion and morality at a frequency about six times that of Moll.  The results of these analyses seem to have implications that bear on critical discussions of Moll Flanders, implications that would be discussed in detail at the conference. There seems to be no stylometric evidence that Moll's voice has been preempted by the voice of the male editor, particularly given the similarity of Moll's voice to that of Roxana. As well, whereas the editor's voice makes heavy use of the vocabulary of morality and religion and relatively little use of the vocabulary of calculation, Moll uses religious language sparingly but uses the language of calculation with great frequency. If the male editor does indeed attempt to preempt Moll's story and her voice, the irony of the novel may be found in his failure to do so.",
        "article_title": "Moll Flanders: Calculating Voice",
        "authors": [
            {
                "given": "Larry",
                "family": "Stewart",
                "affiliation": [
                    "The College of Wooster"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The Brown University Women Writers Project (WWP) has received two years of funding from the National Endowment for the Humanities to create a detailed Encoding Guide for projects wishing to encode early printed books (roughly 1400-1850) using the Text Encoding Initiative Guidelines. This Guide would generalize from the WWP's experience and provide detailed and explicit guidance on the elements and decisions involved in the encoding of early printed books, while taking into account such variables as project needs, funding, and available time. The audience for this effort is one that is not currently well served by the TEI's existing documentation. The TEI Guidelines themselves are a compendious resource which famously provide comparatively little specific guidance on applying the Guidelines to specific project needs; mature projects which have trained technical staff are able to grasp the principles of the TEI and develop project-specific methods, but individual humanities scholars who are unfamiliar with text encoding may find this process much more difficult. There does not currently exist comprehensive documentation written for non-technical humanities scholars on how to develop a TEI encoding practice, or even how to decide what kind of encoding would be appropriate for a given set of materials. The Encoding Guide is thus aimed at individual humanities scholars, directors of small projects, or managers who wish to get a more topical overview of this particular area of the TEI Guidelines. The need for such documentation is now becoming much greater, as new XML publication tools come within the reach of individuals and small projects, and as interest in digital research becomes more widespread. The Encoding Guide has the potential to offer important benefits to text encoding projects of all sizes. It may enable individual scholars and small projects to start work more quickly and with greater confidence, without having to grapple with all of the TEI Guidelines or replicate fundamental encoding wisdom. It may also provide projects of any size with an alternative to TEI Lite. Finally and most importantly, it could help ensure greater consistency between projects working on similar materials, and give them a shared basis for discussion and further refinement of their encoding practice. A number of important issues and questions have already emerged from this effort, and this paper will discuss these in detail. First of all, given that the Encoding Guide will be published electronically, what is the best form for online documentation to take? Even the TEI Guidelines, which describe digital publication in such detail, do not themselves exemplify effective online documentation: in fact, their digital presence is limited to sectioned HTML/XML and PDF files, with some hyperlinking. The Encoding Guide will need to serve not only as a developmental narrative for use during the learning process, but also as a reference work, and one challenge for its writers will be how to handle this dual rhetorical function effectively, economically, and gracefully. Second, to what extent can text encoding principles be separated from a discussion of specific tags and DTDs? One important premise of the Guide is that essential concepts of literary text encoding transcend the TEI and its specific methodsand further, that these concepts can and should be treated as a natural link between the disciplinary methods with which scholars are already familiar and the text encoding issues which they seek to grasp. One underlying problem the Guide will address, then, is the perception that text encoding is about \"technology\", an assumption which often serves to intimidate or misdirect scholars unfamiliar with the field. This problem is especially significant because humanities text encoding projects must involve humanities scholars in their methodological decisions, even if those scholars will not be directly involved in encoding texts. The Guide will therefore attempt to establish a useful high-level grasp of text encoding issues even among scholars who never touch a keyboard. Finally, how can the expertise of the many existing TEI projects best be brought together in a Guide of this sort? Although the Guide will draw specifically on the work and documentation of the Women Writers Project, it will also need to take account of significant work being done elsewhere: work that both duplicates and extends the area of the WWP's expertise. The Guide's authors have compiled a list of projects on which to draw for examples, documentation, and research to enrich the scope and authority of the Guide. The proposed paper will explore all of these issues in greater depth, and will report on the progress made and the challenges encountered during the first year of this two-year project.",
        "article_title": "More Light, Less Lite: A Text Encoding Guide for Literary Scholars",
        "authors": [
            {
                "given": "Julia",
                "family": "Flanders",
                "affiliation": [
                    "Brown University"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Stylometry, the automated examination of texts in search of characteristic patterns that reveal clues to the author (genre, et cetera) of the text, is a well-studied problem with a rich history. In general terms, the standard approaches used involve calculating statistics of linguistically salient events [such as function word distributions (Burrows, 1989), common lexical cooccurences (Hoover, 2002), or character subsequences (Juola, 2002)] in texts of known authorship. These statistics form the basis for similarity-based inferences about texts of unknown authorship; similar methods can be used to \"attribute\" other aspects of text such as genre, time of writing, sex/age/background of author, and so forth. However, not all human-created \"documents\" are, strictly speaking, texts. There is a wide variety of what can be called \"paralinguistic\" data which shares several properties with standard linguistic/textual data. One example (among many) of this sort of data would be music. Both on the printed page and on the CD, music can be clearly seen to be a sequence of (relatively) independent events with a given probability distribution. Furthermore, these events are not uniformly random, but structured in such a way that certain patterns repeat. Even the terms used by musicians, such as \"phrases,\" can be evocative of linguistic phenomena. In this paper, it is argued that the statistical variation present in music can be examined with an eye to performing the same sort of stylometric analysis and to determining the \"author\" (= composer and/or performer), \"genre,\" and so forth. As with conventional stylometry, the analytic process involves three basic steps : canonization of the source \"document\" to insure that similar \"events\" are treated identically determination of the event set of interest and calculation of the statistical distributions inferential statistics to determine the composer/performer Some examples of how each of these steps can be performed in a a computationally tractable fashion will be presented. The results of preliminary experiments are extremely encouraging. Samples were collected of a number of polyphonic recordings from various records of two musical groups. An extremely naive analysis, using histograms of bytes, was nevertheless capable of distinguishing the groups with high accuracy. Even with this level of naivete, the results are \"statistically significant.\" (p < 0.05, using a binomial test) Refinements of various sorts can be deployed to increase the accuracy and reliability of these results. On a more theoretical level, these results show not only that \"composership\" attribution is a practical and solvable problem for non-linguistic musical data, but also that stylometric analysis is practical for other types of paralinguistic data than pure text. From a practical standpoint, this sort of analysis may eventually make it possible to develop musical analysis, search, and retrieval tools similar to existing search engines and text processing tools. Finally, this research raises the question about what other sorts of properties might be useful and accessible for performing other forms of \"document\" classification.",
        "article_title": "On Composership Attribution",
        "authors": [
            {
                "given": "Patrick",
                "family": "Juola",
                "affiliation": [
                    "Duquesne University"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Only one complete version of the Middle Dutch Arthurian Romance Roman van Walewein is handed down to us, in a manuscript written in 1350. The texts as it is consists of 11.292 lines and about 65.000 words (tokens). As stated by the clerk who copied it, the romance was written by two authors: Penninc started the work and Pieter Vostaert finished it by adding the last 3300 lines - or the last 330, as recently has been argued. It is unknown when the romance was composed; most researchers seem to opt for a tentative date between 1230 and 1260, which leaves about a century between the composition of the original text and the manuscript from 1350. If the two authors have known each other and have both written their parts of the text around the middle of the thirteenth century, this might imply that subsequent copying may have smoothed the text as to possible differences between the two authors. Researchers of the text, however, have pointed to significant differences between Penninc and Vostaert. These were elaborately described by G.A. van Es in the introduction to his edition of the text, published in 1957. He based his characterisation of Penninc and Vostaert on a.o. the analysis of syntactical aspects, the use of past perfect and historical present tense, the use of adverbs, the way the authors handled certain motifs in clearly different ways, and on a selective analysis of their vocabulary. This last aspect, he explained, should be researched more deeply in the future because exhaustive research undoubtedly would reveal more about the character and style of the two authors and even may lead to more clues as to the time in which both were writing. Modern authorship attribution techniques will be used to follow up on Van Es's work. I will go into the distribution of parts of speech throughout the text (cf. Holmes 1994: 89-90) and vocabulary analysis of several kinds (type â token ratio, vocabulary distribution, comparing the vocabulary of both authors, vocabulary richness; cf. Holmes 1994: 91-98). Furthermore, because the set of authors is 'closed', limited to two, principal component analysis might yield interesting information in the comparison of both authors (Binongo and Smith 1999, Burrows 2003: 8-10, Somers and Tweedie 2003). The most interesting technique, however, is Burrows 'Delta': analysis of the use of the most frequent words (Burrows 2002; Burrows 2003), a.o. because this kind of analysis was not part of Van Es's research in 1957. Depending on the results of these tests, other (e.g. multivariate) techniques that might supplement the information will be used as well. In order to apply these techniques to the Roman van Walewein of Penninc and Vostaert, the text will be iteratively divided by a cursor into two smaller parts, which will both be analysed with the help of the above-mentioned techniques. The cursor will move through the complete text and the assumption is that the different techniques will concur in locating the place where the first and last part of the text are contrasting most, indicating the place where Vostaert took over from Penninc. Because fourteenth-century Dutch did not have a standard spelling yet, the text is (semi-automatically) tagged on word level, giving each word a modern Dutch headword that will bring all spelling variants and inflectional or conjugational forms of a word under one heading; the text is also tagged for parts of speech. Word counts will be done for both the tagged text and for the 'original' text, to find out in what way tagging will change the results of the tests (cf. Burrows 2003: 10). The goals of this research are twofold: firstly, to find out whether it is possible to get answers to the following questions: is it possible to pinpoint the place where Vostaert took over from Penninc? Did he indeed write about 3300 lines at the end, or only a meagre 330, or will the text as it was written down in 1350 give rise to new assumptions as to the (non-existence of a) break? What will this mean for the suggestions earlier researchers have made as to the lines where Vostaert must have taken over? Another interesting question (cf. Forsyth 1999) is whether it will be possible to find out whether Vostaert wrote only a few years after Penninc or maybe after a longer time - e.g. closer to the year 1350 than to 1250? And to go a step further: could Vostaert possibly be the clerk who wrote the manuscript in 1350, building on a (century-)old manuscript that contained Penninc's unfinished text? Or will this new research lead to new insights into the work of copying clerks? Secondly, the goal is to evaluate the usefulness of the chosen combination of authorship attribution techniques for this specific type of textual problem.",
        "article_title": "Penninc versus Vostaert. Contrasting co-authors by means of authorship attribution techniques",
        "authors": [
            {
                "given": "Karina",
                "family": "van Dalen-Oskam",
                "affiliation": [
                    "Afd. Neerlandistiek"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The Roman stylus tablets from Vindolanda are an unparalleled resource for scholars of the ancient world, giving the sole immediate account of the Roman occupation of Britain (Bowman and Thomas 2003). Unfortunately, the physical condition of the stylus tablets renders them illegible to the human eye. Novel imaging techniques were developed at the Department of Engineering Science, University of Oxford, to analyse these texts (Bowman, Brady et al 1997, Brady et al 2003), but whilst a scrutiny of the document surface using image processing techniques provided new information, it was necessary to develop a computer system to aid the historians in the reading and interpretation of these images themselves, to speed up the reading process. This paper describes the steps taken to understand the nature of the processes used by the papyrologists, in order to build such a computer system. Before designing and building any tools to aid papyrologists in the reading of texts, it is a necessary requirement firstly to ask: just what does a papyrologist do when trying to read and understand an ancient text? Although the readings generated from ancient documents provide one of the major primary information sources for classicists, linguists, archaeologists, historians, palaeographers, and scholars from associated disciplines, surprisingly little research has been carried out regarding how an expert constructs meaning from deteriorated and damaged texts (Terras 2002). This paper discusses an interdisciplinary approach to modelling a complex humanities process, where techniques from artificial intelligence, cognitive psychology, knowledge elicitiation, computational linguistics, and computational content analysis, are combined to result in a proposed model of how experts read ancient documents. This representation was subsequently used as a basis for the development of a computer system which can aid historians in the reading of the Vindolanda texts. The problem with trying to discover the process that papyrologists go through whilst reading an ancient text is that experts are notoriously bad at describing what they are expert at (McGraw and Harbison-Briggs 1989). The primary questions to be asked in this study were: is there a general process that experts use when reading ancient texts? Can this procedure be elucidated? Additionally, what are the differences and similarities between individual experts' approaches to the problem? In trying to answer these questions, computational techniques were employed to interrogate and manipulate any data collected, and assist in developing a model of how experts operate in the given domain. Figure 1 Stylus tablet 836, one of the most complete stylus tablets unearthed at Vindolanda. This text is a letter from Albanus to Bellus, containing a receipt and further demand for payment of transport costs. The incisions on the surface can be seen to be complex, whilst the woodgrain, surface discoloration, warping, and cracking of the physical object demonstrate the difficulty papyrologists have in reading such texts. A process of Knowledge Elicitation (a technique borrowed from artificial intelligence, where the behaviour of experts is analysed to understand the knowledge and procedures they use whilst carrying out expert tasks (Diaper, 1989)) was used to gain an understanding of the general techniques the experts utilise when approaching an ancient text, and specifically, the Vindolanda tablets. Knowledge Elicitation consists of very defined stages. Firstly, the domain literature was researched. Secondly, any other associated literature was collated. Although not a direct comment on the act of reading and transcribing, the two published volumes regarding the Vindolanda ink tablets contain detailed apparatus of the individual texts (Bowman and Thomas 1983; Bowman and Thomas 1994), and electronic versions of these were subjected to Content Analysis techniques, and linguistic analysis (using WordSmith and TACT ), to detect any underlying structures and decision matrices. T Three experts were then identified who were working on the Vindolanda texts, and who were willing to take part in this investigation. Think Aloud Protocols, where each expert is set specific tasks, and asked to describe their thought processes, were carried out, and transcripts from these sessions provided data which enabled an explicit and quantitative representation of the way the papyrologists read damaged and abraded texts to be generated. General procedural information was also collated and analysed, and Automated Knowledge Elicitation techniques were used to help resolve this information into a structure that was used as the basis of a model which would provide the structure of a computer program to aid the historians. Particular issues regarding problems in reading the Vindolanda stylus texts were highlighted, indicating areas in which computational tools may be able to aid the papyrologists in reading such texts. The model that was generated from this process is hierarchical, and recursive, with separate \"agents\" represented which each carry out specific tasks. It is proposed that an expert reads an ancient document by identifying visual features, and then incrementally building up knowledge about the document's characters, words, grammar, phrases, and meaning, continually proposing hypotheses, and checking those against other information, until s/he finds that this process is exhausted. At this point a representation of the text is prepared in the standard publication format. At each agent level, external resources may be consulted, or be unconsciously compared to the characteristics of the document. Although a simple representation, the model shows the overall scope of the process of reading an ancient text. Figure 2: Proposed model of the procedure used to read an ancient text, broken down into individual agents. The computer version of this model was constructed by identifying, adopting and adapting an existing system for the analysis of satellite images; the GRAVA system, which is implemented in YOLAMBDA, a dialect of LISP (Robertson 1999, 2001). This provided an architecture which could easily represent the hierarchical nature of the papyrologist model, and was a successful basis to construct a working computer system which intakes images of ancient documents and generates plausible interpretations of the text of the documents. This paper will discuss the knowledge elicitation procedures and techniques adopted in this research, demonstrate results and conclusions of these procedures, and explain how a model can be resolved from this data. Constructing an explicit model of such a process is the first stage in building a computer system which replicates the process, and it will be shown how the resulting computer program depends on the underlying model which was generated through this research. Finally, it will be shown how the analysis of complex humanities procedural tasks in this manner can result in computer systems which aim to aid experts to carry out those tasks more efficiently.",
        "article_title": "Reading the Readers: Modelling Complex Humanities Processes to Build Cognitive Systems",
        "authors": [
            {
                "given": "Melissa",
                "family": "Terras",
                "affiliation": [
                    "School of Library, Archive and Information Studies, University College London"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Shakespeare's undated plays and Plato's undated dialogues remain among the problematic cases in the area of Stylometry. Their true chronology is not yet known, and probably will never be, since there is lack of sufficient external evidence to support any assumptions. Nevertheless, researchers have attempted to identify various stylistic markers and to test them using a number of traditional as well as computer-based approaches of considerable success. Several recent examples include the dating of Carlo Sigonio's Consolatio (Forsyth et. al., 1999), of Sophocles' Trachiniae (Craik et al., 1987), the sequence of composition of the plays of Christopher Marlowe (Ule, 1982), the development of Euripides' trimeters (Laan, 1995; Gooijer et al., 2001), and the dating of Ars Poetica by Horace (Frischer, 1991). Along similar lines, the present experiment aims to assess the discriminatory power of a number of lexical, semantic, syntactic, entropic, and phonetic predictor variables when used to detect the existence of any non-linear patterns for the purposes of dating in a securely dated collection of texts by four authors, namely Edna St Vincent Millay, Edgar Allan Poe, Christina Rossetti, and William Butler Yeats. The main difference with previous approaches is the use of a non-parametric method, namely regression trees as an alternative to linear regression. Regression trees have been used when a need for detecting non-linear patterns in the data is apparent. In addition, regression trees are known to produce accurate results, although they tend to be not so accurate if the data have a good linear structure (Breiman et. al., 1984: 264). It is hoped that modern methods such as regression trees will be more effective in identifying possible accurate predictors. A regression tree is a treelike structure developed to represent a decision process for regression purposes. Consisting of branches (links) and leaves (nodes), a regression tree aspires to identify the attributes of the objects under examination which function as best discriminators. The aim is to produce a simple yet reliable model of the relationship between one or more categorical/continuous independent variables and a dependent continuous variable. Regression trees are useful when there is need for accurate predictors or understanding of the type of variables and of the interactions among them which underline relevant experiments. The aim is to provide simple descriptions of the conditions under which cases are categorized (Breiman et al., 1984:6). This is achieved by their ability to handle mixed data-types and non-linear relationships since tree-based methods are non-parametric. Permitting one or more variables at each decision node, a regression tree assigns objects to predetermined ordered response groups. When a regression tree is being developed, data for each case traverse down the tree beginning at the root node, splitting nodes as they progress, until a terminal node is reached. If a case satisfies the condition set at each node, the case follows the left branch towards the left node; otherwise the case follows the right branch. Each terminal node contains a constant which is the average value for the response variable based on all cases that reach the node. To use a tree with new data, observations for each case would be dropped down the tree in a similar fashion to the construction of the predictor tree until the terminal nodes are reached. The predicted value for each terminal node would be the sample average of the dependent variable for that particular group. To construct a predictor tree, the following criteria need to be established: a set of binary questions of the form ?is variable j of instance i less than or equal to c?? for all reasonable c values; a splitting rule for each intermediate node; a condition that determines when a node becomes terminal; and a rule for assigning the predicted value of the response variable to the terminal nodes (Breiman et al., 1984: 28-29, 229). GUIDE (Generalized, Unbiased Interaction Detection and Estimation), which was developed by Loh (2002), is a regression tree algorithm based on the previously mentioned criteria. This algorithm can handle categorical predictors and has insignificant selection bias. GUIDE fits the sample mean of the dependent variable at each node, computes the residuals between the observed values at each node and the sample mean, and divides the cases in two groups according to the signs of the residuals. To produce the tree, GUIDE uses the minimal cost-complexity method of CART (Breiman et al., 1984) with V-fold cross validation. GUIDE has been selected as the algorithm for this experiment as the most unbiased, accurate, and readily obtainable. Subsequently, GUIDE was tested on samples of poetry and personal correspondence by the four authors using stylistic markers showing in previous studies to produce accurate results. The stylistic markers were selected according to three criteria; that they were automatically quantifiable, that they had been used by other researchers for similar projects, and that they were of linguistic validity. No distinction was made in terms of whether the markers had been used for authorship, genre, chronology or stylistic studies in general, since it has already been shown that what markers work in one case do not necessarily work on others (Holmes, 1998: 111; Rudman, 2000). To examine whether the observed differences in the accuracy of the trees produced by GUIDE are genuine or are attributed to the natural variability among populations and samples, four four-way ANOVA were performed. The objective was to tease out main effects and all two-way interactions for each factor, and assess whether there is a significant difference in the use of the different factors. The dependent variables were Total Number of Variables in the final trees, Mean Square Error (MSE), Median Square Error (MedSE), Scaled Mean Square Error (MSEscaled), ? the last three being variations of the accuracy measure MSE produced by GUIDE for the trees ? and factors such as Genre, Author, Order, and Variable Type. For the first ANOVA, which used total number of variables as the dependent variable, the result was no significant interaction or significant main effect for any of the factors. It appears that the total number of variables which GUIDE interpreted as important is unpredictable among the different authors. The second ANOVA tested MSE against the four factors. Only one significant interaction was identified, that between author and genre, as well as significant main effects for the same factors. The results produced were almost identical to the ones produced when MedSE is used as the dependent variable. This leads to the conclusion that different authors appear to produce trees of variable predictive accuracy according to the genre they write in, however, it is not possible at this stage to identify which genre is the most difficult to assign. The fourth ANOVA examined the Scaled error against the four factors to assess the size of the improvement for quess work rather than just the raw 'success'. The result was no significant interaction or main effect for the relative error, which implies that no tree consisting of different types of variables for different authors appears to be bigger or more accurate than the other. It would seem that none of these factors produce consistently different results. However, it needs to be mentioned that the interaction effects and main effects detected could be due to the restricted range of Yeats's letters which spread over twenty-eight years and the range of the poems which spread over fifty-four years. This unbalanced productive period ? which is due to the texts availability ? could explain why the significant effects disappear when MSEscaled is used. To summarize, this study has led to the conclusion that authors and genres differ inconsistently, and no similar results are obtained in any of the analyses so far. This suggests that to use personal correspondence to predict poems is a precarious idea. It also advocates that not all authors can be dated, therefore, the accuracy of dating will depend on individuals, which renders the idea of chronological 'fingerprinting' impossible, since not all authors would have such a 'fingerprint'. However, one possible explanation for such an outcome is the small text size used. Therefore, a study on similar grounds but with aggregated datasets as an attempt to verify the present findings is currently taking place, whose results will be presented at the conference.",
        "article_title": "Regression Trees in Stylometry",
        "authors": [
            {
                "given": "Constantina",
                "family": "Stamou",
                "affiliation": [
                    "University of Luton"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "\"Low Density\" languages are those for which relatively few computational resources are available. Not being majority languages, they have received less attention in resource collection. But increasingly, substantial resources can be found on the Internet for many intermediate-sized and even small-sized languages. At the Linguistic Data Consortium (LDC), we are creating basic language 'resource kits' for a number of low density languages. Each such resource kit is to include monolingual texts (in a standard encoding), bilingual texts (in English and the target language), a lexicon (which may be a simple bilingual word list), and where relevant, a morphological parser. Building a resource kit for a particular language involves locating appropriate resources on the web, downloading them, converting from HTML-tagged text to some other form deemed more useful for the end purpose (which often implies stripping out html tags, and tokenization), converting the text to a standard encoding where necessary, merging multiple resources, and assigning meta-data. In this paper, I describe the first step in this process. Specifically, I focus on collection techniques which have proven useful--and some which have not worked as well as we had hoped. We are using these techniques on languages ranging from Hindi (360 million+ speakers) to Chechen (one million speakers). I have also experimented with the use of search techniques on much smaller languages (Tzeltal, a Mayan language with 200 thousand speakers; and Shuar, a language of Ecuador with 30 thousand speakers), with surprisingly fruitful results. For text resources and some kinds of lexicons, one of the most useful search techniques is to enter a few common words in the desired language into a search engine, such as Google. Under good circumstances, this can result in excellent recall and precision. Unfortunately, circumstances are not always good; problems include: Multiple encodings (Hindi); Dialectal variation (Quechua); Non-standard spellings or orthographies (Chechen); Short words (which tend to result in false hits in other languages); Web pages which are purely graphical, with no searchable text (some Burmese sites); and The existence of languages which are closely enough related that they greatly lower the precision of the search, particularly where the related language has a larger web presence than the target language (Bahasa Indonesian hits are returned in much greater numbers than hits for the related language Aceh). Another circumstance which might be thought to make this kind of search work poorly, strongly inflected languages, turns out not to be as bad as one might think, in part because nearly all such languages have some words which do not inflect. However, if the uninflected words are uniformly short (as often happens with pre-/post-positions), inflection can still be problematical. Alternatively, one can 'or' together a few inflected forms of a single word into a search engine. There are several ways of obtaining common words for search purposes. The simplest is to bring up an already found web page in the language, copy a few words, and paste them into a search engine. If one does not know the language in question (this is common in our work at the LDC), it may not be obvious which words are the most common, but a crude heuristic is to copy the shortest words. Another way to find seed terms is to key in a few words from a printed dictionary (assuming that the encoding is simple, or that one has the language-specific keyboard installed on the computer). The best method is to use a simple computer program to tokenize one (or better, several) web pages, and sort them by frequency of appearance. As mentioned, searching for terms in the target language does not work well if web pages in the language do not use a standard encoding. Roman writing systems are reasonably standardized, although issues still arise when the orthography includes non-ASCII characters--even such trivial things as accented characters. Likewise, although there are several competing encodings for Cyrillic orthographies, they tend to be fairly well documented, so that one can transliterate search terms from one encoding to another. But very real problems with encodings surface with many languages which have non-Roman (and non-Cyrillic) orthographies, including languages of India, Ethiopia and Eritrea, and southeast Asia. While Unicode is intended to solve the problem, the fact is that there are multiple competing encodings for many of these languages, and they are often completely undocumented. The result is that there is at present no simple solution for searching across multiple web sites in languages with the multiple encoding problem. For some low density languages, few if any \"ordinary\" web pages exist. This may be because the country where the language is spoken discourages its use (non-official languages of Indonesia), or because there is very little Internet use in the country (many countries of Africa), or because the language lacks a standardized writing system (regional varieties of Arabic). Nevertheless, other sorts of language resources can sometimes be found for such languages; these may include weblogs (Aceh, a language of Indonesia), chat rooms (Arabic 'dialects'), web pages maintained by expatriate speakers (Rwandan), and text collections by other researchers (Mayan languages). Among the techniques which seem obvious, but which turn out not to work well, is searching for websites according the country suffix (e.g. '.tr' for Turkey). This results in both poor recall (since many of the websites we have found for low density languages are not hosted in the country where that language is primarily spoken) and poor precision (because most countries have websites in multiple languages, usually including English or some other regionally dominant language). Another technique which we have tried is to search for the name of the language. While this may result in many hits, precision often turns out badly, resulting in an overwhelming task of sifting through the results. Combining the language name with other search terms (e.g. 'dictionary' or 'lexicon') results in fewer hits, but is still likely to return many uninteresting links. Additionally, languages often have multiple names; the Ethnologue lists seven names for Cebuano (a Philippine language), and our initial search missed an on-line lexicon in this language because the web page title used one of the alternative names for the language. The Open Languages Archive Community (OLAC) is intended to be a repository of the sort of resources we are looking for. Other language resource aggregators include Seven Tones, the Yamada Language Center , Rosetta , and the collections at many Internet search engines (such as Google and Yahoo). However, significant resources--particular text resources--are missed by all these sites. We view our search technologies as supplements to these portals, and intend that our results will be made available for inclusion in OLAC's database.",
        "article_title": "Resource Discovery for Low Density Languages: Internet Search",
        "authors": [
            {
                "given": "Michael",
                "family": "Maxwell",
                "affiliation": [
                    "Linguistic Data Consortium/ University of Pennsylvania"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "Claire",
                "family": "Warwick",
                "affiliation": [
                    "SLAIS, UCL"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "A digital collection has a rich-prospect interface when some meaningful representation of every item in the collection is an intrinsic part of the interface used to access the collection (Ruecker 2003). Rich-prospect interfaces can provide the people using them to access a collection with singular advantages in two areas: an increase in the kinds and amount of information that is visible, and new opportunities for action that derive from tools that are associated with the display. These advantages are not available to people using conventional retrieval interfaces, which focus instead on the precise recall of specified search targets. For example, a retrieval interface to a collection of Victorian poetry might consist of a set of search boxes where the user can specify items such as the title of the poem, the author, or the date of publication. A rich-prospect interface to the same collection might include a similar set of search boxes, but would also show an organized display of every item in the collection. If the collection were to contain, for example, 10,000 poems, then the interface would list all 10,000 poems, perhaps by title, or by a combination of author and keyword, or by some other choice of descriptor that was meaningful. In order to make the rich-prospect display as useful as possible, strategies might also be adopted to allow the user to restructure the items according to some other criteria, such as publishing house, anthology titles, or geographic locations of authors at the time of publication. In order to make 10,000 titles manageable, visual communication design principles might be brought into play, whether through the application of microtext and fisheye cursors, scrolling and zoomable panoramas, or design incorporating a virtual third dimension. The increase in the amount of information immediately present and visible to the user of a rich-prospect interface is through the meaningful representation of every item in the collection. This visible information allows the user to readily perceive what is present and how it has been conceived and identified by the people who created the collection. The increase in the kinds of information stems from the organization of the meaningful representations, whether through sorting, searching, subsetting, or otherwise structuring the material in the display. The opportunities provided by the interface are related to the capacities of the various tools for organizing the representations. Where an appropriate set of rich-prospect tools have been made available, some of the research activities related to collection browsing can be carried out by researchers at the level of the interface, before any of the documents in the collection are accessed. This is true even in cases where the representation of items is not meaningful (Pirolli et al. 1996). However, not all information organization strategies are equally productive. For example, many systems will categorize writers alphabetically by last name. Unfortunately, not many projects require academics to write about all the writers whose last names began with the letter \"C.\" Alphabetical organization does facilitate the retrieval of documents associated with a known author, but prohibits browsing through authors who are related by genre, period, thematic similarity, intertextuality and so on. A better strategy for identifying authors associated in some meaningful dimension is to provide a method for the user to dynamically reorganize the display according to the metric of interest. It is also possible, however, to identify structuring techniques that are more sophisticated, in the sense that they combine more than one metric into a display that allows simultaneous insight into multiple aspects of the information. Through an examination of the organizational principles behind a highly successful visual information design employing rich prospect -- Mendeleyev's periodic table of the elements -- this paper provides a set of principles for designers interested in creating tools for use in association with rich-prospect interfaces. These principles emphasize the value of simultaneous structure across distinct aspects of the available information, and include: compression and inclusion, the establishment of a matrix for pattern-finding, and the creation of potentially productive negative spaces. Interface strategies based on Mendeleyevian principles should be particularly useful for accessing large bodies of digital documents such as those often found in the humanities, maximizing the pre-retrieval research potential of the collection.",
        "article_title": "Strategies for Creating Rich Prospect in Interfaces",
        "authors": [
            {
                "given": "Stan",
                "family": "Ruecker",
                "affiliation": [
                    "University of Alberta"
                ]
            },
            {
                "given": "Susan",
                "family": "Liepert",
                "affiliation": [
                    "University of Alberta"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The TEI Character Encoding work-group recently considered the issue of language identification in TEI XML documents. This paper is a presentation of our deliberations and conclusions along with extensions thereto from the authors imagination. Thus the work-group members can only be held responsible for the conclusion described here; the author is responsible for any logical fallacies or other errors presented. Background In July of 1990 the Text Encoding Initiative released the first public draft of their proposed Guidelines For the Encoding and Interchange of Machine-Readable Texts, commonly known as TEI P1. These guidelines, and the SGML DTD that accompanied them as an appendix, included a provision for an attribute, lang, which was available for use on every element in the entire TEI scheme. This attribute has survived, in pretty much the same form, through the first non-draft release of the Guidelines (TEI P3) to the most recently released version (P4:2002-07), although in that version there is an explicit admission that the mechanism employed by the TEI lang attribute is likely to undergo significant revision at the next release. In November of 1996 the W3C released the first public draft of their proposed Extensible Markup Language, commonly known as XML. This working draft had no provision for language identification. By the time the specification became an official W3C recomendation (February of 1998), however, it included a provision for an attribute, xml:lang, which (if declared appropriately for valid documents) could be used on any element in an XML document. These two attributes serve the same major purpose: to indicate the (natural) language an element is written in. However, they have different mechanisms for acheiving this purpose, and slightly different semantics. Thus, users of TEI P4 XML have a potentially confusing choice as to which method to use. It is perfectly acceptable to use both simultaneously, of course. However this has the obvious disadvantages of extra work, verbosity, and redundancy which serves not as a fallback if there is a problem, but rather as a source of trouble if the two are in disagreement. TEI P4 lang [1] XML xml:lang [2] Pros and Cons Advantages of TEI lang name freedom: Because the value of lang is a reference to an arbitrary identifier, the user is free to choose any token (subject to the limitations of an XML namehttp://www.w3.org/TR/REC-xml#NT-Name) she wishes. Note that nothing prevents the user from using the very same names she would have used if she had the same restrictions as those on xml:lang. descriptiveness: The user can describe the language being discussed as thoroughly as she likes in prose. From within her description she is welcome to refer to ISO 639, IANA, SIL Ethnologue, or any other standardized list of languages. extensibility: The system permits an easy but formal mapping between languages in the document and any language, even those that do not appear in the standard authority lists. This means that the indicated language can be described very precisely. For instance, one might well have different lang identifiers for each of the various dialects of southern American English in an Mark Twain novel. well-defined: The semantics of lang are reasonably well defined. familiarity: TEI users have used lang for years. brevity: it's short. This does not mean that there are no disadvantages to the TEI scheme, of course. The most obvious, more because it is an intractable problem than because of any TEI shortsightedness, is the inability to formally describe a language. But there are others. Suggestions have been put forth, e.g., to permit the specification of the correspondence to multiple authority lists; to permit a hierarchical indication of sublanguages; and to permit the value of lang to point to an external langUsage. However, it is reasonable to assume that if TEI were to continue using lang, at least some of these concerns would be addressed in P5. Disadvantages of TEI lang Little software support, and not much prospect for improved software support in the future. Advantages of XML xml:lang Somewhat better software support, and very good prospects for improved support in the future. Disadvantages of XML xml:lang scope: This is the major complaint against xml:lang: that it applies to attribute values as well as element content. This concern will be discussed in more detail in the full paper, taking each attribute type into consideration. no pointing: There is no formal semantic for what the value of xml:lang is, except that it is a language identifier. It is not possible to point directly into the authority list being used, or to point at any further classifications or descriptions of the language being identified. poor extensibility: While it is entirely possible for a user to develop an identifier for a particular sublanguage or dialect for which no ISO or IANA identifier exists, there is no mechanism to associate any information about the language so identified with the identifier. Results and Reasoning Despite the obvious disadvantages of xml:lang, the TEI Character Encoding work-group chose to recommend that in P5 the TEI drop the lang attribute and declare xml:lang instead. Why use the inferior approach? First it is important to realize that for TEI P5, the scoping problem (which is the most significant disadvantage of xml:lang is drastically minimized compared to P4, as it is expected that most string type attributes will become elements instead. Thus, said elements can bear an xml:lang attribute of their own, and the scoping problem is avoided. Given that the differences would then be minor, the work-group applied the principle that unless your method is demonstrably significantly better than the standard, you should be using the standard[4], and thus chose the more widely standardized xml:lang. The fact that it is far better to only have one attribute for this purpose, and that the TEI could not likely do much to remove the W3C one also influenced our decision. However, in order to make up for the lost functionality of a formal association between the identified language and a description thereof, at least for non-standard languages, a mechanism needed to be developed to link the new xml:lang to a description (language in langUsage in the teiHeader). One such mechanism was proposed in Nancy, however I anticipate some active discussion on the issues, and perhaps some changes in the mechanism, over the next two months. This paper will explore the advantages and disadvantages in more detail, with particular attention to the scoping problem mentioned above. I will argue that in fact the attribution of language to most attributes is not only useless but meaningless, and will present the new mechanism for linking the language identified to its description.",
        "article_title": "TEI: \"xml:lang sucks, let's use it anyway\"",
        "authors": [
            {
                "given": "Syd",
                "family": "Bauman",
                "affiliation": [
                    "Brown University"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "In his Busa Award presentation and two recent articles, John F. Burrows has presented a new measure of stylistic difference that seems quite promising in authorship attribution, and possibly also in stylistic studies (2001, 2002, 2003). This unitary measure, which he calls 'Delta,' like many other techniques, is based on the differences in the frequencies of the most frequent words in a group of texts\"in this case, \"verse by twenty-five poets of the English Restoration period\" (2003: 10). Burrows uses the frequencies of the 150 most frequent words of the entire set of texts in his exposition of the method, comparing the mean frequency of each word in the whole set with its frequency in the texts of each of the authors sequentially. His technique uses z-scores, which are calculated by subtracting the mean frequency of each word in the entire set from its frequency in the test text, and dividing this difference by the standard deviation of the word in the entire set. This method allows all of the 150 words in his set to have equal weight, in spite of their very different frequencies. Burrows then eliminates the sign of the difference (which indicates whether the word is less frequent or more frequent than the mean for the whole set), adds the z-scores together, and calculates the mean. This mean is Delta, \"the mean of the absolute differences between the z-scores for a set of word-variables in a given text-group and the z-scores for the same set of word-variables in a target text\" (2002: 271). Delta proves remarkably effective in identifying authors in a difficult \"open\" test. When texts of sixteen authors who are included in the original set and sixteen authors who are not are tested with Delta to determine likely authorship, \"Of thirty- two long poems, . . . fifteen are correctly identified and another fifteen yield scores that correctly place them outside the main set\" (Burrows, 2003: 15). Although this result is not completely accurate, is very encouraging, and it suggests that delta may be a very useful tool in the early stages of an authorship study in which there are large numbers of possible authors. Because of its great potential, delta deserves further investigation, and I have begun a series of tests on novels published about 1900, to see if Burrows's success with poetry can be duplicated with prose. The choice of 1900 as a central date ensures a large selection of texts for testing without scanning: many such novels, now out of copyright, are already available as e-texts. After cleaning up the e-texts to correct for problems of hyphenation, apostrophes, and single quotation marks, I have extracted samples of approximately 25,000 words of pure authorial narration from more than 100 novels. Using a combination of text-analysis tools, and custom programming, I have collected the most frequent words from each of the samples for comparison. Burrows's analysis (2002) shows that the accuracy of Delta decreases as he reduces the frequency list from the 150 to the 40 most frequent words, and Hoover (2002, 2003) has shown that cluster analyses based on as many as the 800 most frequent words are often more accurate than those based on the traditionally-used smaller lists. Therefore, I have collected the 800 most frequent words of each text, and have set up a complex Excel spreadsheet that accepts as input sets of columns of the most frequent words from up to 80 primary and 80 secondary texts for analysis. A macro calculates Delta for each of the primary texts and then for each of the secondary texts in turn, beginning with the 800 most frequent words, and continuing with the 700, 600, 500, 400, 300, 200, 150, 100, 70, 50, 30, and 20 most frequent. Another macro then moves through the results, calculating the rank of the actual author for texts by authors in the main set and extracting the information in an format for easy graphing. This automation assures the accuracy of the analysis, saves countless hours of tedious and painstaking drudgery, and allows for the testing of many differently selected sets of frequent words. For example, I have tested the following eight different kinds of sets: The most frequent words. Contractions removed. Personal pronouns removed. Contractions and personal pronouns removed. Words for which a single text supplies more than 70% of the occurrences removed. Contractions and words for which a single text supplies more than 70% of the occurrences removed. Personal pronouns and words for which a single text supplies more than 70% of the occurrences removed. Contractions, personal pronouns, and words for which a single text supplies more than 70% of the occurrences removed. Research is still in progress, but preliminary results are available for a primary set of 22 American authors (represented by one third-person novel each) that have been tested against 20 third-person American novels from authors in the primary set. As Figure 1 shows, Delta is quite effective, in two cases attributing 19 of the 20 novels to the correct author. (The 20th, which surprisingly ranks 5th, is one of two novels by Ellen Glasgow in the secondary set. The other is consistently identified correctly. This case invites further investigation, because the novel that is not correctly identified was written about 10 years later than the other two, and is sometimes considered to be Glasgow's first novel in her mature style. The preliminary research also shows that removing contractions reduces the accuracy of the analyses, but removing personal pronouns improves it (for similar results, see Hoover, 2002). Finally, removing words for which a single text supplies more than 70% of the occurrences improves the accuracy quite dramatically, producing the only results with 19 of the 20 texts correctly attributed. Although Burrows showed that texts by authors from the main set usually produce Deltas significantly lower than those by authors from outside the main set, as one would expect, my initial tests on prose show somewhat weaker results: the Deltas of texts by authors from the main set are generally lower, but some show Deltas higher than some of the texts by authors from outside the main set. For this set of prose texts, Delta is more effective in suggesting correct authors than in eliminating incorrect potential claimants. More testing is necessary to determine whether the use of a single novel for each author in the main set may result in less accurate results than those Burrows achieved when most of his authors were represented by a group of poems, and whether work on attribution should be limited to texts with the same point of view or nationality. Nevertheless, Burrows Delta seems poised to become a valuable addition to the authorship attribution toolbox.",
        "article_title": "Testing Burrows's 'Delta'",
        "authors": [
            {
                "given": "David",
                "family": "Hoover",
                "affiliation": [
                    "New York University"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The multilingual and multicultural transmission of ancient Greek mathematics to modern Europe is well illustrated by the case of Theodosius of Bithynia: in the Byzantine east, his work on Spherics was copied in Greek; in the Islamic world, it was translated into and commented on in Arabic; the first Latin translations in Europe were made from Arabic sources in the twelfth century, and were the basis for the first printed edition of 1529; before the end of the sixteenth century, new Latin translations had been published based directly on Arabic and Greek sources, and the first Greek edition was printed in 1558. In this paper I will present work on a corpus of texts representing key moments in the transmission and interpretation of Theodosius's Spherics (including a new English translation that is the first directly from Greek). Textual content of manuscripts and printed editions is represented with TEI-conformant XML. The markup uses the canonical reference system familiar from today's printed editions of Theodosius's work, and uniquely identifies named entities across versions, so that versions can be readily aligned and compared. The TEI texts are made accessible to the internet through the TextServer protocol recently developed by a team of classicists for working with corpora of TEI-conformant texts (http://shot.holycross.edu/projects/TextServer). Applications using this protocol allow users to see the automatically generated equivalent of an apparatus criticus showing differences among versions in a single language; they also allow users to compare passages by canonical reference across languages, or search for named entities across versions regardless of language. Other applications depending on a multilingual corpus of aligned texts could easily be developed as well. The textual content is only one part of a Greek mathematical text, however: our manuscript traditions and printed editions uniformly include figures, too. Traditions of visual presentation may vary, but all our sources agree that figures with labelled elements referred to in the text are an essential part of a formal proof. Modern editors may on occasion make this point explicitly -- in Renaissance Latin translations, the figures can even be called \"demonstrationes,\" that is, \"proofs\" -- but in all cases the claim is implicit in the organization and form of the text. I have therefore developed an XML DTD for describing the semantic content of geometric figures in terms of their logical structure, and the properties of the geometric elements in them. I argue that this is analogous to the principle of semantic markup of texts: the particular visual form of a figure can be considered a presentational variant comparable to the page layout, selection of type face, and other presentational choices made in the copying or editing of a specific version of a text. The DTD organizes figures in the same scheme used in the text's canonical reference system: \"proposition 1 of book 1 of the Spherics\" represents the same notion in the TEI XML of the text, and in the XML for the figure. In addition, the DTD identifies specific elements with the same identifiers used in the text. Just as these identifiers make unambiguous in the text that, for example, \"line AB\" and \"line BA\" refer to the same element, they also make explicit that those elements correspond to a specific element in the XML description of the figure. Thus, computational manipulation or analysis of the text can be associated with the figure, and vice versa. Combining a logical representation of the figures with a TEI XML representation of the text can open new questions about Greek mathematical texts to computational analysis, which I will illustrate with several examples, implemented as XSLT transformations within a Cocoon pipeline. One simple transformation of the figure represents it with a series of English statements (rather than graphically). This output can be paralleled with the corresponding text: the logic of textual presentation can be directly confronted with the logic of the figure. Other transformations create graphics from the logical representation of the figure. (At present, I create SVG graphics that mimic the two-dimensional conventions of Greek manuscript figures; I would like to develop other visualizations of the figures' logic, including OpenGL models in three dimensions for Theodosius's spherical elements.) Corresponding elements in the text and diagram can be highlighted similarly (e.g., with the same color) to make clear their relation. This view recalls the famous edition of Euclid by Oliver Byrne , but unlike Byrne's printed text, can be walked through one step at a time. I believe that these transformations will have obvious pedagogical benefits in helping readers understand the relation of text and figure, as Byrne aimed at doing for Euclid. At the same time, they suggest new approaches to scholarly questions about the transmission of Theodosius's works. Renaissance translations of Theodosius boast about their \"new and improved\" diagrams, when they introduce new features like three-dimensional perspective views, or multi-step views of a construction: the Latin translators literally see themselves as translators of the figures as well as the text. With a formal representation of the figure's logic, it should be possible to test automatically: 1) whether the editor's figure accurately reflects the semantics of the text 2) whether the semantics of two different versions' figures are equivalent or not, independently of whether their visual appearance is identical A systematic, computational approach to these issues should further clarify the relation of text and figure in the textual tradition of Theodosius, and illustrate a method that could be applied to ancient mathematical and scientific texts more broadly.",
        "article_title": "Text and Figure in Theodosius of Bithynia: An XML-Based Approach",
        "authors": [
            {
                "given": "Gabriel",
                "family": "Weaver",
                "affiliation": [
                    "College of the Holy Cross"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "It cannot be denied that the development of information technology poses a profound challenge to traditional scholarship in the humanities. The rapid development of computer technology and of the World Wide Web has opened up immense new opportunities for scholars working in historical disciplines. These opportunities also challenge the traditional ways in which scholarship has been practiced. Yet the challenges presented by humanistic scholarship to information technology and to the World Wide Web are perhaps even greater. In Part One of our paper, we briefly describe four fundamental limitations of the World Wide Web in its current form as a medium of scholarship in the humanities: Language technology. Research in natural language processing has resulted in a proliferation of tools for automatic linguistic analysis of particular languages. There is, however, no way to bring the data produced by these tools into a browsing or editing environment in the absence of standard formats and protocols. What is needed is linguistic middleware that enables user agents to interact with heterogeneous sources of linguistic data. Semantic linking. Scholarly research requires the creation and utilization of meaningful links between source materials. Ordinary HTML links are unidirectional and cannot differentiate between types of semantic relations. Digital library applications must be developed that achieve the ideal of a ``semantic web'' 1 by exploiting the potentials of relevant standards such as the XML Linking Language (XLink). 2 Content creation. Central to the notion of a digital research library is human analysis and annotation of source materials. No matter how sophisticated the algorithms used to generate them, automatically created links cannot take the place of scholarly analysis. The current paradigm of the Web distinguishes the creation and browsing of content as fundamentally separate activities. In a next-generation framework the browsing and creation of content must be more closely integrated. Distributed resources. The free exchange of ideas is a crucial feature of research, whether humanistic or scientific. Any scholar anywhere who has access to the Web should be able to contribute materials to a distributed set of resources and to work with resources produced by other scholars worldwide. Yet the current client-server architecture of the Web limits the free flow of information. Digital libraries must move toward a more fluid distributed or peer-to-peer network model. 3 Software Platform We describe a software platform designed as a first step towards meeting these challenges. This software platform has been developed in the context of the Archimedes Project, an international initiative to create a digital research library for the history of mechanics funded by the National Science Foundation in the United States. Although our software has been developed with a view to solving the problems arising in the course of work on this project, it is not tied to the requirements of any particular area of historical scholarship. Our software platform has three principal components, all of which are freely available in the Internet at http://archimedes.fas.harvard.edu: The Pollux system provides a unified means of access to dictionaries, or any other reference work that is organized by alphabetized headwords, in any natural language. The software is designed to make it possible for users to add new lexica with a minimum of effort. The Donatus system provides a unified frontend to a variety of morphological analysis software and databases. 4 Morphological services are provided both through a Remote Procedure Call (RPC) interface that can be utilized by specialized user applications and through a CGI interface that is accessible in any web browser. Morphological data can be represented in XML, allowing them to be cached on client systems and to be processed by a wide range of software. Backend systems that have already been incorporated include Morpheus, a morphological analyzer for ancient Greek, Latin, and Italian developed by the Perseus Project; 5 the CELEX Linguistic Database for Dutch, English, and German developed by the Center for Linguistic Information of the Max Planck Institute for Psycholinguistics in Nijmegen; 6 and the Xerox finite-state morphological analyzer for Arabic developed by the Xerox Research Centre Europe (XRCE) in Grenoble. 7 Work is currently underway to integrate a morphological analyzer for Sanskrit that is being developed at Brown University and one for Sumerian being developed at the University of Pennsylvania. In addition to providing access to pre-existing linguistic data, Donatus allows for the dynamic extension of morphological datasets by a user. The Arboreal user agent is a powerful and flexible tool for content-based access to and annotation of XML texts. Arboreal includes special features for working with parallel versions of texts, morphology and terminology, and linked images. Integrated language support is currently provided for Latin, Greek, Arabic, Chinese, languages written in cuneiform, and major western European languages. Arboreal supports many standards and is designed as a cross-platform tool that can be used on many different computing systems. Distributions are currently available for Mac OS X, Windows, and GNU/Linux. We envision Arboreal as a prototype for the next-generation web user agent, which closely integrates content browsing and content creation. Arboreal allows for highly flexible navigation of any XML document, using two document views that are presented side-by-side. One pane displays a tree view of the document; the user can control the level of detail shown in the tree by expanding and collapsing nodes and sets of nodes. The other pane offers a detail view of the portions that are selected in the tree. Both views are customizable through a document description language (DDL), which we aim to extend in the next phase of the project. Powerful search capabilities are available, including regular expression searching, lemmatized searching (which takes advantage of morphological data generated by the Donatus system), XPath queries, and the ability to search in an orthographically normalized representation of the text (e.g. a query for the Latin word `uectis' will also find `vectis'). Arboreal can be customized to work with any natural language, by supplying a description of the language in an XML langspec. This description makes possible language-specific features such as word detection and allows for various language-specific views to be defined (e.g. Romanization of a non-Roman script, or fully-voweled vs. non-voweled Arabic script). Close integration with the linguistic services provided by Donatus and the Pollux reference system is provided, allowing the user to access morphological analyses and dictionary entries for any word in a text. Application and Evaluation We present a number of concrete applications of our software platform in the context of the Archimedes Project. First, this platform has proven invaluable in the basic work involved in building up a digital collection that now amounts to some 110 MB of text and 30 GB of associated images. For example, with the aid of the automatic morphological analysis provided by Donatus and the term annotation facilities of Arboreal, a user can easily highlight all words in a document that cannot be analyzed by the Donatus system, allowing for both the speedy correction of typographical errors and the improvement of our morphological datasets. Second, Arboreal's ability to apply arbitrary XSLT transformations allows us to create a range of derived files of immediate scholarly value from a single source text. Third, Arboreal has been extensively used in conjunction with external editing environments to produce scholarly metadata, in particular for the alignment and linking of parallel sections of different XML texts. Fourth, Arboreal's term annotation and editing facilities provide support for scholars attempting to produce consistent translations of source texts. Finally, we have taken preliminary steps towards integrating into our framework sophisticated NLP techniques for the discovery of technical terminology and establishing metrics for assessing the utility of these techniques. A number of cooperating projects and institutions are also currently using components of our software platform. These include the Cuneiform Digital Library Initiative (CDLI), 8 the project Gli anni della cupola 1417-1436: Archivio digitale delle fonti dell'Opera di Santa Maria del Fiore (The Administrative Archives of the Cathedral of Florence), 9 European Cultural Heritage Online (ECHO), 10 and the Digital Sanskrit Library at Brown University. 11 Feedback from these projects has demonstrated the broad range of application of our tools and allowed us to create additional enhancements. Further Perspectives We describe further extension and generalization of our software platform to realize the goals stated in Part One, focusing on two areas in particular: (1) enhancements in core language technology, and (2) the development of tools for ontology creation and visualization.",
        "article_title": "The Challenge of the Humanities to the World Wide Web: Perspectives from the Archimedes Project",
        "authors": [
            {
                "given": "Malcolm D.",
                "family": "Hyman",
                "affiliation": [
                    "Harvard University"
                ]
            },
            {
                "given": "Mark J.",
                "family": "Schiefsky",
                "affiliation": [
                    "Harvard University"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Non-traditional authorship attribution studies are those studies that make use of the computer, statistics, and stylistics. The hypothesis behind these studies is that each author has a unique and verifiable style. However, most non-traditional attribution studies place too much emphasis on the elements of statistics, stylistics, and the computer and not enough focus is given to the overall design of the study as a scientific experiment. One element of this design is the selection and use of controls (aka \"comparison groups\"). Most practitioners do give \"lip service\" to the concept and use of controls -- never explicating their selection, use, or value -- never using all of the necessary controls. The importance of the correct application of controls in authorship studies was made apparent in the recent Donald Foster recantation of his non-traditional attribution of \"A Funerall Elegie\" to Shakespeare. Foster, after traditional scholarship \"proved\" him wrong, now agrees that his much ballyhood attribution study used an \"inadaquate\" base of control texts. [Niederkorn] Each type of scientific experiment demands its own set of controls -- this is one aspect of the scientific method. Controls are an integral part of any experimental design that adequately tests a given hypothesis. Controls are used to check the validity of the experimental design and to ensure the validity of the results. Attribution Types \"Differences in usage of literary elements found in a text are not necessarily an indication of differences in authorship, unless inter-authorship parameters have been compared to intra-authorship parameters to establish critical levels of variation.\" Larry La Mar Adams, p. 86. There are four main types of authorship attribution [Rudman 2000]: Anonymous work -- no idea of potential author Anonymous work -- two, three, or some other workable number of potential authors Anonymous work -- a collaboration Anonymous work -- did author 'A' write the questioned work This paper concentrates on type (4). The determination of controls for the other types will be in \"Appendix A\" and will be available at the conference as a handout. Control Types \"Linguistic Stylistics is the Scientific analysis of individual style-markers as observed and described in the idiolect of a single writer, as well as class style-markers as identified in the language or dialect of groups of writers.\" McMenamin, p. 115. This paper defines the word control as it pertains to authorship studies and discusses four types of controls: The \"author\" touchstone sample If there is any, even the slightest doubt that a work is by the \"author,\" that work must be excluded from this control. A subset of the \"author\" sample This sample is selected and put aside before any analysis begins. \"Non-author\" writings This requires the practitioner to identify every author and work that fits the selection criteria. The number of authors and texts is discussed below. Named \"non-author\" suspects This would include every 'good faith' suspect author of the anonymous text. Control Selection The paper goes on to discuss how to select each type of control: Genre Somers and Tweedie claim that they can mix genres and overcome the problem \"by a combination of a statistical technique...and choosing measures based on low level linguistic features....\"[Somers and Tweedie, p. 412] Other practitioners, such as John Burrows and David Hoover, find it important to separate genres before analysis [Burrows 1992a, p. 175-182] -- even sub-genres (e.g. first person narratives from third person narratives. [Hoover] Burrows also has a telling graph that, \"..shows a complex pattern in which genre transcends authorship.\" [Burrows 1992b, p.101, 102] Gender This is a current \"hot topic. [Woods] The recent work by Argamon, Koppel, Fine, and Shimoni (in fact one of their papers is \"in press\" and another is \"to appear\") forces the practitioner to use gender as a criterion in control text selection. [Argamon et al.] [Koppel et al.] Time period This paper will look at some of the chronology and style studies to show the importance of this item to the selection of controls. The (+/-) time factor is discussed -- from Holmes' (+3/-2 years) to the (+/- centuries) of the Historia Augusta. [Holmes] [Rudman 1998] Randomness Block Sampling, Spread Sampling, Stratified Block Sampling, and Haphazard Sampling are discussed. [Neumann] Johnstone's, \"On the Necessity for Random Sampling,\" is explicated. [Johnstone] Representative There are various ways to select a sample of texts. However, a practitioner cannot simply declare a sample to be \"representative\" -- e.g. \"...forty-three plays of known authorship, which form the comparison samples, HELD TO BE REPRESENTATIVE [emphasis mine] of the usages of six early Modern dramatists.\" [Hope, p.15] Size (number of authors in the \"non-author\" writings) The ideal would be to include all of the authors and texts that conform to the other selection criteria. And, as Foster found out, at some point the sample is too small to be valid. Size (number of tokens in each control) \"...large samples aren't necessarily good samples...the representativeness of a sample is actually more important than sample size.\" [Best] Statistical reasoning A short review of the reasoning behind controls is given. Same native language The importance of restricting controls to texts written by members of the same native group is discussed. Anonymous texts, collaborative texts, pastiche, ghostwriting Why none of these are acceptable in a valid control is discussed. Other criteria such as education, geographical regions within a native language sphere, political convictions, and religious convictions are possible determiners -- but studies to evaluate their importance are yet to be finalized. Not one of the ten listed items are without controversy. Most of them are in dispute. These disputes are analyzed in this paper and a reasoned determination made. ************************************** The availability of electronic texts (on the internet in particular) can be a boon or a bane in the selection and acquisition of controls. However, too many practitioners let what is on the internet and what is easily available from other sources determine the selection and number of authors in their controls. One example of this is Michael Farringdon's work on Henry Fielding. [Farringdon] Use of Controls \"And although controls are purportedly used, the methods are never tested entirely apart from the problem for which they were designed.\" Zimmer, p. 33 This section discusses the way that the various controls should be used and are used and mis-used in a representative sample of experiments. Examples Examples of problem studies are explicated; Historia Augusta \"...the twelve attribution studies of the Historia Augusta.... All provide classic examples of invalid controls.\" Rudman 2003, p. 29. Shakespeare \"But it must be viewed as a major setback when one highly visible study can be cited by skeptics as proof that the whole quantitative enterprise is fruitless, or a playground for fringe theories having no historical or computational validity.\" Foster 1996, p. 255 Defoe \"Most investigators of similar stylo-statistical problems do not divulge how their samples were built up, or how sample size was estimated. It is, certainly, very sensible to leave out such compromising matter, for any attempt to lay down principles in these cases is liable to attract criticism.\" Hargevik, Part I, p. 28. Conclusion \"Nothing can do more to lend an air of credibility to a claim than the suggestion that it has been proven in scientific studies or backed by scieintific evidence. Sadly, however, many claims made in the name of science are founded on misapplications of some aspect of scientific method.\" Carey, p. 6. The use of controls cannot be left to whimsy. Controls of convenience must be avoided if the practitioner expects the nihil obstat of the gatekeepers -- if the practitioner expects the results to be accepted by the community of scholars. Valid controls are not a guarantee of a valid study. However, invalid controls guarantee a suspect study that at best must be taken with a grain of salt.",
        "article_title": "The Determination and Use of Controls",
        "authors": [
            {
                "given": "Joseph",
                "family": "Rudman",
                "affiliation": [
                    "CMU"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Markup is based on mnemonics (i.e. element names, attribute names and attribute values). These mnemonics have meaning, being this one of the most interesting features of markup. Markup allows us to define the structure of a text in a way that can be both processed by computer programs and understood by humans. Human understanding of this meaning is lost when the encoder doesn't have a good command of the language the mnemonics are based on. For example, a Spanish encoder that doesn't know English will find it difficult and error prone to apply or understand TEI markup using the original TEI mnemonics based on the English language. So, by multilingual markup we mean applying marks using mnemonics in one's own language but still following the rules of the original markup vocabulary. In our own experience, a markup vocabulary exactly equivalent to TEI can be developed based on Spanish, Catalan, French and almost any other language, and the tools for translation back and forth to the original TEI core can be built automatically and can be applied in a transparent and easy way. When we build markup vocabularies equivalent to TEI but in the local language, the structural facilities and constraints of the markup scheme remain the same, only the markup terms used by DTDs, Schemas or documents are different, and the document structure becomes remarkably clearer for the encoder. In this paper we will show and defend the benefits of using multilingual markup vocabularies for large digitization projects, like reduced learning times, reduction of errors, and incremented production, all due to using markup tags in the local language. We will also describe our implementation of multilingual markup based on the automatic generation of translating scripts using XSLT (Extensible Stylesheet Language Transformation) for both document instances and also DTDs or Schemas. We will also discuss other alternative implementations to the one proposed that look promising. Finally, we will present the conclusions of the implementation and use of this technology within the Miguel de Cervantes Digital Library. We will also comment on the creation of a TEI Multilingual Markup Special Interest Group (TEI-MM-SIG) and the involvement of the TEI META Workgroup in the development and full implementation of a multilingual term-bank for the TEI. Markup, meaning and multilinguality. One of the key aspects of structural markup is the meaning it conveys, which depends on our ability to understand it. In 1998 Robin Cover wrote: How does XML help with the encoding of information at the semantic level? ... New users sometimes refer to XML as semantic markup, and may be heard to praise XML for its ability to express semantic clarity through markup. ... Someone who uses a text editor to examine an XML document ... will readily judge the XML document more meaningful with respect to the information objects represented by text. The markup itself is a form of 'metadata', explaining to us what the constituent elements are (by name), and how these information objects are structured into larger coherent units. [1] Sperberg-McQueen et.al. [2] supported the usefulness of markup as a source of meaning: The function of markup is not random. Markup has meaning. ... Why worry about this question?: For better markup language documentation, for better QA (verification), for better automated processes (translation, normalization, query), to provide a way to survey current practice (relevance for software developers) ... and because it's interesting. Because markup means something, ... we know certain things. I.e. because we see certain markup, we are allowed (licensed) to make certain inferences. and concluded that: the meaning of markup is the set of inferences it licenses. So understanding XML tags is key to correctly delimit complex text structures for further automated processing. This understanding may be compromised when tag names (elements, attributes and attribute values) are in a foreign language. The largest group of workers in our digital library is by far the proof-reading and markup team, comprised of about 40 persons. They are graduates from different humanities fields, none of them related to the English language. It is in this area where the necessity and importance of translating the original English markup into the local language (Spanish) is made evident. We learned from practice that using a tagset in a foreign language, compared to using a tagset in our own language, increases the learning time and reduces the quality and amount of digital text production, since tag names are mnemonics that may sound familiar to English speakers but are hard to understand and memorize by users of other languages. Giving our encoders the possibility of applying tags in Spanish has increased the amount and quality of digital text production. After successfully using XML-TEI for sometime, we embarked in the project of translating TEI element names, attribute names and attribute values to Spanish. Then we developed the translation tools to grant automatic conversion to and from the main TEI English core. These automatic conversion programs translate not only the markup of XML documents but also the corresponding DTDs. Then we repeated the experience with Catalan and we did some tests with French. Now we are in the process of building other TEI tagsets and translations for several other languages. The purpose is to have many official translations of the TEI tagset, but one core version (the original one). The automation of the language translation of the tags is vital to assure easy interchangeability of documents amongst projects using different languages. In this way, and from the structural and semantic point of view, the tagset is the same, only the names change. We also believe that having multilingual versions of a given tagset, like TEI, can facilitate its acceptance and use in many parts of the world like Latin America where the use of XML for electronic publishing is still uncommon. This may be of special interest for digital libraries and digital publishers worldwide, but specially within the European Union where multilingual projects can benefit in a remarkable way. Automatic generation of markup translators We started by defining the set of possible translations of element names, attribute names, and attribute values to the different target languages. We stored this information in an XML multilingual translation mapping document. An example of this document and its DTD follows: TRANSLATION MAPPING DOCUMENT FOR ENGLISH, SPANISH AND FRENCH (SAMPLE): This mapping document which contains all the necessary structural information to develop the language converters is read by the transformations generator, which was built as an XSLT script [3]. XSL can be used to process XML documents in order to produce other XML documents or a plain text document. As XSL stylesheets are XML, they can be generated as an XSL output. In this way, and for each of the languages contained in the multilingual translation mapping file, we produced both an English to local language XSL transformation and a local language to English XSL transformation. In this way we assured both ways convertibility for XML documents. For each target language we also generate a DTD or a Schema translator. In our first attempts, this took the form of a C++ and Lex parser (see figure 1). Then we changed the approach, and now we first convert the DTD to a W3C Schema, then translate the Schema to the local language, and finally we generate an equivalent translated DTD (see figure 2). This approach has the advantage of not using complex parsers (only XSLT) and also solves the translation of Schemas, which is an interesting goal in itself (see figure 3). We only considered a one way translation from the English DTD/Schema to a local language DTD/Schema, since we assumed that the DTD/Schema would be first built in the original language (English) and then translated to the local language. We saw no need to translate the local language DTD/Schema back to English (dashed line), but this is a transformation that could easily be generated if the need arises, allowing for maintenance and modifications to be done in the local language and then translated to English. Many other markup translators can be built to other languages in the same way, as shown by our tests with Catalan and French. Usage and implementation alternatives We think that markup in the local-language should only be used for tasks which require human intervention, like creation and maintenance of documents. For automated processing and document interchange we think it is more convenient to use markup in the language of the original standard. In this way, processing tools like stylesheets need not be translated to the local language, but the document translated to the original tagset instead. An alternative, and perhaps the most effective implementation of multilingual markup could be a translating interface integrated into an XML editor. In this way, we would have virtual views of the document with markup in different languages that could be toggled at the touch of a button, but without actually having to translate the document file. An implementation like this is possible today, but can only be done by the software companies who build XML editors. This built-in solution would not require the DTD/Schema to be translated. An editor like this would need to load the mapping information (tag-map), as well as the DTD and the document instance (see figure 4). A compromise solution that can be integrated into some XML editors by expert users is to build macros that automatically apply the translation to local language on opening the document, and the translation back to English on closing. This would not be as handy as a one-key language-toggling solution, but can be implemented by users. Additional macro programming would also be required for translation before validation and before applying further processing like XSLT. If multilingual markup becomes a common practice, the mapping structure with the name equivalences for markup translation could well be included as part of a new form of Schema. In any case, this use should be specified and formally integrated into the XML family of standards. Conclusions Are the advantages of using a general and widespread markup vocabulary like TEI lost?: Not at all. The two main advantages of using a general markup vocabulary like TEI are document interchangeability and community support (which includes training and tool sharing). Since markup terms can be very easily and automatically translated to the original TEI tagset, interchangeability is not lost and tools like XSLT scripts can still be used unchanged after markup translation. Training materials, however, may need to be translated or adapted, but this is not due to the use of multilingual markup but to the need of non-English-speaking encoders to have documentation in their language. In our experience, learning times were noticeably reduced. Production times were also reduced, along with an increase in markup quality. Encoders showed themselves satisfied and more confident in their task. By using markup in one's own language, the meaning of markup is not lost, and the document structure suddenly becomes more clear. Scholars and students showed approval for being able to handle documents with markup in the same language of the text. Cooperative multilingual projects may benefit from the possibility of easily translating the markup to each encoder's language. Sometimes new non-standard vocabularies are developed just because it seams comparatively easier than learning a standard vocabulary in a foreign language. Having the possibility of using a standard vocabulary in one's own language plays against developing a new custom vocabulary to fulfil a local markup requirement. This may help spread the use of XML vocabularies like TEI or DocBook in non-English speaking countries. Spreading the use of standard markup vocabularies is good for document interchangeability. Future work A special interest group on multilingual markup (TEI-MM-SIG) has been created within the TEI Consortium to exploit and expand the benefits of using multilingual markup. During its first meeting at the 2003 TEI annual meting, the idea, tools and possibilities of multilingual markup have been introduced, and the objectives of the group have been established. Some of them are: Translate all TEI mnemonics into different languages. This should be done by TEI users from different language zones, with interest in using markup in their own language. This is one of the main reasons to become a member of this SIG. Using the different sets of mnemonics, we should build an official repository of TEI terms, i.e. a multilingual TEI Term-Bank. The TEI META Workgroup will provide technical support for implementing this Term-Bank. Betatest the new term-sets and tools. This is another reason to join this group: to be the first to use this technology and provide feedback to improve it. Study the technical possibilities, limitations and challenges of multilingual markup. There are many aspects to be discussed and decisions yet to be made. To give an example, there may be problems to overcome if we want to build mnemonics using accented or oriental characters.",
        "article_title": "The Future of Markup is Multilingual",
        "authors": [
            {
                "given": "Alejandro",
                "family": "Bia",
                "affiliation": [
                    "University of Alicante"
                ]
            },
            {
                "given": "Manuel",
                "family": "Sanchez-Quero",
                "affiliation": [
                    "University of Alicante"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Libraries have long been perceived as the laboratory the workspace for the humanities scholar (Aboyade), providing access to a wealth of materials that are subsequently analyzed, critiqued and interpreted. Recent advances in technologies from computing to networking are redefining the humanities scholars workspace. Today novel techniques and innovative methodologies are providing new opportunities for cutting-edge humanities scholarship. How has computing changed humanities scholarship? What do humanities scholars need to pursue their research? Under the aegis of the Text Analysis Portal for Research (TAPoR) project (http://www.tapor.ca/), we are undertaking a Web-based survey to better understand the current needs of humanists. Rather than assume what all the needs of our community might be, we have sought the input of our colleagues (this survey builds on an earlier interview-based one with a small group of participants; see Fisher). In addition to helping us to formulate a vision for the development of the TAPoR portal, we believe that such as study is very likely to provide valuable insights of general interest into the current state of computer-based humanities scholarship, as well as provide crucial justification for pursuing grants to respond to specific issues. The survey was made available on November 10th and will be accessible until December 15th. It contains three sections: a) profile of the demographic and research profile, b) use of computing in teaching, and c) use of e-text and text analysis tools. While all participants completed the first part, only those who teach a course in the humanities completed part B and only those who use e-text and/or text analysis tools complete part C. As of November 15th, ninety-six scholars (half male and half female) from more than a dozen countries had responded to the survey. Three quarters were under the age of 45 and most were long term and frequent users of computers and the Web. These respondents came from a range of disciplines working in a range of genre (mostly prose) and primarily using textual material for their research (it should be noted that the survey was especially directed at text-based humanities scholars; this focus is something that subsequent surveys may wish to broaden). Over 80% use e-text and about half use text analysis tools. In general they believe that e-text are available for their use and expect to find them downloadable off the Web. They prefer to find them in a stable, legal form that is freely available from a reliable institution. In terms of mark-up, respondents appear to be a bipolar group with half expecting to acquire text with no mark-up and half with rich XML. In general, respondents believe that they need text analysis tools, although not complex tools, and are not happy with the tools that are currently available. Somewhat surprisingly, over 50% did not know about commonly available tools such as TACT, WordCruncher and Concordancer. The one most highly used was TACT but few found it useful. In addition to our list of about ten tools, participants added another two dozen tools that they employ in their work. These included tools such as the Wordsmith Tools as well as common Microsoft Office products such as Word and Access. We inquired about their collaboration and communication habits. Most use e-mail regularly and subscribe to listservs. But they tend to work as solitary scholars, rarely collaborating with their own graduate students and do not see the need for collaborating with other scholars. That said, they like to communicate with other scholars at various points in the research process. They share some of their materials, but tend not to share notes and tools, although they expect others to share tools. From our data we are developing a picture of the humanities scholar, how they do their research, including the text, tools and techniques they use; the type of collaboration and communication they practise; and their use secondary and primary resources. Our work will also involve a second phase that involves interviews with a select number of humanities scholars. In the final version of our paper, we will combine survey results with the interviews for a rich picture of the humanities scholar in the twenty-first century. Acknowledgments: the authors wish to thank University of Toronto Masters students, Natasha Flora and Imran Hasan who created the survey, and the University of Toronto for providing a small internal SSHRC grant to support the work. Many thanks are due as well to our colleagues who were willing to participate in the survey.",
        "article_title": "The Humanities Scholar in the Twenty-first Century: How Research is Done and What Support is Needed",
        "authors": [
            {
                "given": "Ray",
                "family": "Siemens",
                "affiliation": [
                    "Malaspina U-C"
                ]
            },
            {
                "given": "Elaine",
                "family": "Toms",
                "affiliation": [
                    "University of Toronto"
                ]
            },
            {
                "given": "Stfan",
                "family": "Sinclair",
                "affiliation": [
                    "University of Alberta"
                ]
            },
            {
                "given": "Geoffrey",
                "family": "Rockwell",
                "affiliation": [
                    "McMaster University"
                ]
            },
            {
                "given": "Lynne",
                "family": "Siemens",
                "affiliation": [
                    "Malaspina U-C"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " 1. Background This paper will present results from The Nomen Nescio project (NN) - a joint project for Named Entity Recognition (NER) for three Scandinavian languages: Norwegian, Swedish and Danish. The project includes research groups at The Universities of Oslo and Bergen (Norway), Gothenburg University (Sweden), Centre for Language Technology, and University of Southern Denmark (Denmark). It has been running for three years starting in 2001, and is funded by the Nordic Council of Ministers (The Language Technology Program, administred by NorFA). Before the Nomen Nescio started, only a couple of small NER projects had appeared in Scandinavia, and only for Swedish (Kokkinakis 2001, Dalianis & ÃstrÃ¶m 2001). The NN initialized joined activities in this important supporting technology field and a variety of people (students, engineers, researchers and professors) have been actively involved from the initial stages of the project. 2. Methods The aim of the Nomen Nescio project has been to develop NE recognisers for all the three Mainland Scandinavian languages â Norwegian, Swedish and Danish. Many methods for NER have been used for various languages over the years, and with the diverse background of the participants at the different NN sites, many of these methods have been used (rule-based methods, shallow parsing, statistical methods). While the variety of methods have some well-known advantages and disadvantages, what has proved to be really challenging has been the lack of resources for the different languages. For example, in order to train statistical taggers, an annotated training corpus has to be available in advance, in addition, it turned out that lexicons containing semantic information would be necessary, and again, these were hard to come by. However, none of the sites have used one method only. They have all developed hybrid ones, using at gazetteers in addition to the main method, and often also other kinds of pattern matching and various means of pre- and post-processing â inspired by work by Mikheev et al (1999). The diverse methods that have been used have made it very difficult to calculate and compare the degree of success for the different methods, languages and sites. However, even in the cases where the same methods have been employed, the results are difficult to compare, given that the basic resources, such as lexicons, differ. Even so, evaluation results will be given at the presentation. 3. Name Categories The well known Named Entity sets used in the limited target applications in the \"Message Understanding Conference\" exercises (MUC) recognized not more than seven types of named entities (Grishman and Sundheim, 1996), while in the \"Information Retrieval and Extraction Exercise\" project (IREX) (Sekine and Isahara, 2000) and in the Concerto project (Black et al., 2000), another kind of named entity, 'artifact', was added. While in the \"Automatic Context Extraction\" program (ACE, [EDT, 2000]), two new entities, 'geo-political entity' and 'facility', were added to pursue the generalization of the technology. The main idea behind the NN project's choice of categories was the assumption that our NER tools could be used for Internet search on the world wide web, and that it would be useful to separate names that are often ambiguous, such as song, book or film titles from actual geographical or person names (\"Paris, Texas\" â place or film? \"Angie\" â person or song? \"Harry Potter\" â person or book?). Moreover, in general language, as found in newspaper texts, many more kinds of \"names\" or \"named entities\" are likely to be encountered, and thus in the NN we have further elaborated on a finer taxonomy for names, with six main (person, organization, location, artifact/object, work/art and event) and a large number of subtypes. 4. Challenges for the Scandinavian languages Some of the Scandinavian languages have certain spelling conventions and certain linguistic features which present serious challenges to the task of actually recognising something as a name in the first instance, and to that of separating one name from an other. One challenging spelling convention amounts to names of organisations. All public institutions in Norway and Sweden have at most one initial capital letter, no matter how many words the name consists of (Den norske kirke 'The Norwegian Church, Svenska kyrkan 'The Swedish Church'). This causes a problem in recognising such names at the beginning of sentences, but also to find which words belong to a name at all. The methods we have used to solve this problem are gazetteers, regular expressions and the document centred approach (Mikheev 2000). Moreover, the Scandinavian languages are Verb Second languages, which means that if some constituent other than the subject fills the first position in a sentence, then the subject is demoted to the position after the verb, next to the object or indirect object (Idag sendte Anne Larsen et brev 'Today Anne sent Larsen a letter'). This leads to ambiguity with respect to finding the border between these two constituents. In order to solve this, the Norwegian NE CG recogniser has relied on syntactically tagged input. 5. Evaluation and Summary At the moment, we are on the process of evaluating the different systems, see the relevant sections on the NN project in Holmboe (2002, 2003). In the presentation, we plan to give the full results from all the sites and demonstrate the performance of the systems. We will also present some aspects of the work regarding the identification, classification and annotation of named-entities in the three languages and a detailed account for the named entity classification scheme. The project is described at this site: http://g3.spraakdata.gu.se/nn/, at which an online demo for the different languages can also be found. ",
        "article_title": "The Nomen Nescio Project - Scandinavian Named Entity Recognition",
        "authors": [
            {
                "given": "Janne",
                "family": "Bondi Johannessen",
                "affiliation": [
                    "University of Oslo"
                ]
            },
            {
                "given": "Eckhard",
                "family": "Bick",
                "affiliation": [
                    "University of Southern Denmark"
                ]
            },
            {
                "given": "Janne",
                "family": "Bondi Johannessen",
                "affiliation": [
                    "University of Oslo"
                ]
            },
            {
                "given": "Kristin",
                "family": "Hagen",
                "affiliation": [
                    "University of Oslo"
                ]
            },
            {
                "given": "Dorte",
                "family": "Haltrup",
                "affiliation": [
                    "Centre for Language Technology (CST), Copenhagen"
                ]
            },
            {
                "given": "Ãsne",
                "family": "Haaland",
                "affiliation": [
                    "University of Oslo"
                ]
            },
            {
                "given": "Andra",
                "family": "BjÃ¶rk JÃ³nsdottir",
                "affiliation": [
                    "University of Oslo"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Total of Phonostatistical Distances as Indicator of the Density of Language Taxons: Subgroup, Group, Family, Unity, Phylum, Union and Community. Language taxons (i.e. subgroups, groups, families, unities, phylum, unions and communities) were investigated from the point of view of their dispersion in the way it was first proposed in 1976 and then developed in the further articles and books of the author (Tambovtsev, 1976, 1986, 2003). We consider how compact this or that language taxon (i.e. subgroup, group, family, unity, etc.) on the basis of distribution of certain consonantal groups in the speech sound chain. Therefore, one can speak about a compact or disperse language family. If language taxon is compact, then its internal connections are shorter than its outer connections. Actually, the same notion of compact object is accepted in pattern recognition. The more compact the family is, the more correctly its languages are chosen. If we put in the family a language, which does not belong to the family, then the dispersion of the family rises, thus it becomes less compact. If the language has a similar sound chain, then the dispersion of the group remains the same or becomes less. It means that the family became more compact. In this case we speak about the typological properties of the families. We measure the dispersion of a family by the sum of dispersions of 8 phonostatistical features: frequency of occurrence of labial, front, palatal, velar, sonorant, occlusive, fricative and voiced consonants. It is important that the features do not intersect. The values of the coefficient of variance (V) and T coefficient show the degree of dispersion. The principle is the greater the dispersion, the less compact the family. We have chosen the coefficient of variance and T coefficient since they both keep to the law of commensurability. Comparing different languages of different language families and different morphological structures was possible since all of them have the same 8 phonetic features mentioned above. We have considered the dispersion of the following language taxons: Indo-European (V= 809.13%, T= 9.01; MV= 26.97%, MT= 0.30), Turkic (V= 150.16%, T= 1.66; MV= 18.77%, MT= 0.21), Mongolian (V= 86.24%%, T= 0.66; MV= 10.78%, MT= 0.08), Tungus-Manchurian (V= 148.79%, MT= 1.73; MV= 18.60%, MT= 0.22), Samoyedic (V= 128.04%, T= 1.10; MV= 18.29%; MT= 0.16), Finno-Ugric (V= 193.13%, T= 3.77; MV= 24.14%, MT= 0.47), Paleo-Asiatic (V= 271.14%, MT= 3.71; MV= 33.89%, MT= 0.46), Austronesian (V= 369.65%, T= 7.22; MV= 46.21%, MT= 0.90), Australian (V= 150.59%, MT= 3.83; MV= 21.51%, MT= 0.55), American Indian (V= 346.95%, T= 8.59; MV= 43.37%, MT= 1.07) language families and the Balkan language union (V= 237.89%, T= 2.90; MV= 2974%, MT= 0.36). Analysing the language families, one can come to the conclusion that the most compact is Mongolian (V=86.24%; T=0.66; MV= 10.78%, MT= 0.08), the least compact is Austronesian (V=46.21%; T=0.90). Tungus-Manchurian (V=17.41%; T=0.20) is more compact than Samoyedic (18.29%; T=0.16), Turkic (18.77%; T=0.21), Indo-European (V=28.00%; T=0.61) or Paleo-Asiatic (V=33.89%; T= 0.46). Language groups are more compact in general than language families. Iranian group (MV=13.21%; MT=0.09) of Indo-European language family is the most compact, the least compact is Romanian (MV=27.81%; MT=0.36). Slavonic group (MV=15.21%; MT=0.17) is more compact than Indo-Arien (MV=20.40%; MT=0.23) or Germanic (MV=24.51%; MT= 0.29). Volga group of the Finno-Ugric language family (MV=17.90%; MT=0.13) is more compact than Ugric (MV=27.66%; MT=0.47) or Finnic (MV=29.24%; MT=0.35) group. Altaic super-family is rather compact (MV=25.97%; MT=0.45). However, it is not much more compact than Indo-European (MV=26.97%; T=0.30) or Paleo-Asiatic family (V=33.89%; T=0.46). This fact may support those linguists who consider Altaic as a family, not a super language taxon than a family. Uralic super family taxon is less compact than Altaic (MV=28.31%; MT=0.47), though it is more compact than the Balkan (MV=29.74%; MT=0.36) language union (Sprachbund). Ural-Altaic language taxon (MV=30.98%; MT=0.88) is more (much more) compact than American Indian (MV= 43.37%, MT= 1.07) language taxon. Measuring the typological density of language taxons (subgroups, groups, families, unities, etc.) may help to understand how natural (or correct) these taxons are defined.",
        "article_title": "The Total Of The Phonostatistical Distances As An Indicator Of The Compactness Of The Language Taxons: Subgroup, Group, Family, Unity, Phylum, Union And Community.",
        "authors": [
            {
                "given": "Yuri",
                "family": "Tambovtsev",
                "affiliation": [
                    "Dept of English and Linguistics of KF-NP University"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Paul Caton has recently presented a sustained critique of the claim that certain activities within the text encoding community constitute the evolution of a body of theory about text. According to Caton reflection on markup practice has not lead to theory at all, let alone a Lakatosian progressive research programme, but only to principled practice. Caton's allows that text encoding should be theorized, but he believes that this has not yet happened and that those who believe otherwise are confused about both what sort of theory is possible and what sort is needed. The proposed paper will respond directly to these important criticisms. Specifically it will argues that Caton assumes a false dichotomy between empirical science on one hand, and so-called \"critical theory\" on the other and therefore fails to see that text encoding theory is not failed empirical science, but rather successful \"formal science.\" The Original Claims to Theory Caton takes his texts from two articles. One by Allen Renear, \"Out of Praxis: Three (Meta)Theories of Textuality,\" Kathryn Sutherland (Ed.). Electronic Text: Investigations in Method and Theory (pp. 107-126). Oxford 1997, and one by Renear and Elli Mylonas, \"The Text Encoding Initiative at 10: Not Just an Interchange Format Anymore -- But a New Research Community\" the guest editor's introduction to the TEI anniversary issue of Computers and the Humanities (1999). In \"Out of Praxis\" Renear writes (and Caton quotes): [the text encoding community] ... has evolved a rich body of illuminating theory about the nature of text -- theory that is useful not only to anyone who would create, manage, or use electronic texts, but also to anyone who would, more generally, understand electronic textuality from a theoretical perspective ... the significance of this body of theory and analysis extends well beyond the specific concerns of text processing and text encoding and contributes directly to our general understanding of the deepest issues of textuality and textual communication in general. (Renear, \"Out of Praxis.\") And Caton notes that in \"The TEI at 10\" Renear and Mylonas, after reiterating similar claims, compare this theoretical activity to a Lakatosian \"research programme\" and intimate that it is a \"progressive\" research programme, in Lakatos's sense, exhibiting productive evolution of new theories of increasing explanatory power. Caton's Criticisms Caton begins by analyzing Lakatos's concept of a progressive research program in more detail and develops an argument that the notion of a research program is inapplicable to text encoding theorizing and in any case gives a negative verdict on \"progressiveness.\" Caton notes Lakatos's claim that new theories evolving within a progressive research programme must predict or explain new facts, as well as retain the core assertions. Renear had suggested that ongoing evolution of the OHCO thesis corresponded to this sort of progressiveness, responding to counterexamples (such as the various kinds of non-hierarchical structures), with a natural satisfying explanatory evolution. But Caton points out that Lakatos claims that a truly progressive research programme must predict or establish \"stunning novel facts,\" such as an unexpected astronomical event. Not only does there not seem to be any such predictions forthcoming from text encoding theory, but Caton sees a contradictory theme in the methodology of much of encoding theorizing, which emphasizes adjusting our theoretical claims to our actual intuitions (\"common sense\") about texts -- how could such adjustments ever lead to a \"stunning new fact\"? Caton concludes: In itself, the fact that text encoding's home-grown theory fails to be strictly scientific according to the very criteria it invokes is not greatly significant. Who would think of markup as a science anyway? ... these are all symptoms of a confusion about theory and practice. ... In Renear's version of text encoding history, cumulative practice in representing text leads to experience and then to reflection upon that experience and ultimately to, if not knowledge, then at least theory. I argue, however, that practice has instead led simply to principle. (Caton ACH 2003) And he adds to this account a further deflationary argument to the conclusion that what passes as text encoding theory is in fact \"semi-formal description,\" not explanation. It is arguable that there is anything unexplained about text, but if for a moment we assume complete ignorance and ask of theoreticians \"why are texts the way they are?\" we immediately beg the rejoinder \"you tell us first how they are, then we can hypothesize about why they are that way\". In other words, we need the observation first: we have to define what we wish to explain. Looking at it this way we see that OHCOs 1, 2, and 3, for example, are not hypotheses at all, but attempts at semi-formal description of a phenomenon ... Renear's problems with the word \"theory\" stem from his conflation of multiple signifieds. One signified -- the \"scientific\" one, if you like -- points to theory as speculative propositions, offering answers to questions that arise when we realize what we don't know: why does an apple fall to earth? Why do the planets move in the paths they do? The other points to theory as a set of rules or principles that underpin a particular human practice: a theory of poesie, for example. Caton sees Renear and others as hankering after an empirical science of text that would satisfy their positivist inclinations and their aversion to theory of the literary or critical variety; which is the theorizing that Caton believes text encoding needs. Analysis Caton's critique is brilliant and illuminating, and, ironically, a contribution to text encoding theory. But on almost every important point he is wrong. In our paper we will argue that the fundamental error in Caton's approach is his failure to recognize that text encoding theory, as presented in Renear's examples, is for the most part a formal science, in the sense elaborated by the linguist Jerrold Katz (1981, 1997). In this respect text encoding theorizing is like theorizing in computer science, some parts of linguistics, philosophy, and mathematics, and unlike theorizing in astronomy, sociology, and chemistry. Although the nature of formal science is different from empirical science, there is no reason at all to deny the word, or the concept, theory, to the theories of the formal sciences. Consider an example from linguistics, speech act \"theory\". It is a fact that results in speech act theory provide a sense of new understanding and illumination characteristic of science in the most general sense and that contributions to speech act theory often proceed by, at least in some cases, the familiar process of conjecture and refutation that seems constitutive of science. And although some of the content of speech act theory is empirical, and it is certainly intrinsically involved in much actual empirical theorizing in related purely empirical areas of linguistics, at least much of speech act theory is not empirical at all: consider for instance the specific analyses as carried out by Austin, Grice, Searle, or Bach and Harnish -- in every case the method is non-empirical. It is true that we would not normally say the theory \"explains\", e.g. promising, (and it doesn't, because that particular idiom of explanation is empirical) but speech act theory does \"explain how promising works.\" Does the original theory (say Austin's account), or any of its subsequent improvements predict \"stunning new facts\"? The problem here is both that the role of \"stunning new facts\" in empirical science is exaggerated by Lakatos, and the prospects of \"stunning new facts\" in formal science minimized by Caton. On the one hand much empirical science is incremental improvement, and on the other formal science can provide a surprise: Russell's set paradox for instance, or Godel's incompleteness results, or Cantor's transfinite cardinals. Caton considers Renear's claim to a \"common sense view\" as a decisive indication that no \"stunning new facts\" will be forthcoming. What is true is that formal science is concerned with systematizing and adjusting our formal understanding of how the world. Stunning new facts will not come in the form of a comet, but in the form of paradox, whether that means an actually antinomy or simply the unexpected formal result. Consider Gettier's famous counterexample to the JTB theory of knowledge. In retrospect we might be tempted to say that this can't possibly be either stunning or new as it is simply an unpacking of concepts every ordinary person has; but yet it was stunning, and it was new. Other examples from linguistics, philosophy, mathematics, computer science, can be usefully developed and we will do so in the final paper. In every case there are non-empirical theories that illuminate and, in a sense, explain phenomena, and that are also closely, and perhaps confusingly, connected with empirical explanations as well, playing supportive roles. We note that in the end our result is surprisingly irenic. Caton wants to secure an open field for the application of literary theory and critical theory to text encoding phenomena -- no part of our rejoinder is inconsistent with that agenda. In addition, we believe that this adaption of the notion of a \"formal science\" throws some new light on the old debate over the nature of humanities computing.",
        "article_title": "Theory Restored",
        "authors": [
            {
                "given": "Allen",
                "family": "Renear",
                "affiliation": [
                    "University of Illinois at Urbana-Champaign"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The latest version of the international character encoding standard Unicode, Unicode 4.0, contains over 96,000 characters. This represents a remarkable achievement, for it expands the number of scripts (and the languages using those scripts) that can be represented more fully on the Web, in email, and generally for the electronic transfer of texts. Fortunately, Unicode is increasingly supported in software and fonts. The result of the widespread adoption of Unicode is that text materials for many modern and historical languages are now more widely accessible and capable of being transmitted, without requiring fonts with proprietary (non-standard) encodings. This talk provides an overview of outstanding issues related to character encoding and Unicode in multilingual text projects as observed by the Script Encoding Initiative project at UC Berkeley. It will conclude with a wish-list of ways in which members of the academy can help more fully. The Script Encoding Initiative (SEI) was established at UC Berkeley in 2002 in order to relay various unmet needs of academic multilingual electronic text projects to the character encoding standards bodies and to promote and explain Unicode to scholars. SEI has four objectives. One goal is to give a presence and voice to the academic point of view (as opposed to that of computer companies) within the character encoding standards process, particularly to speak up for the encoding of historic and minority scripts at the Unicode Technical Committee meetings. Currently no university is a full member of the Unicode Consortium; Columbia University and Tamil Virtual University are the only Associate Members, but they don't regularly attend the Unicode Technical Committee meetings. The second goal of this project is to encourage the participation of scholars and other users so scripts missing from Unicode are proposed. The number of missing scripts is still over 90, with approximately one third being modern minority scripts, and two-thirds being historic scripts. The active participation of scholars is critical: proposals need the specialized input from experts in order to arrive at a proposal that covers their needs and is complete. The outstanding scripts are often less-well known, so the task is to locate scholars, explain the standards process, and make clear the need to encode these scripts in the international standard Unicode is a necessity. The third goal is to promote Unicode and an understanding of what it covers -- and what it does not -- more generally amongst academic groups. With the increasing adoption of XML for the Web, Unicode is playing a more prominent role in electronic text projects because it is the default character encoding standard for XML. Text projects now have to grapple with specific problems head-on when dealing with Unicode: How can one include a character (/script) that is not yet in Unicode? How should variants of characters be handled? Given a choice of several Unicode characters, which should be used? How does the font or markup come into play vis-Ã -vis character encoding? Although the revision of the TEI Guidelines (P5) will address some of these issues, it has not yet been published. SEI was established to help provide guidance in these areas, at least from the Unicode perspective. The fourth objective of the Script Encoding Initiative is to raise funds for the writing of Unicode proposals by veteran Unicode proposal authors, scholars, and graduate students, and for font designers to work on the creation of free Unicode fonts. To date, work on Unicode proposals has been largely a volunteer effort, with little financial backing. In order to assure that scripts are proposed in a timely way, funding is needed, otherwise the process will drag on for years to come. A call for donations for script proposal authoring over a number of email lists (i.e., Ancient Near East list and the Unicode email list) has resulted in a small number of donations being received. An application to NEH has been submitted by this project, and additional grant-writing is expected. More remarkable, however, is that no funding or basic support from any university for this project -- or for Unicode proposal work in general -- has been received. The cause may be -- in part -- due to the economic situation in the U.S., and California in particular. However, there appears to be a fundamental disconnect on why the university should be involved in standards work, the one place where the lesser-known scripts are regularly studied. While funding and interest in online text projects (for pedagogical and historical preservation, as well as general communication capabilities for modern language communities) has drawn attention and received funding, script encoding still takes a back seat, though it provides the standard upon which all multilingual text projects should be based. At a time when multilingual capabilities are being touted, attention to the encoding of the outstanding scripts (and missing characters) is needed. A number of general recommendations will conclude this talk: Scholars should work closely with others in their field to arrive at a \"best practices\" set of guidelines on character encoding, realizing that Unicode will not encode variants or precomposed characters, and that Unicode is not intended as a means to capture paleographic details. The guidelines should be posted on a publicly accessible website. Whatever method to cover missing characters is used in a project, document the use fully. If the PUA is used, plan on how conversion to Unicode will be implemented if/when characters are accepted into Unicode. Work with the Unicode Technical Committee (through SEI, if desired) on characters or scripts that are needed but missing from Unicode (cf. http://linguistics.berkeley.edu/~dwanders/alpha-script-list.html). Promote the participation of fellow scholars in the Unicode proposal review process. A list of currently proposed scripts that need comments is posted at: http://linguistics.berkeley.edu/~dwanders/ScriptsNeedInput.html Advocate greater participation and funding from universities (and governments) for Unicode script encoding. ",
        "article_title": "Unicode in Multilingual Text Projects: A Status Report from the Script Encoding Initiative, UC Berkeley",
        "authors": [
            {
                "given": "Deborah",
                "family": "Anderson",
                "affiliation": [
                    "Dept. of Linguistics, UC Berkeley"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Since the release of XML 1.0 in 1998, the academic world, along with the business and scientific worlds, was introduced to a new type of data storage that would overcome the problems of database management systems (interoperability), HTML (lack of description) and SGML (complexity). For our community the result was the ability for scholars to encode their documents in a way that was platform and application independent and allowed for very rich, descriptive markup. Following suit, the TEI released P4 of the TEI guidelines which implemented \"proper XML support\". XML, and TEI P4 have proved remarkably useful in encoding documents and, as was evidenced in the last two TEI meetings (Chicago and Nancy), the interest in XML and the TEI is growing. The one stumbling block that many users of XML and the TEI face, however, is the lack of tools to query, process, and display their XML-encoded documents on the Web. Quite simply, users wish to query their XML documents with the ease of a DBMS and display them on the Web with the ease of HTML. It is not impossible to do these two things now, but the learning curve to do either is quite steep and the tools are nowhere near as robust as they need to be for large projects. On the display side, things are getting easier. XSLT has become established as the language to convert XML to HTML for display. XSLT processors are still a bit difficult to use but once mastered the conversion process is quite easy. Querying is a different story. The W3C is working on the XQuery language and there are a few native XML databases, such as eXist, which are promising, but XQuery is far from becoming a useful language, and eXist, though useful, has proved very slow when querying large number of files. This climate has resulted in those in the community interested in querying and displaying XML documents cobbling together their own systems. Two such promising ones within the community are Peter Robinson's Anastasia and Mark Olsen's PhiloLogic. I've put together a very simple publication package, too, that I feel addresses the needs of many in the community: price, performance, and development time. I would not suggest that this package is THE solution for querying and publishing XML on the web, and in the future I hope to use Xquery or a native XML database when the technology is robust, but, for the time being, this is a very affordable, powerful, and easy to learn solution. The system I have developed uses Apache, PHP, MySQL, and XSLT to query and publish XML documents on the web. It was developed for the web publication of The Public Writings of Margaret Sanger in the department of history at New York University. This online edition is part of a much larger endeavor, The Margaret Sanger Papers Project, which includes an already completed microfilm edition of over 9,000 of Sanger's documents, plus a four-volume book edition of Sanger's papers, the first volume of which has been published with the title The Woman Rebel, 1900-1928. The Sanger documents are TEI encoded and use the Model Editions Partnership DTD. The backend of this system is a MySQL database. The database fields are populated by running a PHP script that parses XML files using the Expat parser. It captures the data of certain elements and inputs this data into a corresponding field in the database. The script for the Sanger project captures title, publication date, document type, category, and body. The ability to parse XML is built in to standard compilations of PHP. This MySQL database is queried by the end user to find appropriate documents in the Sanger collection through web front end written in HTML. PHP is used as the middleware to talk to the MySQL database. For the Sanger project, users can search by full text, title, date range, document type, and category. After performing a search, a user is presented with a list of documents in the web browser that match the criteria of the search. To view a document, he or she clicks on the appropriate link. The document is presented to the user as HTML that is generated from the XML files (not the database) on-the-fly using PHP, XSLT, and Sablotron, an XSLT processor. This system meets the three criteria mentioned earlier: price, performance, and development time. Price: All of the tools used in this system are free and open source (Apache, MySQL, PHP, XSLT, Sablotron). Most are installed with any standard Unix/Linux distribution and all can be installed on Windows. Performance: This system is very efficient. The PHP script that parses the XML and inputs data into the database runs very quickly. It can be rerun on selected XML files if changes are made and can update the data in the database when needed. This data is immediately available for searching in the MySQL database. The XML files are unchanged when parsed by the PHP script and are the same XML files that processed by Sablotron to output HTML for the end users. The queries of the MySQL database are extremely fast and the on-the-fly XSLT transformations range from a few seconds to instantaneous. Development time: This system requires knowledge of XSLT, PHP, and SQL. Of the three, XSLT is the most difficult to learn but must be learned for any system publishing XML files. SQL and PHP are much easier to learn. Since MySQL, PHP, and Apache are proven technologies that are preinstalled on most Linux and Unix distributions, there is little configuration or installation to be done to make this system work. The Sablotron XSLT processor needs to be installed and PHP needs to be reconfigured with the XSLT processor extension. But when that is finished, the user has all the tools needed to query and publish XML. This paper is not theoretical, but is intended to be more than just a software demonstration. The issue of how to process and deliver XML-encoded documents over the web is an important issue in our community and one that warrants serious attention. Though there is ample support for encoding issues through the ALLC, ACH and TEI, many in our community are less confident when it comes to processing their encoded documents. Many of the tools available today such as Cocoon and eXist are still out of reach to Humanists because of the instability and complexity of the tools. And many commercial products are out of reach because the their price. I hope this paper will give an overview of the issues involved in querying and displaying XML-encoded documents that we hope to resolve in the next years and I hope this paper would enable those with small budgets and tight schedules to produce a robust XML publishing with standard open source tools available on most Linux/Unix distributions.",
        "article_title": "Using AMP technology (Apache, MySQL, PHP) for XML publication.",
        "authors": [
            {
                "given": "Matthew",
                "family": "Zimmerman",
                "affiliation": [
                    "New York University"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The Project: Electronic Boethius (http://beowulf.engl.uky.edu/~kiernan/eBoethius/inlad.htm) In October of 1731, a fire swept through the library of the aptly-named Ashburnham house in London, destroying and damaging many of the holdings originally collected by Sir Robert Cotton in the early seventeenth century. Among the artifacts affected by the fire was a tenth-century manuscript, the only known copy of an Old English dual verse and prose translation of the sixth-century Consolation of Philosophy. The Consolation had a rich history throughout the Middle Ages. It was first translated into English by King Alfred the Great, who ruled Anglo-Saxon England from 871-899 and was responsible for the translation of several important Latin texts during his reign. Chaucer made his own translation of the Consolation, as did Queen Elizabeth I. It was Alfred's translation that was damaged in the Ashburnham House fire. Although this tenth-century manuscript (now held by the British Library, Cotton Otho A. vi) was damaged to the point where it was nearly illegible, there are several reasons why the destruction was not a total loss for scholarship. There is one other complete manuscript holding Alfred's translation of Consolation, the Oxford, Bodleian Library MS Bodley 180, although it is a later twelfth-century, prose-only version of the text. More remarkable is that in the seventeenth century, Dutch antiquarian and scholar Francis Junius made a copy of Bodley 180 and collated it with the yet undamaged Otho A. vi, preserving the readings that are different between the two manuscripts. Up until now, this transcript and collation (Bodleian Library MS Junius 12) has been the only source for those parts of Otho A. vi that are damaged to the point of illegibility. There are editions of the Old English Consolation that have been published since the fire, but notably of the two standard editions (both based on the tenth-century manuscript), one edition (Sedgefield's 1899 edition of King Alfred's Anglo-Saxon Version of Boethius, de Consolatione Philosophiae) is entirely in prose, while the other (Krapp's 1932 edition of the Meters of Boethius) is entirely in verse. In recent years, technological advancements have enabled access to Otho A. vi that would have been unimaginable in 1731. Using ultraviolet light and high-resolution digital photography, we are now able to read portions of text that have not been accessible for nearly 270 years. Taking advantage of these effectively new texts, with the support of the National Endowment for the Humanities and the Mellon Foundation we are creating an image-based electronic edition of the Consolation, using Otho A. vi as our base text. Understandably, the final edition will consist of a variety of files including: Daylight and UV images of Otho A. vi Daylight images of Bodley 180 and Junius 12 The prose and verse text of Otho A. vi Supplemental text from Bodley 180, Junius 12, and earlier editions, for those portions of text that are missing or still illegible Pervasive and complex XML encoding that describes many different aspects of the text and manuscript including: Basic codicological features (quires, folios, lines) Basic textual divisions (prose and verse sections, verse and prose lines, half-lines, words) Editorial emendations Metrical information Paleographic descriptions of individual letters Description of the condition of the damaged manuscript Search Tool: for searching XML encoding and text Metadata Requirements for the Edition Because the final edition will be very complex, we wanted to develop a schema for metadata documentation that would simplify the project and not make it more complicated than it is already. There were three issues in particular that we needed to address before deciding on an approach to metadata documentation. Granularity: to what level should the edition be described? In many ways, the question of granularity was the most important, and most difficult, issue to address. On one hand is a MARC record, a relatively simple description of the edition as a whole, including only basic bibliographical information. Though simple, this approach was just not realistic given the complexity and fragility of the edition's contents. On the other hand is a completely pervasive approach, with a TEI header for every text and technical data for every image file. We finally decided to err on the side of caution and provide metadata for every text and image file, but to also provide higher-level description as well. Intellectual Rights Because various elements of the Electronic Boethius are controlled by different parties (the manuscript images, for example, belong to their respective libraries), we wanted to be able to describe clearly which portions of the edition belonged to whom. Organization and Software Given the complexity of the elements of the edition and their relationships with one another -- multiple versions of folio images; folio images in relation to text and markup -- we wanted to be able to express these relationships somehow in the metadata. When someone views the metadata file, it should be clear what files are in the edition, where they are in relation to the other files, and how they all interact. Since we also have some interactive software included in the edition, we wanted to be able to describe that as well. Once we made our decisions concerning these issues, we decided to use the Metadata Encoding & Transmission Standard (METS) to incorporate the metadata we need to describe the elements of the edition, and the edition as a whole. Although METS was almost exclusively designed for implementation by libraries, archives, and digital resource repositories, and not by individual scholars, we decided that since it addresses all our documentation concerns, we would use it to document the Electronic Boethius. This presentation will illustrate how we are using METS to document metadata for the image-based electronic edition. We will visit the various metadata schemas that we are using to describe the elements of the edition: for text, the Schema for Technical Metadata for Text (http://dlib.nyu.edu/METS/textmd.htm); for image technical metadata, NISO Metadata for Images in XML Schema (http://www.loc.gov/standards/mix/); for descriptive metadata, the Metadata Object Description Schema: http://www.loc.gov/standards/mods/; for intellectual rights information, the Schema for Rights Declaration (http://www.loc.gov/standards/rights/METSRights.xsd). We will give examples of how we are taking advantage of METS' file section and structural map sections to describe how the elements relate to one another We will give examples of how we are using the behavior section to describe our delivery software and how it relates to the files within the edition. We hope to use METS to document the Electronic Boethius as completely and thoroughly as possible. If our original files were to suffer a second Ashburnham House, our metadata should be complete enough to rebuild the edition -- as we are re-creating the original manuscript.",
        "article_title": "Using METS to Document Metadata for Image-Based Electronic Editions",
        "authors": [
            {
                "given": "Dorothy",
                "family": "Porter",
                "affiliation": null
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "What makes the best technology for \"presentations\" is a delicate and politically sensitive question. It even receives occasional attention in the media, as schools adopt presentations technologies and as commentators debate their merits. Recently (May 2003) it has gotten special scrutiny from the guru of visual information design, Edward Tufte 1, who has written a diatribe directed specifically at Microsoft PowerPoint but largely applicable to presentations software in general: The Cognitive Style of PowerPoint [Tufte 2003]. 2 This reaction comes only when entire worlds (and not just in the academy, but in government, business, and primary/secondary education as well) have seemingly come to be dominated by PowerPoint; more even than other computer media, presentations packages seem to promise easy \"dazzle\" a seduction we can all recognize. But as its users know (and as Tufte details), Powerpoint comes with its weaknesses and shortcomings as well as its strengths. These include not only features of its design, but in externals as well, such as its cost and its closed data format. There exist several options if one wants to forsake Powerpoint or one of its analogues (Corel WordPerfect Presentations, the new Keynote for the Macintosh, etc.) for sequences of HTML pages, which (especially with the help of CSS) can also make perfectly respectable \"slides\" for presentation; this approach works at least as well as the proprietary packages, and sometimes better, especially when such pages are generated from a stronger authoring format, such as TEI or a custom tag set. 3 But an extremely tempting ultimately, it would seem, the ideal target format is not HTML (which does bullet points readily enough, but whose visual capabilities are limited) but SVG. Scalable Vector Graphics, a W3C Recommendation 4 is a general-purpose declarative tag set specifically designed for describing the rendition of vector graphics, which has already made an impact in both interactive viewers, and print; it is also well-suited for projection and classroom group-interactive work. Several XML to SVG presentations packages exist, the handiwork of various XML pioneers. A discursive series of lists of bullet points is actually a fairly challenging target for SVG, which is not designed to provide for typographic layout features such as line spacing or word wrapping. 5 Nonetheless, by means of various workarounds these problems can be dealt with, and these packages demonstrate how even rudimentary XML can be used effectively to get quite tasteful results [Mulberry 2002]. But if bullet points are what is wanted, HTML is better; SVG's main advantage is that as a native graphics format and, not incidentally, one specifically engineered to scale graphics it also has the potential to take us far, far beyond Powerpoint. Several attached screenshots and SVG files show how SVG can be applied to reinvent the application of \"slideshow\" or projection technologies to the classroom. These images and interfaces (since some of them have dynamic controls to be used in real time, and/or are \"browsable\") are assembled through a combination of routines in the form of an XSLT transformation, operating over source XML files, to create either a standalone SVG image, or a linked sequence of images, calling in libraries of jpegs and SVGs to include as components. 6 These are productions of a larger or looser library of XSLT routines (developed by the author) whose function is to automate the creation of SVG output from mixed text and image input. These XSLT templates constitute a set of primitives (think of it as a library of classes or subroutines), performing various functions such as drawing boxes, sequencing, lining up and arranging text, and so forth. These templates can then be called from others, assigned to match element types. An XML source document is provided with markup that is completely prospective, tied (like HTML but not like a \"descriptive\" tag set such as TEI) to a particular output format there is not even a DTD to validate against, only the processing stylesheet itself to run, and output to inspect but declarative, designed to minimize or eliminate manual labor and leave only those adjustments to be made that cannot be made by the machine unaided. One of these applications is being used in a professional context by Mr. Jim Surkamp, an associate of the author's, whose lectures on the United States Constitutional Convention of 1787 (presented at the US Government's Eastern Management Development Center 7) have provided a perfect occasion for the development and application of a truly dynamic classroom use. The aim here was to provide visuals which should be supporting players, not the center or \"star\" of the lecture. 8 Any projected visuals could be drawn in to provide context; but they should not have to encapsulate the presentation's content on the contrary, if we assume the lecturer knows his material, to repeat it on screen would be redundant at best, at worst an interference. In Surkamp's case, the hour and a half (or sometimes more, at the participants' demand) is conducted in a role-playing mode, with participants assuming the task of representing the interests of the different States at the Convention. Attention is focused not on Surkamp, nor even on the participants themselves, but on the issues faced by the Delegates of 1787. In this context, visuals must be supportive but subdued, working fluidly, not imposing a schedule or sequence; and they should degrade gracefully, allowing for tangents and distractions, or even for the presenter to set them aside and do completely without them. These goals were achieved by a single SVG image. 9 (A photograph of Mr Surkamp in front of the display appears below. That there is only a single image here used in a presentation that goes well into a second hour does not mean that it is simple or that little information is conveyed by it.) Scripting was added in order to provide pre-set zoom positions to show, in detail, a set of images that can be traversed as the presenter chooses. A simple \"one-click\" user interface allows the presenter to bring in or hide content, or zoom in and out, at will, using the dynamic capabilities of the digital medium to serve directly the purposes of focusing attention and capturing the viewers' imaginations rather than being merely gratuitous and distracting, like PowerPoint's \"fly-ins\". A great stress is placed on the detail of images that scale on the screen, so inset images may be scrutinized more closely than would be possible using the limited graphics support of typical presentations packages. Since authoring does not occur directly in SVG, but rather in a simplified custom XML tag set, which distills to its essence the problem of arranging and laying out the content, even an SVG amateur can create these (as is, in fact, the case here). The presenter then runs the source document (ahead of time, in a batch process) through a stylesheet that combines some of the library primitives described above with specialized code for this application. The SVG thus created is more like what comes out of a \"mill\", than anything hand-crafted. But this milling allows for artistic composition of elements at a higher level. The XML source thus serves as a metalanguage with respect to the SVG display, while being strictly \"presentational\" (though high-level and declarative) with respect to the content of the presentation. Its presentational aspect is justifiable when we reflect that its dependence on a particular implementation the stylesheet that goes with it is (far from being a limitation and a design flaw) part of the idea: this metalanguage is not a work of theory, but a practical application. Having developed and demonstrated this technology, the author has found it helpful himself for his presentations and lectures. ",
        "article_title": "Way Beyond Powerpoint: XML-driven SVG for Presentations",
        "authors": [
            {
                "given": "Wendell",
                "family": "Piez",
                "affiliation": [
                    "Mulberry Technologies, Inc."
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The use of digital archives and material collections promises to significantly enhance the way we teach subjects in the humanities. Especially in culture and literary studies, access to a wide array of original source documents can provide students with an unprecedented source for their studies, allowing them to analyze documents within a larger cultural, historical, social, and political context. Whereas most digital archives in the humanities focus primarily on the access to materials, MetaMedia encourages students to work with these materials creatively. MIT's MetaMedia combines an open standards-based digital repository framework with highly flexible annotation, collaboration, sharing, reconfiguration, and upload features. To date, MetaMedia contains mini-archives ranging from Arab Oral Epic, Modern American Dance, and Early American Comics to cross-cultural foreign language projects and the Declarations of Independence. The presentation will discuss MetaMedia's pedagogical concepts, implementation of open standards, its software architecture, and use in the classroom. MetaMedia, developed in the School of Humanities, Arts, and Social Sciences at the Massachusetts Institute of Technology, provides students and faculty with a flexible online environment to create, annotate and share media-rich documents for the teaching and learning of core humanistic subjects. Through collections of media documents pertinent to a specific topic or field, MetaMedia adds the dimension of active archive use and thus considerably extends the notion of multimedia archives. Based on open standards, the MetaMedia framework ensures interoperability with a wide range of current media resources such as text, image, audio, video documents, as well as future media formats. MetaMedia offers a flexible software framework with which users can annotate, share, reconfigure, and contribute media-rich documents. These documents can be organized in the form of highly flexible collections, providing the basis for new modes of classroom discussion, on-line collaboration, multimedia presentation and essay writing, as well as other educational and scholarly activities appropriate to the humanities. These collections combine photographs, text documents, recorded sounds, and moving images to provide a meaningful repository for teaching topics central to the Humanities curriculum. Materials collected and annotated by scholars retain their own, pristine identity, while teachers are able to draw on these resources in classroom instruction and distance education settings; students are able to draw on the same resources as they research, create, and collaborate on multi-media essays or presentations. The result is improved skill at communicating effectively in today's increasingly global world of education and business. Specific learning outcomes include enhanced ability to: Think across traditional boundaries between disciplines See connections across multiple disciplines Build and share knowledge through collaboration Look at problems from unfamiliar perspectives Express their ideas more vividly Collaborate more effectively Understand not just what we know, but how we know it These outcomes, in turn, more generally improve students' abilities to: Think critically Communicate effectively Develop a deeper understanding of complex problems Build learner and instructor communities Architecture: MetaMedia relies on a markup-oriented strategy based on standard formats used throughout the Humanities Computing community. Storing content and associated metadata in Dublin Core, Text Encoding Initiative, SCORM, and MPEG formats presents a number of advantages over older designs based on static web pages or ad-hoc data files. Storing markup in standard formats will allow MetaMedia to separate media content cleanly and simply from its presentation in a given user interface. Separating content and presentation extends each project's potential lifetime, as markup can be output in XML and migrated to new platforms as old ones become obsolete. Supporting open markup standards allows related groups in Humanities Computing to exchange media and annotations, thus fostering collaboration within and across academic institutions. Storing content in rich markup formats from the start allows projects to grow into more sophisticated functionality without starting from scratch. For these and other reasons, digital archiving projects have adopted SGML- and XML-based markup as the de-facto standard. However, MetaMedia expands on traditional practice in two regions: collaboration and media annotation. The MetaMedia framework implements a permissions and authentication management system that allows users to add and share content, thus fostering constructivist models of learning and research rather than those based on simple knowledge diffusion. Also, it can make multi-tier annotations to media files using established markup formats and transition to more fine-grained annotations as multimedia markup formats mature. Tools currently under development include the annotation of parts of large-scale images, multi-level annotation of video and audio files, and a comprehensive multimedia essay tool.",
        "article_title": "Writing with Media Reconfigurable Digital Archives in the Humanities",
        "authors": [
            {
                "given": "Kurt",
                "family": "Fendt",
                "affiliation": [
                    "MIT"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Since the beginning of the use of hierarchical Markup Languages, like SGML and XML, for marking up texts, the phenomena of overlapping structures has become a major concern, because the rather constricted tree structure of these languages does not permit the presentation of overlapping structures. Contrary the OHCO-thesis 1 it is generally accepted, that texts are no ordered hierarchy of content objects\", but rather can have multiple hierarchies or lack of any transparent hierarchy. So the problem of overlapping structures can be distinguished between either multiple concurrent hierarchies that overlap each other or overlapping structures without any hierarchies. In addition to the traditional attempts to solve this problem, during the Extreme Markup Languages 2002\" conference 2, two more, interesting options were introduced. The first option are the Just In Time Trees\" (JITTs) 3 developed by Patrick Durusau and Matthew O'Donnell. The second proposal is a Markup Language explicitly developed for marking up overlapping structures in one document, the Layered Markup and Annotation Language\" (LMNL) 4, invented by Jeni Tennison, Wendell Piez and Gavin Thomas Nicol. Especially the second approach is interesting for documents containing overlapping structures without any hierarchies, but it can also be used to markup concurrent hierarchies in one document. This paper tries to give a short summary and comparison of the known methods of resolution and attempts to outline a solution by combining XML with a non XML data model. In this case it concerns the LMNL data model, which is based on Core Range Algebra\" (CRA) 5 and Attributed Range Algebra\" (ARA) 6. At this time the proposals for possible solutions can be divided into three categories: SGML/XML based solution, i.e. milestones\", fragmentation and references. Alternative Markup Languages with different data models and different syntax, i.e. MECS 7, TexMECS 8 or LMNL. Relocating the assertion of a tree for each hierarchy from the markup to the processing of a document, as the JITTs technology suggests. Traditional Solutions Several possible ways to solve this problem by using traditional hierarchical Markup Languages exist. Solutions like CONCUR, a feature of SGML, milestones\" from the TEI-Guidelines, fragmentation, references, like XPointer, or separate annotationsi 9 are all useful, but do not provide a definite solution to the problem. The main disadvantage of these solutions is the lack of an appropriate data model for describing text with overlapping structures. For this purpose, special systems, with special data models, like the Multi-Element Code System\" (MECS) or LMNL, are developed. MECS has a very complex and therefore complicated syntax, compared to LMNL, which is much easier to learn. LMNL on the other hand has the disadvantages of missing available software tools for creating, editing, validating or even displaying documents and missing other features, like practical query languages. Apparently there is a demand for a solution that takes the advantages of both approaches. On the one hand a data model, explicitly developed for overlapping structures, on the other hand the plurality of existing tools for XML, including features like XSLT, XPath etc. For this purpose we need to distinguish between data model and the syntax, what this paper tries to show on the example by using the LMNL data model combined with XML syntax. Data Model A suitable data model, especially for overlapping structures without any hierarchies, is the LMNL data model, which is based on the Attributed Range Algebra\" (ARA). In ARA a document is handled as a sequence of characters, over which a number of Ranges spans. Ranges have a name, a start index and a length. They are represented in the document through markup. Ranges can have Annotations, which are not allowed to overlap but can be annotated and structured too. A LMNL document is organized in Layers that overlay each other. Every Layer, except the text layer, which is the lowest Layer of every document, has a base layer and can have other layers as overlay. Such a model can apparently not be represented as a tree, so at first glance it seems unsuitable to combine with XML. However using the following Syntax, which attempts an implementation of the flat subset\" 10 of the LMNL data model, might be a practical solution. Syntax According to the statement of the LMNL developers, that LMNL is a data model which can be combined with different syntaxes. It therefore seems self-evident, to combine the advantages of LMNL with the mass of existing valuable tools for processing XML. The intended proximity of LMNL and XML makes a transformation of the data model to XML syntax much easier, than for example a transformation from MECS to XML. However it must be said, that some of the typical features of LMNL cannot be converted to XML. 11 Below is a classic example for overlapping structures, which is no well formed XML: <a>text with <b>overlapping</a> parts</b> In LMNL syntax the example would look as follows: [a}text with [b}overlapping{a] parts{b] The XfOS Syntax would look like in the following way: <a type=\"start\"/>text with <b type=\"start\"/>overlapping<a type=\"end\"/>  parts <b type=\"end\"/> This, at first sight, looks like the milestone solution from the TEI-Guidelines. But if the Ranges are annotated, which can in LMNL be done either in the start- or in the end tag, the markup becomes more complex. LMNL: [a [c}annotation in start tag{]}text with [b}overlapping{a] parts{b [c}annotation in end tag{]] Apparently the resulting marked up document looks like XML, especially since it is well formed, but in fact it is LMNL. The XML for overlapping structures in that case is a mixture of milestones and fragmentation. Every start tag and end tag of a Range is represented by its own XML element, that possesses an attribute \"type\" for indicating whether it is the Ranges start tag or end tag. If the Ranges are not annotated, the XML elements are empty. Annotated Ranges have content, the Annotation, either in the start tag or the end tag. This is possible, because Annotations can be structured but are not allowed to overlap each other. Perspective At this point of development the approach outlined above seems to be a makeshift solution as well, but if we accept the distinction between data model and syntax, we are able to use the benefits of both aspects. Especially the possibility of using a popular syntax most people are familiar with, in combination with a big amount of existing tools, is an important advantage. In this scenario, we only have to learn one more data model, but there is no need to deploy editors or parser. The possibility to use an existing XML parser makes it relatively easy to adapt an application to the LMNL data model as well as developing programing APIs that depends on those parsers.",
        "article_title": "XML for Overlapping Structures (XfOS) using a non XML Data Model",
        "authors": [
            {
                "given": "Alexander",
                "family": "Czmiel",
                "affiliation": [
                    "Historisch-kulturwissenschaftliche Informationsverarbeitung, UniversitÃ¤t zu KÃ¶ln"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "This paper describes a pilot project to evaluate the use of Geographical Information System (GIS) and web-site technology to explore and display information about forced migration in a region of Macedonia (now northern Greece) from 1880 to the present day. The pilot project explored the approaches, techniques and technologies that would be required for a potential large-scale project. It identified challenges that would be encountered and possible novel solutions that could be applied within a limited budget. The products of the pilot project were a web-based digital resource and a desktop Geographical Information System. The project was hosted by Centre for Computing in the Humanities at King's College London where it is one of a number of projects using such technology for both historical and contemporary data. The partners were The Refugee Studies Centre, University of Oxford, The Centre for Computing in the Humanities, King's College London and The Research Centre for Macedonian History and Documentation, Thessaloniki. The Mapping Migration Pilot Project is one of a new style of digital projects in the humanities that make use of image, spatial database and text-based technologies. It fits the Conference theme \"Computing and Multilingual, Multicultural Heritage\" in a number of ways. It took a range of unpublished material and produced a digital museum of cultural heritage aimed primarily at the communities of its subject matter thus exhibiting a broader social role of humanities computing and the resources it develops .It also benefits scholars in cultural studies, refugee studies and social science history. It demonstrates the application of information technology, especially Geographical Information Systems, to cultural and historical studies. The presentation of information that has both spatial and temporal elements presents issues concerning information design in the humanities History Population migrations were among the decisive factors that contributed to the dynamism of Balkan history. Ancient Greece, the Roman and Byzantine Empires, the settlement of the Slavs, their medieval states as well as centuries of Ottoman rule left a deep impact on the ethnic, political and cultural composition of the peninsula. The Eastern Question and the rivalry of the European powers to participate in the Ottoman heritage introduced the Balkans into modern European history, parallel to the birth of national Balkan states in the nineteenth century. The mobility over the new borders of peoples who lived for centuries in multi-national empires made extremely difficult the establishment of ethnic frontiers based on the principle of national self-determination. The Balkan Wars of 1912-13, and in part World War I, had their origins in nationalist aspirations to complete territorial unification. The peace settlements after 1918 established boundaries very similar to those existing today, but left some populations resentful. Ethnic mixtures resulting from migrations blurred national affiliations in neighbouring Balkan national confines. After 1941, the Balkans became a battlefield between the Allied and Axis forces and the scene of resistance movements significant for the post war settlement. The states followed divergent paths from 1945 to 1990, as Communist governments took power in several. Events since 1989, as governments and economies moved into a transition period to capitalism and western type liberal democracy, the rise of nationalism and civil conflict, and the breakdown of Yugoslavia are of major significance. Project Outline The project focuses on a section of the geographical region of Macedonia, which became the present day Greek district of Kastoria. The material is organised in layers defined by a timeline showing significant events and migrations in the period 1880 to 2000. The backbone of the presentation is a series of about forty maps that were generated during the analysis of data by an off-line Geographical Information System. The displays of spatial data also act as a hub technology linking and structuring data throughout the project and allowing users to navigate their way through this structure. The content is drawn from a collection of quantitative and qualitative material consisting of historical documents, photographs (both contemporary and historical), postcards, short explanatory texts, demographic metadata in databases concerning about a hundred communities and multimedia presentations of places, historical events and cultural phenomena. The database material has been extracted from legacy database systems from the Museum of the Macedonian Struggle. This has both preserved the cultural data and opened it up to a much wider audience. An on-line bibliography on the subject is available. The project presents information that has both geographic and temporal elements, with the time aspect being of particular importance. Organising the qualitative material by both time and geography was an early challenge. The representation of time posed some design questions when considered on its own but when combined with space the challenges and opportunities multiplied considerably. A number of existing projects were studied for possible directions at the outset of the project. Much of the material recorded is unpublished and its availability online provides an invaluable resource. Its presentation through the web site is a form of digital museum presenting both archive and modern multimedia material. The GIS is a powerful visualisation tool that allows historical data to be explored and analysed through the use of maps and charts. Both the Web and GIS aspects of the project have highlighted issues concerning the design of information analysis tools and presentation media. The web site was used to integrate the quantitative and qualitative data and provide a medium for delivering interactive maps and animations. Conclusions Many of the perceived barriers to the more extensive use of spatial data in the humanities are due to the common misconception that if a project uses maps it has to use a GIS. However experience in the project has shown that even simple maps can yield valuable insights. The project also shows how spatial data can provide a \"hub technology' linking and structuring data throughout the project and allowing users to navigate their way through this structure. The unavoidably steep learning curve were minimised by utilising the interoperability of GIS software with existing software that one is familiar with. The costs associated with additional hardware and spatial data acquisition can be minimised by utilising features of GIS software. Features such as \"heads up' digitising and \"mosaicing' avoid the need to buy expensive large format scanning and digitising hardware. The project is one of a new style of digital projects in the humanities that make use of image, spatial database and text based technologies. It has explored methods of taking a range of unpublished material, quantitative and qualitative, and producing from it a digital museum of cultural heritage aimed primarily at the communities it describes. In doing so it exhibits the a broader social role of humanities computing and the resources it develops. It also benefits scholars in cultural studies, refugee studies and social science history. It demonstrates the application of information technology, especially Geographical Information Systems, to areas of cultural and historical studies. The presentation of information with both spatial and temporal aspects has presented a number of issues concerning information design in the humanities. The project demonstrates that a great deal can be achieved in the visualisation of spatial data at a relatively low cost in terms of finance, time and expertise. A number of new technologies were explored and some novel web-based solutions to simple cartographic problems were identified. During the project it also became apparent that sometimes producing maps is not important, what matters is compiling and organising information from diverse sources according to geographical location. The process of visualising the spatial data, with or without a GIS, forces this to be done and provides insights in its own right.",
        "article_title": "The Application of GIS to the Creation of a Cultural Heritage Digital Resource",
        "authors": [
            {
                "given": "Martyn",
                "family": "Jessop",
                "affiliation": [
                    "Centre for Computing in the Humanities, King's College London"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The Dilemma of Vocabulary Acquisition A dilemma in vocabulary acquisition is the antagonistic advantages of the commonly distinguished .intentional. and .incidental. vocabulary acquisition. Intentional vocabulary acquisition is memorizing straightforwardly term after term with their respective translations from a list. Intentional learning is quick and therefore usually preferred by learners, but it is also superficial. Learners encounter vocabulary in an isolated, often infinitive form and remain incapable of using it correctly in context. Moreover intentionally learned vocabulary sinks faster into oblivion. Didactically recommendable vocabulary acquisition exposes learners comprehensively to every term, embedding it deeply and solidly in the mental lexicon [1, 10]. Beneficial is also personalized vocabulary acquisition on authentic texts [6, 9, 13, 17]. Incidental vocabulary acquisition, namely through contextual deduction in target language reading, meets these recommendations. Learners encounter terms together with syntactic information, which helps using the accurate words in an idiomatic way. Vocabulary in context often appears repeatedly under different aspects and hence engrains in the learners. minds. Unfortunately it takes long until enough vocabulary for fluent conversations is incidentally gathered [3]. Problematic is, that deduction works best when new terms are mostly surrounded by familiar vocabulary [6]. In fact, with more than 5 to 10 new terms presented simultaneously, our retention capacity declines [12]. Gymn@zilla Combines annotated reading with term lists Gymn@zilla (http://www.eurac.edu/gymnazilla) addresses this dilemma by dynamically annotating authentic text with definitions, translations, pictures, and other descriptive information. When learners access local and Internet documents through Gymn@zilla, server-side processing of texts dynamically adds links from every term to the corresponding entry in an open learning resource. Gymn@zilla employs stemming tools to match inflected word forms with dictionary entries. Learners receive linguistically enriched documents with their original link structure, so that they need only to move their mouse over a term to check it, or continue browsing. Annotated reading is considered as a valuable feature in language learning [2, 11, 16] and implemented in several reading systems [4, 14, 15, 17]. However, all these systems have a closed, annotated corpus and a closed set of dictionary entries. None of them combine Internet browsing with annotation links to local and on-line dictionaries. For intentional vocabulary acquisition, learners can collect terms and their translations from several sites in their personal word list by clicking on term links. In this way vocabulary acquisition occurs both incidentally by reading texts annotated with dictionary information and studying word lists extracted from this text. Depending on the underlying learning resource, Gymn@zilla exposes learners abundantly to new vocabulary in up-to-date, personalized contexts and activates the mental lexicon in several steps and on several levels. Dynamic Interactivity Traditional classroom teaching uses gap-filling and multiple choice quizzes for decades. Their usefulness is generally accepted regardless of the applied methodology. Quizzes can combine to an integrated vocabulary acquisition environment [13]. The main advantage of electronic learning material over traditional paper material is interactivity [18]. Authoring tools allow language teachers to manually create electronic true-false, multiple choice, matching, gap-filling, spelling, or sentence generation quizzes [5]. Learning environments exploit multimedia features [6, 9] and gap-filling quizzes for grammar training and even sentence formation [7, 19]. The effectiveness of such quizzes especially for weaker students has been shown in [8]. Figure 1 to 3 shows the transition from annotated reading via the construction of a word list to the interactive quizzes. To our knowledge, no other system offers interactive practice on annotated internet texts in similar ways. Figure 1: Annotated reading with Gymn@zilla. Figure 2: Vocabulary List with Gymn@zilla. Figure 3: Interactive quiz with Gymn@zilla. The Gymn@zilla project and underlying technology Gymn@zilla has been developed within the LOGOS-GAIAS project. It supports browsing a local text repository and the Internet by dynamically creating and annotating HTML pages with open dictionaries resources. Gymn@zilla is written in Perl. It is an on-line application running on a Linux web server . not a browser. Both components, Perl and GNU/Linux, guarantee the usage of free and powerful modules. The processing of web pages in real-time and generating exercises from it is a complex task, which involves the following steps: (1) mirroring of web pages, (2) linguistic processing and (3) generation of exercises. 1. Mirroring of web pages is done by using Perl.s LWP modules. All Hyperlinks in a web page pointing to other text documents are rewritten to Gymn@zilla.s URL in order to allow continuous browsing with Gymn@zilla. Links to multimedia documents such as audio, video and graphic files are preserved. In a next step encodings other than utf-8 are converted to utf-8. Documents in formats other than html such as *.doc, *.ps or *.pdf are converted to well formed xhtml by the use of GNU-tools. 2. Once converted, the documents language is guessed before starting natural language processing. In order to annotate the text with linguistic information the text is first segmented into its tokens. Stemming is then done by the use of pattern matching techniques. According to the user.s preferences the text is then annotated with translations and terminological information from on-line dictionaries and terminological databases. The annotation is done by insertion of -tags with advanced link titles containing the linguistic information which will show up as a tooltip when the user moves the mouse over a word. With the help of a Javascript function link titles can be formatted like html-documents so that they may contain images and links to further information sources. Information can thus be structured from general to specific. 3. Each user in Gymn@zilla is associated with a session where history information is stored in order to memorize words seen by the user. This information is then used to make editable word lists and to generate cloze texts or other exercises for training. Future steps in the development of Gymn@zilla comprise benchmarking, the expansion of linguistic resources integration of automatic document classification and the integration of a morpho-syntactic parser in order to improve linguistic analysis linguistic annotation. ",
        "article_title": "Bridging the Gap between Intentional and Incidental Vocabulary Acquisition",
        "authors": [
            {
                "given": "Oliver",
                "family": "Streiter",
                "affiliation": [
                    "European Academy of Bolzano"
                ]
            },
            {
                "given": "Judith",
                "family": "Knapp",
                "affiliation": [
                    "European Academy of Bolzano"
                ]
            },
            {
                "given": "Leonhard",
                "family": "Voltmer",
                "affiliation": [
                    "European Academy of Bolzano"
                ]
            },
            {
                "given": "Daniel",
                "family": "Zielinski",
                "affiliation": [
                    "University of Saarland, Germany"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The concept of document architecture (DA) pops up once and again in computer discourse, often with slightly different meanings. Sometimes it refers to the architecture in document managing software, sometimes it denotes the organisation of document components, with a primary focus on the text. This presentation takes its point of departure in how document architecture is defined in the SGML ISO Standard (8879:1986) and what the term document implies there. It then goes on to make an overview of how the document concept has been discussed and employed in various ways within Library and Information Science (LIS) and poses the question of whether this discussion can serve to broaden the document concept in document architecture in a meaningful way. The context in which this question is posed, and for which it is of particular importance, is my Ph.D. thesis, in which I attempt to make use of the metaphor document architecture as the basis for a model that can serve to analytically study documents with regard to media-specific characteristics (cf. Dahlstrm & Gunnarsson 2000). In some ways it bears resemblances to Katherine Hayles Media-Specific Analysis, MSA (Hayles 2002), although its focus is not primarily on literary works, and specifically not on the meaning of the text. Rather, the document is seen as a crucial unit in the organisation of knowledge, serving to contextualise and materialise our interaction with that evasive phenomenon we tend to term information. However, the DA approach shares MSAs interest in how the material of the document influences the text, as well as MSAs applicability in the analysis of different media. Within LIS, the view of what constitutes a document covers a wide span. Smiraglias quite concise document = item (in Smiraglia 1992, quoted in Smiraglia 2001, p. 149), Ranganatans media-restrictive a record on a more or less flat surface (Ranganathan 1963, p. 41, quoted in Buckland 1997, p. 807) and Suzanne Briets encompassing all concrete or symbolic indexical signs [indice, transl. comment], preserved or recorded toward the ends of representing, or reconstituting, or of proving a physical or intellectual phenomenon (Briet 2003) are some examples of how the definitions may differ in explicitness and focus. Implicit in all of these quite diverse definitions is the physical or material aspect of the document. It is often seen as the material item that carries the abstract work; the object of an act of documentation. This is an aspect that is absent in the SGML Standard definition of document, which reads A collection of information that is processed as a unit. (ISO 1986, p. 10) Here, the view of the document lies closer to a logically defined concept and relates closely to the SGML document, which is in essence logical and hierarchical, and which brings to the fore the relationship between the notions of document and text. Although obviously different, all of the definitions above, with the exception of Briets, fit into the same category in the taxonomy of document research suggested by Roger Pdauque and a number of his colleagues (Pdauque 2003); the category of research that privileges the form and material and structural aspects of the document. The wide span within the category is primarily due to the difference in medium of the documents that make up the conceptual basis in the different cases: the print document is described as medium + inscription (p. 5) and the electronic document as structure + data (p. 6). The category concerned here, Form, is one of three categories; the other two are Sign and Medium. Each of the categories is related to one aspect of a reading contract between a reader and a producer (p. 4). The document as form is concerned with an object of communication governed by more or less explicit formatting rules that materialize [the] reading contract (ibid) which is interpreted in terms of legibility. The other two aspects are intelligibility and sociability, the former having to do with the sign (or inscription) and its meaning, and the latter with the social function that the document as a tangible element of communication between human beings (p. 17) plays. Within LIS, document definitions can be found that may fit within one or more of these aspects, and the question is if the notion of a reading contract can be used in a meaningful way in connection to LIS document understandings to inform a model of document architecture, uniting the three aspects of the contract. Another interesting idea when it comes to definitions is the notion of fuzzy sets. Often, definitions constitute an attempt to list the inherent properties that unite that which the concept comprises. Lakoff and Johnson (1981) argue that concepts are defined in social interaction and in language use. Therefore, definition is not a matter of giving some fixed set of necessary and sufficient conditions for the application of a concept []; instead, concepts are defined by prototypes and by types of relations to prototypes (Lakoff and Johnson 1981, p. 125). We may speak of some documents as being close to the centre, and others as being further out towards the limits of the document concept set. Thus, an admittedly quite unscientific, but close at hand, assumption may situate a traditional printed book close to the centre of the set. It has several properties that we intuitively associate with a document intentionality, materiality, storage potential, a communicative function, and artefactuality. This can also serve to explain why we may have some trouble accepting e.g. Chinese water poetry as a document (it lacks storage potential). It is placed a certain distance from the centre of the set, and shares only some of the prototypes properties. In the presentation, I will test the appropriateness of discussing the document concept in terms of a combination of Pdauques reading contract and the document as a fuzzy set. The context for the discussion will be a model of document architecture intended for the analysis of traditional and new media, which may in turn serve to highlight similarities and differences in how we may approach and work with documents in different media. Different disciplines, and indeed schools within the disciplines, have very different answers to the question of what is a document. In conducting a conceptual discussion of the term document, especially in relation to electronic media, aspects of both computer science and a number of diverse humanities disciplines may prove highly relevant as sounding boards. This is one of my incentives for wanting to discuss these questions in the multidisciplinary environment at ALLC/ACH.",
        "article_title": "The Document in Document Architecture",
        "authors": [
            {
                "given": "Helena",
                "family": "Francke",
                "affiliation": [
                    "Swedish School of Library and Information Science"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Word Formation and Second Language Acquisition Word formation is one of the most important ways of extending the lexicon. As opposed to borrowing and semantic extension, the other two main sources of extensions, word formation is largely governed by rules. These rules are not only used for producing new forms, but also for structuring the lexicon. Therefore, there are two reasons why second language learners can benefit from knowing the mechanisms of word formation: for understanding new, unknown words and for perceiving the relationship between known words. As shown by ten Hacken (1998), these advantages do not automatically arise by using electronic dictionaries, but require an appropriate structure. In this paper we show how this problem was approached in the context of the ELDIT project. Word Formation in ELDIT At the European Academy of Bolzano/Bozen an interdisciplinary team consisting of linguists and computer scientists is developing an electronic learner's dictionary for the Italian and German languages called ELDIT. ELDIT is a basic dictionary which contains about 3.500 lemmas for each language and is freely accessible on the web (http://www.eurac.edu/eldit). It is addressed to a well defined target group, namely Italian-speaking students of German and German-speaking students of Italian from beginner to intermediate level (Abel & Weber 2000, Gamper & Knapp 2002b). Each entry of the dictionary contains a wide range of information which is important for the learner. The information is stored in different modules: one or more definitions together with examples and translations, free combinations and collocations, idiomatic expressions, proverbs, and fixed phrases, word fields, word families, usage remarks, grammar and pronunciation. ELDIT is currently enlarged to a complete language learning system that should enhance bilingualism in multilingual regions. The system could be enhanced to other languages. We therefore propose it as a model for multilingual language learning environments. In this paper we report about a very specific aspect of the dictionary, namely word formation: Starting from a single entry-word (lemma) the user can click on the tab \"word family\" and access in this way a list of compounds and derivatives with corresponding explanations. Word formation is represented in ELDIT in the following way (see figure 1): Frequently used derivatives and compounds are listed together. In future we will add head word which is a simplex word that cannot be decomposed further (Augst 1998). The basic part of each element is emphasized. For this basic part we consider the stem of a derivative (e.g. \"Haus\" -> Behausung, Huschen, huslich ...) or this part of a compound word that appears as headword of the entry within which it is listed (e.g. \"Haus\" -> \"Bauernhaus, Hausnummer, Haustr ...\"). Hyperlinks lead the user to further information: derivatives and compound words of the basic vocabulary contained in ELDIT are linked with the relevant word-entry; prefixes and suffixes of the derivatives are linked with explanations of their meaning, use, particularities and with more examples; also the so called \"linking elements\" (Fugenelemente) in German compounds are linked with special explanations. Furthermore, the user can find translation(s) for each element. If necessary, explanations about the word itself are given (for instance if a word is used only in a colloquial sense, etc.) By clicking onto the triangle next to each word, lexicographic examples can be obtained out of the large database of examples in ELDIT (This feature is already working, but not yet enabled in the online version). Figure 1: Derivatives and compound words in ELDIT In the simplest case the module \"word formation\" is helpful for decoding and encoding purposes. Its main objective, however, is to convey the complex system of word formation in a quite simple and transparent way. The user should learn that also semantic similarities exist between formally related words. As a result the learner should be able to analyze the structure of words, to use the L2 in a more creative and productive way by themself (this is important in particular for extremely productive processes such as German compound words and Italian evaluative suffixation) and to draw conclusions on the meaning of unknown words in receptive linguistic situations. Implementation Approaching the previously described ideas in the usual way would require the manual input of a huge amount of data and a very detailed encoding of these data. This extensive and detailed encoding was judged not feasible by the authors of the dictionary. Hence, we supported the authors in the data encoding process by electronically rewriting a hand made, semi structured version of the data into the final extensive version needed by the system. Moreover computational linguistic resources were exploited and as much linguistic information as possible was added electronically. As a uniform data and knowledge representation formalism we use the XML language (for more details see Gamper & Knapp, 2002). The authoring process can be described as follows: The first step is to manually elaborate the language learning content and to submit and save it using an editor in a possibly already semistructured way. Figure 2 shows a derivation in ELDIT developed with a very simple XML-editor in this stage. Figure 2: Semistructured data encoded in XML Then a transfer tool converts the manually elaborated data into the special format needed by the system. Figure 3 shows the same part of the ELDT data as figure 2, but in the fully encoded stage. Figure 3: Fully structured data encoded in XML A lot of information has been added electronically: every XML-element got a unique ID (see for instance id=\"de.n.haus.1.deriv2\" in the element <derivation>). The words have been equipped with citation form and part-of-speech (see the attributes base=\"Behausung\" and ctag=\"N\" in the element <pattern> or the attributes base=\"dimora\" and ctag=\"N\" in the element <w> within <translation>). Wherever possible a reference to the corresponding dictionary entry is added (see for instance the attribute lexref=\"it.n.dimora.1.sense2\" in the element <w> of <translation>). Also references to explaining sections are added (see the attribute explref=\"de.prae.h.be\" which points to a section within which word derivation with the prefix \"Be\" are explained). Thanks to a collaboration between the European Academy of Bolzano and the Scuola Universitaria Professionale della Svizzera Italiana (Funded by the European Union, Interreg IIIA, Italia-Svizzera) involving Word Manager (WM) and ELDIT we can add morphological information to each word. WM is a system for reusable morphological dictionaries. Ten Hacken & Domenig (1996) describe the general architecture and the rule types involved. Lexical tools derived from WM databases are included into ELDIT and allow transforming a word form to its citation form, delivering at the same time its category (Pedrazzini, 1999). Hence, we can equip each word with its citation form (attribute \"base\"), its word form (attribute \"ctag\") and a pointer to the full meaning description of the word in the dictionary (attribute \"lexref\"). WM is also able to suggest new derivations, providing information about their segmentation, derivational degree, frequency within ELDIT, category, etc. These possibilities are currently exploited in an authoring tool for efficiently extending the existing families with new elements and more information for the learner, thus providing rapid access to a rich source of high-quality information. Conclusion By exploiting the possibilities of an appropriate representation of word formation rules and their products, we managed to create an enhanced language learning environment. The potential of the WM system in this respect was noted by ten Hacken & Tschichold (2001). In the realization of this potential, the way different properties of ELDIT and WM collaborate and reinforce each other demonstrates the flexibility of the former as well as the reusability in practice of the latter. ",
        "article_title": "Word Formation in Pedagogical Lexicography: Linguistic and Technical Aspects",
        "authors": [
            {
                "given": "Andrea",
                "family": "Abel",
                "affiliation": [
                    "European Academy Bozen-Bolzano"
                ]
            },
            {
                "given": "Judith",
                "family": "Knapp",
                "affiliation": [
                    "European Academy Bozen-Bolzano"
                ]
            },
            {
                "given": "Pius",
                "family": "ten Hacken",
                "affiliation": [
                    "University of Wales Swansea"
                ]
            }
        ],
        "publisher": null,
        "date": "2004",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    }
]