[
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Distributed multivalent encoding (DME) describes a web-based text encoding practice and the result of that practice. It assumes a digital text resource with a browser interface that allow users to associate N encodings with N texts in the resource. Users need have no other connection with the resource than their using it and constraints should be minimal; hence the encoding is distributed among multiple creators and multiple conceptual approaches. To realize their encoding(s) users may ignore existing encodings; they may apply one tagset to one text, or to multiple texts, or they may apply multiple tagsets to one text, or multiple texts; any element they create can reference any other element created by themselves or by someone else. Thus the resource’s encoding becomes multivalent in part and in whole.A cluster of activities defines the space of distributed multivalent encoding, including annotation, keywording (including “folksonomizing”), editing, and humanities criticism. For any one component of DME anticipatory previous work exists.For example, the resource based around Pico Della Mirandola’s Conclusiones CM publicae disputandae (PICO) features an annotation system which is also employed by the Virtual Humanities Lab (VHL), a resource whose development plan includes implementing a form of DME. Any newness of DME lies only in the surprising fact that the components have never been fully assembled, though they have all been present for some years. Now, however, we can see DME emerging, albeit in hesitant, not-fully-developed forms.See LIMNER, for example: a site intended as a proof-of-concept DME resource, still in an early stage.A DME resource has a base text (or texts), a web interface for creating encoding, and a mechanism for storing encoding; these three things enable all subsequent activities: editing, deleting, retrieving, searching, etc. Considering each of the three parts in a little more detail we will see few real technical obstacles in DME’s path.Distributed multivalent encoding starts with a reference base digital text. A reference base text should be clearly marked as such by the resource managers. Only they may change it (or give someone else permission to change it), and any change must be well publicized. A reference base text must have a well-defined beginning and end, and a well-defined internal base reference structure. While it is certainly possible to regard either the simple byte sequence or the character sequence as the base reference structure and then link encodings to byte or character offsets, this is not a robust solution. Practicality suggests the internal base reference structure should itself be an encoding; convenience and practicality further suggest a presentation-based encoding, on the grounds that presentation in a medium reflects a community’s sense of that medium’s main communicative organization and delivery units (Caton, 2001). A TEI Lite encoding (), for example, or one conforming to DLF Level 4 (), would be suitable. We should stress, though, that the reference structure makes no claim to being representationally definitive. Within the structure, distinctions between tags and #PCDATA are purely structural and do not define either “text” (as general phenomenon) or “the text”. A supplementary encoding – ie. an encoding created by a user – associates with the text as represented within the reference structure, not “the text” as some reified cultural object. If we follow the reasoning of Renear et al on the relationship between FRBR entities and XML documents, we would probably consider the base reference text a manifestationof an expression, especially because we incline towards a presentation-based encoding (which, as Caton argues, is what OHCO-style encoding is)(Renear et al, 2004, Caton 2001). The FRBR vocabulary, however, hardly resolves the problematic semantics of the common phrases “the text” and “a text”. Hence our insistence on treating the internal reference structure as definitive only within the DME resource as a system. Indeed the very point of a DME resource is to allow users to create encodings that they can treat as definitive for their purposes.The web interface presents the base text to the user and allows the user to associate encoding(s) with parts of the text. The encoding must allow for multiple overlapping hierarchies, and so a format such as CLIX/TEI HORSE should be used (DeRose, 2004, Bauman, 2005). While the interface programming might be complex in terms of having to manage numerous details, the required functionality is straightforward. The actual mechanics a resource employs are not important, except as regards the degree to which they make the process awkward. They will vary according to programmers’ preferences and with changes in technology. The proof-of-concept Limner interface, for example, uses HORSE and relies on explicit element IDs and user-selected strings to mark where start and end tags go. DeRose notes that ‘[o]ne often hears that … IDs are somehow \"safe\" pointers into documents (DeRose, 2004). However, this is not true; they are at most “safer” than many other methods.’ His point is well taken. However, there must be some reference system, and unless we resign ourselves to inline markup and unwieldy file sizes, it seems preferable to keep the supplementary markup separate from the base text and use a system that is (to view the cup as half full rather than half empty) at leastsafer than many other methods. A combination of full XPath plus element ID plus a string of sufficient length to have a strong chance of being unique should allow a DME resource to consistently associate a stored out-of-line element with its proper position with respect to the internal base reference system. The Limner implementation is rather crude and already dated; the DOM scripting features of current browsers together with wider availability of XPath handling functions in programming languages offer many opportunities to improve upon it.The actual details of storage are also of limited importance. Relational databases can hold the information (as with Limner) but it seems likely that future DME resources will use native XML databases.Without downplaying the amount of work involved, we can confidently say that DME is perfectly possible with current technology and that DME resources will be built in the near future. The real unknowns (and potential problems) lie on the social side. Who gets to encode? Will all supplementary encodings be equal, or will some be “more equal” than others? Will encodings be moderated? Will differently encoded and competing base reference texts proliferate until the very notion of a base reference becomes utterly compromised? Will a class system of resources emerge, driven by a scholarly fear of non-scholars’ contributions? The relative success of Wikipedia in the face of all the things that could have stopped it should make us optimistic. Probably DME will initially develop in constrained forms, with resources authorizing users and retaining ultimate editorial control over supplementary encodings. In time we hope to see distributed multivalent encoding become a widespread, democratic practice",
        "article_title": "Distributed Multivalent Encoding",
        "authors": [
            {
                "given": "Paul ",
                "family": "Caton",
                "affiliation": [
                    "Brown University"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "multivalent",
            "annotation",
            "encoding"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "AbstractHumanities computing must make conceptual modeling one of its defining activities. This is necessary not only to provide more effective computational support for scholarship in the humanities, but also in order for humanities computing itself to become something more than a “bag of tricks”. This paper describes promising developments already underway in several areas within humanities computing, generalizes from past, if partial, successes (text encoding), draws parallels with recent work in bioinformatics, and argues that the logical identity of conceptual modeling and theory-construction makes conceptual modeling simply a computationally oriented variant of scientific explanation, and, more broadly, interpretation and understanding in general. As an example of how model development can provide fundamental insights into the cultural world a case is described where a set of ontology evaluation criteria developed within computer science is applied to an influential framework for cultural entities and the result that a radical reconfiguration of that framework is suggested.Acknowledgements: There is undoubtedly much here that is influenced by seminal papers on these issues by Willard McCarty and John Unsworth; I acknowledge a debt to them without of course any suggestion that they see things as described here.IntroductionThe nature of humanities computing has long been problematic in the digital humanities community. Part of the reason for this anxiety is that humanities computing is struggling to move out of a pre-scientific phase. “Science” here is of course intended to be taken in the broadest possible sense — the point is not to separate disciplines such as physics on the one hand from, say, literary studies on the other, but rather to distinguish practices that have primarily non-epistemic non-theoretical objectives from those that have understanding, explanation, and knowledge as their principal immediate goals. Nor is there any implication of a clear boundary between scientific practices and non-scientific practices, or that there isn’t a complex and essential mutual involvement — the claim is only that some rough distinction of this sort is possible and useful.If humanities computing is to mature as a coherent field of intellectual inquiry it will need to make a transition from being an ad hoc set of useful techniques, to more systematic general methods and theories that make direct contact with the specific methods and theories of the disciplines it supports. This theme is not new, but our progress has been modest so far and some further discussion and elaboration is in order.First, there are new reasons to be optimistic. For one thing this progress is already occurring in other informatics fields—bioinformatics as an example—and the specifics of these changes provide an indication of how things might go in humanities computing as well. Moreover the growing significance of conceptual models and ontologies that is already in evidence in humanities computing, and the nature of the discourse around them, is a sign that this transition is perhaps already underway. Finally one of the most theoretically productive areas within the humanities computing community, namely text encoding, arguably has been as successful as it has precisely to the extent that it has embodied the features recommended here — and owes its limitations and disappointments to the extent to which it has not. What all these things have in common is the foregrounding of conceptual modeling, explicitly or implicitly.I use the term “conceptual modeling” broadly, meaning any formally defined abstract representation of a domain of interest. Typical examples of modeling languages are the ER and UML diagrams used in business applications, logic-based knowledge representation formalisms such as description logics or the frame languages (such as the KL-ONE family) common in artificial intelligence, and the various formal ontologies now used in computational biology and elsewhereScience in the broad sense is the development of understanding. Scientific understanding comes typically in the form of theories, and theories are at least in part the systematic identification of objects and relationships that are the salient features of some domain. Informatic disciplines must put these theories, formalized as conceptual models that support computational processing, at the center of their identity. And for informatic disciplines to be sciences themselves they must not only exploit such conceptual models, but participate in their development and exploration. This is true for informatics in general, and for humanities computing in particular.Modeling in BioinformaticsOther areas that began as craft-like bag-of-tricks approaches to applying computing to a scientific field have now begun to evolve a more coherent body of method, theory, and metatheory. Perhaps the most dramatic case is bioinformatics. Although initially a haphazard and opportunistic application of computing techniques, there are now a considerable number of families of well-theorized methods tightly tied to biological theory. Of particular interest here is the role of so-called ontologies in contemporary bioinformatics such as the Gene Ontology (Ashburner et al., 2000), the Foundational Model of Anatomy (Rossee & Mejino, 2003), and other ontologies in genomics, molecular function, neuroscience, biodiversity, and other areas of the life sciences as well. An enormous amount of effort is going into the development and use of these formal models.In some cases the influence on biological science per se has still been modest, but generally the results are extremely promising and in a few cases the influence has been stunning. The Gene Ontology in particular, has been enormously successful and provides the overall framework for contemporary genomics. Of special significance for us is the extent to which the fundamental explanatory entities and properties of biological theories are explicitly identified as the core constituents of these conceptual models, blending the work of first order science with its bioinformatics support.Modeling in the Cultural SciencesWithin the humanities computing community text encoding systems such as the TEI can be seen as a step in the same direction as the new ontology-based models in computational biology. Behind the methods of the TEI is at least arguably an informal positing of text models, components, and relationships. However unlike the bioinformatic ontologies the models being indicated by text encoding techniques are only indirectly and implicitly identified. This has been pointed out by a number of critics (Raymond & Tompa, 1992; Raymond, Tompa, & Wood, 1996, 1998, 2001; Buzzetti, 2002), and projects to develop remedies are underway (Sperberg-McQueen et al., 2002; Renear et al., 2003).Another promising sign are the ontologies being developed in museum studies CIDOC/CRM (Crofts et al., 2003) and library science (IFLA 1998). For the most part these are being put forward as frameworks for designing digital information management systems or shaping the development of standards, policies, and procedures, but to the extent that they succeed at efficient, functional, and interoperable systems they may be considered confirmed as substantive theoretical proposals for how to understand the nature of the cultural world. And where these models and ontologies appear to be applications of existing explanatory theories of cultural objects and relationships that is more evidence that they are achievements which advance our understanding of that world.An Example: Revising FRBRWhat follows is an example intended to support two claims. First that the evolution of conceptual models intended to support information systems design can in fact be an activity of first order science (in the broad sense), and second that precisely the same techniques designed for general evaluation of ontologies in natural science can be effectively employed in evaluating cultural ontologies.Several models for cultural objects identify such things as works, texts, editions, and individual items as types of things, fundamental kinds. Not only are various sorts of conceptual arguments advanced in favor of such an analysis, but so is the relative success of such frameworks in guiding the design of effective information systems that are effective, efficient, and interoperable. That is, the conceptual and practical success of these systems can be taken as indicating that they correctly represent how things are. A well-known model of this sort is IFLA’s Functional Requirements for Bibliographic Records (IFLA 1998), a framework designed primarily to support library cataloguing, but which clearly has wider application — FRBR in fact describes itself as a “conceptual model of the bibliographic universe”. A puzzle recently emerged about an assignment of one particular cultural object (XML documents, as defined in the W3C XML standard) to the appropriate FRBR entity type. Some considerations argued for an assignment to the FRBR expression (roughly: text) entity type, and other competing considerations argued for assignment to the FRBR manifestation (roughly: edition, or format) entity type. The proposed resolution was an awkward and unsatisfying exception to the FRBR framework which preserved the ontology of works, expressions, editions, at the cost of complexities elsewhere. (Renear et al 2002, Renear 2005).Two years later however it was noted that when a proposed criterion for ontology evaluation which had been developed for scientific and general purpose ontologies (Guarino & Welty, 2000; Guarino & Welty, 2002) was applied to the FRBR framework the result was an anomaly that suggested refactoring the four Group 1 entity types into new entities and corresponding roles. (Renear, 2006). Under the resulting revised model the manifestation and expression entity types are not treated as true entity types (fundamental kinds), but rather roles (relationships) that entities of other types have in particular circumstances—and in fact this revision was extended to all four FRBR entities (Renear & Dubin, forthcoming). We realized that this new way of conceptualizing cultural objects appeared to be an application of a more general approach developed by John Searle (1995) as well as consistent with some work in aesthetics (Levinson 1980). In short, all four FRBR “entities” can be reinterpreted not as entities but as roles that other non-cultural things have under specific social contexts of “collective intentionality”.That this is not just a practical adjustment in a conceptual model for humanities computing, but a substantive claim in the science of cultural objects in general, can be established by comparing it to familiar claims about cultural objects by scholars such as Ingarden, Wellek, Richards, Fish, Goodman, Tanselle, Schillingsburg and others. In this comparison contradiction will serve as well as convergence to make the point that these assertions fall within the domain of humanist inquiry, and not merely within some auxiliary practice.The Relative Neutrality of these RecommendationsIt may be thought that putting conceptual modeling at the heart of humanities computing and seeing the development of models as first order scholarship in the humanities requires accepting some archaic and dubious philosophical view, a positivistic scientism perhaps, or even philosophical realism. But this is not so. It is possible to engage in ontology without taking a philosophical meta-ontological position. Many different philosophical positions (including constructivism and relativism) are all perfectly consistent with taking conceptual modeling as the natural activity of humanities computing, and at least part of humanities scholarship more generally. Of course the results of modeling will not be neutral vis-à-vis other first order theories, but the activity of modeling need not be exceptionally controversial. Again the experience of bioinformatics, where researchers with different philosophical views (or no philosophical views at all) nevertheless often agree, or more importantly disagree, on specific issues without rejecting the overall approach, is significant. This is not to say that the position is entirely neutral. It may well conflict with some general accounts of humanistic inquiry which hold that such inquiry is radically different than the natural sciences, especially if those accounts are understood as applying in general or without exception to the entire range of humanistic scholarship — although I am not yet convinced that it does in fact conflict with the traditions of hermeneutics and verstehen, despite the differences in tone and direction. In any case that the recommendation is not entirely uncontroversial cannot be taken as a decisive mark against it at the outset. Whether it is sound or not will depend on how it fares against its competitors in improving our knowledge and understanding of the cultural world.Concluding RemarksI have argued that humanities computing must make conceptual modeling its central defining activity if it is to fully realize its promise to substantially advance humanities scholarship, and that such modeling is an intrinsic part of first order humanities inquiry, and not merely an auxiliary activity.",
        "article_title": "Modeling, Explanation, and Ontology in the Cultural Sciences",
        "authors": [
            {
                "given": "Allen H. ",
                "family": "Renear",
                "affiliation": [
                    "Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-03",
        "keywords": [
            "explanation",
            "modeling"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Collex is an online toolset designed to aid students and scholars working in networked archives and federated repositories of humanities materials: a sophisticated COLLections and EXhibits mechanism for the semantic web. It allows users to search, browse, annotate, and tag electronic objects and to repurpose them in illustrated, interlinked essays or exhibits. By saving information about user activity (including the construction of annotated collections and exhibits) as \"remixable\" metadata, the Collex system writes current practice into the scholarly record and permits knowledge discovery based not only on predefined characteristics or \"facets\" of digital objects, but also on the contexts in which they are placed by a community of scholars. Collex builds on the same semantic web technologies that drive MIT's SIMILE project and social bookmarking systems like Connotea and Zotero, but it also brings folksonomy tagging to trusted, peer-reviewed scholarly archives and features an integrated publication system. This exhibits-builder is analogous to high-end digital \"curation\" tools currently affordable only to large institutions like the Smithsonian. Collex is free, generalizable, and open source and is presently being implemented in a large-scale pilot project under the auspices of NINES.Collex is constructed with pragmatic scholarly needs in mind, and under the assumption that \"the general field of humanities education and scholarship will not take the use of digital technology seriously until one demonstrates how its tools improve the ways we explore and explain our cultural inheritance – until, that is, they expand our interpretational procedures\" (McGann, Radiant Textuality xii; my emphasis). Collex facilitates primary interpretive gestures of exploration and explanation in a broad and socially-networked manner, and aims to form a locus for further expansion of interpretive methods in digital humanities.The first formal iteration of Collex (released in February 2007) federates more than 60,000 digital objects of 19th-century literature, art, culture and criticism from the most prominent and acclaimed online journals, archives, and repositories in the field. This pilot project forms the core of NINES, the Networked Infrastructure for Nineteenth-century Electronic Scholarship, a trans-Atlantic federation of scholars and of peer-reviewed primary and secondary materials constituting a federated collective. Endorsed by the NINES steering committee and under development for the past year at ARP, the University of Virginia's Applied Research in Patacriticism lab, Collex is both the central clearing-house for NINES and the interpretive hub around which we hope a vital community of scholars and students will coalesce.Humanists eager to develop new ways to integrate and explore digital works currently lack crucial institutional and technical resources. Even the best models that scholars of (for instance) nineteenth century literature and culture now follow and imitate — the Whitman Archive, Romantic Circles, the Rossetti Archive, the William Blake Archive — are stand-alone projects that can only be loosely integrated through web browsers, even when shared through OAI protocols. As a consequence, what you see now on the web is what you get: an agglomeration of sites and projects whose content is atomized and whose scholarly and educational value is indeterminate. While it is possible for tech-savvy scholars, using ad-hoc tools and methods, to produce and distribute annotated, re-organized, or selected versions of existing online resources, they presently lack coordination within a peer-reviewed digital publishing environment. Because of this, their productions — personal web pages and online course packets — are difficult to maintain, are not readily interoperable or standards-compliant, and are easily dismissed as heterogeneous grab-bags of links. NINES was founded to work against this debilitating situation.The inherent complexity of available resources is a further obstacle to the penetration digital humanities into the disciplines. Collex is designed to aid humanities scholars doing research in complex digital collections like the Rossetti Archive(its initial test case) or within federated research environments like NINES. Such environments often stymie their users through the sheer quantity of information made available to them in top-level tables of contents, sitemaps, and idiosyncratic search engines. Our tool operates under the assumption that the best paths through a complex digital resource are those forged by use and interpretation. A Collex approach works to assist scholars in recording, sharing, and building on the interpretive purposes to which they put their online teaching and research environments.Collex uses semantic web principles and technologies to explore and develop the research potential of the digital scholarship aggregated in NINES. Two critical concepts embodied in a NINES environment shaped by the Collex application fall under the rubrics widely known as \"faceted classification\" and \"folksonomy.\" Facets and folksonomies structure an approach to descriptive metadata. They generate an evolving interface between the fully-integrated peer-reviewed electronic resources that constitute NINES and the user communities that re-imagine NINES content through interpretation, contextualization, and critical and creative re-fashioning.\"Full integration\" means that each of our NINES-participating resources has contributed a package of metadata representing all of the digital objects they wish to make browseable, collectible, and available to users for re-purposing within Collex. An important innovation of Collex lies in the way these objects are defined by their contributing editors. Collex uses a Dublin Core flavor of RDF, the \"resource description framework\" of the semantic web, to define collectible \"objects\" without limiting them to their expression as web pages. Where other social bookmarking tools are designed to allow collection and annotation of whole web pages, Collex allows contributors of resources to make finer-grained distinctions, and users of the system to build collections and exhibits more attuned to the patterns of attention in humanities scholarship.A clear example of interpretive modeling through object definition is the Collexrepresentation of a book of poetry in the Rossetti Archive. Using XSLT transformations, we have created RDF metadata for intellectual and material objects at differing levels in this book. One RDF object (typed as a secondary resource, with supplemental genre and date identifiers) expresses the editor's commentary on the book as a concept. Another object, also articulated in metadata, expresses one particular edition of the book. Within that high-level expression, each page of the book has been shared with Collexusers as a collectible object, as has each poem on each page. Such fine disambiguation ensures that Collex users can locate, annotate, and exhibit objects specifically suited to the scholarship they wish to perform – whether their attention is focused on bibliographic, social, or textual matters. It also ensures that archive maintainers have the fullest control, in the Collex environment, over the use of their intellectual property and the artifacts they minister.Because RDF objects share a common (and relatively simple) metadata scheme, they are discoverable through \"facets\" in the Collex search and faceted-browsing interface. Faceted classification is a non-hierarchical means of expressing ontological relationships. Any given object will share a number of facets with other objects – common dates, genres, authors, etc. Exposing these facets makes it possible not only for users to manually \"drill down\" into certain categories or explore lateral relationships, it also opens possibilities for algorithmic serendipity in research. In other words, Collex can exploit formally-expressed facets to offer more options and avenues to users interested in a particular object: \"more like this\" – more objects in the repository sharing one or more attributes with a researcher's subject of attention. Even more interesting is the ability of Collex to record and analyze user activity, and to translate the products of user interaction into RDF objects within the system itself. In this way, in addition to \"more like this,\" Collex can suggest to recent collectors of a particular object that they view the published collections and exhibits into which other users have placed the object, or objects like it. Because this content can be expressed as subscription-based RSS feeds, a web service, or an API, it is possible for the maintainers of scholarly resources to patch into Collex directly from their individual web page or listserv interfaces, offering information about user annotations and re-mediations for any given object without requiring users to visit Collex at all.All Collex activity takes place within the ordinary web-browsing environment that scholars presently use to access digital resources, and will require nothing in the way of plugins or downloads. The overhead (in terms of initial metadata production) for contributors of resources to the federated collections in which Collex can operate has also been kept purposely low, and is thoroughly compatible with Open Archives protocols. We predict that both of these factors – combined with the strong endorsement and example of NINES – will facilitate the adoption of Collex into day-to-day practices of humanities scholars in networked research and publishing environments.Screenshotspaper_152_nowviskie_1.jpgA Collex sidebar list view (user-collected objects in the \"visual art\" genre) with the same constraint in the faceted browserpaper_152_nowviskie_2.jpgA different constraints set, and a detail view of one object (for tagging, annotation, and knowledge discovery) in the sidebar",
        "article_title": "Collex: Facets, Folksonomy, and Fashioning the Remixable Web",
        "authors": [
            {
                "given": "Bethany ",
                "family": "Nowviskie",
                "affiliation": [
                    "Applied Research in Patacriticism University of Virginia"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "folksonomy",
            "faceted browsing",
            "social bookmarking",
            "semantic web",
            "open archives"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "IntroductionThe numerous visual metaphors that describe cognitive processes hint at the nexus of relationships between what we see and what we think. We say we ‘see’ when we mean we understand, we try to organize and make our ideas ‘clear’ by bringing them into ‘focus’, and so on. When faced with tasks that require substantial thought or organization of ideas we will often reach for a pen and paper to ‘sketch out’ (another visual metaphor) our thoughts. We have a deep understanding that we can enhance our thought processes by finding ways of linking external perception with our interior mental processes. Graphic aids to thinking are not new but the development of computers has provided a new medium with remarkable functionality. This in turn offers the potential for new research methodologies that amplify cognition. These tools serve two distinct purposes. One of these is often described by the hackneyed phrase “A picture is worth ten thousand words”.This is commonly believed to be based on a “Chinese proverb” however Paul Martin Lester believes that it was in fact made up by the advert writer Federick R. Barnard. See  and Printers’ Ink, March 10, 1927. This misses the true point of visualization as what is being described here is just a matter of transmission, of having high bandwidth to transmit large volumes of information. Of far greater importance is the ability of these tools to allow visual perception to be used in the creation or discovery of new knowledge. Knowledge is not transferred, revealed, or perceived, but is created through a dynamic process. This raises epistemological issues concerning visualization and points the way to an intellectual approach to the subject.Computer visualization techniques began as a methodology for understanding the meaning of large volumes of numeric data. Scientists needed a means of visualizing the flood of data that can be collected by modern monitoring and measuring instruments. The National Science Foundation initiative on Scientific Visualization launched in 1985 led in a very short time to Scientific Visualization becoming recognised as not just a methodology but a discipline in its own right. A similar trend may now be developing in the Humanities. For example the London Charter seeks to establish principles for the use of 3D visualization in research and communication of cultural heritage that ensure the intellectual integrity of the methods and outcomes derived from it. How is visualization being used in the humanities at the moment? Is there a potential counterpart to Scientific Visualization in the digital humanities – a field of ‘humanistic visualization’? What issues does it raise? Where is the common ground and what are the intellectual issues involved?Visualization in the Digital HumanitiesThere are many ways of structuring an examination of the use of visualization in the humanities; by discipline, by type of information structure and so on. To set the context for this paper I have chosen to look at the type of data that is being visualized. The boundaries of data types are sometimes blurred but a starting point could be as follows Numbers. Quantitative analysis and visualization has been an established tool in many humanities disciplines for a long time. It is found in generic statistical analysis software or embedded in specialised applications such as text analysis. It is gradually permeating into new areas of the humanities through work such as that of Franco Moretti (2005) who has argued for a wider application of quantitative methods in areas such as literary history.Text: Visualization techniques using tables and graphs have been commonplace in text analysis for many years. These visualizations are sometimes variants of statistical visualizations of numeric data (as in word frequencies) but in other cases they are specialised visual forms of text analysis. Projects such as the TAPoR and NORA Projects are developing imaginative new visual forms and applications. These new methods apply not just to the visual representation of the results of analysis but also to the visualization of the texts themselves. This aims to support interpretive scholarship by allowing areas or relationships of interest to be identified within large volumes of text. Projects have explored specific texts in this way, for example Dante’s Inferno , Hume’s Dialogues and The Shape of Shakespeare but there is substantial scope for a tool that could be applied to any text. The use of the word ‘tool’ here should not be taken to imply that this is a computational problem, this is far from the truth, as what is being grappled with here are conceptual problems. For example, what is a text? How should it be displayed visually? Humanities computing provides a medium with a myriad of possibilities of representation; uni-dimensional or two dimensional physical objects, abstract objects showing relations among words or between words and annotations, and animations. Further questions arise from this work; is there a need for representational as well as interpretive markup? What are the relationships between text visualization and the interpretation of texts? The visualizing of texts is also an area which links humanities computing work to the arts, for example the interactive installation Text Rain by Camille Utterback and Romy Achituv.Narratives and relationships. These can be grouped and referred to as diagrams . Edward Tufte has drawn public attention to this style of diagram; famous examples include the work of Playfair and Charles Minard’s narrative graphic of Napoleon’s ill-fated campaign against Russia in 1812. In many respects this has been a neglected field since the advent of digital tools because diagrams of this type are not easily implemented in software. They are also potentially of enormous value to the humanities as narratives and the study of relationships between people, events and artefacts are studied by many disciplines.Space. The study of spatial relationships and a sense of place occur in many humanities disciplines. This area is dominated by Geographical Information System (GIS) software but this was developed for scientific data and is not ideally suited to the qualitative data used in the humanities (Jessop, 2006). Digital dynamic maps are one of many alternatives offering media that are better suited to humanists (Jessop, 2006). The Electronic Cultural Atlas Initiative (ECAI) with its utilization of Timemap provides an indication of possible future developments.Time. Digital visualization provides a very powerful medium for temporal visualisation. Tools such as timelines allow one to explore the development of complex historical events and the inter-relationships between precursor events. They are of obvious value in the study of history where later events build upon earlier ones but they can also be applied elsewhere. Historiography and literary criticism are both histories of accumulated comments on a subject. Matt Jensen (2006) has developed a number of timeline tools which are intended to answer the styles of questions that are asked by humanists, for example in the case of political scandals questions of the style ‘who knew what and when’ or for exploring the response to an author’s writing over a period of many years3D Visualization. Much of this work has centred upon visualizations of the built environment. It is of interest to not only historians and archaeologists but also anyone who seeks to find out how the buildings of the past worked in human terms, for example the Pompey Theatre and Theatron projects are based on historical and archaeological data but are primarily of interest to scholars of theatre studies. 3D visualization is of especial interest in the context of this paper because there is currently great deal of work focused on defining not only good practice (see ICT Methods Network) but also principles for maintaining the intellectual integrity of such work, for example the London Charter. It may therefore provide pointers for similar work in the development of a broader humanistic visualization as a whole.Any demarcations between the applications of visualization between different disciplines are misconceived. There is much common ground offering considerable potential for humanities computing and the digital humanities. This is where we need to focus our attention if digital visualization is to achieve recognition as a rigorous intellectual activity in research and teaching.ConclusionWe accept that texts and documents are produced as readings resulting from acts of interpretation between the reader and the text; we now need to regard images in the same way. Every representation, visual or otherwise, is an effort to structure an argument and as such it is a rhetorical device. We need to understand the relationship between what is being communicated and how it is being communicated.Information graphics, and indeed images generally, can be considered as historical artefacts themselves, filled with interesting incidental and substantive information embodied in their production, style, and graphical properties. But perhaps more importantly, they are expressions of procedures for generating knowledge through the act of visualization and ways of displaying knowledge embodied in visual imagery (Drucker, 2003).Visualization addresses epistemological and pedagogical issues that are common to the digital humanities and are at the forefront of the developing discipline of humanities computing. This is a vast topic the main aim here is to identify some of the underlying intellectual issues arising from visualization and the use of images in digital humanities scholarship.",
        "article_title": "Digital Visualization as a Scholarly Activity",
        "authors": [
            {
                "given": "Martyn ",
                "family": "Jessop",
                "affiliation": [
                    "Centre for Computing in the HumanitiesKing's College London"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-03",
        "keywords": [
            "images",
            "digital humanities",
            "humanities computing",
            "visualization"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Many operations are more conveniently performed on a graph representation than on a linear representation of a marked up document, and vice versa. Therefore, it is sometimes important to ensure that no relevant aspect of the information contained in a document represented in one of these forms is lost or distorted when the document is converted to the other form.Conventional methods for converting between XML documents and their graph representations [W3C 2000] are typically seen to preserve such information; standards and methods have been established for ensuring what is in most contexts considered full preservation of all relevant aspects of the linearization [W3C 2001].However, what is considered relevant may of course vary, depending on context of use. It would probably be hard to find serious arguments to the effect that literally all aspects of the linear representation of a document are relevant for any generally interesting use. Typically, conventional conversion methods are not guaranteed to preserve e.g. attribute order, declaration order, and insignificant whitespace. But it is not hard to find complaints about, for example, lack of preservation of attribute order in certain applications.Our focus in this paper is on methods for the preservation of element serialization order in marked up documents which make use of mechanisms for representing non- hierarchic complex structures such as overlapping, discontinuous and virtual elements. (For convenience, we use the term \"complex structures\" to refer to such phenomena.) We do not wish to claim that preservation of element order is always or even generally relevant, our aim is limited to providing a method for such preservation in cases where it is considered relevant.The customary graph representation of XML is in the form of an \"XML tree\", a restricted kind of directed acyclic graph (DAG). More specifically, XML trees are DAGs with single parenthood and total ordering on leaf nodes. For certain purposes, however, a different kind of graph representation has been proposed, the so-called Goddag [Sperberg-McQueen and Huitfeldt 2000]: Roughly, Goddags are like XML trees except that they allow multiple parenthood and do not require a total ordering on leaf nodes; leaf nodes may be ordered only relative to their immediate parents. (Thus, XML trees constitute a subset of Goddags.)For certain purposes this data structure provides a more convenient representation of complex structures than XML trees. Documents using different XML mechanisms for representing such structures in linear form (e.g. milestones, fragmentation, virtual elements etc. [Barnard et.al. 1995, Sperberg-McQueen and Huitfeldt 1999]) can be mapped on to Goddags, though not without knowledge of application-specific semantics of the markup vocabulary. The experimental markup system TexMecs [Sperberg-McQueen and Huitfeldt 2001] offers mechanisms for the representation of complex structures which can be mapped on to Goddags independently of such knowledge.However, in both cases, i.e. whether the graph is built from XML or TexMecs, reserialization from the graph is not in general guaranteed possible without changes to the structure and order of elements in the original linearization. For example, if an XML document has used milestones or fragmentation of elements to represent overlapping elements it is possible to build a Goddag representing the non-hierarchic structure of the document. But when reserializing back to XML, the Goddag does not contain any information about which elements to represent as milestones or as fragmented elements.Similarly with TexMecs: Some element structures can be represented by alternative serialization constructs, and the Goddag as currently defined does not preserve information about the choice of construct in each particular case. In TexMecs the problem is made more severe by the fact that the graph does not, in the case of e.g. virtual or discontinuous elements, preserve complete information about the serial orderof elements in the original input.Consider the following example, marked up according to TexMECS, and illustrating how a discontinuous constituent element <q> may be represented. (1)       <s|<q|Why|-q>, he said,<+q| me?|q>|s> In TEI-based XML, the example could e.g. be marked up as:(1’)       <s><q part=\"I\">Why<q/>, he said,<q part=\"F\"> me?</q></s>The resulting Goddag, whether based on the XML or the TexMecs input, would normally look like this:For simplification, we are consciously ignoring certain unresolved issues concerning the representation of discontinuous elements in Goddags [Huitfeldt and Sperberg-McQueen 2006].(2)paper_217_huitfeldt_1.jpgSince the second leaf node (containing the string \", he said,\") does not share any parent with either of the two other leaf nodes, it is not ordered with respect to these. Therefore, the linearization in (1) is equivalent to the following two linearizations, both placing the second leaf node in a position relative to the other two that it does not have in (1):(3)       <s|<q|Why me?|q>, he said,|s> (4)       <s|, he said,<q|Why me?|q>|s> Thus, the Goddag in (2) would be the same whether built from (1), (3) or (4). Similarly, a choice whether to linearize (2) in the form of (1), (3) or (4) will either have to be arbitrary, or based on purely practical considerations.A solution to the linearization problem lies, we propose, in considering the Goddag used for representing marked up text as a path ordered directed acyclic graph; a Podagra. Building a Podagra from (1), we get a graph consisting of three paths, in the order indicated as follows:The simplicity of the example allows us to indicate nodes by their generic identifiers. I.e. the three occurrences of \"s\" all indicate the single node labelled s, and the two occurrences of \"q\" indicate the single node labelled \"q\".(5)1.  s → q → \"Why\"2.  s → \", he said,\"3.  s → q → \" me?\"Building a Podagra from (3), however, produces the following path order:(6)1.  s → q → \"Why me?\"2.  s → \", he said,\"whereas from (4) we get the following paths:(7)1.  s → \", he said,\"2.  s → q → \"Why me?\"The Podagras (5), (6) and (7) all correspond to the Goddag (2), but each maps uniquely  to the linearizations (1), (3) and (4), respectively.In the full paper, we will present an algorithm yielding Podagras from TexMECS  documents containing different linearizations also of overlapping and virtual elements.  We intend to show how path ordered Goddags can faithfully restore the original  linearization of such documents.Since TexMECS is a purely experimental markup language, these results may be said to  have limited practical relevance. However, a number of projects currently build Goddags  from XML-encoded documents (by the use of application-specific semantics). Therefore,  we also hope to indicate how the proposed method may used for preservation of the  original linearization of XML documents using well-known methods for representation of  overlapping, discontinuous and virtual elements.",
        "article_title": "Preserving Information About Linearization in Document Graphs",
        "authors": [
            {
                "given": "Lars G. ",
                "family": "Johnsen",
                "affiliation": [
                    "University of Bergen"
                ]
            },
            {
                "given": "Claus ",
                "family": "Huitfeldt",
                "affiliation": [
                    "University of Bergen"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "Goddag",
            "linearization",
            "DOM",
            "graph",
            "XML"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Description of ProjectMonArch (The Monastic Archaeology Project) is a collaborative project based on the excavations at the Abbey of Saint-Jean-des-Vignes in Soissons, France under the joint directorship of Sheila Bonde (Brown University) and Clark Maines (Wesleyan University). The monastery at St.-Jean-des-Vignes remains with us today in two media: in the physical remains of the abbey and in the textual remains left behind by its inhabitants. The most significant of the latter are the community's Customary, which describes the roles and activities of the abbey's inhabitants over the course of the religious year, and the Abbey's Obituary, which lists, by month and day of death, the members of the monastic community and laics who had provided service or gifts to the monastery, with a description of their donations.Two websites have been developed as part of the MonArch Project. The first was implemented at Wesleyan (), and provides thorough documentation of the excavations, annual field reports, and inventories of finds. The second, which is the focus of this paper, is a research effort in its own right, one that explores the ways in which encoded representations of archaeological data may be used enhance their use. The often complex interrelationships among textual, architectural, and archaeological evidence are difficult to represent and explicate in traditional formats; the work described here attempts to use digital representations to create a new form of scholarly expression.Research QuestionsMonArch has always focused beyond a simple description of concrete artifacts to a thoroughgoing investigation and elaboration of a concept: from the outset, the project was conceived as a multi-disciplinary endeavor that took monasticism rather than the monastery as its object of study.Sheila Bonde and Clark Maines,  Saint-Jean-des-Vignes in Soissons: Approaches to its Architecture, Archaeology and History,  Biblioteca Victorina vol. XV (Turnhout: Brepols Press, 2003). Therefore, to the extent that its digital manifestation is to be a direct reflection of the project's aims, it too must take as its focus an abstraction and not a collection of materials.Such a focus makes designing the digital reflection more challenging. It cannot be a simple linked, digital collection, because such an organization would put undue emphasis on the monastery and its artifacts. And because the site needs to be built around concepts and questions — about monasticism and the archaeology of monasticism — the digital project cannot be conceived or designed without considerable knowledge of those concepts or an understanding of medieval social, economic, architectural, and religious history and the role of archaeology in elucidating it all. It must, in other words, express the archaeological definition of monasticism Bonde and Maines are deriving from their work, a definition that is based on the following factors: The monastic complex as the physical expression of spiritual, social, and economic motives;The functions of the abbey's structures;The communitas who used these structures and the routine of their lives;The economy of the surrounding region — the farms, mills, priories, parishes, and other holdings that constituted the abbey's domain;The place of the abbey (defined as both buildings and community) in the local, regional, and inter-regional networks of power and influence;The way these phenomena changed over time.The relationship of the texts — both primary and secondary — to the archaeological data is complex, because the archaeological material supports and illustrates the texts at the same time that the texts contextualize the archaeology and architecture. These two kinds of remains —  the textual and the physical — become two inter-referential and mutually reinforcing centers from which the concepts of monasticism emerge. The research focus of the current digital project is on designing an infrastructure and and an interactive interface that simultaneously represent the authors' arguments and allow users of the site to form their own interpretations.Considering UseWe argue that it is most fruitful to conceive the digital manifestation of the MonArch project as a reading. Insofar as its goal is to provide a definition, the resource is fundamentally rhetorical: it has an argument, and it supports its claims with evidence drawn from a variety of evidential sources. That argument, generally stated, entails an archaeological definition of monasticism, whose refinement is the ongoing and iterative task of the MonArch project. The task of the digital resource, therefore, must be unusually subtle: it must express this archaeological definition through an intricate blend of textual and material evidence, and it must provide a way for readers to interact with and question the argument and its claims.In fact, insofar as the digital resource is the manifestation of this argument, the mutually referential presence of both encoded texts and artefactual simulacra constitutes a claim in itself, a claim that a definition of monasticism can be manufactured through a reading of monastic text, monastic space, and monastic time, and perhaps only in this way. That is to say, the boundaries established by the site (and here the web site and the excavation site perhaps begin themselves to lose their boundaries) determine how monasticism is to be understood.Research Goals Our research, then, is focused on establishing a layered set of models that enable researchers to articulate their understanding of monasticism and allow scholars (students, readers, users) to interact with that understanding. Underlying the whole project is a fundamental data model that represents the characteristics of the texts, spaces, buildings and artifacts that form the object of study. Overlaying the data model is another model, that of the relationships that embody the intellectual connections that the researchers have made as they work over their materials and which embody their definition of monasticism. These connections are the evidence for their claims. At the topmost layer is a model of user interaction, one consisting of visual juxtapositions that illustrate the relationships among the historical evidence and that enable further questioning. The problem of structuring information in order to enhance argument is part of a larger problem. One of the major powers promised by digital resources is the instantiation of the kind of textuality envisioned by postmodern theorists like Roland Barthes and Jacques Derrida, among others: the simultaneous availability of vast amounts of information in a form that makes the interconnections, both explicit and implicit, traversable. Yet benightedness is a clear danger: it is all too easy to become lost: in hyperspace, in the library, in the labyrinth, or in the wood of error. As digital resources grow in size and complexity, the need for prospects — lookouts, overviews of the textscape — becomes ever greater. Digital resources must therefore become responsive: when a reader examines an argument or claim in a digital publication, the resource should respond to her, helpfully putting on the virtual desk before her materials that are relevant to her evolving inquiry. Thus, in loftiest terms, our goal must be to assist, supplement, and augment a human agent's investigation of accumulated cultural knowledge, a goal congruent with the aims of research over the past half-century in fields from information retrieval to artificial intelligence.",
        "article_title": "The Abbey Inside the Machine: The MonArch Project",
        "authors": [
            {
                "given": "Clifford Edward ",
                "family": "Wulfman",
                "affiliation": [
                    "Brown University"
                ]
            },
            {
                "given": "Elli ",
                "family": "Mylonas",
                "affiliation": [
                    "Brown University"
                ]
            },
            {
                "given": "Anne ",
                "family": "Loyer",
                "affiliation": [
                    "Wesleyan University"
                ]
            },
            {
                "given": "Sheila ",
                "family": "Bonde",
                "affiliation": [
                    "Brown University"
                ]
            },
            {
                "given": "Clark ",
                "family": "Maines",
                "affiliation": [
                    "Wesleyan University"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "rhetoric",
            "archaeology",
            "modelling",
            "xml"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "One of the crowning achievements of the 18th century Enlightenment was the Encyclopédie ou Dictionnaire raisonné des sciences, des arts et des métiers, par une Société de Gens de lettres, edited by Diderot and d'Alembert. Published in Paris between 1751 and 1772, in 17 volumes of text and 11 volumes of plates, it contains 74,000 articles written by more than 140 contributors.The ARTFL implementation of the Encyclopédie is discussed in Robert Morrissey, Jack Iverson and Mark Olsen, \"Présentation: L'Encyclopédie Electronique\" Robert Morrissey and Philippe Roger, eds.,  L'Encyclopédie de réseau au livre et du livre au réseau, (Paris: Champion, 2001): 17-27, and Leonid Andreev, Jack Iverson and Mark Olsen, \"Re-engineering a War Machine: ARTFL's Encyclopédie\"  Literary & Linguistic Computing 14.1 (1999): 11-28.The Encyclopédie was a massive reference work for the arts and sciences, as well as a machine de guerre which served to propagate Enlightenment ideas. The impact of the Encyclopédie was enormous. Through its attempt to classify learning and to open all domains of human activity to its readers, the Encyclopédie gave expression to many of the most important intellectual and social developments of its time.The scale and ambition of the Encyclopédie inspired its editors to adopt three distinct modes of organization which, taken together, Diderot described as encyclopedic: dictionary, hierarchical classification, and the renvois (cross-references). The interaction of these three modes has led modern commentators to describe the Encyclopédie as an \"ancestor of hypertext\" and to depict Diderot as \"l'internaute d'hier\"Eric Brian, \"L'ancêtre de l'hypertexte\", Les Cahiers de Science et Vie 47 (Oct. 1998): 28-38.. D'Alembert underscores the importance of the organization of knowledge in the Discours Préliminaire:  As an Encyclopedia, it is to set forth the order and connection of the parts of human knowledge. As a Reasoned Dictionary of the Sciences, Arts, and Trades, it is to contain the general principles that form the basis of each science and each art ... and the most essential facts that make up the body and substance of each.English translation cited in Nelly Hoyt and Thomas Cassier's \"Introduction\" to Encyclopedia (1965): xxiii (our emphasis). Of the three modes of organization, the dictionary mode (organization of entries in alphabetical order) is certainly the simplest and the most arbitrary. The second mode of organization is classification, wherein each dictionary entry is assigned to a \"class of knowledge,\" placing it within the \"order\" of human understanding, as depicted in the Système Figuré des connaissances humaines. Modeled after Bacon's classification of knowledge and Enlightenment theories of epistemology, all understanding is founded upon memory, reason, or imagination, with numerous categories and sub-categories branching out from these three faculties.For various representations of the Système Figuré and the Editors' description, see  and . However, simply placing an entry into this hierarchy of knowledge was insufficient to indicate the interconnections of knowledge. Thus, Diderot created an extensive system of renvois, the third mode of organization, providing a lattice of interconnections between individual leaves of the tree as well as between classes of knowledge.Blanchard and Olsen examined the structure of the renvois generating a \"mappemonde\" of the cross-references and node level classes of knowledge. See Gilles Blanchard and Mark Olsen, \"Le système de renvois dans l'Encyclopédie: une cartographie de la structure des connaissances au XVIIIème siècle\",  Recherches sur Diderot et sur l'Encyclopédie 31-32 (April 2002): 45-70.The central role of the classification system in the intellectual objectives of the Encyclopédie editors is indicated by the extent to which it has been discussed and debated by both contemporaneous scholars and later researchers. The editors were remarkably diligent in assigning classes of knowledge to each article and sub-article. Of the 73,840 main and sub articles, 55,227 were assigned classes of knowledge. The editors were, however, somewhat less diligent in maintaining a precisely controlled list. Thus the classifications as found in the text are an amalgam of abbreviations, conflations, and even entries that are not found on the Système Figuré. We have recently completed orthographic normalization of the classes of knowledge assigned to each article,This project was accomplished in collaboration with Professor Dena Goodman at the University of Michigan.\" resulting in some 54,289 articles with 2,600 normalized classes of knowledge. The twenty most frequent classifications by number of articles are:5513 Géographie4794 Géographie moderne3084 Géographie ancienne2396 Jurisprudence2304 Grammaire1894 Marine1483 Commerce1277 Histoire naturelle. Botanique1194 Histoire moderne1115 Mythologie1069 Histoire naturelle889 Histoire ancienne796 Medecine730 Architecture689 Jardinage682 Littérature627 Maréchallerie614 Botanique558 Histoire ecclésiastique517 ThéologieLike the Système Figuré, these classifications are a reflection of how knowledge was ordered and classified in the 18th century. Given the assumptions that ontologies are historically contingent and that the Encyclopédie is by far the most consistent and coherent representation of the structure of 18th century knowledge in French, this paper reports the results of our current experiments using machine learning and data mining techniques to understand and exploit this unique resource. Our initial objectives are three-fold. First, we plan to examine the relationship of the classifications to the content of the articles using machine learning techniques to identify feature sets that characterize classes of knowledge in the 55,000 articles classified by the editors of the Encyclopédie. Secondly, we will apply these feature sets to the 19,500 articles for which we do not have a class of knowledge and evaluate the accuracy of classification by randomly selecting articles with known authors which scholars will then inspect. Most contributors to the Encyclopédie worked on fairly specific domains -- Rousseau contributed exclusively on music, for example -- so we can use authorship as one control for judging the accuracy of classification. Similarly, the cross-references will also serve as an evaluation control, since 50% of the renvois link to articles within the same class of knowledge. Finally, we plan to apply these feature sets to the unclassified \"plate legends\" in an effort to determine accuracy by examining the degree to which classification of the plate legends reflects the relationship of particular plate legends to particular articles.For our initial experiments, we have extracted the text from all articles that are more than 100 words in length, and which are categorized within one of the 50 most frequent normalized classifications. Explicit markers of class of knowledge, present at the beginnings of these articles, were removed to ensure that they do not provide facile criteria for classification. The texts are tokenized and lemmatized, and frequencies of words and lemmas are computed both globally and for each article. Words and lemmas with more than 100 occurrences in the entire Encyclopédie were used as attributes, and vectors for each article were generated from the number of occurrences of each attribute in that article.We are using the SMO implementation of a support vector machine in the WekaIan H. Witten and Eibe Frank,  Data Mining: Practical Machine Learning Tools and Techniques 2nd ed. (Morgan Kaufmann, 2005) and  data mining engine for initial experimentation on smaller data samples, and an SVM-LightSVM-Light:  See T. Joachims, \"Making large-Scale SVM Learning Practical\",  Advances in Kernel Methods - Support Vector Learning, B. Schölkopf and C. Burges and A. Smola eds. (MIT-Press, 1999). Note that we are using a parallel implementation. See , G. Zanghirati, L. Zanni, \"A Parallel Solver for Large Quadratic Programs in Training Support Vector Machines\",  Parallel Computing 29 (2003): 535-551 and L. Zanni, T. Serafini, G. Zanghirati, \"Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems\",  JMLR 7 (July 2006): 1467-1492. classifier for larger datasets. While support vector learning algorithms are very effective for classification problemsS. Dumais, et. al., \"Inductive learning algorithms and representations for text categorization\",  CIKM-98, 1998., we are also evaluating several other data metrics and machine learning techniques, including information gain statistics and J48 decision tree classification as implemented in Weka, to examine the the most salient features that are used in the classification process and to test the effectiveness of various feature set selections.See the discussion conformity and uniformity in Chih-Ming Chen, et. al. \"A Hierarchical Neural Network Document Classifier with Linguistic Feature Selection\"  Applied Intelligence 3 (December 2005).Results from preliminary experimentation indicate that SVM classifiers applied to the articles of the Encyclopédie are very effective at distinguishing articles from different classes. We examined 936 unlemmatized articles in our sample dataset belonging to the classes Medecine (499) and Mythologie (437). The Weka SMO classifier using default options with 10-fold cross-validation correctly recognized 98.29% of the articles (920/936). Under the same parameters, the Weka J48 tree classifier achieved slightly lower performance (91.66% accuracy). The decision tree showed a clear split on medical content words, such as maladie, humeurs, inflammation, and so on. Such strong performance may be due to the fact that one would not expect to find similar vocabularies in articles dealing with medicine and mythology. We achieved similar performance by classifying 2,448 articles equally divided between modern and ancient geography. The SMO training achieved 100% accuracy with 92.2% accuracy on cross-validation. Inspection of the most important features in both J48 tree and InfoGain measures shows a strong preference for classical authors (Pline, Ptolomée) and places (Gaule, Thrace), and the strings \"l\", \"lib\" and \"liv\", which correspond to citations of classical authors (e.g. Pline, l. IV. c. xvj.).We checked these using the PhiloLogic build of the  Encyclopédie (), suggesting the importance of checking text mining results with full text analysis systems. Distance and location terms (lieues, long., latit.) are strongly correlated with modern geography. Furthermore, the function words \"selon\" and \"dit\", which are far more prevalent in ancient geography articles as the authors were citing classical descriptions, are given high InfoGain scores. We anticipate assigning classes of knowledge to articles that were not originally classified by the editors iteratively by comparing all unknown articles to specific classes of knowledge rather than trying to classify all unknown articles en masse. To test this approach, we assembled two sets of articles each containing 1,209 instances. The first set contained articles categorized by the editors as belonging to ancient geography, while the second set was constituted by selecting, for each article in the first set, an article as close as possible in length but belonging to a different class of knowledge. Using SMO training, we achieved 97.8% accuracy with standard 10-fold cross-validation. Again, the most heavily weighted features were terms denoting classical authors and place names, along with a greater preponderance of more general geographic terms, comporting nicely with a reasonable human's intuitive understanding of what makes a document on ancient geography distinct from another documents. We further validated our results by running another experiment identical to the first except that each article was randomly labeled as either ancient geography or not ancient geography, irrespective of its true classification. The principle of Random Falsifiability states that if random labels can be learned with the same ease (for SVM, 'ease' can be defined as proportion of support vectors required A. Ruiz and P.E. López-de-Teruel, \"Random Falsifiability and Support Vector Machines\" ().) as true class labels, the method must be rejected as unreliable. After 10-fold cross-validation, SMO achieved a mere 50.2895% accuracy on the classification, barely surpassing random chance. That our method cannot learn the random labels at all suggests that our success in discrimination is in fact based on inherent differences between the two classes and not merely a greedy model's exploitation of arbitrary patterns in the data distribution.The SMO model derived from comparing ancient geography to a random selection of articles in other classes allows us to test classification on a set of unclassified articles. To do this, we assembled 5,000 randomly selected articles containing more than 100 words for which classification was unknown and with attributed authorship. We then applied the ancient geography SMO model to this set in an effort to identify articles pertaining to this category. The recall of this experiment was far too high. In the future we intend to implement a classifier that reports a numeric score rather than a simple binary categorization. There were, however, within the results a number of correctly classified articles such as the river ASOPE and the articles GARAMANTESand Ionique Transmigration. Many of the misclassified articles, such as ADONIES, ou FESTES ADONIENNES and Danse astronomique, pertain to classical history, mythology and other related fields. In addition to implementing a ranking classifier, we will also investigate moving up the tree of knowledge in order to use a coarser classification scheme; e.g., rather than remaining at the leaves of \"ancient\" and \"modern\" geography, we would use the branch of geography itself as a general category. The impressive performance of machine learning algorithms suggests that the editors of the Encyclopédie were quite judicious in their assignments of classifications, a claim which will be tested further in the full paper. Examination of the features most effective in classification tasks will establish a sort of thesaurus which will give scholars a better understanding of the organization of knowledge during the Englightenment. Furthermore, we believe that the creation of well-verified training sets on this large corpus will allow us to test the degree to which we may profitably apply what the systems have learned to articles and plate legends which were not classified at the time, using the contemporary ontologies. If this series of experiments is successful, we would anticipate using the training sets from the classifications in the Encyclopédie to attempt to classify passages in other 18th century French documents.",
        "article_title": "Mining Eighteenth Century Ontologies: Machine Learning and Knowledge Classification in the Encyclopédie",
        "authors": [
            {
                "given": "Russell ",
                "family": "Horton",
                "affiliation": [
                    "Digital Library Development CenterUniversity of Chicago"
                ]
            },
            {
                "given": "Robert ",
                "family": "Morrissey",
                "affiliation": [
                    "ARTFL ProjectUniversity of Chicago"
                ]
            },
            {
                "given": "Mark ",
                "family": "Olsen",
                "affiliation": [
                    "ARTFL ProjectUniversity of Chicago"
                ]
            },
            {
                "given": "Glenn ",
                "family": "Roe",
                "affiliation": [
                    "ARTFL ProjectUniversity of Chicago"
                ]
            },
            {
                "given": "Robert ",
                "family": "Voyer",
                "affiliation": [
                    "ARTFL ProjectUniversity of Chicago"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-05",
        " keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "I shall present my latest work on the theory of phonemic accumulations and its application to the study of poetry. Computerized stylistics tends to concentrate on words: word frequencies, word co-occurrences, word collocations, and word distributions. My computer-assisted study of stylistics concentrates on the phonemic content of the texts. Computerized phonemic analysis is beginning to yield interesting insights into written texts, especially those, such as poetry, for which sound patterning constitutes a significant element. In A Companion to Digital Humanities (2004), Ian Lancashire mentions sound as a future source of inquiry in textual stylistics: “As the implications of cognitive research become understood, text-analysis systems may change, and with them stylistics. Texts in languages whose spelling departs widely from its sounding will likely be routinely translated into a phonological alphabet, before processing, as researchers recognize the primacy of the auditory in mental language processing” (“Cognitive Stylistics and the Literary Imagination”). I have begun undertaking this work with a computer application that translates poems into their broad phonetic transcriptions. The program can then provide visualizations of the phonemic content of the poems, and it can perform calculations on the content.The theory of phonemic accumulations is based on the theory of the persistence of vision: the effect of a phoneme when reading is carried through to the following phonemes. At the same time, its effect tapers off. When the same phoneme is encountered while the effect of the first phoneme has not been nullified, a cumulative effect of the two phonemes is produced. This gives rise to important sound effects in poetry and literature in general, the most obvious of which is alliteration. The cumulative effect of the /s/ phonemes in “silver silent sails” far exceeds the effect of the individual /s/ phonemes if the words occurred at a greater distance from each other. My computer program is able to quantify the impression the /s/ phonemes may have on the reader of these words.While many different possibilities present themselves for analysis, my current work focusses on calculations of the fricative accumulations and the plosive accumulations of a poem. The fricative consonants are often regarded as soft sounds, while the plosive consonants are often regarded as harsh sounds. I have already shown that the percentage of plosives and fricatives in poems is mostly constant across poems and authors. Consequently, the occurrence of these phonemes on their own do not produce a significant effect; it is the groupings or occurrences in proximity with each other that seem to produce effects upon the reader. Graphs of their phonemic accumulations exhibit distinct peaks and troughs, and the differences between the two accumulations reveal interesting insights.For example, Robert Browning’s “Two in the Campagna” has a very intense climax of the difference of the fricative and plosive accumulations in its fifth, sixth, and seventh stanzas. These stanzas are those that are most expressive of true love and passion: while the whole poem is an expression of love, more negative thoughts such as the fleetingness of life and the inability to achieve true love on earth are intermingled with the attempts to seduce. The three stanzas with the fricative accumulation climax, however, are the most rhapsodic, the least tainted with doubt. Similarly, in Browning’s “Porphyria’s Lover,” a poem about the murdering of a woman whom the speaker loves too intensely, the stanza (or five-line set—the poem is not officially divided into stanzas) with the greatest climax of the difference of the fricative and plosive accumulations is the fifth, where the speaker of the poem tells how Porphyria expresses her love for him. The rest of the poem is predominantly heavier on the plosive accumulation side. The pattern that emerges is that when the difference between the fricative and plosive accumulations favours the fricatives, the sentiment of the poem is more positive, loving, and sincere. When the plosives are favoured, the sentiment of the poem is more insincere, withdrawn, and bleak. I shall present my results for these and other poems of a similar structure. These are early results, but the results are very promising.This work is important for the study of literature. This is the first time (that I am aware of) that the phonemic content of a text, i.e. its sound on the page, is put to such advanced calculations. Literary analyses often have to rely upon impressionistic language when discussing the effect of the sounds of the words upon the reader: phrases such as, “the prevalence of s sounds in the final stanza leaves the reader with a soothing, peaceful feeling, one that has countered the chaos of the opening stanzas.” Through the analysis of phonemic content such as I am performing, I can provide critics with quantifiable data upon which to base their claims. Further, the possibility that phonemic accumulations are related to the ideas expressed in the poems suggests strongly that a computer can begin to interpret poetry: that it can distinguish passages with expressions of intense, sincere love from passages with expressions of self-doubt or insincerity. Phonemic analysis may produce significant developments in the field of artificial intelligence.",
        "article_title": "Phonemic Accumulations and the Analysis of Poetry",
        "authors": [
            {
                "given": "Marc ",
                "family": "Plamondon",
                "affiliation": [
                    "Nipissing University"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "phoneme",
            "literary analysis",
            "poetry",
            "phonemic accumulation"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The Black Stage has been an important focus of evolving Black identity and self-representation, touching on many of the most contentious issues in American history since Emancipation—such as migration, exploitation, interracial unity, racial violence, and civil rights activism (Hill 2003). Alexander Street Press (ASP), in collaboration with the ARTFL Project, has developed an extensively tagged database of over 1,200 plays containing 13.3 million words by Black playwrights, from the middle of the 19th century to the present, including many previously unpublished works. Like other ASP datasets, the Black Drama database is remarkable for its detailed encoding and amount of metadata associated with authors, titles, acts/scenes, performances, and characters. Of particular interest for this study are the data available for authors and characters which are stored as \"stand-off mark-up\" data tables. The character table, for example, contains some 13,360 records with some 30 fields including name(s), race, age, gender, nationality, ethnicity, occupation, sexual orientation, performers, if a real person, and type. More extensive information is available for authors and titles. The character data are joined to each character speech, giving 562,000 objects that can be queried by the full range of character attributes. The ARTFL search system, PhiloLogic, allows for joining of object attribute searches, forming a matrix of author/title/character searching. For example, one can search for words in speeches by female, black, American characters depicted by male, non-American authors in comedies first published during the first half of the 20th century.While user-initiated full-text searches on such author and character attributes can help answer specific questions, we believe that advanced text data mining systems have the potential to reveal important new patterns of variation in general language use, broken down by various combinations of author and character attributes. Initial work on racial epithets in this collection has revealed striking differences in the use of such language between male and female authors and characters, as well as American and non-American authors. While illustrative, such micro-studies can do no more than hint at larger discursive and representation issues that we believe can be identified by text mining techniques. Prior studies using text mining for analyzing variation in language use among different classes of authors have succeeded in identifying meaningful linguistic features distinguishing author gender, age, and personality type (e.g. Argamon et al. 2003, Koppel et al. 2002).As might be expected of a collection of a particular class of literary texts, the Black Drama database cannot be considered a \"random\" sample. The database contains 963 works by 128 male playwrights and 243 pieces by 53 female playwrights. Plays by Americans dominate the collection (831 titles), with the remaining 375 titles representing the works of African and Caribbean authors. The database contains 317,000 speeches by 8,392 male characters and 192,000 speeches by 4,162 female characters. There are 336,000 speeches by 7,067 black characters and 55,000 by 1,834 white characters with a smattering of speeches by other racial groups. As would be expected, the predominance of American authors is reflected in the nationalities of speakers in the plays, with 272,000 speeches, compared with 71,000 by speakers represented as coming from a variety of African nations.Using these data, we are examining the degree to which machine learning can isolate stylistic or content characteristics of authors and/or characters having particular attributes—gender, race, and nationality—and the degree to which pairs of author/character attributes interact. The first step to discover if lexical style- or content-markers can be found which can be used to reliably distinguish plays or speeches broken down by a particular characteristic, such as gender of character. A positive result would constitute strong evidence for distinctive, in this case, male and female, character voices in the sample of plays. If distinctiveness can be shown, we then seek some 'characterization' of the differences found, in terms of well-defined grammatical or semantic classes. The experimental protocol which we have been developing for this purpose, as applied by, e.g., Argamon et al. (2003), addresses both goals using techniques from machine learning, supplemented by more traditional computer-assisted text analysis.First, to analyze a corpus of texts for distinctiveness, we need to determine if effective predictive models can be learned from the texts, which accurately classify new texts (that the system has not seen). The standard technique of 10-fold cross-validation can be used to estimate the usefulness of a learning method for constructing models that work for 'out-of-sample' data. The corpus is divided into 10 random subsets, and training is repeated for each of the 10 sets of 9 of those subsets, with accuracy of the resulting model is measures on the remaining subset. The average of these 10 numbers then forms a reasonably good estimate of how a model learned on the entire corpus would perform for new data. If this cross-validation accuracy is high (at least 70% for 50-50 balanced data), we may conclude that the two classes of texts in the corpus are linguistically 'distinctive'.Second, to characterize the difference found (if any), all textual features extracted (frequencies of lexemes, lemmas, parts-of-speech, etc.) are ranked by some measure of how each of them enables prediction of a text's correct class. One such measure is to use feature weights computed during learning (for SVM or Naive Bayes learners, which compute such weights). Features with high weights (positive or negative) are the most influential in classifying a text as one class or the other (dependent on the sign of the weight). While a direct measure of influence, these weights can be difficult to interpret since the effect of a feature must be considered in the context of all the other features influencing classification. Another approach is to use a function that measures the 'distinguishability' of a feature without regard to other features, such as information gain or binormal separation (Forman et al. 2003). The downside here is that some features may be of little use alone, but in conjunction with others may have great discriminating power. In the current study we will be examining the usefulness of multiple measures of both types, and see which approach proves to be the more useful.We conducted preliminary tests using the SVM-Light system (Joachims 1999) with PGPDT (Zanghirati 2004, Zanni 2006) to build the models. We extracted all speeches with character gender attributes from the corpus, splitting them into tokenized word frequency vectors for all authors, all characters, male authors, female authors, male characters, and female characters. For each of these, PGPDT built a model to identify authors and characters by gender.paper_149_olsen_1.jpgTable OneAs indicated in Table One, the system the system correctly identified 88.2% of the authors' gender and 77.4% of the speakers' gender. Performance varied when examining subsets of the corpus, from 86.6% for gender of author in male characters to 69.7% for gender of speaker in female authors. All of these indicators show significant differences in words used by male and female authors and speakers. The differences in accuracy in male and female author/character may, however, result from the fact that male authors tend to include fewer female characters and that the have fewer words as well as having fewer female authors in the sample. This is shown in the Majority row of Table One, which indicates the rate of male instances for each assessment. For female authors, male characters constitute 54.5% of the speakers.We then equalized a test sample for class by discarding instances until we have a balanced set with an attempt to correct for word frequencies as well. As shown in Table Two, author and character gender can be discriminated well.  Furthermore, we see that identification of author gender is consistently more accurate than gender of speaker.paper_149_olsen_2.jpgTable TwoAs shown in Table 3, male authors/speakers are correctly somewhat more often in five of the six cases, with the sole exception of almost exactly the same correct identification of speaker gender in female authors.paper_149_olsen_3.jpgTable ThreeThese results suggest that female lexical choices, both used by authors and depicted in characters, are somewhat less marked (or more varied) than male use of language. The full paper will explore this phenomenon in greater detail. While the accuracy of identification of gender in significant in all the cases we example, we expect that using some sort of feature set selection, as in, for example, Hota, Argamon, and Chung (2006), will improve the precision of the identification.From this preliminary analysis, it would appear that the authorial gender is rather more readily identified than the represented gender in characters. Initial examination of the features that best predict gender of character, as identified by information gain, range from the expected (male characters speak of wives and swear more frequently), to the somewhat opaque, such as the words 'nonsense' and 'reason' being strongly male associated. It is also important to note that both strongly male and female character terms in this sample are used at about the same rates (per 10000 words) by male and female authors. This suggests that male and female authors are able to use certain linguistic gender markers effectively. As noted, it is significant that the machine learning algorithms employed are less accurate in most cases in identifying female as opposed to male authors and speakers.The final paper will report results using similar techniques to examine the the degree to which additional character attributes -- race and nationality -- can be distinguished and, if this proves to be effective, examine the most important features distinguishing between the language of white and black speakers and American/non-American speakers. An initial examination of racial slurs in this dataset suggests that speaker race and nationality may also be readily identified.As we have seen, machine learning and text mining techniques can support higher orders of generalization and characterization than the more traditional user-driven search methods widely used in computer-aided textual research. This approach is most effective when used in relatively constrained experiments where classification criteria are clearly defined, such as the social attributes of authors and their characters. Some results may be trivial on a literary level— of course men talk more of wives and only women tend to call other women hussies —but such common sense results allow us to argue that the technique gives meaningful results, and so odd results should be examined further, using more traditional systems like PhiloLogic. We therefore argue that approaching the interpretative process starting with highly structured and constrained experimental hypotheses, we can take advantage of machine learning methods to find new and unexpected foci for examining literary questions, which may in turn shed new light on critical issues such as race and gender.",
        "article_title": "Gender, Race, and Nationality in Black Drama, 1850-2000: Mining Differences in Language Use in Authors and their Characters",
        "authors": [
            {
                "given": "Shlomo ",
                "family": "Argamon",
                "affiliation": [
                    "Linguistic Cognition LabIllinois Institute of Technology"
                ]
            },
            {
                "given": "Russell ",
                "family": "Horton",
                "affiliation": [
                    "Digital Library Development CenterUniversity of Chicago"
                ]
            },
            {
                "given": "Mark ",
                "family": "Olsen",
                "affiliation": [
                    "ARTFL ProjectUniversity of Chicago"
                ]
            },
            {
                "given": "Sterling Stuart ",
                "family": "Stein",
                "affiliation": [
                    "Linguistic Cognition LabIllinois Institute of Technology"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-05",
        "keywords": [
            "gender",
            "text mining",
            "machine learning",
            "black drama",
            "race"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Abstract:According to the French rhetorician and literary critic Gerard Genette a paratext is a transitional zone: a privileged space of a \"pragmatics\", of \"strategy\", and of action over the audience (Genette 1997). In a digital environment, the idea of the paratext may allow us to recognize the significance and the mode of operation of certain crucial textual formations that might otherwise seem inconsequential. These formations, unlike traditional paratexts, are not always literally visible as part of the legible textual surface, but instead operate through the representational and performative mechanisms of the digital interface. Their separateness is demarcated through markup, which not only creates a boundary between text and paratext but also makes the paratext into a space of function and behavior: of meaning instantiated through action rather than simply through textual signification.Proposal:According to the French rhetorician and literary critic Gerard Genette a paratext is a transitional zone: a privileged space of a \"pragmatics\", of \"strategy\", and of action over the audience (Genette 1997). This paratextual space—instantiated in what are traditionally considered secondary or ancillary texts such as footnotes, commentary, translation, and so forth—produces and ramifies the \"main\" text, the object that is thus presented to our interest as primarily significant. It is precisely through the paratexts that the concept of a \"main\" text emerges at all: in a given historical moment, depending on social, cultural, political, and other factors, a certain text emerges, and slowly starts to circulate and acquire status through the effects of its paratexts: comments, editions, translations, dedications, and so forth.In a digital environment, the idea of the paratext may allow us to recognize the significance and the mode of operation of certain crucial textual formations that might otherwise seem inconsequential. An example of such a formation is the information we might call \"microtexts\": that is, external, contingently visible “microcontents” (Nielsen 2000) that describe, illustrate or complete information within a web page or resource. These include also embedded alternate texts that usually appear when you move the mouse over an image, link or any other semiotic device (Zinna 2005). Another example is what we might call \"metatexts\", a category embracing both formal metadata and also looser kinds of descriptive, normative or regulative information, usually not directly visible to the user, that is added in order to allow search engines and other processing to produce coherent and useful results.These terms arise from an external classification (one which could be extended) emphasizing the different ways these paratexts function as part of a publication architecture. But digital paratexts can be also organized, according to their cognitive and semiotic functions, into at least three flexible (and mutually permeable) categories: 1. descriptive (syntagmatic and paradigmatic axes): those paratexts which contain information about a text, including various kinds of metadata;2. normative: those which constrain the behavior of the text (for instance, schemas)3. pragmatic: those which mediate or represent the text as a discursive object, and which produce its digital phenomenology.These formations, unlike traditional paratexts, are not always literally visible as part of the legible textual surface: their \"strategy\" and \"pragmatics\" operate through the representational and performative mechanisms of the digital interface. In print, the paratext has (despite its marginality) a certain visible presence on the page: its meaning may be wholly in relation to the text it supports, but it is not different in kind from that text (Tomasi 2005). The digital paratexts we are describing, however, occupy a different stratum and have a different kind of visibility and functional effect. Their presence may be felt primarily in what the text does or in how we discover it, rather than in the words we see when we read it. Their separateness is demarcated through markup, which not only creates a boundary between text and paratext but also makes the paratext into a space of function and behavior: of meaning instantiated through action rather than simply through textual signification. In this respect, these categories of paratext (and in particular the last, which is the realm of markup proper) are analogous to the punctuation, formatting, and presentational conventions through which a printed text is realized for a reader, or through which an oral text is concretized in print. As Genette observes, any transcription, including the written transcription of an oral speech, is a form of paratext, and we can usefully extend this idea to digital forms by observing that any encoding—in effect, any transmediation—constitutes a form of paratext as well.Each of the three categories above illustrates a distinctive dimension of the digital paratext, and raises a set of questions and issues that can help us reach a clearer understanding of the role of markup and the nature of digital texts. If we consider markup in Genette’s terms as a “privileged space” of “action” and “pragmatics”, what effect does this have on our understanding of text encoding as a transcriptional activity? To what extent do our disciplinary expectations about documentary evidence run counter to the logic implied by these terms? The boundary between transcription and authoring (and our sense of where the distinction lies) may turn out to be harder to pin down than has previously been suggested.Similarly, to what extent does metadata operate as an active, rather than a passive component of the textual ecology? If we understand metadata creation as part of the authoring of the digital document, does this change our sense of who should be creating it and what its sources should be?Finally, if schemas and stylesheets can also be understood as paratexts (for instance, as suggested in Pierazzo 2006), then we need to rethink our conception of textual meaning and rhetoric to include not only the forms a text actually takes, but also those which its constraints permit it to take: in other words, the potential as well as actual forms through which we apprehend the text.",
        "article_title": "Markup and the Digital Paratext",
        "authors": [
            {
                "given": "Julia ",
                "family": "Flanders",
                "affiliation": [
                    "Brown University"
                ]
            },
            {
                "given": "Domenico ",
                "family": "Fiormonte",
                "affiliation": [
                    "University of Rome"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-03",
        "keywords": [
            "theory",
            "metadata",
            "encoding"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "It is often stated by critics that the Quixote is a theatrical, graphic, and visual book. Thus, visual representations, like theatrical performances, popular iconography and book illustrations, have been recognized as significant contributions to the understanding of Cervantes’ masterpiece.See for instance, Jean Canavaggio, Don Quichotte du livre au mythe (Paris: Librairie Arthème Fayard, 2005) and José Manuel Lucía Megías, “Don Quijote de la Mancha: del libro al mito,” En torno al Quijote: Adaptaciones, imitaciones, imágenes y música (Madrid: Biblioteca Histórica Municipal de Madrid, 2005) 27-55. Nevertheless, the thousands of woodcuts, engravings, etchings, drawings, and lithographs that have accompanied the text are, for the most part, a little known interpretative tradition, and a much neglected critical and artistic treasure.Much has been done in conjunction with the 400th centenary celebration to remedy this situation; in addition to our own project and digital archive at  (accessed November 12, 2006) see the Banco de imágenes del Quijote directed by J.M. Lucía Megías at  (accessed November 12, 2006).Obstacles, such as the difficulty of accessing rare books, have prevented the illustrative tradition from being appreciated by scholars, students, and users in general. In 2001 the Cervantes Project (CP) started the creation of a hypertextual archive to include digital images of the illustrations taken from over 500 of the most significant editions to form the textual iconography of the Quixote (as permitted by copyright limitations). Our main objectives are to make the illustrations more accessible and to establish their contribution to the reception and interpretation of the text. At the present time, the archive has digitized, and made available online more than 7,000 images, supported by a fully searchable database and complemented by rich metadata and innovative visualization tools.Fernando González Moreno et al, “La colección de Quijotes ilustrados del Proyecto Cervantes: Catálogo de ediciones y archivo digital de imágenes,” Cervantes: Bulletin of the Cervantes Society of America 25.1 (2005) [2006]: 79-104 and E. Urbina et al, “Visual Knowledge: Textual Iconography of the Quixote, a Hypertextual Archive.”  Literary & Linguistic Computing 21.2 (2006): 247-58. (Figure 1 shows the reader’s Web-based access to the collection.)The multidisciplinary approach of our project enables scholars to go beyond the literary aspect of Cervantes’ works. As an invaluable pictorial depository, we emphasize supplying information regarding the historic value and artistic significance of the images. The hermeneutic and aesthetic values of each individual image have been carefully examined by art historians and the results incorporated in the archive as scholarly commentary. Additionally, we include biographical commentary about artists and engravers. These rich scholarly commentaries will help to boost the study of book illustration art, which has been to date secondary in Art History, in aspects such as the evolution of techniques, from the first woodcuts (early 17th century) to modern mechanical offset (20th century), and the influence or achievement of an individual engraver, illustrator, or lithographer. We associate the text and the images through a taxonomy of episodes, adventures and narrative motifs for both parts of the Quixote. The 308 taxonomic elements in which we have divided the 126 chapters of the text encompass as descriptive categories the totality of the tradition of illustrations present in the editions in our collection. (See Figure 2 for an example selection from the taxonomy.)The reader’s interface offers access to all the levels of editorial annotation about the artists, textual location, image information and technique, aesthetic and textual commentary. In addition, users can obtain, through links, specific information related to the narrative, biographical information about the creators, and the meaning of engraving techniques. We are in the process of incorporating these and other categories into our current search tool, as well as developing a controlled vocabulary about themes and characters in the Quixote and about generic area content (flora, fauna, architecture, music, etc.) to facilitate the use and exploration of the images in the archive by researchers other than literary scholars. (Figure 3 shows the advanced search interface.) As a multidisciplinary project that involves library staff digitizing the images, computer scientists coding the applications, Hispanic scholars providing textual editing and edition metadata, art historians examining the image metadata, and finally the general public traversing the archive, it is critical to have an efficient production line and a strategy for consistently sharing information among the different parties. After several attempts, we reached a work flow that we found to work efficiently. The procedure starts with Hispanic scholars providing bibliographic information and specifying the editions to be digitized, the pages where the images reside, and the naming scheme for the electronic files. This information is recorded on a work sheet and sent to library personnel. Upon receiving such a work sheet, the library staff responsible for the digitization reviews the information, clarifies any possible discrepancies, digitizes the illustrations, and saves them on a library server as archival TIFF files. A copy of the images is then transferred to the development site where a computer scientist processes them into JPEGs, whose sizes are more appropriate for Web traffic. Afterwards, the references to the JPEGs are added to the database. At that point, the interfaces automatically update and these images become available online. The art historian in charge of image metadata has access to the original illustrations and then remotely enters the information using the online Web form. As soon as these commentaries are finalized, they are published online alongside the images.A Web-based interface is provided for maintenance of the collection. The collection maintenance application supports management functions such as inserting, deleting, or modifying, project-wide data. The editor's Web form also offers a typology of illustrations divided into 17 categories, i.e., frontispiece, vignette, portrait, map, etc., and a 17 item index of engraving techniques. Much of the design effort in preparing this interface has focused on specification of the most appropriate handling of default values and modes. Using an iterative development methodology, we have focused on the art historian’s tasks, refining the means for inheriting and modifying values between entries with the goal of focusing his data-entry activities on differences rather than similarities. The taxonomic and typological categories are part of the rich image metadata approach we have developed to capture and make accessible the variety of image content present in the iconography of the Quixote. They are of value both to collection editor and reader as they are part of the editor's metadata template and of the reader’s advanced search tool. (Figure 4 shows the tools used by the editor in specifying metadata.)The readers’ Web-based interface, introduced earlier, which is separate from the collection maintainer’s interface, also has undergone refinement during the project’s development. At present, it provides a four-layered browsing design: a) an edition index; b) a thumbnail overview of one entire edition; c) a low-resolution image together with meta-data; and d) a high-resolution image. One interesting characteristic in the public interface is the provision of uncommon relationships among the artifacts. One particular example is the ancestor-descent relationship. For instance, an engraver is often related to another as his master or disciple, or even father or son; one illustration or a set of illustrations is inherited from an earlier edition as simply reprints or sometimes after re-engraving. In cases where ancestral and/or familial ties exist, we provide a description of the ties, as well as physical links pointing to the referred artifacts for the purpose of more in-depth investigation. Navigation along the ties provides an intuitive way to look closely at the propagation of engraving skills from one person to another, the effect of the technology evolution upon art works as reflected in re-engravings, and the significance, elegance, and popularity of certain artists or illustrations serving as the archetypes for imitation. At present, the links are manually encoded on a one-by-one basis. We are in the process of setting up a keyword-anchor list for automatic linking.Neal Audenaert et al, “A General Framework for Feature Identification,” Digital Humanities 2006 Conference Abstracts, Association for Digital Humanities Organization International Conference, Université Paris-Sorbonne, (2006) 5-9, and E. Urbina et al, “Textual Iconography of the Quixote: a Data Model for Extending the Single-Faceted Pictorial Space into a Poly-Faceted Semantic Web,”  Digital Humanities 2006 Conference Abstracts (2006) 215-20. (Figure 5 shows the reader’s views of the collection and metadata.)The project’s metadata is imbedded into a Dublin Core framework. We use a MySQL server as the database core to store the metadata and the references to image files. The readers’ and maintainers’ Web-based applications occupy a display tier on top of the database tier.The availability of the archive will contribute to a more complete understanding and appreciation of Cervantes’ novel by initiating new explorations from many perspectives: textual, artistic, critical, bibliographical, and historical. In particular, we provide resources and assistance to examine the reception and evolution of the Quixote’s readings across time, culture, audience, and milieu. Furthermore, the images can be grouped according to several layers of content to respond to the users’ need for information selection of a specific critical focus, i.e, art, geography, history, etc. This is achieved by 1) cataloging each image using a comprehensive taxonomy of the episodes and adventures, 2) including multiple levels of textual annotation about each individual illustration, including technical, historical, and artistic information, and 3) providing descriptive and critical commentary related to the content and textual context of the illustrations. The result is a multi-layered and multi-directional collection of digital objects recombining bibliographical, textual and visual materials (edition-text-image-metadata), and the development of a rich hypertextual archive encompassing a new form of critical apparatus in which the illustrations are newly re-contextualized and re-imag[in]ed.Funding and support provided by the National Endowment for the Humanities, the Cátedra Cervantes at the University of Castilla-La Mancha, Grupo Santander, Texas A&M University, and the Cushing Memorial Library, Texas A&M University Libraries.paper_151_furuta_1.jpgFigure 1: The front page of the collection (left) and the interface to the iconography collection (right).paper_151_furuta_2.jpgFigure 2: An example set of menus showing the selection of an entry in the thematic taxonomy.paper_151_furuta_3.jpgFigure 3: The reader’s advanced search interface.paper_151_furuta_4.jpgFigure 4: Screen shots from the editor’s metadata specification toolkit. At top left is the editor’s view of the collection, which resembles the reader’s. In addition to allowing selection of editions, this screen allows editing of edition metadata, lower left. Selecting an edition brings up the middle display, which flags images still requiring metadata entry. The metadata is then provided using the screen on the right.paper_151_furuta_5.jpgFigure 5: The reader’s view of image magnifications and metadata. At top left is the collection view, seen before. Selecting an edition from the collection brings up its images, lower left. Next, selecting an image brings up more detail about an image, including its metadata, center top. Here, the reader can request detail about persons mentioned in the metadata (center bottom) and an additional image magnification (right).",
        "article_title": "Re-imag[en]ing Cervantes’ Don Quixote: a Multi-layered Approach to Editing Visual Materials in a Hypertextual Archive",
        "authors": [
            {
                "given": "Eduardo ",
                "family": "Urbina",
                "affiliation": [
                    "Cervantes Project &Center for the Study of Digital LibrariesTexas A&M University "
                ]
            },
            {
                "given": "Fernando González ",
                "family": "Moreno",
                "affiliation": [
                    "Cervantes Project &Center for the Study of Digital LibrariesTexas A&M University"
                ]
            },
            {
                "given": "Richard ",
                "family": "Furuta",
                "affiliation": [
                    "Cervantes Project &Center for the Study of Digital LibrariesTexas A&M University"
                ]
            },
            {
                "given": "Steven E. ",
                "family": "Smith",
                "affiliation": [
                    "Cervantes Project &Center for the Study of Digital LibrariesTexas A&M University"
                ]
            },
            {
                "given": "Jie ",
                "family": "Dong",
                "affiliation": [
                    "Cervantes Project &Center for the Study of Digital LibrariesTexas A&M University"
                ]
            },
            {
                "given": "Stephanie ",
                "family": "Elmquist",
                "affiliation": [
                    "Cervantes Project &Center for the Study of Digital LibrariesTexas A&M University"
                ]
            },
            {
                "given": "Sarah ",
                "family": "Tonner",
                "affiliation": [
                    "Cervantes Project &Center for the Study of Digital LibrariesTexas A&M University"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "textual iconography",
            "Cervantes"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Writing is a process that occurs in time. This simple and obvious consideration involves many issues both from a theoretical and practical point of view. A critical evaluation of timing is really crucial in the case of modern authors' autograph draft manuscripts because different layers of corrections, deletions and additions can give insight into an author’s way of working, key to the interpretations of his/her works and the evolution of the author’s Weltanschaung, as highlighted in genetic criticism over the last few years.The question is too complex just to try to give some basic references; anyway Gresillon 1994 will offer a good starting point. Medieval manuscripts copied by one ore more scribes are also, of course, the result of a process that occurred in time, but the different kind of authorship involved in such cases seems to involve a difference in the evaluation of the cultural weight of recorded variants. When a scholar inspects a written text, especially a manuscript, s/he has in his/her hands the final result of that process and can choose whether to approach it from a codicological/documentary point of view, “photographing” the resulting product, or from a genetic point of view, trying to describe the teleological flow of authoring. The first approach is more typical of scribes’ copies, the latter of autographic (draft) manuscripts.Practically, time based editions are really difficult to represent in printed works, because of the bi-dimensionality of paper sheets.On voudrait représenter dans la bidimensionalité des pages un processus génétique dont on s’est pourtant appliqué à montrer que sa proprieté est d’ajouter à l’écrit, qui est bidimensionel, une troisième dimension, qui est celle du temps!Gresillon 1994:121In a digital framework bi-dimensionality can be overruled by a hypertextual/multimedia approach that can allow the creation of a more flexible context for the presentation of a genetic edition. The problem of a time based encoding has been discussed in several circumstances (i.e. TEI Manuscripts SIG Meeting Report 01, Vanhoutte 2002 that suggests the usage of the markup solution employed in the transcription of speech for the purpose), but – up till now – a coherent encoding model for such editions has not been proposed. And for a good reasons: while it is possible to describe a relative-timed process, it is very complicated, if not impossible, to draw a general absolute-timed framework.Analysis and time-based encoding of authorial interventionsLet us consider a couple of examples taken from page 3595 of Zibaldone of Giacomo Leopardi.Ex. 1 line 6paper_172_pierazzo_1.jpgFig. 1paper_172_pierazzo_2.jpgIn this line we can detect two corrections: 1) deletion of the line under “si” and 2) deletion of “dotti” corrected into “denti”.We can try to draw the timing of the creation of the segment as follow:Time 1: writing “che e’ si rechi a dotti”Time 2: deletion of “dotti”; consequent writing of “denti”Time 3: underlying of “si”Time 4: deletion of the line under “si”Time 5: underlying of “rechi” and “denti”Time 6: writing of “l’un d’essi cibi”Other timetables are also possible, but let’s assume this is the more probable one. The text can be encoded using a TEI-based mark-up with the help of a new global attribute (@time), intending that it’s just an example to help the conceptualization of the problem:  <seg time=\"1\">che e' <del type=\"underline deletion\" time=\"4\"> <hi rend=\"underline\" time=\"3\">si</hi>> </del> <hi rend=\"underline\" time=\"5\">rechi a'</hi> <del type=\"overstrike\" time=\"2\">dotti</del> </seg>> <seg time=\"2\"> <hi rend=\"underline\" time=\"5\">denti</hi> </seg> <seg time=\"6\">l'un d'essi cibi</seg>Such transcription tries to model the real flow of writing, but such a model may not be workable. In fact, it will fragment the flow of the plain writing in potentially infinite pieces. To simplify it, we can assume as Time 0 (default) the time of the normal plain writing flow, timing just editorial interventions.The schedule will then be modified as follows:Time 0: writing “che e’ si rechi a dotti”Time 1: deletion of : “dotti”Time 0: writing of “denti”Time 2: underlying of “si”Time 3: deletion of the underline under “si”Time 4: underlying of “rechi” and “denti”Time 0: writing of “l’un d’essi cibi”A further simplification is also possible: assuming that – in genetic criticism terms – so-called “writing variant” (deletion of a single word substituted by another that immediately follows on the same line) occurs during the normal writing flow, the following model can be drawn:Time 0: writing “che si rechi a dotti”; deletion of : “dotti”; writing of “denti”; writing of “l’un d’essi cibi”Time 1: underlying of “si”Time 2: deletion of the underline under “si”Time 3: underlying of “rechi” and “denti”This will be the consequent new encoding: che e' <del type=\"underline deletion\" time=\"2\"> <hi rend=\"underline\" time=\"1\">si</hi></del> <hi rend=\"underline\" time=\"3\">rechi a'</hi> <del type=\"overstrike\">dotti</del> <hi rend=\"underline\" time=\"3\">denti</hi> l'un d'essi cibiThe last possibility should not imply that any inline correction is to be considered as done in Time 0, but just the one followed by the correction. In fact, in the case of a deletion of an adjective or of any other word not essential from a syntactical point of view, the correction can occur in any time.Ex. 2 line 11paper_172_pierazzo_3.jpgFig. 2paper_172_pierazzo_4.jpgThis passage can be timed as follow:Time 1: writing “che di due Eroi a”;Time 2: deletion of “a”;Time 3: writing of “quanto si voglia pari di”Time 4: interlinear addition of “o più” after “Eroi”Time 5: deletion of the additionTime 6: interlinear addition of “o più” after “due”Or, in the simplified version, as follow:Time 0: writing of “che di due Eroi a”; deletion of “a”; writing of “quanto si voglia pari di”Time 1: interlinear addition of “o più” after “Eroi”Time 2: deletion of the additionTime 3: interlinear addition of “o più” after “due”The encoding model (simplified version): che di due <add place=\"intralinear\" time=\"3\">o più</add> Eroi <del type=\"overstrike\">a</del> <del type=\"overstrike\" time=\"2\"><add place=\"intralinear\" time=\"1\">o più</add></del> quanto si voglia pari di By applying different stylesheets to the encoded texts, it will be possible to show all the different stages and to give the user the possibility of browsing among them.paper_172_pierazzo_5.jpgRelative or absolute?The two above examples occur on the same page: shall we then consider Time 1 of the first example the same of Time 1 of the second example? The answer should be: no; very little can be said about the timing of editorial/authorial intervention in two different segments. The possibility of establishing an absolute timing for correction is applicable only where we have strong palaeographic evidences or authorial declarations dating or describing a revision.For instance, we can imagine an author used to typewrite his/her texts and then to correct them by hand: in this case the assumption of an absolute time is possible. But as different layers of hand corrections can also occur, there will be, in this case also, the necessity of considering relative-timed interventions. This situation can be represented in encoding distinguishing absolute and relative timing for instance by the application of two different attributes, i.e. @timeRel and @timeAbs In a digital framework an incorrect consideration of time as absolute or relative can bring to display texts that never existed. Let us imagine for a moment that we build an XSLT based tool able to display at a time either Time 1, Time 2 or Time 3 etc. variants: the results would be the display of variants that might have lived in different moments. I think that, in absence of explicit authorial declarations or of palaeographic evidence, the only possible display would be to show timing of variants segment by segment, i.e. to give evidence just of relative timed corrections.In the presentation I will present some examples from different authors (Giacomo Leopardi, Jane Austen, Virginia Woolf and some others) to examine the complexity of representation of temporal factors in digital genetic editions.",
        "article_title": "The Encoding of Time in Manuscripts Transcription: Toward Genetic Digital Editions",
        "authors": [
            {
                "given": "Elena ",
                "family": "Pierazzo",
                "affiliation": [
                    "King's College London"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "genetic criticism",
            "XML",
            "timing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "An online community may be defined very simply as any group of people who collaborate regularly and/or formally using internet technologies. Such communities are becoming increasingly important in arts and humanities research. They include blogs, wikis, mailing lists and other such fora. The Jiscmail and Listserv services (), and the availability of well-known open source wiki and blog software (e.g.  and ) have made it so easy to establish online communities that most projects in the digital and humanities now have one of some kind or another. However such fora are founded on relatively basic technologies. The various arts and humanities e-science and cyberinfrastructure programmes underway in many countries offer huge opportunities to enable online communities in ways that go far beyond what is generally available now. These approaches will be based on incomparably more complex tools, methods and technologies such as Virtual Organizations (VOs), Virtual Research Environments (see ) collaborative virtual workspaces, and Semantic Web. If these tools, methods and technologies are to be exploited by practice-led arts researchers and humanists to the greatest effect, they will need to be grounded in a strategic, rigorous and systematic understanding of the behaviours of current online communities, as complex online systems will clearly evolve from collaboration tools currently in use. The proposed paper will offer an overview of current usage of various online fora, and propose a high-level mapping from that overview onto the capabilities of the e-science-based collaborative systems of the future, such as MyVocs (). Of immediate importance to the humanities are VREs, as they help geographically dispersed research groups to come together in virtual laboratories that allow modelling and experimentation as much as discussion of research results. The e-Science concept of a Virtual Organisation on the other hand is seen as a set of institutions and/or individuals defined by resource sharing policies. Foster and C. Kesselman,  The Grid 2: Blueprint for a new computing infrastructure, (Morgan-Kaufmann, 2004). A VO is a community with the will to share resources and information across the internet, while VREs enable VOs by adding tools and methods that help the community to work together and share resources in a secure manner. VREs attempt to bring together researchers across disciplines and administrative boundaries. Many argue that humanities computing in general constitutes such an attempt, where the ‘glue’ is to find ‘methodological commons’See Willard McCarty  Humanities Computing, (Basingstoke: Palgrave McMillan, 2005). to present the disciplinary kinships among different disciplines in the humanities and in computing. A hi-tech VRE for arts and humanities computing could therefore make these computational methods the subject of online discussions enabled by advanced information discovery technologies. As a case study, this paper will report on our efforts to set up such a VRE. The Arts and Humanities e-Science Support Centre (AHESSC) is in regular and direct contact with many online communities, and is in an excellent position to offer an overview. We will present findings from detailed research on the recent history of a range of existing discussion fora. These will include both moderated and unmoderated groups critically and qualitatively assessing the difference this makes, and extrapolate an agenda for the management systems needed for VREs. We will examine both direct and indirect links between messages, semantic commonalities, disciplinary vocabularies and threading patterns, and hyperlinking behaviour. We will also examine other well-established wiki and blog communities of relevance to the humanities in much the same way. The paper will draw out commonalities, identify points of conflict and the reasons underlying them, the implementation and practice of “netiquette’ codes, and the importance of sustaining archives and maintaining access to them. We argue that regardless of the technology, these are essential questions if the adoption of advanced collaborative tools is to be a success. A key remit of AHeSSC itself is to help develop a community of e-science ‘early adopters’, and the research presented forms a central part of this mission. We believe that more complex collaborative technologies will not only enable the transmission of ‘traditional’ scholarly communication in the form of text and images as in the current basic systems; but also they will facilitate access to large scale, complex and fuzzy data, and allow scholars to work together in real time with that data. For example, instead of sending a complex dataset as an email attachment to a group whose members then manipulate it according to their expertise and locally available software, and then send it back to the originator, the new architectures will allow two or (many) more to work on it in real time, to discuss it, expose it to analytical tools, and to annotate it online, whilst preserving a complete record of the workflow. We will show how close critical analysis of the existing lo-tech systems should inform the design of such architectures.This will be illustrated by considering a VRE for arts and humanities computing methodologies for which funding is currently being sought. This project will continue the community building efforts of the UK’s AHRC ICT Methods Network (see ), with a taxonomy of computational methods developed by the Arts and Humanities Data Service (see ). This is seen as a first step to build a semantically enriched VRE, using the taxonomy as the foundation of an ontology. The taxonomy will be verified against papers from online databases or websites presenting tools and methods. Ontological approaches are used that allow not only the semantic integration of different text and multimedia resources, but also the tracking of exchanged arguments that help users better understand decisions about methodologies. Our paper will illustrate the evolutionary relationship between an advanced environment such as this, and the basic environments with which we are all familiar. ",
        "article_title": "The Anthropology of Knowledge: From Basic to Complex Virtual Communities in the Arts and Humanities",
        "authors": [
            {
                "given": "Stuart ",
                "family": "Dunn",
                "affiliation": [
                    "King's College London"
                ]
            },
            {
                "given": "Tobias ",
                "family": "Blanke",
                "affiliation": [
                    "King's College London"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "VRE's",
            "fora",
            "communities",
            "methods"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "A survey study Bei Yu and John Unsworth,  Toward Discovering Potential Data Mining Applications in Literary Criticism, Digital Humanities 2006 Conference Abstracts (Paris: CATI, Université Paris-Sorbonne, 2006): 237-239. shows that text classification is a typical scholarly activity in literary study, and automatic text classification methods can be used in three scenarios. The first is information organization - a classifier can learn the target category concepts (e.g. news article about trade, acquisition, etc.) from the training documents, and then assign new documents into these predefined categories. The second purpose is knowledge discovery - a successful classifier can provide insights to understand a target concept by revealing the correlations between the features and the concept. The third purpose is example-based retrieval - a classifier might be able to learn a concept from a small number of training documents with the help of semi- supervised learning or active learning methods, and then retrieve more documents similar to the training examples from a large collection. Text classification techniques have been well developed in the past twenty years. With the availability of many text classification methods, empirical evaluation is important to provide guidance for method selection in applications. Because of the sub jectivity in the class concept definition, analytical evaluation of text classifiers is difficult. Therefore empirical experiments became the common text classification evaluation methods.Fabrizio Sebastiani,  Machine Learning in Automated Text Categorization, ACM Computing Surveys 34.1 (2002): 1-47. The major text classification methods have been evaluated on topic classification tasks using some benchmark data sets, such as the Reuters-21578 news collection and the Usenet newsgroup collection. Some topic classification evaluation results have been widely accepted. For example, SVM is currently the best text classifierThorsten Joachims,  Text Categorization with Support Vector Machines: Learning with Many Relevant Features, Proceedings of ECML-98, 10th European Conference on Machine Learning 1998, and Yiming Yang and Xin Liu,  A Re-evalution of Text Categorization Methods, Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’99),  (Berkley, CA: AMC, 1999): 42–49; no feature selection improves SVM performance  George Forman,  An Extensive Empirical Study of Feature Selection Metrics for Text Categorization, Journal of Machine Learning Research 3 (2003): 1289–1305, and Dunja Mladenic and Marko Grobelnik,  Feature Selection for Unbalanced Class Distribution and Nave Bayes,Proceedings of the Sixteenth International Conference on Machine Learning (ICML) (199): 258–267.; SVM feature selection is better than Odds Ratio for naive Bayes, etc.Dunja Mladenic, Janez Brank, Marko Grobelnik, and Natasa Milic-Frayling,  Feature Selection Using Linear Classifier Weights: Interaction with Classification Models, ACM SIGIR ’04 (2004): 234–241. There are mixed conclusions regarding some document preprocessing techniques, such as stemming and stop word removal. However, these evaluation data sets were limited to news and web documents; the evaluation tasks were limited to topic classification for information organization purpose. The target concepts in literary text classification range from topic to style, genre, emotion, and more. These different types of target concepts can also be called document properties. Previous study  J. Morato, J. Llorens, G. Genova, and J. A. Moreiro,  Experiments in Discourse Analysis Impact on Information Classification and Retrieval Algorithms, Information Processing and Management 39 (2003): 825– 851. showed that document properties interact with clustering methods. Will the various document properties in literary text classification tasks affect the classification methods? Are the previous evaluation results still valid for literary text classification? This paper describes an empirical evaluation of text classification methods for literary study. We choose a new kind of data - the literary documents - to evaluate classification methods. Because no benchmark data is available in the literary domain, we select two literary text classification problems - the eroticism classification in Dickinson’s poems and the sentimentalism classification in early American novels - as two cases for this study. Both problems focus on identifying certain kinds of emotion - a document property other than topic.We also choose two popular text classification algorithms - naive Bayes and Support Vector Machines (SVM), and three feature engineering options - feature merging (stemming), stopword removal and statistical feature selection (Odds Ratio and SVM) - as the sub jects of evaluation. We aim to examine the effects of the chosen classifiers and feature engineering options on the two emotion classification problems, and the interaction between the classifiers and the feature engineering options. As a special case of feature merging, we also examine the impact of Dickinson’s unconventional capitalizations on classification performance. We choose bag-of-words (BOW) model for document representation.We seek empirical answers to the following research questions: 1.Is SVM a better classifier than naive Bayes regarding classification accuracy, new literary knowledge discovery and potential for example-based retrieval? 2.Is SVM a better feature selection method than Odds Ratio regarding feature reduction rate and clas- sification accuracy improvement? 3.Does stop word removal affect the classification performance? 4.Does stemming affect the performance of classifiers and feature selection methods? Our experiment results show that SVM is not a universal winner in literary text classification. After fea- ture reduction naive Bayes achieves high accuracies in both cases while SVM succeeds in the sentimentalism classification only. Figure 1 and 2 show that SVM and naive Bayes select their top features from different frequency ranges. Naive Bayes tends to pick unique words, which are often not frequent. The large number of low frequency words results in the success of naive Bayes in the eroticism classification. These unique words also surprised the Dickinson scholars, who finally found some new erotic indicators from them. SVM tends to pick high frequent and discriminant words, which are scarce in the Dickinson collection. These words (such as personal pronouns) are within the scholars’ expectation and therefore not interesting anymore.paper_157_yu_1.jpgpaper_157_yu_2.jpgFigure 1: Dickinson feature ranks and frequenciespaper_157_yu_3.jpgpaper_157_yu_4.jpgFigure 2: Sentimentality feature ranks and frequenciesDespite the high classification accuracies, the learned naive Bayes eroticism classifier, and also the SVM classifier with low accuracy, are useless for example-based retrieval purpose. Figure 3 shows that the concept of eroticism can not be learned from small number of examples, and the classifiers’ prediction confidence drop quickly with the expanding prediction coverage.Both classifiers achieve high classification accuracies in the sentimentalism classification task, which in- dicate that sentimentalism is a more straightfoward concept than eroticism for bag-of-words representation. The two classifiers still choose different top features but reach comparable performances for the sentimen- talism classification. So for the purpose of feature-category correlation analysis the two methods should be used as complemental to each other rather than one over the other. This time the unique words picked by naive Bayes are so strange that the scholars can not make sense of it. The common but discriminant words picked by SVM are still within the scholars’ expectation. The learning curves and confidence curves in figure 4 show that both classifiers yield high potential for example-based retrieval.The experiment results also show that self feature selection helps both naive Bayes and SVM improve classification accuracies. For SVM the improvement is not as significant as for naive Bayes. Odds Ratio is better than SVM as feature selection method for naive Bayes. However Odds Ratio cannot improve the SVM performance. Without feature selection the stemmed and unstemmed features obtain similarly low classification accuracies in both cases, so did the case merging in the Dickinson case. The micro level analysis finds that the effects of good mergings and bad mergings are neutralized overall. Stemming does not affect both feature selection methods in the eroticism classification case, but we are surprised to find that stemming negatively affects both feature selection methods, especially SVM, in the sentimentalism classification case.We have found that the stop words obtained from the Brown corpus are also overly common and useless in sentimentalism classification. However, the Brown stop words are mostly uncommon in the Dickinson collection. Personal pronouns - the group of function words usually treated as stop words - turns out to be highly relevant features for eroticism classification.Our study extends the empirical evaluation of text classification methods to emotion classification tasks in the literary domain. Some conclusions are consistent with what are obtained in previous research, such as Odds Ratio does not improve SVM performance and stop word removal might harm classification. Some conclusions contradict previous results, such as SVM does not beat naive Bayes in both cases. Some findings are new to this area - SVM and naive Bayes select top features in different frequency ranges; stemming might harm feature selection methods. These experiment results provide new insights to the relation between classification methods, feature engineering options and non-topic document properties.paper_157_yu_5.jpgpaper_157_yu_6.jpgFigure 3: Figure: potential for Dickinson example-based retrievalpaper_157_yu_7.jpgpaper_157_yu_8.jpgFigure 4: Figure: potential for sentimentalism example-based retrievalOur experiment results also provide guidance for classification method selection in literary text classifi- cation applications. We suggest that both SVM and naive Bayes be used for feature-category correlation analysis purpose. The number of support vectors in the SVM model indicates the complexity of the target concept. A complex concept is hard to learn from small training set. Feature reduction produces smaller and more generalizable models, but statistical methods are a better choice than the arbitrary feature reduction (like stemming and stop word removal) which are insensitive to particular classification tasks.",
        "article_title": "An Evaluation of Text Classification Methods for Literary Study",
        "authors": [
            {
                "given": "Bei ",
                "family": "Yu",
                "affiliation": [
                    "University of Illinois at Urbana-Champaign"
                ]
            },
            {
                "given": "John ",
                "family": "Unsworth",
                "affiliation": [
                    "University of Illinois at Urbana-Champaign"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "text classification",
            "sentimentality",
            "stemming",
            "feature selection",
            "Dickinson eroticism"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Introduction The catalogue raisonné, or reasoned catalogue, has long been a standard tool for representing large art collections. A typical catalogue raisonné includes images of artworks along with descriptive metadata, commentary, and background information (often a biography of the artist) about the collection. More recently, technological and infrastructural advances (in particular, cheaper secondary memory, increased network bandwidth, computational power, and digitization technology) have enabled development of the digital catalogue raisonné. Some of these catalogues have been developed primarily to support the construction of print-based catalogues (Lanzelotte, et al., 1993; Gladney, et al. 1998), while others are intended for online use (for example, The Vincent van Gogh Gallery and Gemini G.E.L.). The Picasso Project (Mallen, 2006) has developed a digital catalogue raisonné containing 11,000 of Picasso's artworks, along with 7,000 biographical entries. It is the most complete and up-to-date collection of Picasso's prodigious body of work.Building on the premise that the logical structures of the book do not support scholarly inquiry adequately (McGann, 1997), we are using the Picasso digital catalogue to facilitate scholarly work with art collections. Researchers, students, and teachers in disciplines such as art history, painting, drawing, history of art, and art appreciation deal with art collections. They analyze and critique individual works and compare and contrast these with other works. They identify similarities between pieces of art and trace threads of influence between artworks, artists, styles, materials, themes, and social, geo-political, or personal events. These scholars interpret artworks, identify missing links, and communicate their findings. In the context of the Picasso collection, we support scholars in expressing and visualizing the complex, multifarious relationships between artworks via a Web-accessible software interface.ApproachIn a series of informal interviews with faculty members from art education, history, Hispanic studies, art history, and with local K-12 art teachers we found a diverse set of needs, interests, and approaches to working with artworks in both education and research settings. One key theme running through each of these areas is the need to discover and present relationships between artworks, although the specific relationships of interest varied by discipline. The art history scholar wishes to investigate relationships between artworks displayed together in an exhibition or to study works composed when an artist was with a particular lover. The historian wishes to view art in the context of significant historical events, for example, artworks created while Europe was anticipating World War I. K-12 teachers are interested in identifying artworks that provide good examples of specific drawing or painting techniques, such as the two-point perspective or the use of complementary color schemes.In addition to the interviews, we also attended sessions of two college-level art history survey courses. We observed that instructors typically showed one or two examples of artworks from different artists or art movements, discussing each for a few minutes. In subsequent interviews, the instructors explained that lack of time constrains their ability to include additional works. Creating thematic sub-collections based on the relationships discussed in class could alleviate this problem, enabling students to study additional examples of materials covered in the classroom. These observations of classroom interaction and feedback from educators and researchers have informed our enhancements for supporting the representation and visualization of diverse relationships over the Picasso project’s artwork catalogue.Picasso’s works cover a broad range of themes, topics, and materials, thus presenting a rich substrate of artworks for building a network of semantically diverse, meaningful relationships. In addition to the image collection, the Picasso project includes extensive metadata related to these works, such as its place and date of creation, medium, dimensions, current location, as well as exhibitions and books in which it has appeared. We leverage much of this metadata to express relationships based on ownership, materials, patronage, or chronology.Interactive Relationship VisualizerThe Interactive Relationship Visualizer (IRV), an interactive, Web-based application, enables visualization of relationships that exist between artworks in the archive. The IRV interface displays image sub-collections connected by the relationship of the viewer’s focus. In addition, it presents connections that exist within artworks in the sub-collection as well as those with others in the archive, enabling users to navigate the intricately interconnected hypertextual web defined by these relationships. While browsing, the the display changes to reflect the dominant relationship being displayed. In order to express a rich set of relationships, we are augmenting existing metadata to include type (such as still life or portrait), art movement (cubism, fauvism, surrealism), and content (woman, nude, vase, mirror).The IRV distinguishes two broad categories of relationships, \"inferred\" and \"specified.\" Inferred relationships are those which can be expressed in terms of the metadata elements provided for each artwork. Some inferred relationships can be expressed in terms of a single metadata value, such as \"artworks created with oil on canvas.\" Others require mapping a range of metadata values onto a higher-level concept and require definitions involving multiple metadata fields. For example, identifying \"paintings created in Paris around the time of World War II\" is a two-step process. The system must map the timeframe of World War II to a portion of the traditional calendar and locate paintings created during this time. It then selects from this set, those that were created in Paris Finally, relationships such as \"expensive paintings\" involve subjective, theory-driven, and potentially variable definitions. A price that would be expensive in one art market might be comparatively inexpensive in another. Inferred relationships provide a powerful mechanism for exploring, discovering, and expressing relationships between artworks that leverages existing metadata.paper_213_furuta_1.jpgFigure 1: Interactive Relationship Visualizer system design.In contrast to relationships inferred directly from existing metadata, other types of relationships must explicitly be stated. We refer to these as \"specified\" relationships. For example, Picasso sketched several rough drafts of large works, interspersed with smaller works. Thus, a chronological view of artwork around the time the Guernica was painted results in a sub-collection that includes these preparatory works as well as other, unrelated works. Hence, this \"preparatory work\" relationship must be expressed between early sketches of the Guernica and the final masterpiece. Another specified relationship is images based on a shared subject, for example, Picasso’s interpretive series of works of Diego Velásquez’s Las Meninas. Specified relationships afford us the ability to define and represent relationships between artworks that are difficult to derive from the descriptive metadata associated with each work. This category of relationships is critical for the expression of concepts based in established and novel analytical approaches to Picasso's work—allowing relationships based on information beyond that which is encoded in the collection. The drawback is that participation in specified relationships must be manually encoded.Figure 1 displays the IRV system design. The Web interface employs specific visualizations for displaying different kinds of relationships. For example, the display of Guernica and its preparatory images uses a visualization that illustrates the centrality of Guernica relative to the other images displayed. In contrast, the display of all artworks in the Las Meninas series uses a table-like view, since no image is clearly central to this sub-collection. We employ artwork images from the Picasso project, reinterpret and extend existing metadata to express myriad connections between these artworks, and facilitate visualization of these relationships to support art scholars from various disciplines.Future WorkWe continue to add new metadata to enrich the relationships expressed in our archive. While new attributes enable us to express additional relationships, the growing number of relationships gets increasingly difficult to represent visually. We are investigating mechanisms to display secondary visualizations without overwhelming the presentation of the primary relationship views. As scholars analyze Picasso’s works and life, the relationships of their interest are likely to increase in complexity as well as variety. It is not possible to express all imaginable relationships among these artworks a priori, nor is it possible to have all the necessary metadata. Enabling scholars to define useful metadata as well as supporting them in forming new relationships will engage them as partners in this project rather than as mere users.The IRV has potential for exploration of artwork relationships in the classroom as well as for evaluating student performance via homework and papers. For example, students could explore a relationship and write a short paper about the artworks it encompasses. An instructor could create a relationship and ask students to identify the relationship embodied by the included artworks. Educators need assistance in the form of targeted features for successful use of the IRV in the classroom setting. We continue our dialog with instructors to channel the IRV’s expressive power for enriching education.",
        "article_title": "Relationship Mapping for Art Education and Research",
        "authors": [
            {
                "given": " Unmil ",
                "family": "Karadkar",
                "affiliation": [
                    "Texas A&M University"
                ]
            },
            {
                "given": "Neal ",
                "family": "Audenaert",
                "affiliation": [
                    "Texas A&M University"
                ]
            },
            {
                "given": "Adam ",
                "family": "Mikeal",
                "affiliation": [
                    "Texas A&M University"
                ]
            },
            {
                "given": "Scott ",
                "family": "Phillips",
                "affiliation": [
                    "Texas A&M University"
                ]
            },
            {
                "given": "Alexey ",
                "family": "Maslov",
                "affiliation": [
                    "Texas A&M University"
                ]
            },
            {
                "given": "Enrique ",
                "family": "Mallen",
                "affiliation": [
                    "Texas A&M University"
                ]
            },
            {
                "given": "Richard ",
                "family": "Furuta",
                "affiliation": [
                    "Texas A&M University"
                ]
            },
            {
                "given": "Marlo ",
                "family": "Nordt",
                "affiliation": [
                    "Texas A&M University"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "Picasso",
            "art history",
            "relationships"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "A reservation I should wish to express is that customarily levelled at digital projects, which is that while the technology (brilliantly, beautifully, wonderfully) enables and indeed encourages the presentation of multiple points of view, so putting the burden of interpretation onto the individual reader, there is a concomitant loss of genuine decision-making by those claiming to be responsible for the work as a whole... Editors should edit: will they?Our epigraph comes from the report of an external assessor to the original funding application for our project. We cite it because it represents a common question asked of the Digital Humanities by traditional scholars: “Can it be as significant as it is pretty?”For editors of text-based digital projects, the answer is increasingly clear. The last decade has seen the development of a relatively solid consensus as to basic technological and generic expectations (see O'Donnell 2004 for a summary). Best practice now expects that text-based digital projects will be encoded using XML, preferably TEI. It assumes they will contain an archive with transcriptions and full colour facsimiles of primary sources; that some means will be provided for comparing variant readings; and that users will be able to test editorial assumptions by comparing or constructing alternative editorial texts. While there is some debate as to whether text-based projects have yet lived up to their original promise (Robinson 2005), there can be little doubt they are beginning to be recognised as important works of scholarship in their own right.For developers of projects that depend heavily on multimedia or collaborative technologies, however, the answer to this question is far less clear. While open standards exist for the encoding of image, moving pictures, and sound, there is little agreement as to how these are to be presented to the end user: unlike text-based projects, multi- and mixed media projects still commonly rely on proprietary software or specific operating systems (e.g. Foys 2002, Reed-Kline 2000; British Library Board, n.d.). And while many digital projects propose using collaborative technology in their design, there is as yet no agreement on the fundamental issue of how such collaboration can function in a research culture based on peer review and the preservation of authorial integrity. A number of exemplary projects are beginning to show how such technologies can be applied in specific contexts or to solve specific research problems (e.g. Ó Croinin et al. [n.d.], Toth et al. [n.d.]). But we are still far from agreeing as to how they can be used more generally to support day-to-day research by working humanities scholars.The Visionary Cross project addresses this problem by treating it as a research question. Our goal is to produce a mixed-media and extensible edition of a key group of Anglo-Saxon artefacts associated with the “Visionary Cross” tradition in Anglo-Saxon England: the eighth-century Ruthwell and Bewcastle standing stone crosses, the tenth-century Vercelli Book dream of the Rood poem, and the eleventh century Brussels Reliquary Cross (for this tradition, see Ó Carragáin 2005).These objects include some of the best known and most studied of the period. The Ruthwell Cross is a 17 foot high stone cross erected near a former Roman military site in Dumfriesshire Scotland. It is perhaps best known to Anglo-Saxonists for a runic inscription that may be the oldest known record of an Anglo-Saxon vernacular poem, versions of which can be found in the tenth-century Vercelli Book and eleventh-century Brussels Cross (see Ó Carragáin 2005, 58-60; O'Donnell 1996, 287-288, for bibliography).The Brussels Cross is a reliquary that once contained a fragment from the supposed True Cross. It is built on an oak core that was covered with precious metal and jewels and perhaps a crucifixion (stolen sometime before 1793; see Van Ypersele de Strihou 2000; Webster 1984; Ó Carragáin 2005). Gilt silver decoration on the cross’s back and side bands bearing a vernacular inscription have survived. On the centre of the back of the cross is a depiction of the Agnus Dei; symbols of the four evangelists are found at the terminals. An Old English inscription around the edge quotes from a version of the same poem found on the Ruthwell Cross and in the Vercelli Book. A second inscription explains that the cross was made by two brothers in memory of a third. On the back we are told the name of the artist responsible for its manufacture.The Bewcastle Cross is a standing stone cross found, like the Ruthwell Cross, at a former Roman military site. Approximately the same size as Ruthwell and belonging perhaps to the same artistic school, the severely weathered Bewcastle Cross still stands in its original location (see Bailey and Cramp 1988). It has the remains of a sundial on its side and may have been painted and decorated with other metalwork or glass attachments. The west face is carved with three figural panels, of which two also appear on Ruthwell. The east side of the cross is decorated with a continuous vinescroll similar to Ruthwell; its north and south sides are carved with panels of interlace, geometric, and foliate ornament. The lowest panel on the west face shows a falconer wearing secular dress. This usually is understood to represent the deceased man commemorated in a now largely illegible runic inscription.The Vercelli Book Dream of the Rood poem ties the members of this collection together (ed. Swanton 1996). The Dream poem describes an encounter with an object that is at once and alternately a tree, a beacon (a word used to describe the Cross on the Bewcastle Cross), a sign, and a cross sometimes covered with blood, and sometimes covered (as in Brussels) with gold and jewels. It ends with the Cross instructing the dreamer to tell what he or she has seen and with the dreamer reciting an expression of devotion and commemoration. The Dream is one of only about 25 poems and poetic fragments known to have survived the Anglo-Saxon period in more than one copy (O'Donnell 1996; see also Orton 2000). If the runic carving on Ruthwell is coeval with the rest of the monument, then the poem has a textual history that is longer and more geographically and linguistically diverse than almost any vernacular poem in the period. The citation of a couplet from the text on Brussels, moreover, suggests that it occupied a very significant place in the vernacular literary imagination: the only other known example of a similar verse citation in the period is from the translation of the Psalter.Together, these objects form a cultural matrix whose members are associated along a number of textual, art historical, liturgical, and archaeological planes. The goal of this project is to use new technology to study these objects and their relationships in ways impossible in print—or even in person. Just as a textual edition improves upon witnesses by contributing an interpretive apparatus, so to our edition will improve on readers' knowledge of this matrix by placing it in a hypermedia apparatus that will assist in its interpretation.The value of this approach is perhaps most obvious in the case of the crosses, which can be understood as multimedia objects in their own right. In all three cases, the monuments gain meaning from the interaction of text, image, and context. The stone crosses appear to have been “read” by walking around in a direction determined by their geographical orientation and the order of the Liturgy. The Brussels cross—depending on one's view of the object's original function—would likely have been seen by contemporary audiences either as an altar piece or carried in procession (On this spatial aspect see especially Ó Carragáin 2005).The new technologies also allow us to ask new questions about the objects relationships with each other. Had an Anglo-Saxon observer been lucky enough to see all four in a single lifetime (an impossible proposition given their temporal and geographic distribution), he or she would have understood them both as individual works of art and as part of a larger web of cultural traditions and references extending along various textual, art historical, and generic planes. By taking advantage of hypermedia's strength in the representation of arbitrary connections, we as editors can now represent these connections to modern scholars in a way that translates and augments the original artefacts—in our edition, linking becomes a type of hypermedia collation. In our edition, scholars will be able to both to study the individual objects as objects in their own right and follow the connections among them. In doing so they will have access both to a collection unavailable to any single Anglo-Saxon observer and the benefit of immediate access to the best of recent criticism and centuries of secondary scholarship.By using recent developments in collaborative technologies, finally, we hope this project—like the cultural knowledge it attempts to capture and represents—will be open to augmentation as our knowledge develops. By using standoff markup, we intend to allow developers and users to anticipate connections to other objects in the matrix or discover new connections among existing objects in much the same way contributors to the Wikipedia can predict the existence of articles that have yet to be written or contribute “stubs” for subsequent elaboration while retaining intellectual ownership of their contributions (see Ore 2004 for a discussion of collaborative editing; O'Donnell 2006 discusses some strengths and weaknesses of the model for scholars).If multimedia projects are going to answer our reviewer's question, they must learn to do more than simply display—they must also learn to edit. This paper discusses the approaches we are and will be taking to this important problem in developing a complex multimedia “edition” of a cultural matrix.",
        "article_title": "The Visionary Cross: An Experiment in the Multimedia Edition",
        "authors": [
            {
                "given": "Daniel Paul ",
                "family": "O'Donnell",
                "affiliation": [
                    "University of Lethbridge"
                ]
            },
            {
                "given": "Catherine ",
                "family": "Karkov",
                "affiliation": [
                    "University of Leeds"
                ]
            },
            {
                "given": "James ",
                "family": "Graham",
                "affiliation": [
                    "University of Lethbridge"
                ]
            },
            {
                "given": "Wendy ",
                "family": "Osborn",
                "affiliation": [
                    "University of Lethbridge"
                ]
            },
            {
                "given": "Roberto ",
                "family": "Rosselli Del Turco",
                "affiliation": [
                    "Università degli studi di Torino"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "medieval studies",
            "editing",
            "encoding"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "IntroductionThe Picasso Project is a web-based, dynamic catalogue raisonnécurrently containing digital images and descriptive metadata for more than 11,000 of Picasso’s works (Mallen, 2006). The catalogue includes commentary for many artworks, along with notes detailing sales of the item, exhibitions in which it has been displayed, and scholarly literature in which it has been cited. In addition to these works, the catalogue also provides extensive biographical information including nearly 7,500 entries. The biography is linked to artworks, photographs of key people, maps of the various places Picasso lived and worked, and related documents (for example, Picasso’s birth certificate). Tools are provided to support side-by-side comparisons of two artworks and to build custom subcollections from which an illustrated, color catalogue can be automatically generated for printing. Within this context, we have become increasingly interested in exploring Picasso’s writings which have, until recently, been largely neglected in favor of his more well known work in painting, sculpture, and collage.Picasso, first and foremost an artist, wrote texts that are strikingly visual, both in terms of the design of the texts themselves, as well as the decorative elements that adorn the pages on which these texts are found. These intensely visual writings present his editor with significant challenges—challenges that exemplify the TEI consortium's guidelines for when TEI should not be used (Lavagnino, 2006). As Lavagnino points out, \"to make the [TEI] edition work as intended it is generally necessary to interpret features and not merely reproduce their appearance.\" Picasso's writings do not readily yield to a single fixed interpretation that can be understood by an editor and transcribed in some definitive form. Indeed, their interest stems, in part, from their complex and indeterminate nature. Furthermore, transcription involves the selection of relevant features of a work and production of a digital (text plus encoding) description of them. In cases where this digital description has little value for analysis, alternative forms of presentation must be pursued. The two examples of this cited by Lavagnino, \"works intended as mixtures of words and images, and very complex draft manuscripts in which the sequence of text or inscription is difficult to make out,\" are both characteristic of Picasso's writings.Despite these difficulties, there have been a number of efforts to produce transcriptions of Picasso’s writings as published books (Bernadac, 1998; Michaël, 2005). This approach gives primacy to the textual content of these works, at the expense of losing almost all of the visual elements. To remedy this, most editions include facsimile reproductions of illustrative samples of his writings alongside the transcriptions. While this is helpful in conveying a sense of the original context of these works, it remains inadequate for many purposes. Beyond limitations of scale (it is only feasible to include a limited number of facsimiles), this approach treats the literal textual elements of Picasso’s writings as  paper_179_furuta_1.jpgFigure 1: Two of Picasso's intensely visual texts. primary, bringing in the images of the original, visually constructed pages in order to illustrate and elaborate. This approach to remediate the writings of Picasso divorces Picasso the writer from Picasso the artist, limiting the productive interchange of ideas that might result from a blending of literary and art history-based approaches.While Picasso’s writings provide a compelling example of the limitations of a purely textual approach to representing manuscripts and other documents, this problem is not unique. Within the digital textual studies community there is an increasing recognition of the need to pair robust text encoding with access to images of the original source material (Dicks, 1997; McGann, 2001). Over the past decade, a number of projects have focused on presenting documents as images while providing additional support via transcriptions (McGann, 1996; Viscomi, 2002). Others have supplemented textually-oriented systems by providing access to digitized images of the original documents in a variety of formats (Furuta, 2001; Robinson, 1996). Image based representations of documents have placed particular strain on hierarchical methods for representing and encoding the features of a text and alternative formal models have been proposed (Dekhtyar, 2006; Renear, Durand, and Mylonas, 1993).ApproachWithin the Picasso Project, we have encountered the problem of developing digital representations of texts from a different perspective than that found in either print based transcriptions or in digital textual projects—namely, we have treated Picasso’s writings, first as works of art, and secondarily as art that contains text. By conceiving of and contextualizing the writings of Picasso in a form common to traditional approaches to art (that is, the catalogue raisonné) we are able to reap immediate benefits for understanding these texts in ways not readily supported by the tools grounded in a textual approach to these works. This approach to visualizing Picasso's texts places them squarely within the context of the other works that he was painting and thinking about at the same time. This helps inform our understanding of both the artworks that Picasso produced as well as his writings. For example, Figure 2 shows a typical thumbnail view of the last fifteen items in the catalogue for the year 1935. In the poem in the center of the last line (OPP.35:004), Picasso refers to a small girl. In the context of drawings made by Picasso around the same time we see connections with Picasso’s daughter, Maya, at three and three and a half months old (OPP.35:031 and OPP.35:032), followed by Marie-Thérèse, Maya’s mother (OPP.35:034). Similarly, using the comparison tool to compare a text dated 28 November 1935 with a painting made earlier that year highlights possible connections between verbal imagery of the text and the visual imagery of the painting. References in the text to \"tongue of fire,\" \"stabbing,\" and \"the eye of the bull\" take on new meanings when seen in this context.In addition to visually contextualizing writings in relationship to Picasso’s other works, the digital catalogue raisonné (unlike a corresponding print version) allows us to make accessible images of Picasso’s writings suitable for reading and analysis. Closer examinations of the text enables scholars to consider multiple states of a text, to see annotations, deletions, and additions to a text, to explore Picasso’s use of color to provide structural divisions or graphical bars rather than traditional punctuation to divide conceptual segments of the text. These tasks, which are difficult or impossible to performpaper_179_furuta_2.jpgFigure 2: la petite fille\" with drawings of Maya and Marie-Thérèsepaper_179_furuta_3.jpgFigure 3: Courses de taureaux and \"lengua de fuego\"using transcriptions alone, are encouraged by the online presentation. To further enable access to the textual component of these works, we are initially adding transcriptions of these texts to the biographical section of the catalogue. Since the online catalogue presents the biographical text in parallel with the artworks, this technique permits easy cross-referencing between the transcriptions and images while we investigate more sophisticated means for encoding and presenting the content of these writings.Implications and Future WorkFraming our approach to Picasso’s writings in terms of artworks that contain text, encourages us to look for text in artworks in general. Like many other artists of his day, Picasso began incorporating words into his artworks in a variety of ways and forms, notably in the newspaper clippings pasted into his papiers-collés. These works reinforce our conviction that we need develop tools for working with text in art grounded to the needs of the artistic disciplines rather than those of the traditional textual studies community.Picasso's unique works offer fertile ground for exploring the techniques and tools that can be applied to visually constructed texts but much work is needed—both in terms of understanding the needs of the scholars and other readers interested in Picasso’s writings as well as formulating new models for representing and working with these texts in a digital environment. We are currently investigating the potential for techniques based in spatial hypertext research for interpreting and presenting these texts. In spatial hypertext, Figure 3: Courses de taureaux and \"lengua de fuego\" nodes of information that are connected by visual elements (for example, text style, 2-D position, color) rather than by explicit links (Shipman, 1999). Our current efforts are focused in understanding how the texts might be sub-divided into their constituent parts and manipulated in a 2-D space in ways that enhance understanding. We are also looking at how formal relationships between these parts can be incrementally added as an expression of an editor/reader's evolving understanding. In addition to purely image based approaches, we are interested in studying methods of encoding the textual content of Picasso's works to support content based retrieval, enable automatic processing, and facilitate reading.SummaryBy translating a traditional print-based approach for working with a large corpus of artworks, the catalogue raisonné, into a digital format, we increase the level of support provided for three scholarly primitives (Unsworth, 2000): comparing (either two works side by side, or many works in a thumbnail view), sampling select artworks from the collection as a whole, and representing the artworks not merely as thumbnails, but also with higher resolution images. With these enhancements, the digital catalogue raisonné, though not its printed counter-part, provides a natural medium for presenting the writings of Picasso. In this context, his writings are presented against the backdrop of other artworks while enabling the textual elements of these artworks to be read and carefully studied as texts. In addition to the immediate benefits that this approach brings in terms of accessing Picasso’s writings, it also offers a new paradigm for working with these texts that suggests several promising directions for further work.",
        "article_title": "Viewing Texts: An Art-Centered Representation of Picasso’s Writings",
        "authors": [
            {
                "given": "Neal ",
                "family": "Audenaert",
                "affiliation": [
                    "Texas A&M University"
                ]
            },
            {
                "given": "Unmil ",
                "family": "Karadkar",
                "affiliation": [
                    "Texas A&M University"
                ]
            },
            {
                "given": "Enrique ",
                "family": "Mallen",
                "affiliation": [
                    "Texas A&M University"
                ]
            },
            {
                "given": "Richard ",
                "family": "Furuta",
                "affiliation": [
                    "Texas A&M University"
                ]
            },
            {
                "given": "Sarah ",
                "family": "Tonner",
                "affiliation": [
                    "Texas A&M University"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "Picasso",
            "information presentation",
            "texts and images"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The widely divergent languages and cultures of Russia, Eastern Europe & Eurasia (hereafter Eastern Europe), present a rich terrain for digital scholarship, but owing to a number of factors—including poorly-endowed or mutually-incompatible infrastructures, language barriers, and different approaches to the construction of metadata—the corpora already in existence and those in development are often poorly known elsewhere, and in many cases are hidden from international resource discovery and data sharing.In view of this situation, it was clear to the Digital Projects Subcommittee of the American Association for the Advancement of Slavic Studies () that improved documentation would constitute an important step toward the promotion of knowledge sharing and interoperability.Other points in the group’s charge include: (2) fostering informed participation in future initiatives; (3-4) increasing digital information and training opportunities for Slavic librarians; (5) complementing national efforts to establish digital repositories. See  The Slavic & East European Library at UIUC is therefore building an international Inventory of Slavic, East European and Eurasian Digital Projects (). The Inventory already describes at least 360 collections in 110 projects, and we also foresee web submissions from international partners to promote collaborative registration. But even now the Inventory is heavily used; recent statistics indicate that in the period July 2005-June 2006 it received around 25,000 hits. It has also become an OAI Data Provider, and although inventory contents (unlike the digital collections they record) are sometimes not reflected in aggregated search services like OAIster (), the Inventory’s records have indeed been harvested and now display in OAIster searches—likely to improve dramatically the visibility of substantive digital projects in the Slavic field. But the project goals go far beyond the compilation of a mere inventory. Besides the implementation of intensive content development, promotion of metadata standardization, expanded reference assistance and interactive user options, the Inventory will focus on an expansion of the OAI data provider system and item-level harvesting. (It also hopes to explore customized delivery services and an archival framework for at-risk collections in the Slavic field, currently being explored through an inter-institutional grant proposal).Nevertheless, the project team acknowledges that metadata practices in Eastern Europe are still very diverse, and far from standardized. At one end of the scale, where relatively simple encoding is involved, even schemes like Dublin Core are poorly recognized. This makes the implementation of relatively well-established data-sharing protocols like OAI-PMH (Open Archives Initiative: Protocol for Metadata Harvesting) little known in Eastern Europe. A registry of OAI data providers at UIUC demonstrates that while OAI data providers are well established in the US and Western Europe, they are much less widely encountered further East.See . U.S. providers are proliferating: in the past year the number of “edu” domains increased from 265 to 332, and “org” domains from 149 to 182. Likewise in Western Europe: U.K. DPs went from 89 to 114, Germany from 73 to 99. But in Eastern Europe it is different; the totals for Hungary (4), Czech Republic (2) and Slovenia (2) stayed the same this year, and Russia increased only slightly, from 3 to 4. Only Poland increased significantly, from 4 to 18. True, the implementation of OAI data interchange is not necessarily well standardized in countries with far more providers.As reported at the Oct. 2005 OAI workshop in Geneva, the German Initiative for Networked Information (DINI) has installed a certificate system in order to bring OAI providers into greater standardization. Furthermore, the whole question of the efficacy of OAI-PMH as a metadata transfer protocol is still somewhat open to question.The Open Archives Initiative OAI-Implementers group announced on Nov. 3 2005 that it is “studying the effectiveness of OAI and some other related methods of creating interoperable online libraries” and has “posted a questionnaire 'Survey on Common Interface Frameworks for Online Libraries' on the web.” Nevertheless, its power is generally well-recognized, and its further penetration into Eastern Europe is certainly a desirable goal.At the other end of the scale, the level of deep encoding is even less widely standardized in Eastern Europe. The decade-long Institute of World Literature project in Moscow known as FEB (Fundamental'naia elektronaia biblioteka) () has an enviable collection of over 50,000 Russian literature texts, all heavily encoded in SGML—but not according to the Text Encoding Initiative (TEI) Guidelines. True, the TEI is not a standard, but interoperability with such an impressive collection would be highly desirable. Other such examples are available. To be sure, there are exceptions: the recent TEI meeting in Sofia, Bulgaria revealed the extent to which TEI projects are beginning to gain root in South Eastern Europe.See Milena Dobreva, “TEI in South-Eastern Europe: Experience and Prospects,” TEI Members Meeting, Sofia, Bulgaria, October 28, 2005. There are also similar developments in a more abstract sense. The TEI Guidelines have been translated into Russian,See . and efforts are also being made to promote the further internationalization of the Guidelines, and to ensure their availability in a greater number of languages, including Bulgarian. Yet even when East European scholarly projects adopt the TEI, standardized approaches to the conversion of metadata to OAI-compliant formats are also at issue, and there is evidently much ground still to be covered. Hence, the object of this paper will be to:Design and conduct a survey of metadata practices in a number of East European digital centers (including existing UIUC partners) that are sponsored both by institutions and by individual faculty teams;Produce an up-to-date analysis of their awareness, evaluation and observance of international metadata standards;Identify problems and practices preventing their implementation;Suggest ways in which steps can be taken to ameliorate this situation, including crosswalks and other technical procedures.The results of this exercise will not only assist in a practical sense the development of the Inventory of Slavic, East European and Eurasian Digital Projects (the only registry of substantive, scholarly East European digital initiatives), but will also result in better awareness and information-sharing among Slavic digital practitioners. This is especially vital as the institutional repository movement gains ground.Here again the number of East European repositories lags far behind. In July 2006 there were 144 DSpace instances worldwide, and less than half were in the U.S. and U.K. (62), indicating that many non-English-speaking countries (like Japan) are aware of and attracted to this increasingly popular open source software. But in Eastern Europe only 2 DSpace sites were listed (in Russia). This lack of awareness was substantiated at an e-text conference in Eastern Russia in July 2006 entitled Modern Informational Technologies and Written Heritage: From Ancient Manuscripts to Electronic Texts at which I gave a presentation entitled “Promoting the TEI in scholarly communities: OAI harvesting, digital repositories.” Conference attendees described many sophisticated projects, but there was little familiarity with OAI or digital repository software used in the West. One scholar was familiar with these technologies, but in a presentation entitled: “A Computer System for the Creation and Maintenance of Electronic Collections of Ancient Texts,” V. S. Iuzhikov (Kazan State University) wrote: “For the creation and development of electronic libraries there are several systems. Among the best known are Greenstone and DSpace. But they are oriented mostly toward text libraries with small number of illustrations, which are poorly suited for the collection of old printed works. Therefore it is less time-consuming and gives better result to build a local system.” Given the specialized nature of his material, this conclusion was understandable. Yet the general lack of East European involvement with these international standards and methodologies remains true. Experience shows, particularly in West Europe, that the issue of standardized metadata is paramount, and that subject repositories are becoming of even greater interest since they tend to be populated by scholars working on their own projects and producing more substantive metadata. In this environment, East European scholars should not excluded from the pool of informed experience. ",
        "article_title": "Digital Text Projects in Eastern Europe: Promoting International Interoperability",
        "authors": [
            {
                "given": "Miranda ",
                "family": "Remnek",
                "affiliation": [
                    "Slavic & Eastern European LibraryUniversity of Illinois at Urbana-Champaign"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "collaboration",
            "interoperability"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Since the codification of the Text Encoding Initiative standards in the mid-1990s, the process of the creation of digital editions and archives is largely one of \"marking up\" existing texts in XML. Originally, this was often done \"by hand.\" Scholars would add XML tags to existing text documents using a text editor-a tedious process often fraught with errors. Over the last decade several tools have been produced which make this process somewhat more efficient and accurate, though most still require more than a beginners familiarity with XML encoding, and few are open source. Moreover, many digital humanities projects have, of late, become far more multi-medial--relying on image, video, and audio files as well as text. Existing markup tools have only begun to work with these non-textual artifacts. As digital archives continue to grow, the markup tools used to encode them must become more flexible and easier to use. The Ajax XML Encoder (AXE), developed at the Maryland Institute for Technology in the Humanities (MITH) is intended to be the tool suite that meets this need.AXE is a free tool suite that will allow users with limited technical skills to deeply tag text, images, video, and audio files for inclusion in digital archives. The program combines and extends the functionality of proprietary online tools such as YouTube and Flickr in a (mostly) open sourceAlthough all code written at MITH will be published on the web, the use of Adobe's Flash prevents the project from being truly open source., web-based platform for scholarly use. The tagging tool is designed in AJAX and Macromedia Flash and generates and uses a MySQL database. Like the mythological Ajax's axe, MITH's AXE provides users with enough power and flexibility to accomplish their tasks without a great deal of assistance from higher powers (the technical or professorial \"gods\" graduate student workers must often invoke when using earlier encoding tools).Users of this tool are divided into two classes: managing editors and editor-users. Managing editors are permitted to define the sorts of tags all users can use and are able to remove items from the database. All users, however, are able to add new multimedia content to the database, tag it, and search for preexisting content in the database. Tagging is naturally accomplished in different ways depending on the media. For sound files, an mp3 file is played through a Flash plug-in. When the user pauses a sound file once, the timing is recorded. When the user pauses the file again, the end time is recorded and the user is presented with a web-based form to tag the selection. If appropriate, a textual transcript of the sound with corresponding times can be made. Providing a tool for image tagging was more difficult. Although popular websites like Myspace and Facebook provide some models for folksonomic image tagging, the tag-spaces are usually limited to regular polygons (like squares) and do not store the resulting data in a form that can be easily shared. A better model for image tagging is the ARCHway Project's \"Image Tagger,\" described in the June 2006 issue of the International Journal on Digital Libraries by designers Dekhtyar, Iacob, Jaromczyk, Kiernan, Moore, and Porter . The program is hindered, though, by the clumsy Java-based interface that requires users to install and learn specialized software. Our program uses an AJAX website which allows the user to add points to an image map (represented on the image itself via a 1 pixel div element with a red border) [see figure 1]. Once the image map is drawn, the user can describe it with tags defined by the editor (or editors) of the project. The entire image can also be so describedpaper_145_reside_1.jpgFigure 1Video tagging, as one might expect, uses a combination of the image and audio tagging methods. The video is first converted to flash movie format using FFMpeg. The audio portion of the movie is handled in exactly the same way as stand alone audio. The visual portion of the movie is handled with a combination of the techniques used for images and sound. The user can tag, for instance, the start and end times of a particular segment and add metadata to this portion. AXE even allows users to tag images within the frames of movie files. If the user wished, for instance, to tag a tree in the background of a shot, the user first marks the time period in which the appropriate image appears. A user-defined number of the frames from this segment are then stacked and rendered translucent. With some adjustments, the user can then click a series of coordinates which define the selection over the space of the segment. This process is rendered through Javascript and Macromedia's Flash.Once all the tags have been created, they are parsed into a mySQL database (the XML is generated first to allow users to work offline and then feed the XML to the database later). This database can be searched through traditional keyword or tag cloud searches, but the interface also allows guided browsing. When the user views a multimedia object, the user is also presented with a series of \"related\" objects (after the e-commerce model in which customers are presented with a series of products related to one they are currently viewing). If the user selects a sub-element in the document (perhaps something like a tree in the background of a photograph), the \"related\" documents will change to reflect relationships centered on the user's choice. The user can also set global limits to what suggestions are made. The related documents will change to reflect user selection of sub-elements. The user can set global limits on what relations are presented (e.g. related documents must be dated after the current document). New tags and documents from remote sites can also be added to the database by editor-users. Later users can decide whose tags they trust and block those by anonymous or unreliable taggers.AXE is currently being used in a project headed by Angel David Nieves to create a multi-perspective history of a 1976 student uprising in Soweto, South Africa. Dr. Nieves has an enormous wealth of multimedia primary documents surrounding the event and hoped that the mass of materials would provide new ways of telling the story. We needed, however, to process and present this material in an unbiased way that allowed for multiple interpretations of the events. AXE is being used to create an interface which, as much as possible, left the arrangement and interpretation of the materials up to the individual users.AXE may eventually prove essential in the creation of digital library archives. Although academic libraries are now fairly adept at the digital preservation of textual material, few libraries provide searchable digital archives of sound and video. As the average available bandwidth of both users and institutions increase, software will be needed to allow the cataloging and access of this material. AXE seeks to fill this need.",
        "article_title": "The AXE Tool Suite: Tagging Across Time and Space",
        "authors": [
            {
                "given": "Doug ",
                "family": "Reside",
                "affiliation": [
                    "Maryland Institute of Technology in the Humanities (MITH)"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-03",
        "keywords": [
            "tools",
            "AJAX",
            "tagging",
            "video",
            "XML"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "In working across and between disciplines, it is the tacit assumptions that may be most destructive to meaningful collaboration. Ours is a state of mutual ignorance, and the goals and practice of the professional literary historian and the machine-learning researcher are equally obscure. But in collaboration mutual ignorance becomes an opportunity for self-reflection, clarification, and the speaking of what is usually unspoken. Willard McCarty writes, “Computational form, which accepts only that which can be told explicitly and precisely” proves “useful for isolating ... tacit and inchoate” knowledge (256). Collaborators are forced to set out a program in detail, one that is mutually comprehensible but also one that delivers results that are simultaneously meaningful in two disciplines. In this paper, we discuss the tacit assumptions that accompany data set preparation, hypothesis testing, and data exploration in order to deliver prescriptive claims. We propose a communication protocol designed to bring hidden and tacit assumptions into plain view where they may be discussed and analyzed. This paper is the third in a series of collaborative efforts undertaken by the two authors. It is informed by real experience working together: working often at cross purposes, garbling a common language, but ultimately producinresults that are of interest to both computer scientists and literary scholars.Transforming literary content into data for machine learning methods requires the adoption of a number of initial assumptions, each of which significantly impacts the final results. First, the collaborators must select or design an appropriate data representation. The selection of a bag of words model may be one such decision, but other feature mappings such as parse trees or link structure graphs may be more informative for a given task. Once the textual material is represented, we must decide upon a method of feature weighting; that is, we must decide if some features are more important than others and how much so. Because many learning methods prove intractable when working with very large numbers of features, feature selection is necessary in order to enable computation. Sophisticated, ambiguous, unstable texts must be normalized to make comparisons across texts meaningful—so that the choice of normalization method is critical. So, too, are methods of noise filtering. There are no clear objective choices among methods, because each choice introduces a set of assumptions and biases. We demonstrate these difficulties with experiments on a range of literary data. A loose analogy is here drawn as the literary scholar may choose to cite post-colonial theory to the exclusion of queer theory or practice close reading to the exclusion of historical analysis. We do not argue that structuring assumptions be minimized or eliminated--this is impossible--but we do make the case that in interdisciplinary work especially, it is important for the impact of each assumption to be assessed and reported at the outset. The critic is often a bricoleur, borrowing from literary theory in promiscuous fashion. In preparing data, bricolage is not a ready option and the collaborators must make painful compromises.The use of machine learning for the testing of literary claims also has several potential pitfalls. The broadest is the impact of the No Free Lunch Theorem, which states that there can be no single machine-learning algorithm that gives optimal performance on all data sets. The choice of a learning algorithm entails, again, the adoption of tacit assumptions about the underlying structure of the data. We may assume that data is linearly separable (which is often a true assumption in text classification), or that the data examples are statistically independent of one another (which is often false in the text domain). As we demonstrate experimentally, these assumptions carry significant impact on the results of the data mining. In the literary domain, selection bias seems particularly problematic as we navigate the politics of canon formation, the difficulty of defining of genre, and the vagaries of influence--all of which trouble the initial selection of texts.An important question in both machine learning and literature is that of generalization. Do the results and models we discover apply only to our particular data set (as in the case of rote learning), or do these patterns also describe new periods and genres, data we have not yet investigated? In truth, machine-learning methods can never guarantee generalization. However, they do offer statistical bounds on the probability that a model will generalize. According to the Probably Approximately Correct paradigm of computational learning theory, a model that achieves a given level of accuracy on a training data set will likely achieve a predicted level of accuracy on a test data set from the same distribution. The computer scientist emphasizes that generalization bounds are only valid under assumptions of statistical independence in the training data. Care must be taken in the literary domain to ensure that probabilistic assumptions are satisfied. Otherwise, the findings may reflect little more than the selection bias of the investigator. We provide concrete examples of these issues using data from literary analysis, and give guidelines for determining when a generalization assumption may or may not be valid.The literary scholar often turns to computational methods to explore large numbers of texts--more texts than one human could ever read closely. In this last case, the scholar may not have a hypothesis to test, but is instead looking for new perspectives on literary history. In a word, the literary scholar hopes to be surprised by the computer scientist. However, surprise is too easy a commodity to supply in data mining. Consider that some of the first literary data miners were the Dadaists and Surrealists, who produced poetry by cutting a printed text nto pieces and pulling those pieces randomly from a bag. In machine learning, this method of textual analysis is known as Gibbs sampling (Duda), and has been used in recent work on probabilistic author-topic modeling (Steyvers). This sort of surprise, however, may not be that which a literary scholar desires--it may not be a meaningful surprise. Thus, the scholar must define for the machine-learning specialist exactly what sort of surprises are desired, so that the appropriate data mining methods may be applied. This is a curious hermeneutic circle--the critic worries that requesting a particular kind of surprise effectively removes true surprise from the process. Data exploration requires a bound on the unknowns to be meaningful and productive. We adduce examples of this need with experiments in anomaly detection on literary data.Data exploration may be performed by employing data visualization techniques, or by using unsupervised methods of machine learning such as clustering. In both of these situations, it is important to keep the cartographer's dilemma in mind. In order to understand large data sets in high-dimensional space both the literary scholar and the computer scientist require some form of dimensionality reduction. While reductive methods may, indeed, enable new insights, they may also produce artifacts--strange islands analogous to the distorted, massive projection of Greenland on most two-dimensional maps of the world--that give a distorted view of the underlying structure.Two specific dangers, then, accompany data exploration. The first is that a distorted artifact, a picture, may be mistaken for an underlying truth. The second is that once a data set has been fully explored, it may no longer be valid to use it for hypothesis testing. An exhausted data set prompts us to move on to a new set of texts, to generalize as discussed above. But moving to a new set of data, we often discover that our hypothesis is not portable and fails to generalize. The history of literature is a \"collective system,\" as described by Franco Moretti in Graphs, Maps, and Trees (4), but law-like generalizations are difficult to frame and even harder to transport from text collection to text collection: we discover patterns, yes, and congruent patterns may be discovered in different collections. To say more is to abandon many of the certainties that the literary scholar had hoped machine learning would provide him with. In preparing a data set we construct a system; leaving that data set behind we leave behind its artificial systematicity as well. Here the literary scholar is surprised to find the computer scientist a more thoroughgoing poststructuralist than himself. It may seem that data mining offers no more claims to objectivity than literary scholarship -- and indeed, from a certain perspective, this is the case. At its worst, data mining and visualization techniques produce mere inkblots that do little more than manifest the hidden (and indeed, perhaps even unknown) biases of the researchers. However, these explorations gain in consequence as the tacit assumptions of both the literary scholar and the data miner are clearly stated. To assist in foregrounding assumptions, we propose a protocol for researchers in these disparate fields. This protocol includes ways of talking about patterns in a common language, for defining meaningful data representations, and for selecting appropriate statistical assumptions. Only when careful preparatory work is done can data mining have a claim to meaning in the humanities.",
        "article_title": "Meaning and Mining: the Impact of Implicit Assumptions in Data Mining for the Humanities",
        "authors": [
            {
                "given": "D. ",
                "family": "Sculley",
                "affiliation": [
                    "Department of Computer ScienceTufts University"
                ]
            },
            {
                "given": "Brad ",
                "family": "Pasanek",
                "affiliation": [
                    "Annenberg Center for CommunicationUniversity of Southern California"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "collaboration",
            "data mining",
            "metaphors"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "IntroductionElectronic scholarly editions which mimick conventional models of scholarly editions as prescribed by different theoretical and methodological schools provide denotative models (Geertz, 1993, p. 93) which thrive on the otherness of the digital medium but re-emphasize the computational aspect of the 'computer-based' (Steding, 2002), 'Computergestützte' (Kamzelak, 1999), or computer-assisted scholarly edition. The isomorphism between the digital and the print medium aimed at by the application of computational techniques to the praxis of scholarly editing confirms what we already know. What is interesting, however, is not the degree to which the computer can assist the editor in digitizing, creating, and publishing an edition, but the intentional artefacts which are built by using the computer as a modelling tool (Smith, 2002). They are instrumental in two crucial activities of humanities research, that is, the discovery of meaning and the making of meaning. As products of (experimental) modelling, their purpose is 'to achieve failure so as to raise and point the question of how we know what we know' (McCarty, 1999), 'what we do not know', and 'to give us what we do not yet have.' (McCarty, 2004, p. 255) This paper will address the role of experimental modelling and the assessment of exemplary models of scholarly editions in the development of a useful classification, typology, and description of electronic editions.ClassificationAs psycholinguistic research has shown, categorization is innate in human cognition (Giannakopoulou, 2003) and involves the formation and use of patterns in a self-maximizing system (deBono, 1978, pp. 25-43). Operations which can be classified under this scholarly primitive are naming, labelling, classifying, cataloguing, indexing, sorting, etc. Categorizing as a mind process can result in the production of formalized instruments such as bibliographies, indexes, catalogues, classification schemes, and taxonomies for which advanced subject analysis is needed. For the most part, however, it remains a culturally determined mind tool particularly where it is used for the selection of usefulness. This means that categorizing is not determined by how the world is, but tries to develop convenient ways in which to represent it (Hacking, 1999, p. 33).Traditions and TypologiesTextual scholarship is fragmented by the development of different theories, methods, and praxes which are based on a diversity of attitudes and perspectives (author, language, audience, function, format, etc.). This becomes especially clear when studying current typologies and classification schemes for scholarly editions. Heinrich Meyer (1992) surveyed the literature on textual scholarship in Germany in the twentieth century and listed more than forty names for different types of editions that were used. As he argued, the 'ausgabentypoligische Terminologiewirrwarr' (Meyer, 1992, p. 17) is the result of a methodological pluralism both inside and across editorial traditions.As a consequence, there is no one theoretical paradigm for textual scholarship across all traditions, periods, languages, and authors and there is no one universally applicable taxonomy of editorial types. Moreover, the existing taxonomies are seldom internally consistent in their applied perspective. The simplified representation in the German school, for instance, offers a taxonomy which runs from the archive edition over the historical-critical edition to the study and the reading edition (Kanzog, 1970, pp. 9-44). Where 'archive edition' denotes the archival function of this type of edition and hints at the extent of the documentary set presented, 'historical-critical' refers both to the method used to create the edition and the format in which that edition comes before the user. The study-edition and reading edition, on the other hand, identify the envisioned function of the product and its intended audience in their naming. In the Anglo-American tradition, the copy-text edition refers to a specific theory of establishing a text whereas the types of scholarly editions David Greetham mentions in his Textual Scholarship. An Introduction mainly refer to the format or appearance of the edition, such as 'parallel print edition', 'variorum edition', or 'type facsimile edition', or to a combination of format and method such as 'Eclectic Clear-Text Edition with Multiple Apparatus'. (Greetham, 1994, p. 383)The least useful typology of scholarly editions is based on the publication medium. Here we have print edition, hybrid edition, and electronic edition. Especially this last one is often presented as a meaningful class while it is widely used to name almost anything which is available in an electronic format. A sad example is, for instance, the édition électronique of the correspondence of René Descartes which is nothing more than a 35 page MSWord file which has been made available online  . Electronic editionsWith respect to the classification of electronic editions, it becomes difficult to maintain the application of conventional typologies and taxonomies, or ignore them altogether. The danger of a normative typology and hence a rigid theoretical frame for textual scholarship is that it establishes its principles firmly without allowing the advancement of its theories, methodologies, and practices. However, as a scholarly discipline, scholarly editing should be interested in both. Especially when, in the case of electronic scholarly editing, exemplary modeling is employed as a scholarly method to generate electronic editions rather than the epigonuous application of rigid theory and method to the electronic edition.Classification GeneratorFor reasons of identification and bibliographic research on electronic editions (Lavagnino, 1996; Dahlström, 2002; Kirschenbaum, 2002; Van der Weel, 2005), there is a need for some integrated scheme by which editors of electronic editions can describe their edition according to several parameters. With the classification generator which we propose here, we believe we have developed a tool which can be of aid to that purpose.The classification generator is an on-line tool which allows the editor to input the details of the electronic edition atomized in meta-information on the edited text (language, period, genre) and information on the edition via a user-friendly form. The latter minimally contains details about method, intended audience, content, format, encoding, technology, function, and functionality of the edition. Once the edition is described according to these parameters, a descriptive classification code is generated that can be included in the published edition. This classification code is an alphanumerical string that exactly describes the electronic edition from multiple perspectives. The classification generator serves at least three goals. First, it liberates the field of electronic scholarly editing from the conventional text-editorial theories with their rigid and inconsistent prescriptive typologies. Instead the classification generator atomizes the different facets of the electronic edition and presents the sum total of this documentation as a description of the product. Second, the user confronted with an electronic edition gets a detailed description of the kind of electronic edition one is using on inputting the classification code in the classification generator. Third, the codes derived from the classification generator can be of use for an (analytical) bibliography of electronic editions. The description of an improved re-release of an electronic edition will generate a different classification code which could be collated against the codes of other releases of the same edition.A last feature of the classification generator is the option to register an edition's classification code together with a formal bibliographic description in a database. This database will allow theorists of electronic scholarship and bibliographers of new media to perform interesting forms of analysis on its contents.",
        "article_title": "A Descriptive Classification Generator for Electronic Editions",
        "authors": [
            {
                "given": "Edward ",
                "family": "Vanhoutte",
                "affiliation": [
                    "Centre for Scholarly Editing and Document StudiesRoyal Academy of Dutch Language and Literature"
                ]
            },
            {
                "given": "Ron ",
                "family": "Van den Branden",
                "affiliation": [
                    "Centre for Scholarly Editing and Document Studies Royal Academy of Dutch Language and Literature"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-03",
        "keywords": [
            "classification",
            "typology",
            "modeling",
            "electronic edition"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "1 MotivationBlogging has become a new and disruptive communication medium. Blogs have changed the way people and organizations express, interact, and—quite unforeseen—exercise influence. David R. Ellis’ film Snakes on a Plane (2006) starred by Samuel L. Jackson became the first movie to incorporate materials suggested by bloggers long before the movie finished filming. A social mass of blog-based fans influenced the Hollywood creation providing ideas about plots and scenes that finally made it into the released movie. The digital nature of the blog media provides access to an always-expanding corpus of information. It would take more than a lifetime to read all the available blogs necessary to answer questions such as what were the more relevant plots suggested or what key concepts were managed by bloggers in their ideas. However, human-centered analysis and visualization techniques may help users navigate such enormous corpus. This paper presents how human-centered analysis and visualization techniques help identifying relevant post portions and visualizing concept relations in the blogosphere—Google blogs in particular are used for illustrative purposes.The rest of the paper is structured as follows. Section 2 presents a brief overview of the techniques and visualizations proposed to track the blogsphere. We describe in section 3 how such tools can be applied to track the blogosphere. Finally, we present some conclusions and further research directions in section 4.2 Snakes, Bloggers, and Human-InnovationTracking the blogosphere requires at least (1) gathering blog posts and (2) storing them in a structured metadata store before any analysis and visualization can take place. Blogs rely on syndication feeds, usually incarnating in the form of RSS or Atom feed—both based on XML (Miller, 2001; AtomEnabled, 2006). The first step is to properly process blog feeds by retrieving, annotating, and storing the posts’ contents in the feeds for later analysis. Our approach stores the posts in a RDF-based (Shadbolt, 2006) metadata store⁚Mulgara (Gearon, 2006)⁚waiting to be analyzed. Then, we use the extracted text as the input of three different analysis and visualization techniques. It is important to mention here that our approach is based on statistics instead of more traditional approaches based on natural language processing—from which we may benefit in future stages.2.1 BITS: Getting the relevant terms and excerpts of a postBITS (blog induced topic selection) is a ranking algorithm for words and sentences in a blog. Higher ranked words may be regarded as main topics used in a blog. Similarly, higher ranked sentences express how key concepts are used in the posts. BITS is inspired by HITS (hypertext induced topic selection) algorithm proposed by Kleinberg (1999). BITS ranking is based on mutually reinforcing relationship between sentences and words: important sentences include many important words and important words are included by many important sentences. The rankings are obtained by an iterative calculation⁚further details can be found elsewhere (Kleinberg, 1999). Each iteration we update the score of each sentence using the sum of scores of all the words of the sentence; we also update the score of words using the sum of scores of all the sentences containing the word.This mutually recursive calculation provides two important outputs: (1) the ranking of relevant words for a blog, and (2) the ranking of relevant sentences. The ranking of words can be regarded as a summarization of the topics discussed on a given blog. On the other hand, we regard the ranking of sentences as an excerpt extraction technique capable of providing relevant excerpts of a blog and, hence, a summarization tool.2.3 ISNP: Modelling posts elementsThe text contained in a post can be turned into a n-dimensional vector of features using text mining techniques (Weiss, Indurkhya, Zhang, &Damerau, 2006). Each feature is a word in a blog post once stop words are removed. Each vector entry represents a frequency measure for the a given word—TFIDF in our particular case (Weiss, Indurkhya, Zhang, & Damerau, 2006). This simple transformation enables the usage of machine learning techniques as tools for exploring and understanding the processed blog posts. ISNP (Identifying Self/Non-self Post) is an algorithm and visualization technique to create predictive models of posts on a given blog. ISNP uses the postbased feature vectors to learn models that describe and predicts what post belong to a feed. In particular ISNP induce linear models based on support-vector machines (Vapnik, 1999; Cristianini & Shawe-Taylor, 2000; Shawe-Taylor &Cristianini, 2004). Once the models are learned, we can use them to: (1) predict pertinence to a feed given a blog, (2) compare multiple feeds to measure degrees of topic overlapping, and (3) visualize the key elements that identify self in a post.The proposed visualization based on ISNP results allows the analyst to quickly distinguish the main topics that characterize a feed, and also obtain a measure of the existing overlap between feeds from different posts. The visualization presents a polar arrangement of the terms that distinguish self and non-self blog feeds and the strength of each them—see Figure 1. ISNP also provides another visualization of how topics change as new posts are added to the blogs feed stream by displaying sliding windows of the TFIDF values across the sequences of post of a blog—see Figure 2.2.3 KeyGraph: Visualizing concept relationsWhen applied to blogs, KeyGraph (Ohsawa, Benson, & Yachida, 1998) is a chance discovery technique (Ohsawa & McBurney, 2003) which provide a visual map of the contest of the posts of a blog feed. A KeyGraph is a graph where nodes are words on the blog posts and links indicate co-ocurrence of words in sentences. KeyGraph has been widely used as tools to support human innovation and creativity in on-line scenarios (Llor`a, Goldberg, Ohsawa, Matsumura, Washida, Tamura, Masataka, Welge, Auvil, Searsmith, Ohnishi, & Chao, 2006). KeyGraph starts computing high-frequency terms and high-frequency links among them given the sentences of a blog. Then, relevant low frequency terms (key terms) and links (key links) are identified. A key terms and key links bridge high frequency clusters together, flagging interesting transitions between the concepts described by those clusters. Finally, ranking high frequency and key terms based on the connectivity degree allows the KeyGraph to identify keywords.KeyGraph visualization represents concepts and their relations as visual maps, favoring humanreflection. Moreover, it provides a simple exploratory method to evaluate bridges between concepts, fundamental building blocks of innovation and creativity. KeyGraphs are usually presented nodes and links using three colors: grey to identify high frequency terms and links, red to display key terms and links, and green border nodes to identify keywords—as shown in Figure 3.3. Tracking the Google BlogTo illustrate the capabilities of BITS, ISNP, and KeyGraphs we tracked the Google Blog () from November 10th to November 14th. A detailed description of the methodology and results is beyond the scope of this paper and can be found elsewhere (Llor`a, Yasui, Welge, &Goldberg, 2007). However, we present some illustrative examples of the blog analysis and visualization techniques proposed in this paper. Unless noted otherwise, the results described below present the analysis of the post entitled “Old world meets new on Google Earth”BITS. The BITS ranking provide the following terms as relevant: map, earth, historic, explore, old, world, tool, and cartography. The more relevant and descriptive sentence of the post was: “I was able to explore and fly around the old maps and use the transparency slider to compare the old world and the new; as I did this, I thought to myself that this is the perfect marriage of historic cartographic masterpieces with the innovative contemporary software tools of Google.” After reading the complete post⁚not reproduced due to its length⁚it became clear that the terms provided by BITS were an accurate description of the topics discussed in the post. Moreover, the extracted excerpts by BITS acted as a relevant summary of the posts in the blog.ISNP. ISNP was used to learn models that uniquely identify posts. The linear model based on support-vector machines was able to accurately distinguish between the ten post of the feed during the period of observation. Moreover, the visualization of such models—see Figure 1— correctly identified topic overlapping with two other posts talking about Google Earth. ISNP also provided a simple visualization of how term relevance changed through time, identifying recurrent topics—Figure 2 displays recurrent topics as wide areas on the accumulated vertical axis graph.KeyGraphsFinally, the last analysis and visualization tool⁚KeyGraph⁚provided a clear map of the concepts managed in the overlapping posts “Old world meets new on Google Earth” and “Know where you are”., as well as their bridging relations⁚see Figure 3. KeyGraph clearly visualize the two main discourse clusters provided by the overlapping posts, and also made explicit the connection between them.4. Conclusions and Further WorkThis paper has presented how human-centered analysis and visualization techniques used to support innovation and creativity can also help to identify relevant post portions and to visualize concept relations in the blogosphere. BITS, ISNP, and KeyGraphs were introduced and used to analyze the posts on the Google Blog for illustrative purposes. The proposed techniques showed how humancentered techniques can easily assist tracking the blogosphere for relevant information, concepts, and relations, filtering the amount of information that the analyst need to review by providing meaningful summaries and visualizations.AcknowledgmentsWe would like to thank the Automated Learning Group at the National Center for Supercomputing Applications for their friendship and support while hosting this joint collaboration. This work was sponsored by the Air Force Office of Scientific Research, Air Force Materiel Command, USAF, under grant F49620-03-1-0129, and the National Science Foundation under grant IIS-02-09199. The US Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation thereon.paper_256_llora_1.jpgFigure 1: Radial map of the key terms involved in the ISNP models for each of the posts. Different posts are displayed in different colors. The area provided by the measure of relevance of the terms provide a qualitative measure of model overlapping for the different post. Post number 5 corresponds to the analyzed “Old world meets new on Google Earth”.paper_256_llora_2.jpgFigure 2: ISNP visualization of term dynamics across the different post.paper_256_llora_3.jpgFigure 3: Visual map of the concepts involved in the two overlapping posts “Old world meets new on Google Earth” and “Know where you are”. KeyGraph clearly visualize the two main discourse clusters provided by the overlapping post, and also makes explicit the connection between them.The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Air Force Office of Scientific Research, the Technology Research, Education, and Commercialization Center, the Office of Naval Research, the National Science Foundation, or the U.S. Government.",
        "article_title": "Human-Centered Analysis and Visualization Tools for the Blogoshpere",
        "authors": [
            {
                "given": "Xavier ",
                "family": "Llorà",
                "affiliation": [
                    "National Center for Supercomputing ApplicationsUniversity of Illinois Urbana-Champaign"
                ]
            },
            {
                "given": "Noriko Imafuji ",
                "family": "Yasui",
                "affiliation": [
                    "Industrial and Enterprise System EngineeringUniversity of Illinois Urbana-Champaign"
                ]
            },
            {
                "given": "Michael ",
                "family": "Welge",
                "affiliation": [
                    "National Center for Supercomputing ApplicationsUniversity of Illinois at Urbana-Champaign"
                ]
            },
            {
                "given": " David E. ",
                "family": "Goldberg",
                "affiliation": [
                    "Industrial and Enterprise System EngineeringUniversity of Illinois Urbana-Champaign"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "text analysis",
            "visualisation",
            "blog"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "This paper presents the methods and tools designed to appraise a digital archive. The attributes characterizing the archive at hand led to the development of the concept of natural electronic archives that would allow transforming the archive as a whole into a unit of analysis. The methodology for appraising the archive combines traditional archival concepts while adding tools –such as text mining and social network analysis– taken from other fields.The digital archive belongs to a philanthropic agency whose activities in support of the arts, sciences and social welfare in Argentina span from the mid 1980s until its closure in December of 2005. In the early 1990s the institution implemented a networked server to store, retrieve, and share electronic texts, email, applications, and databases. In it, each employee had a virtual folder with his or her initials to store the files that they created. To comply with legal requirements the archive must be kept closed but accessible for the next 10 years. In parallel, appraisal needed to be undertaken to decide on the archive’s long term destiny. An initial survey of the archive led to the development of the natural archive concept and suggested an appraisal method to establish archival evidence.The natural electronic archive concept grew while I was surveying the server and from the interviews conducted with staff members who had worked in the organization since it opened. Based on my analysis, I concluded that the manner in which records were generated and kept could not easily be ascribed to digital archiving models currently discussed in the literature. These models focus more on the creation of sound electronic records, the design of electronic record-keeping systems, and on institutional repository archiving models than on the way in which digital archives exist “in the wild” (Bearman & Trant, 1997; Cox, 1997; Duranti et al., 2002; InterPARES Authenticity Task Force 2002; Jones et al., 2006). The natural archive concept builds partly on that of “natural collections” proposed by Phillip Cronenwett to describe collections of literary manuscripts as they leave the hand of the creator (Cronnenwett, 1984, p.106). I suggest that this concept is relevant both to the case study at hand, as well as to archives of public or private persons and institutions showing similar characteristics.Creation of natural electronic archives involves a set of ad-hoc practices developed as people adjust to and learn how to use information technologies. A natural archive is not designed or managed by records managers or archivists. Instead, it is what those working in institutions, in different capacities, using different technologies, and making decisions, make of it. In a natural archive, records are created, named, destroyed, or retained according to individual work-practices. Each record creator decides on structure and naming conventions for files and folders, spontaneously or consistently, according to individual mnemonic rules or the spur of the moment. Within the virtual folders, images, spreadsheets, texts, web sites, databases, back-ups, and applications live together under the same roof, placed or misplaced, in organized or disorganized fashion, in general without descriptive clues.In a context without explicit recordkeeping rules, bits and pieces of text are ubiquitous inhabitants. Either shared by different members of a network or used repeatedly by their creators, they constitute the core of many records. This repetition of fragments afforded by the cut and paste function of the text editor, speaks as much of provenance, group collaboration and fair use, as of hierarchies and corporate culture. As a consequence of all of these phenomena, records within a natural archive are difficult to identify and lack formal documentation. This creates doubts about their capacity to provide evidence.An appraisal method that uses Text Mining and Social Network Analysis was designed to determine what type of evidence of the organization that created it is provided by a natural electronic archive. The method is rooted in concerns expressed by Peter Boticelli in his study of networked organizations (Boticelli, 2000). It considers the need to document dynamics and changes in organizations and it explores the meaning of evidence and archival bond –understood as the “network of relationships between records” – in an ambiguous environment (Duranti & Guercio, 1997). Also, it highlights the importance of preserving evidence of the archive’s formation process that will allow the study of its technological history and social uses (Lubar, 1996; Parezo, 1996). Its main departure from other appraisal methods is that it uses digital tools to analyze a large corpus of records inductively.Determining archival evidence implies being able to map the organization through its records. Text Mining and Social Network Analysis use computing algorithms to discover knowledge about the relations among electronic records. By measuring the similarity between texts produced and co-produced by staff members within frameworks of time and provenance, the strength of relationships between records and between the staff members and/or functions that created them can be established. In turn, by averaging the similarities between the records of all pairs of staff members or functions across time, organizational structure and functions as well as correspondent changes in dynamics emerge. To confirm the validity of the findings, results are contrasted against the narratives of staff members about who they collaborated with, when, and in what. In this way, the evidence provided by the electronic records in this natural archive can be attestedA proof of concept was conducted to determine the feasibility of the appraisal method. For this, copies of electronic text records from the archive were used, while the original archive, kept with its directory structure intact, remains as guarantee of provenance and original order. Pre-processing documents involved using file management software to sort files within directories and sub-directories to construct sets belonging to a group of staff members that worked in the organization during a one year period. After conversion to .txt format, the sets were submitted to Rainbow, an open source text classification and retrieval tool, to obtain a vector space model (McCallum, 1996). In this phase, several trials were conducted to find the best way to narrow the vocabulary without losing language subtleties. From this model, pair-wise distances between documents where calculated using the cosine similarity formula in MatLab on a UNIX server. The resultant matrix was submitted to the social network analysis software UCINET to obtain a network drawing of the distances between texts (Borgatti et al., 2002). In turn, the average of distances corresponding to each staff member were calculated to obtain a matrix of relationships between staff members during one year.The experiment suggested that relationships between staff members do emerge from the similarities and differences between the texts that they create. Testing showed that staff members who were leaving the organization and wrote farewell or personal records were less related to those cooperating in the preparation of monthly or annual reports. Also, project proposals written by grant applicants and stored in the shared server were barely related to reports or appropriation requests written by members of the organization. These preliminary results indicated that shifts in functions and consequent relationships between staff members can emerge from electronic texts.The proof of concept also examined the use of cluster analysis to explore the concept of archival bond in natural electronic archives. Analyzing the content of strongly and poorly related records can explain what characterizes relationships between records –provenance, date, type of record, contents, topic – and whether these features can be mapped onto theoretical conceptualizations of archival bond. It will also explain the role of drafts, versions, and non-records by finding the proportion in which they exist in the natural archive and how close or not they are to complete records.Before issuing the final appraisal protocol, changes had to be implemented and concerns addressed. Since the archive contains formats as old as Microsoft Word 5.0 for DOS, a converter with broad file format support was found to transform old files to ANSI text so they can be processed by Rainbow. Because there are various pieces of software involved, processes need to be automated and simplified as much as possible and issues related to the size of the text sets and matrices vis a vis the power of the processing tools have to be considered. Through a research grant from the University of Texas at Austin a programmer was hired to modify existing applications and develop new ones. Rainbow’s tokenizer was modified to recognize Spanish characters and to include the Oleander Spanish stemmer (Oleander Solutions, 2006). The cosine similarity algorithm was coded in C++ so that bigger matrices can be processed efficiently in a UNIX server. To improve the ability to distinguish the characteristics of individual texts, Tf-idf capabilities were added to the script. The program outputs both a matrix of cosine similarities between every other document and a matrix of the averages of cosine similarity distances between every other author in the sample. Current testing involves processing sets with all the texts produced in one year by every author to determine changes in collaboration dynamics. After processing the matrices with UCINET, preliminary results for author’s yearly averages show relationships that concur with their functions in the institution (See Fig. 1 and 2).paper_136_esteva_1.jpgFigure 1. Network drawing of averages of cosine similarities between a set of 544 records of different staff members during the year 1996. The director is at the center of the network which corresponds with his functions in the organization and the data gathered in the interviews. Most of the records produced by the financial manager are non-textual and remained within the database systems which explains the distance between him and the director.paper_136_esteva_2.jpgFigure 2. Network drawing of averages of cosine similarities between 719 records of different staff members during the year 1997. As new staff members added their records in the networked directory and started their own relationships, the majority of the director’s previous relationships and his status in the network remained stable.The use of Text Mining and Social Network Analysis promises to allow archivists to explore and define the meaning of evidence in natural electronic archives. Instead of intuition and art, as appraisal has been characterized, the opportunity exists to use inductive quantitative methods to make of appraisal a research endeavor (Eastwood, 1992). Moreover, the use of these methods opens the doors to the enormous potential of digital tools in the analysis and processing of digital archives.",
        "article_title": "Bits and Pieces of Text: Appraisal of a Natural Electronic Archive",
        "authors": [
            {
                "given": "Maria ",
                "family": "Esteva",
                "affiliation": [
                    "School of InformationUniversity of Texas at Austin"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "archive appraisal",
            "social network analysis",
            "digital archiving",
            "text mining"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "It is well documented that men and women use informal language, such as conversation and correspondence, in rather different ways, reflecting a wide variety of cultural forces and practices (Tannen 1990, Eckert & McConnell-Ginet 2003). Recent work has suggested that gender differences may also be found in more formal, written communications. Koppel, Argamon, et al (2002) have shown that gender of author can be accurately predicted between 70 and 80 percent of cases of published samples from the British National Corpus, using machine learning and text mining techniques. Using simple statistical and collocation techniques, Olsen (2005) has argued that there are distinct gender differences in literary French from the 17th to the early 20th centuries. He (Olsen: 2004) further proposed that the differences between male and female writing during this time does not appear to support a \"two cultures\" model (Liberman 2004), but that it was a more conscious political activity of domain specific writing in which men and women deployed the same language.We are addressing several issues that arise from our previous work on gender and public writing in this study. The first is to apply machine learning and data mining techniques to the same sample used by Olsen to examine the degree to which conclusions found in Koppel, Argamon, et al. can be replicated for larger sample of French literary texts. The methodologies used by Koppel, Argamon, et al. and Olsen may have privileged identification of particular features, with Koppel focusing on more stylistic aspects of writing and Olsen examining significant frequency differences of content words. Use of a common approach will allow us to examine the degree to which gender differences can be compared between cultures and time periods. We also expect use of machine learning techniques will allow us to test Olsen's claim that gender differences in French literature are best explained as examples of what Liberman describes as \"dominance\" theories of gender distinction. If the features that best characterize gender difference are more \"stylistic\", this would suggest that \"difference\" or two culture models are more applicable, but if differences are shown in content words and these words are used in similar ways, we would argue that the \"dominance\" or political theory may be a more effective explanatory model. And finally, we are comparing several distinct statistical techniques, most notably Support Vector Machine (SVM) and information gain models, to see which are most effective at extracting weighted features that can be used to interpret observed differences in male and female writing, comparing these with more traditional examinations of differences in term frequency. Becuase of the need to pick apart the learned model to examine its inner workings, with the model itself as the end product of the learning process, some Digital Humanities applications of machine learning techniques will differ in emphasis from more common data mining tasks, where the end goal is often to maximize performance on the classification of unknown data, and the model may be viewed as a black box.This study is based on the creation of two samples of 300 texts roughly balanced by genre, collection and time period driven by texts by French women writers available to us. For each of the 300 texts by 67 female authors (18.5 million words), we selected the chronologically closest male document by genre and, where available, by collection, leading to a comparison collection of 300 texts by 170 authors (27 million words). As noted in Olsen (2005), the samples are largely drawn from the 18th-early 20th centuries with strongest representation in the 19th century, owing to the predominance of romantic novelists in the available collections of female writers. The sample is also skewed by the presence of many works by particular notable authors, in particular George Sand. The ARTFL project is currently adding a large block of texts to our collection of French Women Writers (FWW) and these will be integrated into the full study, depending on how much is available in time.Each text in the corpus has been prepared by first tokenizing it and running it through TreeTagger to determine lemmas and parts-of-speech for each token. In cases of uncertainty, each possible assignment is assigned a probability value between 0 and 1 by TreeTagger; currently, we only consider the most likely analysis for each token (though we will apply more sophisticated techniques in the sequel). We consider in the study several types of lexical features by which to represent the texts numerically: Word Frequencies (WF), Lemma Frequencies (LF), Part-of-speech Frequencies (PF), Word Bigram Frequencies (WBF), Lemma Bigram Frequencies (LBF), and POS Bigram Frequencies (PBF). For each such feature set, we compute a vector of numeric values for each text, where elements of the vector represent the relative frequencies of various features in the set. For example, a WF vector may contain entries for \"la\", \"l'\", \"femme\", and \"femmes\", while the corresponding LF vector will contain entries just for \"la\" and \"femme\", and the corresponding LBF vector may contain an entry for \"la/femme\". Features that occur less than 10 times in the entire corpus are elided; in some experiments, more stringent criteria for feature inclusion will be used, to see how few features we can get away with.For classification, we are applying the SVM method; SVMs are one of the most successful modern methods for text categorization (Joachims 2002). We are currently working with the Weka (Witten & Frank 2005) implementation, using a linear kernel and the default parameters. (Other options do not appear to improve accuracy by much, so we used the simplest option, which also enables easier interpretation of the results.) As the work progresses, we will be evaluating computational efficiency, and may switch to other, faster, software codes if necessary. The classification model constructed is a \"linear model\", which means that it assigns a single numeric weight to each input feature, positive for one input class (say, \"femme\") and negative for the other (say, \"homme\"). The magnitude of the weight corresponds to the importance or influence of the feature's value for classification; high weights indicate those features with the most effect on the classification of a given text (in a specific model). Once classified by the SVM method, we apply the information gain and other metrics (Forman et al. 2003) to identify those features that are most relevant to the classification task. Tokens and lemmas found in this step will be compared to differential frequency tables already used by Olsen.Preliminary classification results using Weka Sequential Minimal Optimization (SMO) implementation of a support vector classifier with 10 fold cross-validation confirm that author gender in this sample can be detected with surprising reliability. For the entire sample, gender of author was correctly identified 85.9% of the time using word lemmas and 85.7% on tokens, with lower performance for both generic POS (73%) and more specific POS (75%). A second test on paired 100 document male/female collections, to reduce the number of works by Sand and smooth other main sample anomalies, achieved slightly better performance, with correct classification by author gender at 87% for both surface word forms (tokens) and lemmas, with POS again performing less well at 76.5% and POSgroups (abstract POS) at 72%. The accuracy performance of classifications based on tokens and lemmas (86% and 87%) is somewhat higher than the 70-80% performance for a sample modern English documents reported by Koppel et al. (2002) This may be due to our use of all of the words in this experiment, suggesting that men and women tend to write about different things and use somewhat different vocabularies.Table 1 shows the confusion matrix from the Weka SMO classification expressed as the correct classification rate broken down by feature type. In both samples, the highest performing features (tokens and lemmas) are considerably better at identification of male authors than female authors. We have noted this difference in other recent work on modern English data. Surprisingly, the opposite is true (in 3 of 4 instances) for part of speech features, though it is unlikely that the differences are statistically significant.paper_161_olsen_1.jpgTable 1Using a variety of techniques, we can effectively distinguish between male and female authors. Examination of the highest weighted features using the information gain (IG) measure on the 2x92 subset sample reveals clear agreement with previous work. Female authors show a strong preference for writing about women (noted by the pronoun elle), adopt a more personal and reflective frame (je, me), and address (vous). This is consistently shown in both selected tokens and lemmas, and is also indicated in the part of speech analysis by a preference for the use of personal pronouns, indefinite pronouns and possessive pronouns. Examination of tokens assigned a high information gain in the subset selected to correct for sample biases reveals the same female concern for descriptions of internal, subjective and emotive states described by Olsen (2005). These terms include émotions, amitié, chagrin, courage, craint* esprit* desire. generosité, larmes, motifs, peines, penser, plaisirs, réflexions, sensible, sentiment, sentis, soin*, and souffrance. The IG measures are notable for some striking absences from Olsen's examination, including terms like amour* and aim* which were strongly correlated to female discourse, as well as many kinship terms (oncle, maman, enfant, parents, etc.). Terms with high information gain values that are more common in male than female writers are less clearly grouped and are missing the strong preference noted in Olsen for abstractions and numbers. However, when examining the PoS measures, ordinal numbers are preferred by male writers. These variations arise from using the IG measure as opposed to differences in relative frequencies to assess \"importance\" and examination of a small but more random sample, issues which will be explored in the full paper.Text mining techniques can identify gender of author using a variety of models with impressive accuracy. The features ranked most highly in learned classification tend to confirm previous results and point to the possibility of cross-language patterns of gendered language use. The full paper will explore the reasons for the higher accuracy performance in identifying male authors and examine in more detail the male and female author feature sets with specific reference to the differences in ranked features arising from different techniques. Finally, we will address the larger question of whether the clear distinctions of male and female public writing in this sample arise from a \"two cultures\" interpretation or more conscious political stances by a systematic analysis of word collocations around particularly gender specific terms.",
        "article_title": "Discourse, Power and Écriture Féminine: Text Mining Gender Difference in 18th and 19th Century French Literature",
        "authors": [
            {
                "given": "Shlomo ",
                "family": "Argamon",
                "affiliation": [
                    "Linguistic Cognition LabIllinois Insitute of Technology"
                ]
            },
            {
                "given": "Jean-Baptiste ",
                "family": "Goulain",
                "affiliation": [
                    "Linguistic Cognition LabIllinois Insitute of Technology"
                ]
            },
            {
                "given": "Russell ",
                "family": "Horton",
                "affiliation": [
                    "Digital Library Development CenterUniversity of Chicago"
                ]
            },
            {
                "given": "Mark ",
                "family": "Olsen",
                "affiliation": [
                    "ARTFL ProjectUniversity of Chicago"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-05",
        "keywords": [
            "text mining",
            "gender studies",
            "French literature"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "IntroductionThe study on the roles of electronic texts (e-texts) in the research process in the humanities investigates how academics in literary and historical studies work with electronic textual resources and how interaction with electronic texts affects their research processes. It is situated in the context of discussions about the cultural revolution triggered by the development of digital media. The common comparison with the print revolution suggests that textual sources are at the centre of the change, which is particularly significant for scholars in the humanities. Since text is the basic working material for research in the humanities, the provision of text in an adequate electronic form is crucial for qualitatively new applications of computer technology in humanistic disciplines. At the same time, an understanding of scholars’ interactions with e-texts is critical for decisions on how to present existing sources and for any work on further developments.In his seminal book, Radiant textuality, Jerome McGann talks about the ‘material revolution’ in which we reconceive the entity of our cultural archive of materials. Since these repositories provide the basis for all traditional scholarly work, institutional changes have been having a radical effect on the traditional scholarship (McGann 2001). The acceptance of digital resources in the humanities has not been as clear and as decisive as in sciences, but reports from different countries over the last few years point to a positive shift in the quantity and quality of scholars’ engagement with information and communication technologies (Houghton, Steele, and Henty 2003; The British Academy 2005; American Council of Learned Societies 2006)A range of studies explored the information behaviour of the humanities scholar; some were focused on the use of digital resources in the humanities but none dealt primarily with the use of electronic texts. Several studies have been conducted, dealing briefly with the use of electronic texts as part of a broader investigation (Massey-Burzio 1999; Brockman et al. 2001) or surveying the use of a particular electronic resource (Flanders 1998; Porter 1998; Duff and Cherry 2000; Cherry and Duff 2002). The literature suggests that electronic texts, while much appreciated by those who use them, have not become widely accepted, even in disciplines that are heavily based on textual studies (Warwick 1999; Brockman et al. 2001). Studies of citation patterns, such as Graham’s investigation of citations in historians’ professional publications, show that electronic resources do not rate highly in published works (Graham 2000, 2001). However, citation patterns may indicate intellectual exchanges to some extent, but they are not an accurate reflection of the use of electronic resources. Palmer (2005) points out that there are scholars who have begun to create digital resources for themselves, which is one of the indications of how scholars wish to engage new technologies in their research.There is a gap in our knowledge about the roles of electronic texts in the research process so we need to explore various aspects of scholars’ interactions with e-texts and explain how these interactions contribute to the research process.MethodologyThe exploratory study on the roles of electronic text in the research enquiry used qualitative methodology to investigate research projects in which e-texts have been used and the nature of academics’ interactions with these texts. The following research questions guided the development of the study: How do academic researchers in literary and historical studies work with electronic texts?How and for what purposes do researchers interact with electronic texts?How do researchers think and feel about the research context in which they work with e-textsHow are the interactions with e-texts integrated in the research process?What is the contribution of e-texts to the research process?What are the obstacles and aids in engagement with electronic texts?The study has dealt with the use of electronic texts as a resource and tool, as opposed to projects that aim to produce electronic textual editions or enhance electronic texts in any way. The participants saw their work as traditional humanities research or discussed their projects aiming to have traditional outputs. Investigated research projects were in the areas of literary and historical studies, because both fields are known for extensive and sophisticated use of textual resources. Participants from six universities in two Australian cities and one participant from a university in the USA (altogether 16 participants) participated in the study and discussed thirty research projects.The term ‘electronic text’ in this study means any textual material in electronic form, used as a primary source in literary and historical studies. Primary materials are usually poetry, stories, novels, plays, and a variety of historical documents — government, public or private. Digitised archival copies of magazines and newspapers, as well as web-sites and blogs, could be electronic texts as defined here when they are used as primary sources. Electronic texts could be written or spoken (e.g., oral histories), digitised or created electronically, stand-alone documents or part of electronic databases and editions.The study has had two phases. The first phase included in-depth semi-structured interviews, examination of participants’ manuscripts and published works as well as examination of some e-texts they mentioned during interviews. The second phase involved detailed data-gathering from a small group of academics drawn from the participants in the first phase. The grounded theory techniques described by Strauss (1987), Strauss and Corbin (1998a; 1998b) and Glaser (1998) were used for data analysis.Roles of e-textsThis paper presents findings directly related to roles of e-texts in the research process but the understanding of the roles is based on other findings that emerged from the study. Firstly, scholars in the study perceived e-texts as fluid entities, which combine different media and formats in a way that does not match the traditional divisions of library materials. The perceived fluidity of electronic textuality leads to converging and transformed practices of networking and information searching. These practices combine aspects of networking, chaining, browsing and web-surfing in traditional and new ways so that the pattern of a new information behaviour emerges. I called this new practice netchaining (Sukovic 2006).The roles of e-texts are based to a large extent on working with e-texts as fluid entities. Four main roles of e-texts in the research process emerged from the study.1. Support in finding documents and information. Search capabilities combined with the provision of full text documents provide a powerful aid in information discovery. E-texts provide support in information retrieval and discovery of primary materials; they lead to other sources and aid in working with analogue sources; supplement hard copies and contribute to the current awareness. This is the most fundamental role. Not only do information discovery and retrieval provide a basis for all other roles, but the nature of scholars’ interaction with e-texts during the retrieval determines other roles to a large extent (Role 3, for example).2. Aid in managing the research process. Access to e-texts allows scholars to plan visits to remote collections; aids the publication process and provides sources for some research activities (e.g., ordering digital images, confirming publication rights, and exchanging files with collaborators and publishers).3. Aid in investigation of the topic. The multiplicity of sources, formats and textual information that could be quickly brought together is a basis of exploration that allows scholars to see different meanings and aspects of the topic. Exploration of research questions through interactions with e-texts took four main forms: exploration of patterns and connections by searching and comparing diverse bodies of electronic texts;production and/or interrogation of textual databases to explore research questions;exploration of electronically born literature andexploration as part of the academic research to be used in creative.4. Contribution to writing and presenting research results. The use of e-texts improves the speed and accuracy in writing by allowing copying and pasting of passages. Interactions with e-texts and digital media in general promote new, less structured and linear ways of thinking about the topic, which influences the academic writing style. From subtle changes in presenting the argument to more radical combinations of academic and creative writing styles, the participants reflected on different ways in which electronic textuality was influencing their traditional academic writing. Interactions with e-texts also encourage thinking about alternative modes for presenting research findings that do not fit traditional academic genres. E-texts contribute to the final research stages in a complex process of negotiation with the research tradition.These roles serve two main functions: aid in providing basis for research (support roles) oraid in exploring the topic and presenting research findings (substantive roles).E-texts provide basis for research, or play support roles, when they make some aspects of the research process quicker and easier. The speed and convenience, or frustration sometimes associated with working with electronic sources, may influence the process, but they normally do not affect the scholar’s intellectual engagement with the topic in a significant way. The first two roles, Support in finding information and Aid in managing research process, are support roles. The fourth, Contribution to writing and presenting research results, plays a support role when e-texts help in improving the speed and accuracy.E-texts aid in exploring the topic and presenting research findings, or play substantive roles, when they take part in shaping the scholar’s thinking process. Interactivity is an essential element in following hunches, testing hypotheses and making connections in a way that was impossible or impractical without e-texts. The scholar’s thinking about the topic develops in the interplay with the e-text and this experience can influence the presentation of research results. The third role, Aid in investigation of the topic, is a substantive content-oriented role. In Contribution to writing and presenting research results, e-texts have a substantive role when they influence the writing style and presentation of research results.ConclusionUnderstanding of the roles of e-texts in the research process contributes to our understanding of the envisaged scholarly change as well as information needs and behaviour in the humanities. It confirms the recent reports on the change of research practices resulting from the use of ICTs and explores the impacts of interactions with e-texts on the research process. The study can have practical implications for the development of digital collections and software applications, approaches to text encoding and development of training programs and institutional policies.",
        "article_title": "Scholarly (R)evolution: Roles of E-texts in the Research Process in the Humanities",
        "authors": [
            {
                "given": "Suzana ",
                "family": "Sukovic",
                "affiliation": [
                    "University of Technology, Sydney "
                ]
            }
        ],
        "publisher": null,
        "date": "2007-03",
        "keywords": [
            "historical studies",
            "electronic texts",
            "literary studies",
            "information behavior in the humanities"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "In 2002, Computers and the Humanities dedicated an issue to “Image-based Humanities Computing” at a time when “a majority of first generation image-based humanities computing projects have reached at least an initial plateau of completion.” (Kirschenbaum, p. 3) Since that time interest in incorporating primary source images into “text” editions has blossomed, as can be attested by recent threads on the TEI listserv and work on the TEI council to develop recommendations for specific methods for integrating image files – and pointers to areas of image files – into traditional text encoding projects. The number of image-based projects has multiplied in that time as well, although it takes some effort to find who is working on such projects. There is not (yet) a central listing of all image-based TEI projects under development.Practical work has been done on tool development since 2002 as well. As of March 1, 2007 there are no less than six tools of which I am aware that serve to edit or display images within the context of text editing. The majority of these tools are for linking text and image for digital display.Edition Production and Presentation Technology (EPPT), developed by Kevin Kiernan under the aegis of the Electronic Boethius project, Image processing services, developed by Neel Smith and Christopher Blackwell through Harvard's Center for Hellenic Studies, at Digital incunabula: a CHS site devoted to the cultivation of digital arts and letters, Juxta, developed through the NINES project (networked infrastructure for nineteenth-century electronic scholarship), Florian Thienel, “Konzept für einen editionsphilologischen EDV-Arbeitsplatz auf der Basis von XML und verwandten Standards” Diplomarbeit im Fach Informatik, Universität Würzburg Add to these additional tools for the simple annotation of imagesUVic Image Markup Tool (1.3.0.3),  and tools for tagging multimedia.Doug Reside at the Maryland Institute of Technology in the Humanities (MITH) at the University of Maryland is developing still-unnamed tool to tag not only images, but video and audio files as well Undoubtedly, the proliferation of tools focused on image editing and display reflects a growing interest in incorporating images into digital editions.The number of tools available for working with text plus image in digital editing highlights a simple truth: projects and their sources are different, and technologies that will work for one project might be incompatible with another. On the other hand, technologies applicable in simple circumstances might be expanded and combined with other technologies to suit much more complex situations. In this presentation, I will describe the sources of two digital projects with reference to their requirements for becoming viable digital projects. One is quite simple and the other complex, but the same methods inform both projects.MS Cambridge, Pembroke College MS 25, the subject of the Digital Edition of Cambridge, Pembroke College MS 25 , (Pembroke 25 project) directed by Paul Szarmach, Director of the Medieval Academy of America, and Thomas N. Hall at the University of Notre Dame. Pembroke 25 is a collection of Anglo-Latin homilies, copied at the scriptorium at Bury St. Edmunds in the late eleventh or early twelfth century by a scribe – or perhaps two scribes – who used the round English Caroline minuscule common there rather than the more pointed Norman Caroline minuscule that came to prominence in England in the period immediately following the Norman Conquest. Following the disillusionment of Bury St. Edmunds in 1538, Pembroke 25 disappeared for a time, but it was given to Pembroke College, Cambridge, at the end of the sixteenth century, and it still lives in that library today. It has been well maintained, it is not damaged, and the script is clear and easy to decipher.For an edition of this sort, a single text from a single manuscript, the encoding requirements are relatively simple. This manuscript is purely textual, not illustrated or illuminated in any way, but we are noting all abbreviations and distinctive paleographical aspects in the manuscript (including scribal emendations), as well as marginalia. The TEI Header contains some descriptive information, notably a descriptive list of all abbreviation types that are linked to the individual abbreviations throughout the project. We are using the EPPT for the text-image linking in this project, and I will give a brief demonstration of the project as it stands at the time of the conference.The Electronic Aelfric , directed by Aaron Kleist at Biola University and developed by a large group of collaborators, seeks to edit eight Old English homilies by Ælfric of Eynsham, who was arguably the most educated and prolific writer of tenth century England. These homilies cover the period from Easter to Pentecost, and trace their development through six phases of authorial revision and then through nearly 200 years of transmission following Ælfric’s death: twenty-four sets of readings or strands of textual tradition found in twenty-eight manuscripts produced in at least five scriptoria between 990 and 1200.The contrast between the Electronic Aelfric and Pembroke 25 is great: while Pembroke 25 is one manuscript, the Electronic Aelfric draws from twenty-eight manuscripts. Although no single homily out of the eight occurs in more than ten of these manuscripts, it is still a great number of textual variants to deal with. In addition to the text, the project also needs to address the individual manuscripts – six of which are from the infamous Cotton Collection (now housed in the British Library), damaged by fire in 1731. Those manuscripts that are not damaged still have singularities, such as marginalia, that we also wish to encode and link to image. We are using the EPPT for this project, partnered with the TEI Apparatus tags, to bring together the text and images of several different manuscripts. I will show examples of corresponding manuscript pages, as well as sample code illustrating multiple variants partnered with image-text linking.",
        "article_title": "Examples of Images in Text Editing",
        "authors": [
            {
                "given": "Dorothy ",
                "family": "Carr Porter",
                "affiliation": [
                    "University of Kentucky"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "text encoding",
            "TEI",
            "images",
            "medieval manuscripts"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "IntroductionThe Centre for Scholarly Editing and Document Studies (CTB) is preparing a digital edition of De Trein der Traagheid, a novella by the 20th century Flemish author Johan Daisne. The project initially aimed at a print reading edition, involving the constitution of a reading text based on a text-critical analysis of 19 witnesses of the novella's print history. However absent from the original project proposal, the TEI markup scheme was adopted early in the project as the means for digitally representing the edition. Its provisions for marking up textual variation with the so-called parallel-segmentation method informed the construction of a single XML source text containing the transcriptions of all 19 text witnesses under consideration as well as the constituted reading text, records of their mutual textual variation, and editorial annotations. The subsequent development of the electronic edition proved this unitary source text's potential for modelling a microcosm of user-generated editions. This paper will focus on characteristics, difficulties, and theoretical challenges of this particular editorial constellation, as well as the tools developed for probing into it. ModellingRefocusing the goal towards an electronic edition added an experimental dimension to the project. The lack of well-established models for (creating) electronic scholarly editions forced us to conceptualise the boundaries of this particular electronic edition in the course of its development. Initially, traditional notions of scholarly editing (explicitly formalised as 'a print reading edition' in the initial project proposal) provided a good starting point for the development process. At first, development was guided by mimicking the familiar print edition model, aimed at generating a reading text with apparatus variorum from the XML source text. However, this denotative model of the print edition (McCarty, 2004) soon evolved to a guiding principle itself for conceptualising new ideas, an exemplary model for electronic editions. The added potential of an interactive edition framework allowing for user-driven input opened up new ways of exploring possible engagements of the user with the textual tradition. On a theoretical level, this exemplary model for electronic scholarly editions informed some challenging insights and rethinking of the nature of this model's object (the edition). Technology and toolsOn the most basic level, the seminal potential of the XML source text for our edition could be realised through the use of several open source XML-related technologies and tools that are currently being adopted as a standard amalgam for accessing XML resources. Key technologies for deploying XML texts like the Extensible Stylesheet Transformation Language (XSLT) and XML Query language (XQuery) allow for flexible manipulation and retrieval of XML encoded information, commonly achieved through dedicated XSLT and XQuery processors, and native XML databases.The advent of XML publishing environments like the Cocoon web development framework has made it possible to integrate these functionalities in dynamic user interfaces for presenting and querying XML content via easily accessible delivery technologies such as a web browser. This integrative potential stimulated the development of our XML text processing scripts initially developed as a tentative instrument for a specific task, to what we named the 'Morkel system', a generalisable suite of XSLT and XQuery scripts for driving electronic scholarly editions in an open source software environment.Views on textual traditionIn the course of its experimental development the Morkel system became a tool facilitating a multi-faceted user-driven view on the textual tradition captured in the unitary XML source text.The scope of this view can be adjusted from micro- to macro-level. Users can have access to singular texts in the tradition, by requesting specific versions of the text as orientation version which presents itself as a faithful reconstruction of this text version. A broader view on the tradition can be accomplished by selecting a parallel edition, in which different episodes in the textual tradition can be viewed and contrasted, literally next to each other. This parallel presentation mode of different text versions for visual comparison resembles that of the Versioning Machine, developed by the Maryland Institute for Technology in the Humanities. Finally, the entire textual tradition can be taken into account when a variorum edition is selected. In its ability to compare any number of text versions with an orientation version, this variorum edition is similar to the Juxta tool, developed by the Applied Research in Patacriticism group at the University of Virginia. The focal point of this text comparison in the Morkel system is the contextual external apparatus variorum containing only the relevant variants for the selected comparison set and providing a locus for reorienting the edition. This scope on the textual tradition can be further refined on an intra-textual level. Complete text versions can be compared, as well as separate text divisions (one of the 33 chapters or the dedication). Where applicable when comparing different text versions, an entry to a generated apparatus is provided both at chapter level and at paragraph level. Edition formatsOne end of the delivery spectrum features the dynamic XHTML visualisation discussed so far. The versatility of XML equally allows for the generation of a PDF visualisation of the (different) edition(s), closely resembling a traditional print view of the textual tradition. A PDF rendering consists of an orientation version, either as an integral text or as a chapter sample, possibly compared to any number of comparison versions, as reflected in an inline contextualised apparatus variorum. However dynamic this generative edition frame is (Vanhoutte & Van den Branden, forthcoming), its boundaries are still present. To cater for this limitation and to enhance scientific independence, the other extreme of the delivery range is offered as well: the Morkel system equally allows users to generate pure XML renderings of the selected comparison version texts or chapters, containing their parallel-segmented inline record of the textual variation. These source texts can then be used in completely different usage scenarios, perhaps featuring completely different software environments.ChallengesIn short, the Morkel system enables users to generate their own edition(s) along 3 axes (comparison set (19 text witnesses and 1 reading text), textual scope (all or 1 of the 34 text divisions), delivery format (3 possible formats)), combining to 58 different visualisation parameters. This generates the potential for 53.477.376 different views on the text, and problematises some traditional text theoretical concepts, as well as the defining role of tools for the electronic editions they facilitate or constitute. An obvious consequence of this generative edition paradigm (Vanhoutte & Van den Branden, forthcoming) is the promotion of each text witness to a candidate orientation version, instead of the adoption of one text version as a base text for the edition against which all other versions are calibrated. Instead, this calibration itself is made relative by the possibility of restoring each different textual witness as an autonomous landmark in the textual tradition, thence allowing a forward or backward look into the tradition. As a matter of fact, the constituted reading text itself has become integrated as 'just' a (commented) view on the textual tradition, against which all variant versions of the text can be plotted. A dynamic selection of a comparison set not only transforms the apparatus variorum to a dynamic, contextualised rendering of the relevant textual variation, but equally promotes it to a performative instrument for reorienting the edition to another point in the textual history. Due to the dynamic selection of comparison sets, the notion of variable classification becomes relativised. Discerning different types of textual variants becomes irrelevant: a variant can hold as a spelling variant in one comparison set but can change classes and become a semantic variant when compared to another version in the textual tradition. The search capabilities of the Morkel system even extend the view on the textual tradition from text level, by allowing simple search operations inside one text version (intra-textual), to collection level, by allowing complex search operations over different text versions (extra-textual). To conclude, this generative paradigm for electronic scholarly editions seems to articulate the defining role of the specific tools for accessing electronic texts more sharply. On its own, the XML representation of text-critical research is a valuable record of scientific labour, but it is the specific (generative) interface which instantiates it as an editorial microcosm by providing a range of user-driven access methods that enable dynamic exploration of a textual tradition. The characteristics and exact nature of this user-driven scholarly edition or constellation of editions is strongly determined by the boundaries this generative interface provides, allowing for a microscopic, telescopic, stereoscopic, or kaleidoscopic view on the textual tradition.",
        "article_title": "Through the Reading Glass: Generating an Editorial Microcosm Through Experimental Modelling",
        "authors": [
            {
                "given": "Ron ",
                "family": "Van den Branden",
                "affiliation": [
                    "Centre for Scholarly Editing and Document StudiesRoyal Academy of Dutch Language and Literature"
                ]
            },
            {
                "given": "Edward ",
                "family": "Vanhoutte",
                "affiliation": [
                    "Centre for Scholarly Editing and Document StudiesRoyal Academy of Dutch Language and Literature"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-03",
        "keywords": [
            "generating",
            "modeling",
            "scholarly editing",
            "text technology",
            "XML"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Instructional use of interactive three-dimensional computer models is transforming undergraduate education at the University of California, Los Angeles. A surge of faculty interest in virtual environments over the past ten years has resulted in a broad spectrum of projects now making their way into Humanities and Social Sciences classrooms. Art history and architecture students can now interactively explore the digital Roman Forum developed by UCLA’s Experiential Technologies Center (the successor organization to the Cultural Virtual Reality Lab) in place of filmic slides or PowerPoint presentations. Near Eastern Languages and Cultures students can tour through Qumran, the settlement associated with the Dead Sea Scrolls, in a computer reconstruction developed by the Qumran Virtual Reality Project, or compare the first century Herodian Temple Mount with the eighth century Umayyad structures on the site through the real-time visual simulation model developed jointly by the Urban Simulation Team at UCLA and the Israel Antiquities Authority. Archaeology students can experience the ancient Egyptian sites of Karanis and Karnak created with support from UCLA’s Office of Instructional Development and Academic Technology Services. Spanish and Portuguese students can culminate their studies of the pilgrimage route in Spain with a virtual visit to the Romanesque Cathedral of Santiago de Compostela, complete with authentic period music. In American History classrooms, students can experience the wonders of the World’s Columbian Exposition of 1893 by interacting with a model developed by the Urban Simulation Team.The proposed paper will describe the results of over 600 student surveys administered in the past two years by the UCLA Experiential Technologies Center (ETC) staff to solicit reactions to this new form of instructional technology. The survey instruments were completed following regularly scheduled class meetings held either in a technology-enabled classroom or UCLA’s Visualization Portal (a campus facility with a 160 degree spherically wrapped projection screen specifically designed for displaying these virtual environments). In the surveys, Likert-style ratings gauged the students’ overall experience with the computer model, their understanding and interest in the content of the virtual environment, and their reactions to the technology as a learning tool and compared to more traditional types of instructional technologies. Multiple choice and ranking questions explored the students’ interest in using the virtual environments outside of the classroom and the aspects of the environment most important for creating an engaging learning experience. Short answer questions delved into the students’ likes and dislikes, and thoughts on the learning benefits of interactive computer models.The paper will also explore instructor reactions to the classroom use of interactive computer models. Concurrent with the student surveys, ETC staff administered instructional technology questionnaires to twenty five undergraduate instructors from around the country who participated in an NEH Summer Institute focused on “Models of Ancient Rome” and conducted personal interviews with twelve instructors actively using the models in their classrooms to explore faculty reactions to teaching with virtual environments. The results of these surveys identify the perceived challenges and benefits to classroom use of interactive computer environments, general concerns about instructional technology, curricular integration, perceived and experienced pedagogical impacts, and instructor expectations for virtual environments.The paper will conclude with an analysis of how the student and instructor reactions to the UCLA environments are informing ongoing project development, and a discussion of future research regarding digital pedagogy in the Humanities.Associated websites:UCLA Experiential Technologies Center ()UCLA Academic Technology Services ()The Urban Simulation Team at UCLA ()UCLA Office of Instructional Development ()",
        "article_title": "Digital Innovations in Teaching and Learning: Interactive Computer Environments in the Undergraduate Classroom",
        "authors": [
            {
                "given": "Lisa M. ",
                "family": "Snyder",
                "affiliation": [
                    "University of California, Los Angeles"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "virtual reality",
            "simulation",
            "technology",
            "pedagogy"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "With The Making of AmericansGertrude Stein,  The Making of the Americas: Being a History of a Family's Progress, (Normal, IL: Dalkey Archive Press, 1995). First published by Contact Editions, Paris, 1925., Stein’s goal was to record “being” as it is manifested in repetition. Lauded by some critics who thought Stein accomplished what T.S. Eliot demanded of all writers—to make art, literature, and language “new”—she was also criticized by others like Malcolm Cowley who said Stein’s “experiments in grammar” made her novel “one of the hardest books to read from beginning to end that has ever been published.”Malcolm Cowley, \"Gertrude Stein, Writer or Word Scientist\"  The Critical Response to Gertrude Stein, (Westport, CT: Greenwood Press, 2000): 148.  More recent critics have attempted to aid interpretation by charting the correspondence between structures of repetition and the novel’s discussion of identity and representation. Yet, the use of repetition in The Making of Americans is far more complicated than manual practices or traditional word-analysis programs (such as those that make concordances or measure word-frequency occurrence) could indicate. The text’s large size (almost 900 pages and 3183 paragraphs), its particular philosophical trajectory, and its complex patterns of repetition make it a useful case study for analyzing the interplay between the development of text mining tools and the hermeneutics employed in interpreting literary texts in general.  The reason that text mining procedures might be particularly illuminating for Stein’s The Making of Americans is not that these procedures make reading the text easier or because text mining might “solve” a literary conundrum. In fact, such a reductive approach would be very uninteresting to the literary scholar. Many Stein critics argue that the “difficulty” the repetition engenders in The Making of Americans is valuable precisely because it is rooted in indeterminacy: that is, the text’s use of repetition represents a postmodernist project that challenges readerly subjectivity and deconstructs the role language and the process of writing plays in determining meaning. The more “difficult” the text is to read, the more it meets its philosophical goals of making the reader question acts of representation and interpretation in general. Thus the text mining process is useful for literary analysis for three reasons: Text mining may be used to determine relationships (clusters and patterns) in large bodies of data (in this case, the confusing network of Stein’s repetitions ).Text mining can be used to identify relationships within the text that are not predetermined by meaning but are based on structural elements of the content (such as repetitive phrases or words).Text mining also requires “subjective human evaluation” as an essential part of the analytical process.This description of text mining is detailed further in Sholom M. Weiss et al.,  Text Mining: Predictive Methods for Analyzing Unstructured Information, (New York: Springer Science+Business Media, Inc., 2005) Analyzing The Making of Americans has already provided rich opportunities for thinking about both tool development and processes of literary analysis. For example, initial analyses on the text using the Data to Knowledge (D2K) application environment for data miningDeveloped by the Automated Learning Group (ALG) at the National Center for Supercomputing Applications (NCSA), alg.ncsa.uiuc.edu (which was used in the nora projectThe nora project (www.noraproject.org) is a Mellon-funded collaborative (including computing, design, library science, andd English departments at the University of Alberta; University of Illinois, Urbana-Champaign; University of Maryland; University of Nebraska; and the University of Virginia) which is developing text mining and visualization software in order to \"explore significant patterns across large collections of full-text humanities resources.\") has yielded clusters based on the existence of frequent patterns. Instead of using single words for analyzing the text, the features used were phrases (i.e. ngrams). Executing a frequent pattern analysis algorithmJ. Pei, J. Han, R. Mao, \"CLOSET: An Efficient Algorithm for Mining Frequent Closed Itemsets (PDF)\",  Proceedings of the 2000 ACM-SIGMOD International Workshop on Data Mining and Knowledge Discovery (DMKD'00), Dallas, TX, May 2000. produced a list of patterns of the phrases co-occurring frequently in paragraphs. This frequent pattern analysis generated thousands of patterns because slight variations generated a new pattern. Because the number of frequent patterns was so large, another algorithm was applied that clustered these frequent patterns.Xifeng Yan, Hong Cheng, Jiawei Han, and Dong Xin, \"Summarizing Itemset Patterns: A Profile-Based Approach,\"  Proceedings of the 2005 International Conference on Knowledge Discovery and Data Mining (KDD 05), Chicago, IL, August 2005.Although the analytics that were used are sophisticated, the results are not presented in a manner that makes them easy for scholars to understand. For example, one cluster has 85 frequent patterns (see Figure 1). A scholar could piece clustered phrases back together to form a longer repetitive phrase (“part of Gossols where no other rich people were living”), but this is difficult, and, in addition, the results do not indicate where the phrase occurs in the document. The impact of Stein’s repetition, after all, is always contingent on context, on what comes before and after a particular string of letters, on what those strings (or words) mean in the context of a sentence, a paragraph, and the larger narrative or project according both to Stein and to the scholar/user’s theoretical perspective.Thus, this first attempt at producing clusters was only marginally useful for scholarly study because the rules by which the items are clustered are not readily apparent and the results are detached from the context of the text. One method for helping scholars understand the rules is by including the scholar in identifying the features by which the frequent patterns are determined. For instance, she could develop a “productive” question about the text, a question that correlates to specific linguistic and narrative features that can be extracted and eventually mined. For example, a question might be: how do linguistic and syntactic variations reflect, advance, or deter from the progression of the text’s more abstract features such as argument and narrative? Specifically, do changes in repetition correspond to the novel’s evolving theories about identity and representation? And how? Features which might be considered include, but are not limited to, repetition across words and phrases (including “trivial” and “motivated” recurrences);\"Trivial\" and \"motivated\" recurrences are used by Calvin Brown in his book  Repetition in Zola's Novels, (Athens, GA: University of Georgia Press, 1952). Examples of such recurrences include \"tags\" (repeated descriptions such as \"her face was worn, her cheeks were thin,\" \"her worn, thin, lined determined face,\" \"her lined, worn, thin, pale yellow face\"), \"key passages\" (a relatively long repetition of a fundamental idea) and \"hammer word\" (a strong or exclusive obsession with any single idea). grammatical features (such as shifting perspectives, oscillating verb-tenses, or dropped pronoun referents); syntactic features (such as diagrammic versus run-on or fragmented sentences); and narrative features (such as shifts between description or “telling” and narrative or “showing”). More work needs to be done to create the features described above for use in the analytics. This paper will discuss our attempts to augment these analyses.In addition, in order to link these patterns back to the context, it is essential to develop an environment that facilitates a self-reflective, critical evaluation of the scholar’s theoretical perspective and ultimately, her ability to influence how data is culled, to assert what Ben Shneiderman has called “user control.” Thus, the text’s features must be visualized in a meaningful way so the scholar can “see” the text mining results and engage with the techniques to improve the results. For instance, in our first attempt, we created frequent patterns without using stemming. After reviewing the list of results, we realized that stemming was valuable to regulate pluralism, verb tense, etc. Stemming was deployed. Our experience with the norainterface has also demonstrated that providing easy access to context within the text is important to understanding the results of classification algorithms—a task which is relatively easy when the entire text is a short poem (see Figure 2). A scholar who is attempting to study repetition across the 900-page text, however, needs new ways to access the context of each instance of repetition. She needs tools that take her beyond searching tools which list the context of a single line (see Figure 3 for results using “grep” on a file). To this end, we have explored various visualization methods such as a weighted centroid (Figure 4), a scatterplot (Figure 5), a heat map (Figure 6), and a line graph (Figure 7).After our team considered the advantages and disadvantages of each of these tools, Anthony Don led the effort to design FeatureLens (Figure 8), a tool which allows the user to see features derived from text mining results within the context of the original text. This tool will serve as the main focus of this talk.Ben Shneiderman calls the powerful combination of text mining and visualization, “discovery” based because it empowers users “to specify what they are seeking and what they find interesting.”Ben Shneiderman, \"Inventing Discovery Tools: Combining Information Visualization with Data Mining.\"  Information Visualization 1.1 (2002): 11. Gertrude Stein has this to say about the same topic: It is very likely that nearly every one has been very nearly certain that something that is interesting is interesting them . . . The only thing that is different from one time to another is what is seen and what is seen depends upon how everybody is doing everything. This makes the thing we are looking at very different and this makes what those describe it make of it, it makes a composition, it confuses, it shows, it is, it looks, it likes it as it is, and this makes what is seen as it is seen.Gertrude Stein,  Composition as Explanation, (London: The Hogarth Press, 1926).  Text mining allows the literary scholar to “see” the text “differently” by facilitating analytical approaches that chart repetition across thousands of paragraphs and visualizations that empower her to tweak the results, focus her search, and ultimately (re)discover “something that is interesting.” This paper will discuss how this process of analyzing repetition in The Making of Americans has impacted the ways in which nora has combined text mining and visualization applications. paper_176_clement_1.jpgFigure 1: A list of frequent patterns for one cluster.paper_176_clement_2.jpgFigure 2: With the nora interface, users can review the results of the data mining, see which documents contain the feature words returned by the algorithm, and see the location of the words in a selected document. () paper_176_clement_3.jpgFigure 3: Identifying repetition as it emerges in a list of frequent pattern clusters in a simple \"grep\" text file.paper_176_clement_4.jpgFigure 4: Using Text Arc, we are able to see where the word \"Repeating\" is repeated throughout the entire second section of The Making of Americans. The text of the section is represented by the lines in the outer ring. The word \"repeating\" is situated in the circle according to where it appears most often in the outer ring; the green lines represent lines in which the word appears; and the orange lines point to word's occurrence in those lines. () paper_176_clement_5.jpgFigure 5: Common phrases displayed on a scatterplot, with frequencies in section 1 on the X axis and frequency in section 2 on the Y axis. We can see than \"men and women\" and \"I was saying\" is a lot more common than any other phrases, and used equally in both sections. ()paper_176_clement_6.jpgFigure 6: This heat map allows us to compare the frequency of phrases in five sections. Each line is a different phrase. The red lines show when a phrase occurs more than 100 times in a section. ()paper_176_clement_7.jpgFigure 7: This line graph shows frequent phrases compared across five sections. Each line represents a different phrase. Here we can immediately see that \"men and women\" appears almost as frequently in two sections of the text. ()paper_176_clement_8.jpgFigure 8: FeatureLens, created by Anthony Don.",
        "article_title": "‘Something that is interesting is interesting them’: Using Text Mining and Visualizations to Aid Interpreting Repetition in Gertrude Stein’s The Making of Americans",
        "authors": [
            {
                "given": "Tanya ",
                "family": "Clement",
                "affiliation": [
                    "University of Maryland, College Park"
                ]
            },
            {
                "given": "Anthony ",
                "family": "Don",
                "affiliation": [
                    "Human Computer Interaction Lab (HCIL)University of Maryland, College Park"
                ]
            },
            {
                "given": "Catherine ",
                "family": "Plaisant",
                "affiliation": [
                    "Human Computer Interaction Lab (HCIL)University of Maryland, College Park"
                ]
            },
            {
                "given": "Loretta ",
                "family": "Auvil",
                "affiliation": [
                    "National Center for Supercomputing Applications (NCSA)University of Illinois at Urbana-Champaign"
                ]
            },
            {
                "given": "Greg ",
                "family": "Pape",
                "affiliation": [
                    "National Center for Supercomputing Applications (NCSA)University of Illinois at Urbana-Champaign"
                ]
            },
            {
                "given": "Vered ",
                "family": "Goren",
                "affiliation": [
                    "National Center for Supercomputing Applications (NCSA)University of Illinois at Urbana-Champaign"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-05",
        "keywords": [
            "text mining",
            "user needs",
            "visualizations",
            "literary studies",
            "text analysis"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "This paper will focus on the use of technologies traditionally associated with knowledge management and ontological representation to express complex associations between entities in historical texts that have been marked up in XML according to the Text Encoding Initiative guidelines. In particular, we will describe our exploration of the potential use of RDF (Resource Description Framework)/OWL (Web Ontology Language) technologies and will reflect on the role of an ontology in facilitating the interpretation of implicit and hidden associations in the sources, examining its use and limits in a digital humanities project in connection with editing tools and delivery issues. We will demonstrate our findings based on the Henry III Fine Rolls project, where an RDF ontology is being developed to make explicit information about person, place and subject entities marked up as “instances” in the core texts themselves.The Henry III Fine Rolls project and the need for authority listsThe Henry III Fine Rolls is a three-year collaborative project between King's College London and the National Archives (UK) that aims to represent the complexity of a historical object known as the 'fine rolls' which chart offers of money made to King Henry III of England in exchange for a wide range of concessions and favours. A total of 64 rolls containing around 800 parchment membranes, one for almost all of the fifty-six years of Henry III’s reign from 1216-72, survive in the UK National Archives. Each fine roll was compiled in Latin by a handful of scribes and taken as a body of documentary evidence, the rolls are of “prime importance in the study of political, social, and economic history and of government and administration at a local and national level.”Dryburgh, Paul. “Henry III Fine Rolls Project” (paper presented at the Institute of Historical Research, London , February, 9, 2006).The project will cover the first thirty-two years of Henry III’s reign down to 1248 and will result in both print and digital editions of the rolls, using as a model the format of the traditional printed 'calendar' (an English summary of records, plus a set of indices) in connection with the digitised images of the rolls themselves. The digital edition will have a sophisticated search interface and both print and digital editions will provide a series of indices for people, places and subjects that include complex associations between the various entries.The core texts were encoded in XML using the TEI guidelines and include mark-up related to some aspects of the physical structure of the rolls as material artefacts.For a more detailed presentation of the mark-up model see Ciula, Arianna. “Searching the Fine Rolls: A Demonstration of the Electronic Version” (paper presented at the International Medieval Congress, University of Leeds, July 10-13, 2006). Particular attention has been given in the mark-up to the occurrences of names of persons, places and institutions. Furthermore, the project researchers have identified and marked-up relevant subjects in the fine rolls. Therefore, while the general mark-up in TEI XML provides a framework for theoretical analysis and practical encoding of the text of the calendar as it is being edited and summarised in English, the need for additional modelling has emerged so as to: associate textual instances of the same person, subject occurrence or place to their correspondent logical authority —whether that be a person (identified or anonymous), a subject class or a place;express the mutual relationships between the relevant authorities (e.g. individuals, locations, institutions and subjects).Why RDF/OWL?In previous work on projects requiring an expression of the associations between sources in core texts marked up using TEI XML we have taken more traditional approaches using XML structures or relational databases to solve the problem, but have increasingly found these wanting. There has been some research in the TEI community in this area lately, particularly around 'biographical and prosopographical data' See “Biographical and Prosopographical Data”. In Sperberg-McQueen, C. M., and Burnard, Lou, eds. TEI P5 Guidelines for Electronic Text Encoding and Interchange.  (accessed 15 November 2006)., but at the time of writing this was not mature enough for us to make a commitment to using it and in any case our attention was drawn to other standards whose main objectives are closer to what we are trying to achieve.After conducting a comparative evaluation of possible standards to create authority lists structures that included research into RDF/OWL, Topic Maps and MADS (Metadata Authority Description Schema), we opted for RDF/OWL.The main reasons for this choice were the following:OWL is a language developed on top of RDF by W3C to write ontologies. For W3C RDF specifications on RDF see . For an overview and further resources on OWL see RDF/OWL models provide a logical organisation of data together with the possibility of a flexible manipulation of meanings (e.g. rich expression of relationships among objects/entities mentioned in the source text, where an object/entity might be a person, a place or a subject);RDF/OWL decreases ambiguity by identifying unique entities independently from their instances in a decentralised way.While the more practical advantages are that: as a set of standards which are at the heart of the Semantic Web, RDF/OWL is internationally recognised and supported - its models could facilitate the interconnection between humanities computing projects in general and between directly related data in particular;there is a relatively good selection of tools for RDF/OWL compared to the other technologies that we have explored;RDF/OWL can be expressed in XML format, thus allowing the re-purposing of data for web delivery (e.g. creation of indices through the use of XSLT).Ontology: Structure, Tool and DeliveryThe RDF schema and OWL are used to define the knowledge domain around the core materials on the project (the fine rolls) using classes and predicates. Already existing predicates are used as extensively as possible (e.g. CIDOC-CRM, Dublin Core, Friend of a Friend, Simple Knowledge Organization System).The connection between the TEI source files and the ontology play an important role in our model. For a similar approach see Eide, Øyvind and Ore, Christen-Emil. “TEI, CIDOC-CRM and a Possible Interface between the Two” (paper presented at Digital Humanities 2006, Université Paris-Sorbonne, July 5-9, 2006) . Indeed the XML files not only feed the ontology with some data (e.g. variants of a certain person name), but they are themselves part of the ontological model. The fact that they exhibit names and assert facts related to those names can be modelled in the ontology together with the statement that a person name (proper name or in general reference string) is associated to a specific person as an instance of the entity person (e.g. that has a gender, a possible status etc.). Quoting Eide,Eide, Øyvind. “The Exhibition Problem. A Real Life Example with a Suggested Solution” (paper presented at Digital Humanities 2006, Université Paris-Sorbonne, July 5-9, 2006) . we include “explicit statements of the sources of the assertions exhibited in the text”: paper_196_ciula_1.jpgpaper_196_ciula_2.jpgThe concrete connection between the XML files and the ontology is maintained through the use of identifiers, as in the example below:<persName key=\"arsic_robert\"> Robert Arsic </persName> → <frh3:Man rdf:ID=\"arsic_robert\"> … </frh3:Man>Evidently, the value and granularity of the ontology depends on various factors, which include the richness of the source files themselves (e.g. how well do the fine rolls express the reality of the roles/occupations in the thirteenth century?), the specific interests of the researchers (e.g. in the types of fines rather than in the amount of money they keep record of) and the definability of the knowledge domain itself (as shown in the contrast between the identification of individuals and the much more blurred identification of subjects).In addition to expressing complex associations in an abstract intellectual sense, it was also crucial to the project to create a system endowed with sophisticated facilities for editing and final publication. We will describe our experience in creating such tools, in particular taking into account such project requirements as: the facilities for managing/editing data: the addition of new information, new classes/predicates/literal values; the definition of relationships between entries, the possibility to merge instances of classes or subclasses (e.g. while editing we may realise that person A and B are actually the same);the connection and synchronisation between the ontology structure and data on the one hand, and the TEI source files on the other (e.g. links from classes in authority list to actual references to those classes as exhibited in the mark-up);the facility to browse information alphabetically or categorised in some way (e.g. by date, county, role, kind of relation) and to search for particular associations as expressed in the ontology (e.g. all the locations connected to the person called 'Robert Arsic' or all the relatives of Robert Arsic).ConclusionsIn the case of the fine rolls (as is true of many projects involving complex primary sources) it is not enough to mark-up an occurrence of a person name if you wish to create complex indices or to create structured search functions.Moreover, an authority-based approach is essential in order to make the resource interoperable; external authority lists are needed to record information in a systematic, comprehensive and possibly standardised way, so as to: store additional information related to persons, places and subject (exhibited and marked up as corresponding instances in the TEI XML files);make explicit the multiple connections between places, persons and subjects;merge or disambiguate identifications and eventually correct the original mark-up of the rolls.Our paper will describe how, for the Henry III Fine Rolls project, an RDF/OWL ontology was used to model complex associations and how this has assisted the project researchers in the interpretation of the material they are editing and facilitated the production of a digital resource that will allow future users to browse the material under different perspectives, to explore the relationships among mentioned individuals, locations and subjects, and to foster new understandings.",
        "article_title": "Expressing Complex Associations in Medieval Historical Documents: The Henry III Fine Rolls Project",
        "authors": [
            {
                "given": "Arianna ",
                "family": "Ciuala",
                "affiliation": [
                    "Centre for Humanities ComputingKing's College London"
                ]
            },
            {
                "given": "Paul ",
                "family": "Spence",
                "affiliation": [
                    "Centre for Humanities ComputingKing's College London"
                ]
            },
            {
                "given": "José Miguel ",
                "family": "Vieira",
                "affiliation": [
                    "Centre for Humanities ComputingKing's College London"
                ]
            },
            {
                "given": "Gautier ",
                "family": "Poupeau",
                "affiliation": [
                    "École Nationale des ChartesUniversité Paris-Sorbonne"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "authorities",
            "text encoding",
            "historical texts",
            "ontological representation"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Libraries, archives, museums, and other cultural heritage organizations are creating digital representations of their artifacts and collections at an astounding rate (Institute of Museum and Library Services, 2005). The potential of increased access to digital collections has tremendous implications for digital humanities researchers, and many recent projects have focused on the technical mechanics of digitizing cultural heritage resources. While these considerations are extremely worthwhile, there is an equally important need to explore how people use these digital resources once they become available (Cameron, 2003; Knell, 2003).This paper explores the role digital museum resources play in the lives of the users of those resources. It presents results from an international survey (administered to nearly 1500 visitors at nine different online museums) that addressed the relationship between digital and physical museum resources in the lives of museum visitors. The survey questions focused on how museum professionals can encourage their visitors to form lifelong relationships with museums: visiting in person when they can, and visiting online when they cannot. The results of the survey help museum researchers and professionals understand the complementary, cyclical relationship that exists between digital and physical museums from a user-centered perspective.A growing number of researchers are interested in studying how the many different users of museum resources (museum visitors and professionals, information providers and consumers) employ digital museum resources in their everyday lives (White, 2004). Museum professionals today work in a changing environment where information resources are becoming more technically-complex, and where the users of those resources are becoming more information-savvy. Over the past decade, the needs and expectations of museum visitors have become increasingly sophisticated, and museum professionals are increasingly concerned with ensuring that the right information resources are available to all users, inside and outside the museum (Marty, Rayward &Twidale, 2003).To meet the changing needs of their visitors, museum professionals have dramatically changed the way museum visitors interact with museum resources. Increased access to digital collections has removed many traditional barriers between museums and their visitors, offering new opportunities for interacting with collections and information resources. Douma and Henchman (2000), for example, discuss an online exhibit that allows visitors to digitally “strip away” layers of a painting (Bellini’s Feast of the Gods), examining earlier versions using simulated infrared or x-ray lenses. Gillard (2002) explores how the National Museum of American History’s HistoryWired project encourages visitors to manipulate a collection of artifacts, uncovering connections between objects along temporal, cultural, and thematic lines.These changes have led museum professionals to express concerns about the impact new information technologies have on the relationship between museums and their visitors. Some worry whether providing online access to digital museum resources will result in a decrease in visits to physical museums and a corresponding loss of financial revenue (Haley-Goldman & Wadman, 2002). In the process of providing access to digital museum resources, someone usually asks, “if visitors can access these resources over the Internet, will they still come to the museum?” The simple answer to this question is “yes, they will,” and a number of recent surveys have shown that online visitors are also physical visitors (Kravchyna & Hastings, 2002). This makes sense; nobody asks, “if people can look at pictures of beaches online, will they still vacation in Florida?” Similarly, the ability to access digital museum resources online should in theory serve as a lure, encouraging potential visitors to come to the physical museum.Despite this commonsense approach, worries about the relationship between physical and digital museum resources persist. Why? The truth is that these worries have far less to do with financial remuneration than with understanding how the users of digital museum resources perceive the integration of those resources into the sociocultural fabric of their everyday lives. To understand the relationship between digital and physical museums, one must ask, “what role do digital museum resources play in the life of the user of digital museums?” Framing the question in this manner reflects a shift in perspective away from the “user in the life of the museum” to the “museum in the life of the user”—a shift that parallels one that has taken place in the library and information science community over the past few decades.Very little is known about why users seek digital museum resources or how users integrate digital museum resources into their everyday lives, despite valuable studies that have explored what visitors do at museum websites (Thomas & Carey, 2005). Understanding the relationship between digital museums and their users becomes critically important as more museums offer digital resources online, and the number of visitors to online museums increases to be five to ten times the number of visito.rs to physical museums. If one wants to encourage a situation where visitors make museums part of their everyday lives and feel connected to museums whether they are physically there or not, there is a need to explore the digital museum in the life of the user.To meet this need, this study explored the following research questions: What is the role of the digital museum in the life of the museum visitor?How can museums use museum websites to build stronger relationships with their visitors, before and after a museum visit?What needs do digital museum resources meet for museum users outside the museum?When and how do people use museum websites before and after visiting museums?What do visitors prefer to do on museum websites vs. in the museum, and vice versa?How do museum websites influence the visitors’ desire to visit the museum?To answer these questions, the researcher developed an online survey that asked online museum visitors about their use of digital museum resources before and after museum visits, and how they integrate digital and physical museum resources in their everyday lives. The survey was advertised on the websites of nine different museums, including the Fine Arts Museums of San Francisco, the Science Museum of Minnesota, the Australia War Memorial Museum, the Victoria and Albert Museum, and the National Museum of Wildlife Art. From October 2005 to October 2006, 1464 online visitors responded to the survey.The survey results provide valuable insights into the behavior of visitors to online museums around the world. Certain key findings from the study are summarized in the following list: Online museum visitors consider it very important for museums to have a website.They are very likely to visit a museum’s website before visiting the museum.They are likely to use a museum’s website to determine whether they want to visit the museum.They have strong preferences for what they want to do in the museum vs. what they want to do using the museum’s website.They are likely to visit the museum’s website after visiting a museum.After leaving a museum, they expect to be able to find the museum’s website easily, and they rely on the museum’s website to answer questions about the museum.They are likely to establish a relationship where they visit a museum and its website repeatedly, visiting the museum when they can, and its website when they cannot.They are likely to visit museum websites in their daily life, independent of planning or returning from a museum visit.These results indicate that online museum visitors view museums and museum websites as complementary, and that digital museum resources are not likely to replace physical museum resources in the lives of museum visitors. The users of digital museums are constructing, mostly on their own initiative, a complicated relationship between digital and physical museum resources in their own lives. By focusing solely on the user in the life of the museum, one sees only how visitors use resources there, not how they make use of all museum resources, digital and physical, in the museum and online, in their own lives. Studies of the digital museum in the life of the user, therefore, are more likely to paint an actual picture of the complex interactions the users of museum resources experience at the boundary of physical and digital museums.This study explores the broader implications of access to digital culture by addressing the relationship between physical museum objects and digital information resources. Much of the world’s cultural resources are located in small museums, historical societies, and community heritage associations—organizations that are just now digitizing their collections. Without a solid understanding of how people use digital museum resources in their everyday lives, what good does it do those institutions to create those same resources? If one is to create a sustainable future for the digital humanities, one must improve the overall understanding of how individuals use digital information resources in ways that augment their appreciation and understanding of physical artifacts and cultural heritage worldwide.",
        "article_title": "The Digital Museum in the Life of the User",
        "authors": [
            {
                "given": "Paul F. ",
                "family": "Marty",
                "affiliation": [
                    "College of Information Florida State University"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-03",
        "keywords": [
            "digital museum resources",
            "user studies",
            "museum informatics"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "At the 2005 meeting in Victoria, I presented a paper with the revised title, “A Macro-Economic Model for Literary Research.” That paper marked an early phase in a project aimed at leveraging large, digital, text corpora for what I now call “Macro-Analysis.” The work presented in Victoria was largely a proof-of-concept using an electronic bibliography of 775 works of Irish-American literature. For that paper I performed a dumbed down sort of macro-analytic text analysis using metadata fields and the titles of works in the bibliography. Today the software has improved significantly, and in place of the title analysis of 2005, this paper offers results derived from a macro-analysis of the full text of 1125 British and American novels from the 19th century. In the presentation, I provide a general overview of the tool(s) and an explanation of the methodology employed in the analysis.The tools and techniques I have develop utilize both supervised and unsupervised text-mining techniques. The supervised techniques allow for a focused analysis in which a researcher probes the corpus for items meeting a specific research criteria. A very simple example might involve tracing the behavior or “frequency” of some “signal” (linguistic pattern, literary theme, or author style) over the course of the corpus. I should note here that while this process sounds similar in some ways to the supervised machine learning approach being used by the NORA project, it is specifically not like NORA in that I am not employing machine learning or utilizing previous identified, “marked,” training data. Instead the “signal” is developed ad hoc by the researcher/user.An easy way to understand the project is to visualize the tool being used: A user is given an interface that allows for the usual sort of corpus searching. The user performs a corpus wide search for some term (or other feature such as a word cluster or syntactical pattern). The result page reports all of the texts in the corpus in which the search term(s) is found, and then the user is given a “toolbox” of macro-analytic tools with which to process and analyze the result set. These tools are varied and perform diverse sorts of analysis.A topic-modeling tool, for example, provides the ability to harvest the salient themes from the text in the result set. The user is thus able to say, for example, that works in the corpus that contain word “x” show a predominance of the \"n\" topic. In my own work with ethnic American literature, I have found this tool valuable in assessing and quantifying the dominant themes that occur in works where ethnic markers (words denoting race or ethnicity) occur. This technique is derived from the work of David Newman and his research team at the University of California—Irvine.Newman’s work is profiled on line at the following sites: Another tool offers a type of literary time series analysis. Figure 1 shows a graph produced by my “timeline” tool.paper_147_jockers_1.jpgFigure 1With the timeline tool, the results of any query of the corpus can be mapped over time. Say, for example, that I am interested in how some textual feature evolves over time. I perform the necessary query to isolate the occurrences of that feature and then choose the timeline tool. The resulting graph provides a visual time-series analysis of the frequency of the pattern. The graph shown here was produced after a simple search for occurrences of the word “romance” in the titles of 7300 novels from the 18th and 19th century. The raw counts are displayed in red beneath each year. In addition to providing this timeline of raw hits (figure 1), a second graph (figure 2) is also produced that shows a normalized result in the form of “hits-per-100” texts.paper_147_jockers_2.jpgFigure 2In this case, the normalized graph is particularly revealing because it shows that the frequency of “romance” as a key word in titles is not especially noteworthy. Aside from a very brief period (1800-1810) where the word appears in 2-3% of all titles, its use is steady at 1% or 1 occurrence per 100 titles in a given year.The macro-analytic tools developed in this research exist as both command line applications and as a (beta) extension to the open-source eXtensible Text Framework (XTF) application developed by Martin Haye and the California Digital Library.See  The successful implementation of the tools into XTF has been achieved with the assistance of Stanford Undergraduate digital humanities major Jenny Loomis who will spend the final five minutes of this presentation giving a live demonstration of the tool as implemented into the XTF application. ",
        "article_title": "Macro Analysis (2.0)",
        "authors": [
            {
                "given": "Matthew ",
                "family": "Jockers",
                "affiliation": [
                    "Stanford"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-05",
        "keywords": [
            "literary history",
            "text analysis",
            "corpus",
            "tools"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "e-Science technologies have the potential to enable large-scale datasets to be searched analysed, and shared quickly, efficiently, and in complex and novel ways. So far, little application has been made of the processing power of grid technologies to humanities data, due to lack of available large scale datasets which would warrant such high performance computing, and little understanding of or access to e-Science technologies. The ReACH workshop series, funded by the UK’s Arts and Humanities Research Council, was established in June 2006 at University College London to investigate the potential application of e-Science and high performance computing technologies to a large dataset of interest to historians, humanists, digital consumers, and the general public: historical census records.The ReACH series consisted of various workshops undertaken over the summer of 2006 to investigate the academic, technical, and managerial aspects that would have to be taken into account in order to set up a large scale project which would utilise UCL’s high performance computing facilities to analyse large scale historical census datasets from the UK’s National Archives, in conjunction with the genealogy firm, Ancestry. By undertaking a scoping study in this manner, it was hoped to determine the academic merits of such a proposal: it may be feasible to undertake this analysis, but would it be useful to historical researchers? What would the analysis do? What would the technical implementation of such a project involve? What staffing and funding costs would be required? The workshop series featured input from various project partners, and interdisciplinary experts, to ascertain whether a full scale project would be worthwhile to undertake. Moreover, the workshop series aimed to ascertain if and how e-Science (defined as “a specific set of advanced technologies for Internet resource-sharing and collaboration: so-called grid technologies, and technologies integrated with them, for instance for authentication, data-mining and visualization. (AHRC ICT 2006)”) can be applied to the arts and humanities.Public interest in historical census data is phenomenal, as the overwhelming response to mounting the 1901 census online at The National Archives demonstrates (Inman, 2002). Yet the data is also much used for research by historians (see Higgs 2005 for an introduction). There are many versions of historical census datasets available, covering a variety of aspect of the census, and digitised census records are one of the largest digital datasets available in arts and humanities research. In the Arts and Humanities Data Service repository collection alone there are currently 155 datasets pertaining to historical census data (from the UK and abroad) created for research purposes (AHDS 2006). Commercial firms dealing (or having dealt) in genealogy information (such as Ancestry, Genes Re-united, QinetiQ , British Origins, The Genealogist, and 1837Online ) have digitised vast swathes of historical census material (although to varying degrees of completeness and accuracy). There is much interest from the historical community in using this emerging data for research, and developing tools and computational architectures which can aid historians in analysing this complex data (see Crocket, Jones and Schürer (2006) for an advanced proposal regarding the creation of a longitudinal database of English individuals and households from 1851 to 1901, see also the work of the North Atlantic Population Project). However, there have been few opportunities for the application of high performance computing to utilise large scale processing power in the analysis of historical census material, especially analysing data across the spectrum of census years available in the UK (7 different censuses taken at 10 year intervals from 1841-1901). Although certain digitized datasets of the UK census are in the public domain (1881The 1881 Census for England and Wales, the Channel Islands and the Isle of Man (Enhanced Version) was deposited in the Arts and Humanities Data Service repository by K. Schürer (University of Essex. Department of History) in 2000, and is available from ) most were digitized by commercial companies and are unavailable to the academic researcher. Most historians do not have access to, or do not know how to use, high performance computing facilities. The aim of the ReACH series was to bring together disparate expertise in Computing Science, Archives, Genealogy, History, and Humanities Computing, to discuss how e-science scale techniques could be applied to be of use in the historical research community. The project partners each brought various expertise and input to the project: UCL School of Library, Archives and Information Studies, who have expertise in digital humanities and advanced computational techniques, as well as digital records management,The National Archives, who select, preserve and provide access to, and advice on, historical records, e.g. the censuses of England and Wales 1841-1901 (and also the Isle of Man, Channel Islands and Royal Navy censuses)Ancestry.co.uk, who own a massive dataset of census holdings worldwide, and who have digitized the censuses of England and Wales under license from The National Archives. The input of Ancestry was central to this research to gain access to the complete range of UK census years in digital format. UCL Research Computing, the UK's Centre for Excellence in networked computing, who have extensive high performance computing facilities available for use in research. The project aimed to investigate the reuse of pre-digitised census data: presuming there was not funding available to be in the business of digitisation of other record data for any pilot project. The project also wished to investigate the use of commercial datasets (as many of the large census data sets are owned by commercial firms: in this case, Ancestry), and the licensing and managerial issues this would raise for future projects. The project also wanted to establish how feasible, and indeed useful, undertaking such an analysis of historical census data would be.The results of the well attended workshop series was a sketch for a potential project, and recommendations regarding the implementation of e-science (high performance computing) technologies in this area. However, at this time, it was not thought possible to pursue the potential project at this time in the following e-Science call which emanated from the AHRC in October 2006 due to a variety of reasons which are elucidated in this paper. Reasons for not taking the project forward at this time were not technical or managerial, but historical: it will be a few years before all the digitized data required to make this project a success will be available (or be of high enough quality, see Holmes 2006). Nevertheless, the scoping nature of this project did highlight interesting aspects of the application of high performance computing to humanities data: discussing the nature, size and quality of humanities datasets (as opposed to scientific datasets), and managerial and technical expertise in data management, security, and licensing. Importantly, the nature of working with a commercial company on their sensitive data was also explored from a legal aspect, highlighting issues regarding use and reuse of digital data for the arts and humanities: who “owns” resulting datasets from collaborative projects?This paper describes the methodology of the workshops, reporting on suggestions made during the series regarding potential applications of high performance computing which would benefit academic historians, sketching out a future project regarding how historical census material can be analysed utilising high performance computing, and extrapolates recommendations that can be applied in general to the use of e-Science and high performance computing in the arts and humanities research sectors.",
        "article_title": "Researching e-Science Analysis of Census Holdings: The ReACH project",
        "authors": [
            {
                "given": "Melissa ",
                "family": "Terras",
                "affiliation": [
                    "School of Library, Archive and Information StudiesUniversity College London"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "e-science",
            "census",
            "high performance computing",
            "history",
            "database analysis"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "1. IntroductionCan computational analysis better reveal how Shakespeare's words and phrases construct characters clearly gendered as male and female? What happens when stylistic analysis is brought to bear on a longstanding notion in literary and cultural studies that gender identity is a discursive (i.e. a culturally-decodable lexical and semantic) construction? How helpful can linguistic style be in explaining aspects of literary style?Last year we presented our first results (Hota et al. 2006) in analyzing lexical items of character gender in Shakespeare. Our observations on gender character were in line with previous work (Argamon et al. 2003) on discriminating author gender in modern texts, supporting the idea that Shakespeare projects character gender in a manner consistent with patterns of authorial gender projection found in other texts both literary and nonliterary.In this abstract, we extend and refine our methods by focusing on lexical and semantic language use by Shakespeare for determining the gender of his literary characters. Here we used a better version of Shakespeare corpus, the Nameless Shakespeare. In the Nameless Shakespeare, each lexical item is fully tagged with lemma entries. Our accuracy has improved to 81% using lemma features, compared to last year’s results. We observe that lemmas and tri-grams help identify a Shakespearean character’s gender. In addition, discrimination models using lemmas and tri-grams may allow literary and cultural scholars to see discursive patterns that, while impossible to find noncomputationally, nonetheless portray or construct a character’s attitudes as male or femaleThe fact that these patterns hold across literary and nonliterary texts, and from early-modern to modern English, supports their possible significance in understanding discursively-formed gender identity. We further observe other distinguishing features, including the fact that some feature constellations match well to previous reports of features that distinguish male from female authors (Argamon et al. 2003). We have analyzed the concordance lines of lexical and lemma tri gram occurrences from the corpus and found patterns of phrasal usage that indicate significant gender differences in language use in the plays.We are interested in understanding gender characterization based on major characters and minor characters, and in understanding how prose and verse forms impact gendered speech in Shakespeare. We are also interested in how words highly-associated with a particular gender may also help with plotting, dramatic tension, and closure. We will present our findings at the conference.2. MethodologyWe applied text classification methods using machine learning with feature sets described above, under the umbrella of a well-tagged corpus. If reasonable classification accuracy is achieved with these new sets of features, it will show that Shakespeare used words differently for his male and for his female characters. If this is the case, then examining the most discriminating features should give some insight into how gender stylistics flows into socio-linguistic and literary gender construction.2.1 Corpus ConstructionWe constructed a corpus of characters' speeches from 35 Shakespearean plays, collected from the lexically-and-lemmatically-tagged Nameless Shakespeare. The plays are in XML (Extensible Markup Language) format. To import them into our system, we extracted the speeches and the gender of each character automatically, cleaning the stage directions. A text file for each character in each play was constructed by concatenating all of that character’s speeches in the play. We only considered characters with 200 or more words. From that collection, all female characters were chosen. Then we took the same number of male characters as female characters from a play, restricted to those not longer than the longest female character from that particular play. In this way, we balanced the corpus for gender, giving a total of 101 female characters and 101 male characters, with equal numbers of males and females from each play (see Table 1). We balance the corpus to avoid bias in the automated learning procedure; this introduces other issues which we address below.paper_193_argamon_1.jpg2.2 Feature ExtractionWe processed the text using the ATMan system, a text processing system in Java that we have developed. The text is tokenized and the system produces a sequence of tokens. Each token corresponds to a word in the input text file. We used lexical and lemma features with n = 1, 2 and 3 gram combinations. In order to understand gendered language more deeply, we extracted those n-grams most linked to gender (Tables 2, 3). We collected most frequent 500 words, bigrams (2670) and trigrams (356) from the lexical entries. In the same way 2001 unigrams, 2860 bigrams, and 571 trigrams from lemmas were collected. We calculated the frequencies of these various features and computed their relative frequencies. The list of various feature sets with their counts is given Tables 3-5.paper_193_argamon_2.jpgpaper_193_argamon_3.jpg2.3 Text ClassificationThe classification learning phase of this task is carried out by Weka's (Frank & Witten 1999) implementation of Sequential Minimal Optimization (Platt 1998) (SMO) using a linear kernel and default parameters. The output of SMO is a model linearly weighting the various features. Testing was done via 10 fold cross validation. With this methodology, we ensure that each character is tested on at least once with training that does not include it. Table 4 presents the results obtained by running various experiments.paper_193_argamon_4.jpg3 Results: Accuracy and Feature AnalysisMany feature combinations give classification accuracies near or above 70%, which is quite good (random would be 50%, since the corpus is balanced). The highest accuracy of all (80.69%) was attained using the 500 most frequent word lemmas as features. Lexical (surface tokens) also worked well, with unigrams plus bi grams plus tri grams combination as a whole giving the highest accuracy (75.74%). The accuracy is captured in Table 4.The feature analysis phase is carried out by taking the results obtained from Weka’s implementation of SMO. SMO provides weights to the features corresponding to both class labels. To discriminate binary class labels, SMO uses positive and negative weight values in a linear model. After sorting the features based on their weights, we collected the top ten features indicative of each gender. We have also computed the average value of each feature for each gender (Tables 5-10). For reasons of space we consider here just those feature sets giving the most insight (as well as good classification accuracy). We also ranked features using information gain (IG), defined as the expected reduction in entropy caused by partitioning the training set according to the attribute.Lemma Unigrams:In Shakespeare, several meaningful clusters of words emerge. Female lemmas indicate family relationships (‘husband’, ‘mother’, ‘court’) , feelings (‘sick’, ‘merry’), emotional injections (‘alas’, ‘o’, ‘prithee’), and integration of personal context (‘he’, ‘you’). Male features indicate concern with quantification (‘three’) and social status (‘noble’, ‘solemn’, ‘savage’). Male lemmas also include some less-clearly interpretable verb forms (‘begin’, ‘alight’, ‘beat’). Lemma Trigrams: More specific meaning patterns can be seen in lemma triples. Female trigrams mostly indicate construal of self and others (‘I/see/you’, ‘for/I/to’, ‘I/know/I’, ‘be/he/not’, ‘say/I/be’), politeness (‘thank/you/for’), conditionals (‘if/he/have’), and questions (‘who/be/that’). Male trigrams focus on assertions (‘I/say/to’) mainly about personal/social status (‘but/I/be’, ‘be/a/very’, ‘be/a/ass’, ‘I/be/ he’), possessions (‘have/no/more’, ‘I/have/lose’), and manner (‘the/manner/of’). Analysis of Concordance Lines: For both lexical and lemmatic trigrams, we contextualize usage by examining concordance lines. For males, 'the name of'’ is followed usually by 'truth', 'hero', 'love', 'justice', 'whore', while for females, this trigram is followed by 'wife', 'jesting' and sometimes with the name of a female character. Men use this to invoke overarching abstractions (or to insult women), while women talk about “names of” in a less metaphorical and more neutral fashion. For 'do you know', males tend to follow with another question, but not females. Strikingly, males use 'the manner of,' to set up dramatic contrasts or even shifts in dramatic action, but female use of this trigram is entirely unremarkable in this way. paper_193_argamon_5.jpgpaper_193_argamon_6.jpgpaper_193_argamon_7.jpgpaper_193_argamon_8.jpg4 DiscussionThese findings capture word patterning in Shakespeare inaccessible to non-computational methods of literary analysis, because of the scale of data processing involved. Literary scholars work almost exclusively with well-elaborated methods of semantic analysis (New Criticism, structuralism, and post-structuralism), developed with all the strengths and limitations posed by a book-only, eye-centered, subjectivity-dependent research context. In contrast, these findings encourage comparisons between non-computational and computational approaches. It is remarkable that these findings support aspects of non-computational methodology (words linked together in meaningful patterns like informational discourse/male and involved discourse/female), while also bringing to light new structural features of Shakespeare’s discursive gender construction through language: parts of speech use, tri-gram combinations of words. These findings may, in addition, capture creative and literary patterning in greater detail than is possible with noncomputational literary methods alone. Since Shakespeare’s plays depend greatly on gender-identified characters, the words linked to gender most likely also serve literary purposes. In the plays, heterosexual romance is linked not only to characterization, but also to action, and in the case of romantic comedies, to how the plays conclude. In the cases of complex characterization, a character’s misuse of gendered speech may also be central to how Shakepeare develops dramatic action.Limitations:The editorial procedures for The Nameless Shakespeare are sound and practical for the project’s purposes and for the work of this paper, but they need to be read and understood fully: both by literary scholars wanting to apply these findings to particular words in particular plays, and by computational scholars thinking through the problem of establishing textual accuracy prior to inviting a wider community to conduct searches. Also, with respect to Shakespeare’s literary art, the findings here do not at this stage account for the impact of blank verse dialogue (for high or elite characters) versus prose dialogue (for low or common-born characters) on word choices and the numbers of words. It may be that blank verse fosters semantically significant tri-gram constructions because Shakespeare needed short words to complete plays primarily written in ten-syllable lines. But at least the question can be asked, and the answer will tell us something about Shakespeare both as dramatist and as poet. In addition, our work focuses on the heternormativity clearly present in Shakespeare’s plays, but does not exclude nonheteronormative gender construction. Finally and significantly, the gender-balancing used here had the odd result of excluding from the corpus all major male characters in Shakespeare, including every male character named in a play’s title, because all these characters speak more than 600 lines. All major female characters (including females named in titles) are included. We now know that very-long speech length per play efficiently identify characters as male, but we also will test our findings on the males excluded so far and report the results at the conference.5 ConclusionsThis is the first work, to our knowledge, in analyzing various textual features (lexical and lemma) collected from a single source in understanding literary character gender. We see, as in our earlier work (Hota et al. 2006) that the male and female language in Shakespeare’s characters is similar to that found in modern texts by male and female authors (Argamon et.al 2003). Here we also observed the importance of trigrams for lexical and lemma features. Trigrams are few in number, so they are information rich and computationally efficient for identifying gender. The true import of the features identified by this analysis need to be confirmed by more traditional digital humanities methods such as examining concordance lines, to allow a more properly contextual interpretation. In any case, we believe that this study shows how classification learning can be used as a tool in developing new ‘statistical’ interpretative methodologies for bodies of literary works.Acknowledgements:Many thanks to Dr. Martin Mueller for providing us the Nameless Shakespeare corpus and many helpful comments in gender characterization in Shakespeare.",
        "article_title": "Understanding the Linguistic Construction of Gender in Shakespeare via Text Mining",
        "authors": [
            {
                "given": "Sobhan Raj ",
                "family": "Hota",
                "affiliation": [
                    "Linguistic Cognition LabIllinois Institute of Technology "
                ]
            },
            {
                "given": "Shlomo ",
                "family": "Argamon",
                "affiliation": [
                    "Linguistic Cognition LabIllinois Institute of Technology"
                ]
            },
            {
                "given": "Rebecca ",
                "family": "Chung",
                "affiliation": [
                    "Lewis Department of Humanities Illinois Institute of Technology "
                ]
            }
        ],
        "publisher": null,
        "date": "2007-05",
        "keywords": [
            "text mining",
            "gender",
            "computational stylistics",
            "Shakespeare"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The National Panel Report released by the Association of American Colleges and Universities (2002), calls for a “dramatic reorganization of undergraduate education” to address the challenges faced by higher education in a time of transformation from an industrial to a knowledge-based society (vii). The report states that “education practices invented when education served only the few are increasingly disconnected from the needs of contemporary students”(viii) and the demands of citizens of a diverse and interconnected world.The Report recommends an invigorated and practical liberal education offering knowledge that all students, regardless of backgrounds, fields, or chosen higher education institutions, should acquire. The college student of the twenty-first century needs to become an “intentional learner” who can thrive in a complex world, and who can adapt to new environments, integrate knowledge from different sources, and transform information into knowledge and knowledge into judgment and action (xi.). The Report urges an “end to the traditional, artificial distinctions between liberal and practical education” and advocates a kind of instruction and learning that looks beyond the classroom to the world’s major questions” (xii).The changing nature of colleges and universities and the reconstruction of education it calls for is in great part conditioned by what Douglas Kellner calls the “Great Transformation,” powered by one of the most dramatic technological revolutions in history (2003, 51). The revisioning of higher education is increasingly seen in correlation with the development of digital technologies, which are changing not only traditional models of work, leisure and communication, but also the nature of knowledge itself, as well as models of acquiring and processing information cognitively. In the Report it is asserted that the “intellectual and practical skills that students need are extensive, sophisticated and expanding with the explosion of new technologies” (xi). Like the AACU Report, Kellner too argues that traditional and specialized aspects of education need to be overcome in order to develop alternative pedagogies and multiple “literacies” to meet the challenges of an interconnected global society. The study of interrelationships, connectivity, transfer, and integration, leading to the development of critical judgment, is proposed as the basis of the new liberal curriculum in all disciplines of the humanities.The five key concepts for the new curriculum are in fact not so much concepts as they are markers of cognitive processes. As such, the reform of higher education is to take place primarily in terms of methodology of teaching. Given that “hypertext is a mental process, as well as a digital tool” (Gilster, 137), and the WWW is “an embodiment of human knowledge”(W3C), exploring the relationships between cognition and technology in the context of the new humanities and pedagogy can be useful. I would like to suggest that the digital media offer a useful concrete, but also a cognitive tool for teaching the five processes that are to be the core of the new humanities. This claim has theoretical and practical implications. Theoretically, it calls for becoming more aware how the computer is altering our ways of engaging with specific disciplinary questions cognitively and methodologically (McCarty, 1). My concerns in this paper, however, have to do with the practical applications of the impact of digital media on cognition within the area of the humanities. Practically, the claim calls for explicit instruction within the context of each discipline in the methods of organization and manipulability that underlie the presentation of material on the WWW.The main vehicle of the digital media, the WWW, in its nature embodies, illustrates and enables through its functioning all four of the processes suggested as the basis for the new liberal curriculum--interrelationships, connectivity, transfer, and integration. By their very nature “the new media technologies externalize and objectify reasoning” (Manovich, 59). The WWW resists attempts at standard systematization, and demonstrates the co-existence of and interrelationships between multiple and apparently contradictory perspectives on a single issue. One GOOGLE keyword search will retrieve hundreds of documents linked by one single term, but applied variously in different contexts, emphasizing the importance and nature of connectivity, transfer and integration.In relation to a given subject of inquiry or task, the non-linearity in the presentation of material and ideas on the WWW encourages “intentional” involvement on the part of learners, as there is no longer one “solution” or a single “interpretation,” but a variety, all situated within their own context and knowledge. In order to find solutions to given questions or problems, learners have to engage in a process of discovering connections among apparently disparate materials and contexts, then find ways of transferring and integrating parts of materials into a new context. The WWW also offers alternative models of grouping materials, such as scaffolding, and it promotes the idea that conceptual knowledge cannot be separated from the contexts in which it is represented (Wiles and Littlejohn, 2003; Campbell, 2004; Cole, 2000; Carr, 1998;). The ready availability of various information on the WWW turns research and interpretation not so much into an exercise that depends upon finding information, but one that strongly emphasizes cognitive operations depending on critical thinking: classification of it (finding meaningful interrelationships) and making use of it (transferring it) by arranging it meaningfully in a give context (integrating it). Scholarship and instruction in the humanities has always relied on these processes; however, with computer connectivity and the speed with which these processes happen, the WWW amplifies them, enables them “physically” and “on-demand” and thus makes them more explicitly and self-consciously “teachable” than before.In addition, because the WWW offers multiple presentations of information, it illustrates that knowledge and heuristics are not absolute, but situated within various communities of knowing. Conceptually there is no “closure” or “ending” online, but rather a constant process of evaluation of materials (Rhodes and Sawday, 12) that are open to revisions, additions and remodeling. Becoming aware of the implications the nature of the WWW has for understanding what constitutes knowledge, argument, opinion, analysis and interpretation, leads to the development of critical discernment, evaluative capacity and judgment.While traditional “mass education tended to see life in a linear fashion based on print models and developed pedagogies which broke experience into discrete moments and behavioral bits,” a new critical pedagogy of the digital humanities could produce skills that “enable individuals to better navigate the multiple realm and challenges of contemporary life” (Kellner, 9). Making the WWW, its capabilities and operating functions explicit models of intentional learning can help educators in the digital humanities illustrate how knowledge in the humanities is positively affected by the digital medium, and how the new pedagogy leads to learning as an active, social process bound up with experience connected with wider socio-political paradigms of change.",
        "article_title": "The WWW as Curricular Method in the Digital Humanities",
        "authors": [
            {
                "given": "Tatjana ",
                "family": "Chorney",
                "affiliation": [
                    "Saint Mary's University"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-05",
        "keywords": [
            "'intentional' teaching and learning",
            "WWW",
            "digital media",
            "humanities for the global age"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Following my own comparative stylometric analyses of originals and translations of literary works (Rybicki 2006, 2006a), I have decided to expand the scope of this research to more than just two languages. This has been made possible by the fact that Burrows’s well-established method, first used in his study of Jane Austen (1987), and later developed, evaluated and applied by a number of scholars, including Hoover (2002), can be applied to the most frequent words of any text, also in a language unknown (or less-known) to the researcher. Quite self-evidently, Shakespeare’s Hamlet was chosen for its status of a crucial work of English literature and its numerous translations. The fact that the English Hamlet exists in three primary versions (First Quarto, Second Quarto, First Folio) was also a serious incentive.Character parts were extracted from the three above-mentioned originals and nine translations: Czech (by Josef Jirí Kolár, 1855), French (François-Victor Hugo, 1863), German (August Wilhelm Schlegel, 1798), Hungarian (János Arany, 1864), Italian (Goffredo Raponi, 1999), Polish (ca. 1875), Portuguese (1966) Russian (Mikhail Morozov, 1954), and Spanish (Leandro Fernandez de Moratin, 1798). The selection of the translations was a compromise between their age (the older the better) and availability in electronic form. The three originals were taken from the collection of exemplary electronic texts on TACT CD (1996). Most frequent words were identified in each version, basing on the rule that a given word was allowed if it appeared in at least 5 of the major character parts: these included Claudius, Gertrude, Hamlet, Horatio, Laertes, Ophelia and Polonius. As a result, the number of the most frequent words included in the analysis varied from 157 (First Quarto, obviously the shortest text) to 282 (the Italian translation). Relative frequencies were then obtained for each character’s use of each of the above-mentioned words; matrices of the relative frequencies were then used to produce multidimensional scaling (MDS) graphs for each version of the play. The same material was also treated with cluster analysis, usually with good agreement with MDS.The following observations have been made: Graphs for each version of the play always included at least two peripheral data points, invariably including the two female characters, Gertrude and Ophelia;Laertes and Horatio frequently joined Gertrude and Ophelia in their peripheral orbits around the data point for Hamlet, usually central in all graphs (with the notable exception of the Czech translation);In many translations, the idiolect of Hamlet was more or less similar to those of some other characters: its closeness to Polonius (or Corambis) in the First Quarto lessened in the Second, only to be replaced by a much stronger similarity to Laertes. An even stronger similitude was observed between Hamlet, Polonius and Claudius in the Portuguese translation, or to Claudius alone in the Italian version. In general, Polonius and Claudius were the idiolects most consistently similar to that of the main character.The closest similarity of pattern between entire graphs was that between the First Quarto and Schlegel’s German translation (Hamlet and Polonius surrounded by even-spaced peripheral characters);The general pattern was also quite similar between the Second Quarto and the Folio (peripheral Gertrudes, Horatios and Ophelias, central Hamlets, other characters in-between); it was also roughly reproduced in the translations by Arany, Moratin and Paszkowski.The clearest gender division was observed in the Czech, Hungarian, and Russian translations.This introductory study seems to offer several promising avenues for development. The similarity of pattern between the Second Quarto and the Folio is not surprising; even less so is its above-mentioned reproduction by three translations, since their source material was usually a compilation of the two original versions. On the other hand, Schlegel’s similarity to the First Quarto seems pure coincidence, as the so-called “bad quarto” was discovered more than twenty years after the early German translation. The peripheral position of female characters in all graphs is perhaps the most consistent feature of most of European writing so far analysed with MDS; this effect also travels very well in translation.Yet the greatest potential for expanding this study lies in the sheer number of Hamlet translations. In the languages included in this study, they are at least five, and very often more than ten; cultures such as Polish, German, or French, have produced almost twenty Hamlets each. The inclusion of at least some of these would provide a fuller picture of patterns of stylistic differences between Shakespeare’s fundamental play and its various realisations in other languages.",
        "article_title": "Twelve Hamlets: A Stylometric Analysis of Major Characters' Idiolects in Three English Versions and Nine Translations",
        "authors": [
            {
                "given": "Jan ",
                "family": "Rybicki",
                "affiliation": [
                    "Pedagogical University"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-03",
        "keywords": [
            "stylometry",
            "Shakespeare",
            "multivariate analysis",
            "Hamlet",
            "translation"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The author of the presentation makes an attempt to summarize problems and prospects concerning terminology used in computer supported description of Slavic manuscripts in two Slavic languages – Bulgarian and Russian, and, at the same time – in English and German. The report is a result of a working team in the Bulgarian Academy of Sciences and Sofia University: Anisava Miltenova (Institute of Literature, director of the project), Andrej Bojadžiev (University of Sofia), Margaret Dimitrova (University of Sofia), Irina Kuzidova (Institute of Literature), Regina Koycheva (Institute of Literature), Maya Petrova (Institute of Literature), and Svetla Koeva (Institute of Bulgarian Language). They are working in the frames of the project Metadata and Electronic Catalogues (2004–08), a component of the Repertorium of Old Bulgarian Literature and Letters. The project is oriented to create electronic catalogues and authority files that will serve as integrated repositories of terminological information that has been developed and applied successfully in already existing projects in the realm of medieval Slavic languages, literatures, and cultures. One innovative feature of this project is that beyond serving as a central repository for such information, it will expand the organizational framework to support a multilingual superstructure along the lines of I18N initiatives elsewhere in the world of electronic text technology in general and humanities computing in particular.The project presumes the possibility of linking the standardized terminological apparatus for description, study, edition, and translation of medieval texts, on the one hand, to authoritative lists of key-words and terms used in bibliographic descriptions, on the other. This will allow the integration of scholarly meta-data and bibliographic references under a single unified framework. Another aim of the project is to create a mechanism for allowing the extraction of different types of indices based upon the imported documents even when the languages of encoding may vary. The primary manuscript description texts are encoded in a TEI-based XML format in the context of the broader Repertorium initiative, and their utility for the type of multilingual authority files, bibliographic databases, and other broad reference resources illustrates the multipurposing that is characteristic of XML documents in the humanities, but on a broader scale than is usual.One of the project’s main aims is to propose an approach with the help of which the main terms of the Medieval culture could be further explained with the original texts, translations and current research activities. In this respect we could distinguish several areas of knowledge: name of the various texts and principles of such a naming, the author of the text(s) and problems related with the transliteration and transcriptions of the names; terms related to the text history its structure and function; the language(s) of the text; the relationship between the text and the codex; the Medieval mankind represented in the texts, etc. This project is based on distinguishing the meanings of particular terms and notions that appear in the text of medieval written documents both within the primary corpus and in comparison to established scholarly terminology.In the frame of the project several types of indices have been created: names of the texts (in four languages), genre terminology (in the field of hymnography, hagiography, homiletic literature, different kinds of instructions, etc.), types of manuscripts (concerning their function), palaeographic and codicological terms, linguistic terminology These indices could be divided into two types: Entries in the form of lists which contain information for usage of particular notion in the original Medieval texts .Entries in the form of terminological XML tool, which contain research metadata, in the form of thesaurus.The main difference between the both is the absence (in the former) and the presence (in the latter) of definitions, and a strict hierarchy of concepts. The first type of the information represents a simple multilingual list of terms. The function of this list could be explained as “It could be the standard way to say this or to write this”. The entries of the second type have more sophisticated structure, very close resembling to the encyclopaedic article. A compulsory element of this type of entries is the reference to the authority source, from which the material is extracted and which is very appreciated among scholars. In the informational area the specifications to the particular standards are used. The project is very actual in the context of discussions in the frames of ISO activities for definitions of markup of the terminological information (cf. TMF: Terminological Markup Framework). Our model is based on the approved standard ISO 12000. The report includes demonstration of the current project’s results both in the philological and technical aspects.",
        "article_title": "The Encoding of Terminology Related to the Medieval Slavic Manuscripts: Philological and Technological Results and Perspectives",
        "authors": [
            {
                "given": "Andrej Todorov ",
                "family": "Bojadžiev",
                "affiliation": [
                    "University of Sofia"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "Medieval Slavic manuscripts",
            "descriptions",
            "terminology",
            "XML encoding"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Theories of the signThis consideration begins in the theories of signification proposed in Structuralism, particularly as elucidated by Ferdinand de Saussure and interpreted by Roland Barthes (drawing on Hjelmslev and others). It will further be informed by the general theory of “autopoiesis” as articulated by Maturana and Varela (1987) and by other recent studies of language and signification influenced by systems theory.Of particular importance to this treatment are the following Structuralist principles: Composite nature of the sign - a sign is an event or a relation between two components, a signifying part (signifier, expression, or token) and a signified part (the content or meaning of the sign).Arbitrary nature of the sign - the relation between signifier and signified in a sign is arbitrary, not inevitable or given by nature. Thus slippage is possible in principle and, in some systems, is common. This potential for slippage is what accounts for much of the flexibility, adaptability and power of sign systems.Signs work in combination: individual words, for example, have their significations, but when words are combined into sentences they become more useful and more expressive.Complete signs (considered as signifier/signified pairs) can also enter into signifying relations. For example, a metalanguage is a system of signs that describes another system of signs; expressions in the metalanguage represent signs in the signified sign system. Metalanguages provide channels of regulation and feedback that are conducive to the development of the sign systems they describe, and eventually (when formalized sufficiently) enable automated processes to manipulate signs systematically. Conversely, a connotative system is a system of signs in which the signifying parts of signs are themselves signs (one might think of literary texts). Connotative systems demonstrate the reciprocal relations between layers in a sign system and the way significations at higher levels can condition and affect signification at lower levels, even apart from the application of metalanguages. While metalanguages abstract and formalize the sign systems they describe, connotative systems work by deploying signs as they are used concretely in other contexts, bringing alternative significations into play. Likewise, while metalanguages systematize and formalize, and thus indicate where automated rules-based processing is possible, connotative systems draw on and reflect particular significations available only in specific local contexts, and indicate where automation by traditional means is difficult or impossible.paper_188_piez_1.jpgFigure 1: In both natural and artificial systems, a combination of simplicity of design with complexity, versatility and adaptability of application is achieved through a layered structure in which components with distinctive functions are made by combining simpler components.The Semantics of Layers and the Layering of SemanticsA very useful distinction can be made between two different modes of signification, which we can call operational and representational semantics. The representational semantics of a sign corresponds to the conventional notion of what a sign is and how it functions, that a sign “stands for” something, reflecting and naming an actually existent “thing” in a real or imagined world. In contrast, the operational semantics describes not what the sign refers to in a purported world (which may or may not be present to the senses), but rather simply how it operates within the signifying system — which generally includes the world, or at least the present and active circumstances of the sign's use. (This in itself is the major difference between “human” and “machine” semantics [Piez 2002].) The rules of combination that allow any given set of signs to be assembled into a higher-order organization, which itself has signifying potential, are part of their operational semantics; but so also are any disambiguating “incidentals”. At all layers, operational semantics may be conditioned or constrained by their context of operation: the “meaning” of a sign is not built into the sign, but is established ad hoc, by means of those distinctions between it and possible alternatives that result in particular outcomes in application.Interestingly, careful consideration suggests that while we often consider that the representational aspect of a sign is fundamental to its operation and provides part of its operational semantics, in general the opposite is the case: operational semantics are primary, and representational semantics can only be established once operational semantics are set and perhaps codified. Once a sign's operation has been established repeatedly it can start to “carry its context with it” — that is, its context can begin to include prior contexts, implicitly recognized — and this is the beginning of representation. (A compelling description of this process in human language development as well as in the more rudimentary linguistic capabilities of chimpanzees, bonobos and gorillas, may be read in [Greenspan and Shankar 2004].) Thus, what any given sign represents must be inferred from its context of operation — which may include the traces of other occasions of its use — and much energy is given, in the application of sign systems to doing useful work in the world, to negotiating these inferences.The construction of layered sign systems, in which complex representations may be reliably constructed by a rules-based assembly from simpler components, has the precise advantage of managing and reducing this expenditure of energy. Metalanguages (such as an orthography, formal logic, or the grammars of natural or artificial languages), which describe sign systems themselves and stipulate rules for their operation, are more than sterile intellectual exercises. By abstracting and codifying the application of rules to signs, they reduce the need to rely on sheer brute force methods (memorization or negotiation) to support the assembly of signs into sign systems, thus reducing overall complexity and enabling more sophisticated operations at higher levels.Accordingly, media can be built out of building blocks constituted of other media. Within a (relatively highly evolved) language, utterances take the form of sentences or statements, composed out of words; words are composed of phonemes (or of letters, in the case of written words). When sentences or statements or propositions are combined, they constitute yet another medium at a higher level, which might be identified with argument or narrative. Thus, media are built in layers, each layer a hierarchy of subsystems on a lower layer. This is characteristic of complex systems generally in both the natural and artificial worlds (see [Simon 1996]).In general, this layering is characterized by two related phenomena:The distinctions between the layers becomes clearer over time: media formats evolve and become more systematic and coherent in composition. More and more complex and comprehensive structures become progressively easier to realize, at the cost of a certain kind of expressive power characteristic of early or individual experiments. The higher layers, as they solidify and consolidate, induce a process of simplification and rationalization at lower layers.Likewise, as you go up the stack, the distinction between layers becomes less clear. The difference between a letter and a word is almost always clear, but the distinction between a statement and an argument is less so.As media ascend in complexity, each layer is capable of more “meaningfulness” than the layer it is built on; or at least it has greater representational power, due to the consolidation of operational and (on that basis) even of representational semantics at lower layers. In turn, the context provided by the combination of any set of tokens at any layer provides information relevant to the construing of the local meaning of each of the constituent tokens. This interpenetrating influence between layers, familiar to all students of language and literature, is an important feature of the entire system.The most complex and “highest” of these layers currently might go by the name of information architectures. Until recently we have not really needed to have a name for this layer or members of it (signs that appear in it), as they have for the most part been identified with print media: we have spoken of indexes, cross-references, tables of contexts, abstracts, summaries and bibliographies without being aware that these things might not have to be printed on paper. This is because traditionally, in order to stabilize complex bodies of information, particular features of print media have been indispensible — temporal persistence and asynchrony; “random” or holistic access; conventions of citation; graphic and typographic queues representing organization and providing for navigability; economies of scale afforded by mass production; and so on. Now these features are available in the formats provided by digital media — along with elaborations of them such as the hyperlink, dynamic display and device independence — it becomes useful to consider this higher layer without identifying it with print technology.paper_188_piez_2.jpgFigure 2: Various different forms of communications media are formed by layering.paper_188_piez_3.jpgFigure 3: At lower layers, alignments between constituent parts are generally possible, leading to representational correspondences not only between media and the world, but across media themselves. At higher layers, media find their own particular strengths, constituting 'worlds of their own' whose representational capabilities are both presumably greater, and more problematic.The Emergence of the DigitalLike all media (cf. Marshall McLuhan), digital formats begin as representations (operational refigurations) of prior media. But just as alphabetic literacy, following the logic of its own peculiar operational semantics, quickly assumes forms distinct from the oral forms it begins by mediating, digital media soon become something other than the print counterparts that were to be “transmitted” by telecommunications technologies. Again, it achieves these forms by layering.Despite the highly developed form of the processing stack when it comes to the computer's own processing (its operations), however, when it comes to digital media we are still at a point when these layers are inchoate — although the standardization efforts of the last decade (especially as regards HTML, CSS, XML and XSLT) are providing a level of metalinguistic control conducive to their development and maturation. Yet for the most part, applications of digital media are still either derivative of other forms (in the sense that a page on the web may be almost entirely analogous to the same document in print) or directly in service to them; digital media have not fully come into their own.Nonetheless, the usefulness and power of layering in this context too has long been recognized: we only need to recall the familiar dogma (and the discussion that has surrounded it) of the “separation of format from content” in the design and deployment of markup-based publication systems (see [Sperberg- McQueen and Burnard, 1994], [Durand et al. 1996], [Piez 2001], [Sperberg-McQueen et al. 2002], [Piez 2002]). This tradition recognizes that so-called “descriptive encoding” works by anticipating and expressing at a lower layer (in what we call source code), the rationale for structures to be expressed at higher layers through site organization, page layout, typography, screen widgetry, linking and all the apparatus of a full-blown architecture. In this respect, the tagging of an XML document whose encoding is descriptive and data-oriented rather than prescriptive and application-oriented proves to be a connotative system, as the signifiers (the element names “title” and “p”) that describe the data (this chunk of text is a nominal paragraph; that one a title) are themselves signs, to be transformed by a heuristic and rules-based process into renditions that will themselves signify to readers that they are paragraphs or titles. This anticipation or prolepsis by markup of further signification elucidates the confusion as to whether we consider descriptive encoding to be at a “higher” or “lower” layer, as indeed it is paradoxically both. Within a classical three-tiered architecture, the XML encoding is “below” its HTML (or print, or SVG, or ODF) rendition; yet we also describe the conversion from descriptive XML into a presentational format such as HTML as a “down-conversion” (since it “loses information”), by rendering only indirectly in presentational features, if at all, what is explicit in the source. The reason we can, in effect, go down to go up, is that here the lower layer achieves its aim of scalability and reusability by working to describe a higher layer that it is not yet practical (if it ever will be) for the computer to infer on its own: it provides, in representational form by a kind of \"sleight of hand\" (the operational semantics being invisible to the machine and left to the stylesheet designer), information that would ordinarily be available only by a processing context not yet available — the act of reading itself. In fact, the transformation from descriptive XML to HTML is not actually taking it “up” the ladder towards richer information design; it is merely transposing the data into another stack altogether, where, since the operational semantics of HTML are more tightly bound to standard processing contexts (the browser), its implicit functionalities can be elaborated, while the representational aspect can be deferred to where it makes more sense — where, because the tagging now signifies “large and bold”, the reader can be trusted to infer “title”.Yet the same discussion has also masked deeper problems and issues (see [Buzzetti 2002], [Caton 2005]) stemming from the limitations in current markup systems, which can gracefully handle only a single organizational hierarchy at a time, and thus lack the representational and expressive power necessary to take full advantage of the computer's capabilities for useful automated processing of complex textual artifacts.Nevertheless, understanding digital text encoding technologies as complex sign systems elucidates how and why they function without resorting to the metaphysical appeal that “good” encoding should be designed to describe the “thing itself”. Especially given the limitations inherent in XML's design, such a position proves soon to be untenable; yet XML systems succeed in doing useful work notwithstanding, and provide the foundations for sustainable, scalable and navigable information resources, whose presentational features (interfaces) can be improved over time. This in itself represents a major advance over what was ever possible in the past.More generally, a consideration of digital text encoding as a distinctive semiotic system, with its own metalanguages and its own relation to media artifacts, suggests why the humanistic study of digital media remains so foreign to traditional disciplines in the humanities. Likewise, it points the way to the future, as it becomes clear what, and how much, still remains to be done.paper_188_piez_4.jpgFigure 4: While metalanguages can be expensive to maintain, they provide means not only to describe but also to maintain and control the communications media they describe.",
        "article_title": "Form and Format: Towards a Semiotics of Digital Text Encoding",
        "authors": [
            {
                "given": "Wendell ",
                "family": "Piez",
                "affiliation": [
                    "Mulberry Technologies, Inc"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "text encoding",
            "XML",
            "semiotics"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Computer-assisted text analysis has over 50 years of history in providing tools to help scholars interpret texts (see, for instance, Potter, 1991, Burrows, 2004, Bradley, 2004). The tools themselves are the products of a large range of circumstances in computing and text criticism, from the availability of certain hardware to the fluctuating fortunes of structuralist approaches in literary criticism. In this paper we will reverse the usual interpretive flow from tools to text by attempting to interpret tools themselves as artifacts of human creation. We will frame this interpretive exercise as a Gedankenexperiment; in particular: if a scholar one hundred years from now were to study the TAPoR Portal as cultural artifact, what would it reveal about its theoretical presuppositions, the methodological practices of its times, its cultural value, and even its authors?This thought experiment is predicated on the assumption that tools can be studied as cultural artifacts in ways similar to, say, literary texts. To examine this assumption more closely, we will outline several ways in which tools may be studied and interpreted, including: as functional systems fulfilling an identified needas code that corresponds to certain expectations in terms of structure, brevity, creativity, etc.as interfaces that may have an aesthetic appealas pedagogic tools that are intended to assist users in developing skillsas artifacts that express an author's perspectiveas artifacts that express characteristics about a communitySeveral differences are evident between literary texts and tools as objects of study. For instance, tools manifest themselves at two (at leaset) layers of visibility: the code layer, generally reserved for developers, and the interface layer, generally intended for users. Literary texts, in contrast, have only one layer of exposure. It may also be that text analysis tools are fundamentally too different from texts to be considered using approaches of literary criticism. A similar debate has raged in game studies for the past several years (see, for instance Frasca, 1999): can games be studied as narratives (the literary camp) or do they require an entirely different approach (the ludology camp)? We will argue for a hybrid approach: while literary criticism can be useful to interpret certain aspects of tools, other aspects require their own framework of study. Moreover, this hybrid approach leads to certain practical consequences when considering how to peer-review tools and their development (see Sinclair et al., 2003). As a case study for interpreting tools, we will use the TAPoR Portal, an initiative to build a web-based gateway to electronic texts and tools (see  and Sinclair, 2002). The TAPoR Portal is reaching the end of its development cycle, after approximately five years. It serves as a convenient case study for several reasons, including its duration (a five year project), its collaborative nature (involving several dozen researchers at five Canadian universities, its development model (a blend of academic and private sector contributions), its interdisciplinarity (representing several branches of the digital humanities and computer science), and, of course, its size and complexity (over 165, 000 new lines of code). Indeed, given the size of this \"corpus\", examples can be found for just about any interpretation that might be proposed, but such is the open-ended nature of interpretation. ",
        "article_title": "Reading Tools, or Text Analysis Tools as Objects of Interpretation",
        "authors": [
            {
                "given": "Stéfan ",
                "family": "Sinclair",
                "affiliation": [
                    "McMaster University"
                ]
            },
            {
                "given": "Geoffrey ",
                "family": "Rockwell",
                "affiliation": [
                    "McMaster University"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "text analysis",
            "tools development"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Teaching Spanish colonial literature to undergraduate students is a difficult pedagogical task, because students in that level don’t have an extensive knowledge of the language of the times or the books’ historical context. Indeed, graduate classes and even scholars writing in the field rarely grapple with the early modern editions themselves, because of their accessibility and because they can be so difficult to penetrate. A scholarly collection of essays by Danny Anderson and Jill S. Kuhnheim, that discusses strategies for teaching this material, encourages an approach to these texts based on cultural theory and literary criticism rather than on a close examination of the texts themselves. At Wheaton College (Norton, MA), on the contrary, we have spent the last two years experimenting with a different approach: students’ direct involvement with a digital portion of a colonial text (supported by digital cartographic evidence from the period, and mediated by the faculty’s suggestions) enhances their comprehension and interest for the actual original book.In three literature classes undergraduate students have contributed to the creation of a digital critical edition of a 16th century Spanish rare book: Libro de los Infortunios y Naufragios (Book of the Misfortunes and Shipwrecks) by Gonzalo Fernández de Oviedo, partially published in Seville in 1535, as part of the La Historia General de las Indias (General History of the West Indies). To date Fernandez de Oviedo’s shipwreck narrative has received only scant scholarly attention by literary critics because of the rarity of the book and accessibility to the XVI century printed edition (Kohut).The class goal is to produce a digital edition of Fernandez de Oviedo’s book. The process comprises a variety of steps, aimed to direct students’ attention to the texts themselves, their structure, the meaning of words and phrases, and even of individual letters. Students first learn to use accurate transcription techniques and electronic textual editing practices, and then move on to the text encoding based on an XML encoding scheme developed by the Text Encoding Initiative (TEI). We have found that using the TEI encoding is an effective pedagogical tool because, as Allen Renear notes in a article, the standard can “improve our ability to describe textual features…. The TEI Guidelines represent an elucidation of current practices, methods, and concepts that open the way to new methods of analysis, new understandings, and new possibilities for representation and communication” (235).The TEI encoding project functions as a pedagogical tool to study an Early Modern Spanish text; it helps students become more knowledgeable regarding the primary text and the context in which it was written. By using TEI students focus their attention on three main levels: linguistic, geographical and historical. While working directly with a text with the purpose of encoding one of its portions, students have a chance to learn across multiple disciplines, e.g. learning about the ancient Spanish denomination of a place, and creating a link to its location on a map of the times.The encoding projects are divided into several stages. First, students use text editing software to create a transcription from digital images of original print editions – a process that involves deciphering difficult 16th century typography and finding codes to represent the many characters that are no longer used in contemporary Spanish. After completing the transcription, the students employ a rigorous, descriptive “tagging” process, using the TEI XML encoding scheme. They begin by marking the structural parts of the text – where each chapter begins and ends, each section heading, each paragraph, and so on. They then use TEI encoding to tag historical people names, places, and unfamiliar or archaic vocabulary in the text. And as a final stage of the project, the students perform appropriate research about their texts, and using TEI, define all of the tagged people, places, and vocabulary – essentially providing electronic footnotes to the digital editions of the text. During this stage of the project, students also work with scans of original maps from the period – locating on the maps many places they have tagged, and linking segments of text in their documents to the scans.This process presents many pedagogical advantages. Students are extremely motivated by projects like this: they work so closely with the text and end up creating their own annotated edition, thus feeling a sense of ownership of the documents. Many students are excited that this project reaches beyond typical Humanities class work and that they see the results of their hard work very quickly published on the World Wide Web. The students' digital editions of the texts also help preserve and eventually widen the distribution of out of print texts. And finally, this project introduces an academic rigor in studying this literature, which shows in the accuracy taken to encode and validate the text encoding. Within the last few years, Wheaton has been instituting a new curriculum that emphasizes that students should gain a breadth and depth of knowledge through their course work. The process of creating a digital edition of texts like these ensures that students have an in depth experience with a text unlike most other experiences they have had in the humanities. This approach to a text encourages them to apply a systematic, almost scientific approach to humanities scholarship. This work, therefore, is well in line with the goals of many of digital humanities scholarship. A goal, which Susan Hockey expressed in a comprehensive article about the history of the Digital Humanities: “to bring the rigor and systematic unambiguous procedural methodologies characteristic of the sciences to address problems within the humanities that had hitherto been most often treated in a serendipitous fashion” (3).Our experience teaching with TEI at Wheaton has been very positive: students learn to understand and appreciate the original rare book, only once they have had their experience with creating the digitally enhanced version of it. At that point they’re ready to visit a prestigious library like the John Carter Brown Library at Brown University, and look at the book and at the maps of the period. Undergraduates benefit from the text encoding experience in many ways: better understanding of the topic of study, better understanding of the foreign language, and interest, or at least curiosity, for the rare book itself. Assessment on student learning has been very positive: students retain a lot more than with traditional teaching, and can sometimes also spot irregularities and even fake news in the texts they study. The load on the faculty member and on the library liaison is very high, but it pays off with the students’ satisfaction.",
        "article_title": "Exploring New Worlds in Old Texts: Text Encoding Projects for the Undergraduate Study of Spanish American Colonial Literature",
        "authors": [
            {
                "given": "Domingo ",
                "family": "Ledezma",
                "affiliation": [
                    "Wheaton College"
                ]
            },
            {
                "given": "Phoebe ",
                "family": "Stinson",
                "affiliation": [
                    "Wheaton College"
                ]
            },
            {
                "given": "Scott ",
                "family": "Hamlin",
                "affiliation": [
                    "Wheaton College"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "Spanish American colonial literature",
            "humanities computing",
            "pedagogy",
            "encoding"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "[Schools] can only teach those patterns which have proved successful. If one is going to do new business the patterns cannot help, though one does not deliberately go out to do that. My Ántonia, for instance, is just the other side of the rug, the pattern that is supposed not to count in a story.Willa Cather, 1925Tools for text analysis have long been a focus of many digital humanities scholars, yet the results produced by those tools are rarely utilized in typical literary and cultural criticism. Though the reasons for this disconnect are varied, we believe two primary hurdles are visibility and accessibility. Specifically, most text analysis tools and research are not found on the sites—like thematic digital text archives—that scholars use most, and most text analysis tools are not designed for ordinary scholars, but are meant for those with more technical sophistication. Our essay discusses a novel approach to filling this gap between scholars and text analysis, an ongoing collaborative experiment in humanities computing to bring a \"new business\" that willfully counters the \"patterns which have proved successful\" in literary criticism in order to add a new dimension to literary research. In \"The Other Side of the Rug: TokenX on the Will Cather Archive,\" we discuss the application of Brian Pytlik Zillig's text analysis, visualization, and play tool, TokenX () to the Willa Cather Archive (), a free, educational resource dedicated to the study of Willa Cather's life and writings and edited by Andrew Jewell. When this application of TokenX debuts in the summer of 2007, scholars will be able to analyze the entire corpus of Cather's fiction, from her first college publications to her final short story. In many ways, our project is in the tradition of research that seeks to use text analysis tools to arrive at, in Ramsay and Steger's language, \"suggestive patterns\" to \"enable critical reflection in literary study.\" However, in creating this tool for application on the Cather Archive, we were faced with two distinct challenges: (1) how to develop TokenX to make it capable of the envisioned crossdocument analysis which is sensitive to changes over time, and (2) how to design an interface that would make sophisticated text analysis a manageable, useful tool for the widest possible range of Cather scholars, scholars unaccustomed to using such tools. The project is a interdisciplinary collaboration between Pytlik Zillig, a Digital Initiatives Librarian with specialization in XSLT and text analysis, and Jewell, Assistant Professor of Digital Projects with a Ph.D. in American literature, both of the University of Nebraska-Lincoln's Center for Digital Research in the Humanities. The paper, likewise, is a collaborative work that represents two distinct but complementary perspectives on the issue.Pytlik Zillig asserts that, for digital humanities, the development and ready availability of tools to assist in the noticing and appreciation of texts is of increasing importance. Unsworth has observed that “by paying attention to an object of interest, we can explore it, find new dimensions within it, notice things about it that have never been noticed before, and increase its value.\" For the first iteration of the TokenX/Cather collaboration, the TokenX tool has generated a word frequency data set containing nearly half a million data cells. These data reveal the frequency of usage of words in fifteen TEI-encoded XML texts, representing Cather’s complete corpus of book-length fiction. This data set will be available for dynamic, user-centered queries to assist in formulating theories and facilitating explorations of Cather’s changing diction over time. (See Figure 1 for initial results on ten sample terms within Cather's corpus, detailing her usage of certain \"body\" words within each of her published books of fiction.) If a text’s common words are, as John Burrows suggests, \"a barely visible web that gives shape to whatever is being said\" (323), then it must be the ambition of tools such as TokenX to expose the dimensions of that web for further inquiry. (Plans are underway to add TokenX to the Text Analysis Portal for Research [TAPoR], which will enable TokenX users to visualize, analyze and play with documents stored on the TAPoR portal.)Jewell argues that introduction of and experimentation with new tools for engagement with literary texts are an important way to make author-centric sites, like the Willa Cather Archive, models of innovation. The audience of the Cather Archive is not one inherently inclined to think about literary texts numerically. The onus on the designer, then, is to consider the sorts of research questions driving Cather and American literature scholarship and to make this tool something that would contribute to tackling such questions. For example, how might text analysis contribute to a scholar exploring Cather's work with a cultural studies or gender studies approach? What about a scholar looking at contexts, themes, or references within a single novel, or the fiction of a finite span in Cather's career? How might this tool aid a researcher interested in tracking evolutions in Cather's prose over a long period of time? (See Lindemann and the program of the 2005 International Cather Seminar for examples of these approaches). By designing an interface that is sensitive to a range of scholarly inquiry, one that allows for a significant amount of flexibility and user input, TokenX on the Cather Archive can represent an innovative use of digital research that is brought into the mainstream of scholarship. By demonstrating sensitivity to researcher interests and seeking to design broadly useful tools, we demonstrate to colleagues that such tools are not just for specialists, but can enrich diverse arguments, any that find foundation— as most literary scholarship does—on the use of words.",
        "article_title": "The Other Side of the Rug: TokenX on the Willa Cather Archive",
        "authors": [
            {
                "given": "Andrew Wade ",
                "family": "Jewell",
                "affiliation": [
                    "University of Nebraska-Lincoln"
                ]
            },
            {
                "given": "Brian L. ",
                "family": "Pytlik Zillig",
                "affiliation": [
                    "University of Nebraska-Lincoln"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "Willa Cather Archive",
            "text analysis",
            "XSLT 2.0",
            "TokenX"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "1. IntroductionThe explosive growth of digital and digitized text creates opportunities for scholars and students to conduct new analyses and develop unique insights about our written culture and heritage. To effectively use large collections of textual data, scholars and students need flexible, easy to use tools that provide powerful analysis and visualization.1.1 GoalsThe Automated Learning Group is developing interactive tools for mining large, complex semantic networks, which were automatically extracted from a text corpus. The corpus could be a collection of documents or a stream of messages. The semantic network represents the concepts in the collection as entities and relations. The visual environment allows the user to query the semantic network to retrieve portions of the graph. The display allows the user to view and navigate these networks to discover patterns in the collection.1.2 Technical ApproachOur approach to the visual analysis of text documents implements extraction and visualization of important entities and the relations among them. This is accomplished in the following high-level steps:Perform lexical analysis of textual document setExtract key entities and relations from the textCompose entity-relation triples into semantic network model summarizing key information in document corpus“Publish” the semantic network in a repositoryVisualize semantic network model as graphSearch and prune visual graph through appropriately structured queries against semantic network model.2 Text AnalysisTextual processing is accomplished by passing documents through a series of syntactic, semantic, and functional analysis tools. These tools primarily consist of D2K and T2K, our own analytic software, working in concert with GATE and MontyLingua . These tools are seamlessly connected together into the D2K visual programming environmentThe GATE toolkit is utilized for standard syntactic processes including tokenization, sentence splitting, and part-of-speech tagging. Syntactically annotated documents are passed to GATE’s Named Entity (NE) tagger. The GATE NE tagger identifies proper nouns in the text that belong to relevant categories such as “Person”, “Organization”, and “Location.” NE tagging passes through three modules that perform different kinds of co-reference. The final output of the NE tagging and co-referencing processes are a set of annotations that identify references to “Person”, “Organization”, and “Location” entities.After NE extraction, we identify key relations in the text using a tool within MontyLingua. This Jister tool extracts sentence structures called “jists,” which carry thematic-role information such as verb, subject, and object. This is similar to semantic role labeling , but less formally structured. An example jist:Tiger Woods wrapped up the tournament at noon.(Verb: ‘wrap up’, Subj: ‘Tiger Woods’,Obj1: ‘tournament’,Obj2: ‘at noon’)The next step involves the normalization of references in the MontyLingua jists with entities identified in the analysis. For example, “Tiger Woods” may have been identified as a entity:PERSON, and “tournament” referenced back to “Masters Golf Tournament” appearing earlier in the text. Tagging the verb “wrap up” as an entity:ACTION, and “at noon” as a TIME relation to the verb, we can generate triples as follows: <Tiger Woods> <is-a> <entity:PERSON> <Masters Golf Tournament> <is-a> <entity:EVENT> <wrap up> <is-a> <entity:ACTION> <Tiger Woods> <actor> <wrap up> Figure 1 shows the D2K toolkit with a view of the text processing itinerary. The itinerary is a dataflow graph: the nodes in the figure are D2K modules (major processing blocks), connected by edges representing the flow of data during execution. The itinerary can exploit both data parallelism (multiple documents at the same time) and task parallelism (different modules in parallel). The D2K itinerary can be run on a desktop or scaled up to multiple computers or High Performance Computer systems.Figure 1 shows the D2K toolkit with a view of the text processing itinerary. The itinerary is a dataflow graph: the nodes in the figure are D2K modules (major processing blocks), connected by edges representing the flow of data during execution. The itinerary can exploit both data parallelism (multiple documents at the same time) and task parallelism (different modules in parallel). The D2K itinerary can be run on a desktop or scaled up to multiple computers or High Performance Computer systems.3 Semantic Network Storage and RetrievalThe triples generated from the semantic extraction process are stored in an RDF metadata store. We use Kowari, developed by Tucana Technologies . Additional triples are generated to represent metadata in conformance with a common vocabulary, and user annotations can be included as well. Queries are coded in an SQL-like query language called iTQL. The result is a set of triples, which represents a semantic graph . This architecture demonstrates a key design principle for robust Cyberinfrastructure. The analysis is decoupled from the visualization, so that a large scale analysis can asynchronously update the triples as new results are computed, while interactive tools will automatically pick up the new data by refreshing the query. The triples generated from the semantic extraction process can be combined with many other similar relation triples from many sources, and additional triples are generated to represent a common vocabulary.4 Visual Investigation of Semantic NetworksUsing the visual environment, investigators can perform searches over the semantic networks extracted from a text corpus. The familiar web browser paradigm was employed in the user interface design. The user interface allows one to construct more complex queries by incorporating multiple rules and filters. Each user query is converted into iTQL and executed by the Kowari server . The query history is available as a pull-down menu, just as query histories are in a web browser. Investigators can directly observe semantic relationships between entities in an interactive link-node graph visualization. (Figure 2) Relations between entities in the resulting semantic network graph are displayed as a link-node graph visualized using Prefuse , and also as a hierarchical tree of entities conforming to the common vocabulary. The subject and object entities in the relations are displayed as nodes in the graph visualization, predicates are displayed as links. This simplification of the more complex semantic network, stored in Kowari, provides a compact and usable abstraction of the important relations extracted from the text streams. The Entity pane at the upper left displays lists of entities. The Relations pane at the lower left displays a list of relations, with additional options to highlight synonyms, antonyms, hyponyms or hypernyms based on WordNet. Selecting an entity in the left pane also highlights the corresponding node or edge in the visualization.Every entity in the graph maintains a link back to the original text document from which it was extracted. By right-clicking on a node, and selecting View Source Documents, the text of the original document will be retrieved from the repository and displayed.5 CollectionsThis tool can be applied for many different types of text, across one or many collections. In addition, it can analyze evolving collections of text, such as documents from one or more RSS feed.This tool has been used as part of the Nora project, a multi-institution collaboration to produce software for discovering, visualizing, and exploring significant patterns across large collections of full-text humanities resources in existing digital libraries. For example, we are experimenting with the digitized text of the novel Uncle Tom’s Cabin, available as part of the Early American Fiction Collection from the University of Virginia. The system performs feature extraction in order to determine shared characteristics of the selected documents, such as chapters of the novel. The resulting arc-node graph can be viewed and navigated to discover patterns and relations within and among the texts. Figure 2 illustrates an example analysis of text from nineteenth century novels, showing the use of concepts related to “Mother” and “Man”.Tom Horton, Kristen Taylor, Bei Yu and Xin Xiang, Quite Right, Dear and Interesting”: Seeking the Sentimental in Nineteenth Century American FictionDigital Humanities 2006, pp. 81-82 6 ConclusionThis paper described interactive tools for mining large, complex semantic networks automatically extracted from a text corpus. These approaches can be applied to news feeds, technical literature, or literary collections.We believe this is a promising approach, although we are only beginning to develop these tools for use by humanists, who will be the ultimate judges of the utility and validity of this approach. The general purpose semantic analysis is widely applicable to many types of text, though it is difficult to predict the impact of such analysis on our understanding of texts.The query interface and graphical displays are still under development. The entity-relationship graphs may be quite complicated, so we must find new visual methods and metaphors to enable scholars and students to understand the information in the graphs, and to use them formulate hypotheses, and answer questions.7 AcknowledgementsThe Nora project is funded in part by the Mellon Foundation. This work was funded in part by the National Center for Advanced Secure Systems Research (NCASSR) at the University of Illinois at Urbana-Champaign (UIUC), a multi-institutional cybersecurity research team. NCASSR is led by the National Center for Supercomputing Applications (NCSA) and supported by funding from the Office of Naval Research (ONR). Substantial portions of the code were implemented by David Clutter and Fang Guo. Thanks to Patricia S. Taylor.paper_206_mcgrath_1.jpgFigure 1: D2K Itinerary showing text processing.paper_206_mcgrath_2.jpgFigure 2: Visual interface showing result of a query.",
        "article_title": "A Flexible System for Text Analysis with Semantic Networks",
        "authors": [
            {
                "given": "Loretta  ",
                "family": "Auvil",
                "affiliation": [
                    "National Center for Supercomputing ApplicationsUniversity of Illinois at Urbana-Champaign"
                ]
            },
            {
                "given": "Eugene ",
                "family": "Grois",
                "affiliation": [
                    "National Center for Supercomputing ApplicationsUniversity of Illinois at Urbana-Champaign"
                ]
            },
            {
                "given": "Xavier ",
                "family": "Llorà",
                "affiliation": [
                    "University of Illinois at Urbana-Champaign"
                ]
            },
            {
                "given": "Greg ",
                "family": "Pape",
                "affiliation": [
                    "National Center for Supercomputing ApplicationsUniversity of Illinois at Urbana-Champaign"
                ]
            },
            {
                "given": "Vered ",
                "family": "Goren",
                "affiliation": [
                    "National Center for Supercomputing ApplicationsUniversity of Illinois at Urbana-Champaign"
                ]
            },
            {
                "given": "Barry ",
                "family": "Sanders",
                "affiliation": [
                    "National Center for Supercomputing ApplicationsUniversity of Illinois at Urbana-Champaign"
                ]
            },
            {
                "given": "Bernie ",
                "family": "Acs",
                "affiliation": [
                    "National Center for Supercomputing ApplicationsUniversity of Illinois at Urbana-Champaign"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-03",
        "keywords": [
            "automated learning",
            "text analysis",
            "analysis environments",
            "data mining"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The international, interdisciplinary and multilingual LICHEN project, initiated by the Department of English and the MediaTeam research group (Dept. of Electrical and Information Engineering, MediaTeam 2006) at the University of Oulu and the SCOTS corpus project at the University of Glasgow (Scottish Corpus of Texts and Speech, 2006), focuses on the languages and cultures of the northern circumpolar region. Its underlying assumption is that language and culture are as important to the survival and well-being of populations as more obvious ecological, social and health issues and thus it is also a member of the Circumpolar Health and Wellbeing research programme run by the Centre for Arctic Medicine, University of Oulu (Thule Institute 2006).The aim of the project is two-fold: firstly, the project aims to collect, preserve and disseminate information about the languages spoken in the circumpolar region, thus also enabling research on them. This will also help to promote the linguistic confidence and self-image of the speakers of these languages, strengthening their cultural awareness and facilitating cross-cultural communication between these peoples in an age of rapid global change (Winsa 1998).Secondly, and more importantly, the project aims to create an electronic framework for the collection, management, online display, and exploitation of existing corpora of the languages of the circumpolar regions, which is also applicable to other corpora that represent regional, social and other varieties of languages. Humanities computing researchers, in particular, have long recognized the need for new, more sophisticated tools to aid scholarly research of textual data, not to mention tools that would be able to handle multimodal data. Although a number of tools have been developed, they suffer from various restrictions, e.g. they are only applicable to the data they were developed for, importing data is laborious, user interfaces and encoding standards are outdated, considerable expertise in programming is assumed, no support for multilinguality is included, or they promise more than they offer. While there have been some very promising advances made in this direction (e.g. TAPOR Tools 2006), it is clear that more tools are needed.The framework being developed in this project is intended to be the equivalent of an extendable toolbox for corpus linguists. It will attempt to offer much-needed functionality in an easy-to-use package, which is shaped and built-on according to real user needs. Initially emphasis will be given to the implementation of the text capabilities of the system, but other modalities (such as audio and video) are also taken into account. The idea is to facilitate queries into a multimodal database using both proven and novel ways of finding and displaying information (Seppänen 2006). Metadata and metadata visualisation, particularly in conjunction with the new modalities, will be essential in achieving this. While we support the use of best practices for the collection, preservation and presentation of corpus data, we also recognize that some data, particularly legacy data, may not be in a position to do so and the shell must also support such data (Kretzschmar et al. 2006).The system will also make migration to and from other tools straightforward by offering import and export features for commonly used programs. It will enable users to bring in their own data, which they can keep private or make public using the built-in web functionality. The database will also be capable of handling several different versions of any document (for example, revisions, interpretations or translations); these are linked, a feature that can be made use of in queries. Queries can be made using regular expressions, which may combine free-form text (words, phrases) and part-of-speech tags, for example.The system is implemented in Java making it platform independent and taking advantage of the many technological components developed for that language. The ultimate goal of the development of the computing tools is a shell which can be adapted to any language. Therefore, support for multiple languages and a variety of character encoding schemes are important.The main focus of the project is on Meänkieli and Kven, two Finnic minority languages spoken in Sweden and Norway, respectively, and Scots and Scottish English. At present we have about 150 hours of tapes in Meänkieli and 100 hours of tapes in Kven. More Kven material is currently being collected. We also have access to both the structure and contents of the Scottish Corpus of Texts and Speech (SCOTS) at the University of Glasgow, currently totalling 3.5 million words of spoken and written Scots and Scottish English.The project began in 2004 and a prototype of the shell has now been constructed. We will demonstrate this shell, showing some of the basic functionality of the system while looking into concrete research questions focusing on the image of Scottishness as presented in Irvine Welsh’s Trainspotting (1996). Since the story is found in novel, play and movie versions, it affords an excellent testbed for a toolbox that can handle multimodal data. Using a few key scenes as examples, our research focuses on the questioning of national stereotypes in terms of landscape, language and culture. We are thus interested in the comparison of the images presented in the three versions, which will also demonstrate the ability of the toolbox to support versioning. While we concentrate on linguistic features of Scottish English, we also demonstrate how easy-to-use access to sound and images linked to the transcription of the movie and the linking between the three versions of the text greatly facilitate research such as ours that must take into consideration images created both on all levels of a text and across texts. Finally, we demonstrate the possibility of making use of online dictionaries as an added tool in the analysis of data from within the toolbox.Since the shell is still in prototype form, we would welcome this opportunity to discuss our needs and goals at DH2007, thus drawing on the considerable expertise of the conference participants in order to ensure that our tools benefit as wide a range of users as possible.",
        "article_title": "The LInguistic and Cultural Heritage Electronic Network (LICHEN): A New Electronic Framework for the Collection, Management, Online Display, and Exploitation of Multimodal Corpora",
        "authors": [
            {
                "given": "Lisa Lena ",
                "family": "Opas-Hänninen",
                "affiliation": [
                    "University of OuluFinland"
                ]
            },
            {
                "given": "Matti ",
                "family": "Hosio",
                "affiliation": [
                    "Univerisity of OuluFinland"
                ]
            },
            {
                "given": "Ilkka ",
                "family": "Juuso",
                "affiliation": [
                    "University of Oulu Finland"
                ]
            },
            {
                "given": "Tapio ",
                "family": "Seppänen",
                "affiliation": [
                    "University of Oulu Finland"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-03",
        "keywords": [
            "multimodal data",
            "corpora",
            "tools"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "AbstractThis paper proposes to use citation networks as a tool for mapping out links among disciplines and research areas in & around humanities. By collecting papers from main journals in diverse areas like cognitive science, art history and psychoanalysis, this study aims to build a small citation network that will be visualized and published as a 3d web page. The end-map will be the backbone of a dissertation, which tries to analyze the influence of various movements and disciplines onto the art historical canon. Used as such, a citation network can become more than a frozen map that has more than a one-time use value.Access to online electronic databases and tools such as Google scholar have led to a significant improvement in the discovery of secondary literature. However, organizing the vast amount of bibliographic information from various discipline-specific databases continues to be an impediment to truly interdisciplinary work. My project attempts to construct virtual maps of electronic databases or digital libraries capable of providing scholars with significant links between disciplinary relations, interdisciplinary research areas, or tendencies, approaches and methodologies inside a single discipline. Moreover, as I will show, if the third dimension, namely history, is added to these maps, the transformation of the disciplines, the merging of research areas, and the changes in the taxonomic structure of the academy will be revealed to the expert eye. As an example of how such a virtual time-map could become a valuable analytic tool during the research process, I will construct such a tool and, using my dissertation as a test case, demonstrate how it can be applied to map intersections between art historiography, psychoanalysis and cognitive science.As a PhD candidate in UCLA's Art History program, my dissertation traces the changes of the critical discourse from the 1970s, when the so-called \"New Art History\" clashed with traditional art historians, and gave rise to a whole new approach—one that has now become known as Visual Cultural Studies. However, in such a broad context, one needs to handle a deluge of texts and interrelations. A simple timeline, a linear outline constituted of chapters and subchapters is not enough to depict the map of overlapping layers, concepts and relations between opposite but—in this case still—allied methodologies. In order to render all these visible, I would like to create a multidimensional space of such relations. To that end I propose to collect significant papers, extract citation information using various text-analysis programs and visualize the end results as a citation network that runs along three different trajectories, namely the history of psychoanalytical and cognitive scientific methodologies and their impact on the evolution of New Art History into Visual Cultural Studies.History:A relatively new research venue, called ‘scientometrics’ or ‘bibliometrics’, specializes in creating such maps for delineating growth, relations and interactions in scientific fields.To read more on the history of scientometrics see Katy Börner, Jeegar T. Marus, and Robert L. Goldstone. The Simultaneous Evolution Of Author And Paper Networks, PNAS 101 (suppl.1) (2004): 5266-5273. Bibliometrics uses text analysis to extract citation data from papers and makes use of this data as a way of evaluating scientific publications. To obtain information from scientific papers and using he results in mapping out scientific relations has a long history. As early as 1964 Garfield and his colleagues suggested using citation data to evaluate the development of science. All these search engines providing citation index information are products of Thomson ISI. The original foundation was called simply Institute for Scientific Information. Garfield launched it, again in 1964, see ibid. From 80’s on, the research in this area accelerated with the advancement of computers and various combinations of statistical methods used to extract and evaluate information such as citations, co- citations with reference of various bibliometric data. The end-results are usually rendered as so-called ‘citation networks’ which are a variation of social networks.Doreian Patrick, A Measure Of Standing Of Journals In Stratified Networks, Journal of the American Society for lnformation Science 8.5-6 (1985): 341-363. Now it is a common practice to evaluate a scholar or a journal according to how many times it/he/she is cited. Moreover, there is so much research done using citation networks as a methodology to analyze the disciplines, many scholars are now questioning the efficacy of this approach.See R. E. Rice et al., Journal-To-Journal Citation Data, Scientometrics Vol. 15.3-4 (1989): 257-282; Lindsey D, Using Citation Counts As A Measure Of Quality In Science Measuring What's Measurable Rather Than What's Valid, Scientometrics 15.3-4 (1989): 189-203; Nederhof, A. J., and Zwaan, R. A. Quality Judgments of Journals as Indicators of Research Performance in the Humanities and the Social and Behavioral Sciences, Journal of the American Society for Information Science 42.5 (1991): 332– 40; Nederhof Anton J., Bibliometric Monitoring Of Research Performance In The Social Sciences And The Humanities: A Review, Scientometrics 66.1 (2006): 81-100; Leydesdorff Loet, Can Scientific Journals Be Classified in Terms of Aggregated Journal-Journal Citation Relations Using the Journal Citation Reports?, Journal Of The American Society For Information Science And Technology (March 2006): 601-614. Among the ample publications in this area, two general approaches distinguish themselves: the citation networks are either built to support an idea or to enhance the way in which such networks are composed. In the first instance, a search is done to filter out unnecessary papers. The maps generated in this way are limited with a scope, and generally give an overview of the research topic. These maps are not created from a relational database and are not published for further use. That means they are not applicable to other research questions and the enormous work put into collecting papers and preparing them for network analysis is done on a case-by-case basis because access to the databases is restricted and therefore the datasets themselves cannot be made available. In the second instance papers are extracted from the electronic database without any filtering. Instead a time limit is imposed. These types of publications tend to focus more on the technical details and explore the mathematical substructure of social networks. Usually open-source databases are preferred, since the main idea is to test the application’s performance on huge datasets. Papers about such studies do not interpret the resulting map and instead detail the mathematical innovation of the applications.Despite the popularity of this approach in the sciences, I have yet to find a paper that uses Humanities databases. Even the most comprehensive citation network, a data set that encompasses \"7,121 journals covering over 1 million documents in the combined Science Citation and Social Science Citation Indexes” does not delve into the Arts and Humanities Citation Index.Boyack, Kevin W., Klavans Richard, Börner Katy, Mapping The Backbone Of Science, Scientometrics 64.3 (2005): 351.374  Rather than simply applying the same techniques to Humanities materials, a fresh approach, one that is not only suits to humanities scholarship, but addresses some of the issues raised in the scientific communities seems desirable.Even though the end results of the “citation networks” give a scholar a good overview, they are still not integrated into the academic research facilities like Web of Science, Science Citation Index, Social Citation Index or Arts & Humanities Citation Index etc. The main reason is that once a citation network is derived, it becomes a static entity; it covers a limited time and scope. Thus citation networks, by their very definitions and aims, fail to keep up with new publications.  Although my long-term goal is to create a dynamic citation network that can become a part of an ongoing research project in digital humanities, in this paper I will focus on creating a static citation network using open-source tools. My research plan to achieve this aim is as follows: Collecting papers: I have already collected around 2000 papers, mostly from prominent journals in Art History (such as Art Bulletin, October, Art Journal and Leonardo), and in Cognitive Science (Trends in Cognitive Sciences). Beside these resources I would like to include the classical texts in psychoanalysis; which can be found in electronic format in the Psychoanalytic Electronic Publishing database. The main criterion used for selecting pertinent papers is to use keywords that are relevant for the research topic. For example I used keywords such as \"Freud\", \"psychoanalysis\", \"aesthetic\", \"artist\" while searching in a cognitive science journal whereas I chose keywords like ‘cognition\", \"cognitive science\", \"vision\", \"artist\" etc. for the search in the domain of psychoanalysis. Preprocessing the database for text analysis & extraction the needed information: The acquired corpus is preprocessed for text mining and analysis. Preparing a list of keywords and bibliometric data (author name, date, journal name, title, etc.) will be enough for the preprocessing stage. We will experiment with different text- mining and text analysis programs and report on those that work well at this stage of data preparation.  Construction of social networks: A social network is a graph representation of social relations. Graphs are the most popular and widely researched data structure for representing and processing relational data. In a graph, each node represents one entity (a person in a social network; a researcher or a work in a citation network) and the edges (or arcs, if they are directed) of the graph represent some relation. One can also indicate the strength of the relation by associating weights with the edges of the graph. Then, by using tools like PajekYou can find more information about Pajek, its history  and application areas at , last accessed 2006-11-15., the graph nodes can be placed in a 3D space in such a way to minimize an energy term. Thus, the nodes that are close to each other semantically (through the interpretation of graph edges) are placed in proximity, even when they don't have actual links. On a social network, clusters and cliques can be identified, indirect relations can be uncovered, and relevance judgments can be made based on quantitative or qualitative measures. Even the location of the nodes (center vs. periphery) can be informative for a person thoroughly acquainted with the represented structure. The use of such a graph tool is simple; nodes and arcs are read from a file, and the graph visualization is accomplished with a few commands. For a citation or semantic network, text mining tools can be employed to derive the entities in relation automatically. Once such a network is built, Pajek can import the graph in 3D file formats like 3xd and VRML; both are now becoming standards for internet publication in 3D.Please check  to see the latest  standards of World Wide Web in relation to 3d publishing, last accessed 2006-11-15. ConclusionThe proposed tool serves not only as a bibliographic aid, but will become the main framework for my dissertation. By incorporating the 3D virtual map into my dissertation, I hope to demonstrate a new form of digital scholarship--one that springs from the new possibilities that digital technologies affords scholars in the humanities whose work is inherently, and exuberantly, interdisciplinary.",
        "article_title": "Citation Networks: A New Humanities Tool?",
        "authors": [
            {
                "given": "Almila ",
                "family": "Akdag",
                "affiliation": [
                    "Department of Art HistoryUCLA"
                ]
            },
            {
                "given": " Zoe ",
                "family": "Borovsky",
                "affiliation": [
                    "UDHIGUCLA"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "Pajek",
            "3d visualization and web publishing",
            "scientometrics",
            "citation networks"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "AbstractIn this paper, we discuss how researchers can benefit from tools that allow them to work with visualizations that rely on XML data at different levels of granularity. For these visualizations, we propose using a single interface that draws on the underlying structural information at both the collection level and the level of the contents of the individual documents. We compare two models of this kind of interactivity that are the subjects of our current interface design and prototyping activities. One of these systems is predicated on displays relying on sequence, and the other on visualizing the structure of items and facilitating their traversal.DiscussionThe common goal of these interface systems is to provide researchers with experimental means of combining overviews of a document with tools for manipulating the display. First is our multilevel document visualization system (Ruecker et al. 2005) that combines three or more simultaneous displays, including a microtext column showing collection items, a separate microtext column showing the contents of a particular document, and a reading view of the document. These different levels of display are combined with tools for selecting and manipulating a portion of the text in a subsequent display.The purpose of the multilevel system is to allow the user to work with a digital document within the visual context of related material. We have currently installed it as a feature in the prototype for watching digital scripts, where the standard reading view is supplemented with a digital stage that allows the reader to see both the dynamic text playback and the blocking of the characters as they move around the stage. We are currently carrying out a user study of actors using the system to learn their lines.In addition to the dynamic text, we have recently begun to consider the possibilities of using the microtext columns as opportunities for providing overviews that can provide further data. For example, they might be colour-coded and re-organized according to some useful principle. In the Watching the Script demo, one form of colour-coding currently available – for plays that have been encoded in XML that marks the character names – is to help the reader differentiate between characters.For purposes of a director planning the play, this overview allows exploration of the various relationships of the characters on the stage. It can be use to address questions such as who is on stage when, and with who, and how many lines does each actor have? By selectively applying colour only to the character or characters of interest, and having the remainder display in regular black text, the director has a tool for studying the entire play from the perspective of the staging.Another overview panel in the prototype provides a list of the characters. As a simple display it does not provide many affordances, but with the addition of some further information and the ability to sort, this list also has potential benefits. For example, if we give the user the opportunity of adding next to each character the number of lines that character speaks, or a total of time spent on stage, or both, and then rearranging the characters according to those numbers, we have another tool for use in understanding the play at an overview level, and for planning production. Combined with the blocking tool, these overviews and their related tools begin to create a tool suite that we hope will prove useful to actors, directors, and students of theatre.The second prototype produces what we are calling a Document Blueprint: a compact visual representation of the markup in a document. This blueprint can be used to suggest encoding particularities of a document by colour-coding tags and attributes in flexible ways. The identity – and therefore colour-coding – of sections can be defined in ways that ignore certain nodes in an XML tree, including elements, attributes, and text.Our first application of this system is to generate a document view where the table of contents is initially displayed in a legible font size and the markup of sections are displayed in small, blueprint mode (where visual cues from the text emerge but the text itself is too small to be legible). The Document Blueprint system allows the user to define which elements to toggle between legible and blueprinting modes. Depending on which elements the user chooses, the system provides a variety of overviews that can help both in understanding and navigating the document, much as the conventional table of contents has always done, but with the important differences of dynamic display and interactivity (Ruecker 2005).Future DirectionsFor the multilevel display, we are currently working to extend the affordances of the various overview items. It may be useful, for instance, to allow the entire surface to be temporarily subsumed by the overview that currently only occupies one column. The more complex display would show the data, still not at a reading view, but at a smaller level of granularity. Colour-coding and reorganizing this form of display could prove both interesting and useful.The document blueprinting project is the youngest of our prototype projects and will likely evolve the most in the coming year. We are developing this system in collaboration with the Orlando Project, which in its next phase will be producing literary critical material in volume form. The dynamic table of contents in this case will be used as a navigation aide for readers interested in browsing through the extended prose of the three volumes. As has been the case with each of the prototypes, we will pursue an iterative cycle of design, prototyping, and development, with user study and experiment at each stage of the cycle.",
        "article_title": "Multilevel Displays and Document Blueprints: Dynamic Browsing Using XML Structures and Text Features",
        "authors": [
            {
                "given": "Stan ",
                "family": "Ruecker",
                "affiliation": [
                    "University of Alberta"
                ]
            },
            {
                "given": "Stéfan ",
                "family": "Sinclair",
                "affiliation": [
                    "McMaster University"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "text visualization",
            "human-computer interaction",
            "interface design",
            "rich-prospect browsing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "As questions and concerns about digital preservation and sustainability become increasingly audible in the spheres of digital humanities and humanities computing, the necessity to build strong ties between digital humanities and digital libraries only intensifies. Howard Besser, in his essay, The Past, Present, and Future of Digital Libraries,” underscores the connections between these two fields. He explains digital libraries not only “provide access to original source material, contextualization, and commentaries, but they also provide a set of additional resources and service”.From Besser’s essay, “The Past, Present, and Future of Digital Libraries” in  A Companion to Digital Humanities, 2004, p. 557. Besser then delineates some of these resources and services, including digital collections of traditional print materials, lexical analysis, and increased accessibility. In addition to these important contributions to humanities research, I would like to highlight the emerging role libraries play in processing, preserving, maintaining, and providing access to important archives that house born-digital content. This role will not only contribute to humanities research in decades to come, but will also impact how research is performed by directing what content is made available and how researches may access it.In this paper, I will examine the Emory University Libraries’ acquisition and processing of a singular personal archive as a case study to explore the methods and practices of handling born-digital archival materials and the implications such methodologies and their outcomes may have on humanities research.Emory University’s acquisition of Salman Rushdie’s personal archive represents an important addition to the Manuscript and Rare Books Library, and contributes significantly to the University’s digital library resources and research. Rushdie’s rich personal archive includes traditional manuscript materials such as journals, personal correspondence, and notebooks, as well as less traditional archival materials, namely a series of personal computers that cover a significant span in his personal and literary life. This digital archive includes five computers, one early Macintosh desktop and four Macintosh laptops, including both obsolete and current models. While MARBL has previously acquired collections containing some digital materials, Rushdie’s computers represent the first significant, sizeable digital component to the University’s extensive holdings of rare and unique materials.Such an acquisition requires archivists to engage with technologists to ensure that the library can most effectively serve current and future researchers and scholars. The curation of such an archive raises important questions about how libraries should process, index, and present these materials while simultaneously addressing preservation and authenticity concerns. Such questions include: What is the research value of such an archive? How important is the physical artifact? Do researchers need access to exact systems emulation? Is providing search and browse access to the data sufficient or will researchers be interested in Rushdie’s original directory structure? Once data is migrated from the original environments, do we continue to maintain those outdated systems? How do archives sustain both master and access instances of born-digital archives?As digital librarians and archivists at Emory begin processing the born-digital components of this important archive, they must keep these questions, and the host of secondary concerns circulating around them, in the foreground of workflow and process discussions. In this paper, I will argue for the importance of balancing the urgent needs of data and system stabilization with the more long-term challenges of considering the ideal outcomes and products of processing and providing access to a rare and unique born-digital archive. This talk will track the early stages of processing the physical and digital materials comprising Rushdie’s digital archive, outline approaches to handling the more complex processing requirements, discuss proposed approaches to presenting the archive to both local and distant researchers, and generalize observations drawn from the experiences with this born-digital archive to broader implications within digital libraries, digital curation, and humanities computing.With the acquisition of any archive, a research library takes on multiple responsibilities to preserve, index, and provide access to the rare and unique materials. Libraries—digital, brick and mortar, or otherwise—must “incorporate the component of stewardship over a collection.”Besser, p. 559. Such stewardship carries with it important responsibilities, especially in cases of archival materials. Thus, with the arrival of the first shipment of Rushdie’s digital archive, consisting of three out of the five computers, our library and archive staff faced both immediate preservation demands and distant challenges for archival curation. We elected to produce a workflow that is both staged and modular, which I will only summarize here. The first task is to provide a secure and stable environment for the machines themselves. As our archives had not previously included technological artifacts, this first step required some intensive space and environment analysis. Once we stabilized the physical objects, this born digital archive next challenged staff with questions of data recovery, data preservation, and data duplication. Such challenges prompted us to develop a partitioned data architecture that duplicates and preserves all master data. The original is preserved, a master duplicate is generated and stored darkly, a duplicate collection of the master database is housed in a secure repository for in-house processing and staging, and, finally, a fully-processed instance of the data is made available through a production database. Such an approach provides for preservation of original artifacts and master data, while ensuring a level of security for data while it is being processed for embargoed material.In addition to preservation and security, authenticity of the archived data is of particular importance to archivists, digital librarians, and humanities researchers. Graham Barwell’s discussions about originality and authenticity within the fields of textual studies and electronic textuality resonate with archives such as Rushdie’s born-digital materials.Graham Barwell, “Original, Authentic, Copy: Conceptual Issues in Digital Texts” in  Literary & Linguistic Computing 20.4 (1995) see pages 416, 418, and 419. How can we most authentically represent the digital archives included in the Rushdie collection? Is the data the only component of real research value or is the context that holds this data, the paratextual elements, if you will, of equal importance to researchers? I will explore these questions and provide illustrations from our processing of the Rushdie archive to offer some preliminary insights. ",
        "article_title": "Rushdie's Computers: Born-Digital Archives and Humanities Scholarship",
        "authors": [
            {
                "given": "Erika Leigh ",
                "family": "Farr",
                "affiliation": [
                    "Emory University"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "digital libraries",
            "archives",
            "digital curation"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "In his intriguing “All the Way Through: Testing for Authorship in Different Frequency Strata”, John F. Burrows follows up his much-discussed Delta (Burrows, 2002a, 2002b, 2003; Hoover 2004a, 2004b, 2006) with two new measures of textual difference: Zeta and Iota (Burrows 2006; see also Burrows 2005).Both measures begin with a full word frequency list for a sample of Restoration poetry (approximately 20,000 words) by a single primary author. The sample is then divided into five sections of equal size, and word frequency lists are created for them. Zeta deals with words of moderate frequency, words occurring in at least three of the five sections. To compare two poets, the word list is reduced further by removing any occurring more than three times in the second poet’s sample. Where many authors are being compared, the list is reduced by removing any words present in the text samples of most of the other authors. Both methods remove from consideration the most frequent words of English that have been the focus of so much recent work. Whether there are two or many authors, the result is a list of words that are moderately frequent in the primary author but much less frequent in the other author(s).For Iota, the word list is first limited to words appearing in at most two of the primary author’s sections. To compare two authors, the list is further limited to words that are completely absent from the second author’s sample. Where many authors are being compared, the list is further reduced by removing words that appear in more than half the other authors. In either case, very frequent and moderately frequent words are eliminated, leaving words that are not very frequent in the primary author but are rare or non-existent in the other author(s).Zeta and Iota are remarkably effective in attributing poems as short as 1,000 words to the correct authors. Even more important, they allow the analyst to concentrate on a relatively small subset of characteristic words, nearly all content words. These lead back to the text and to important questions of interpretation and style.Both Zeta and Iota will require further testing before they can be confidently applied to genuine questions of authorship and style, and we can begin with a study of twentieth-century poetry. For these tests my corpus consists of samples of 14,000 to 129,000 words of poetry by twenty-six poets as the primary set and fifty-six independent poems from 900 to 21,000 words long as the secondary set, thirty-six of these by poets in the primary set and twenty by other poets (poems by primary authors are removed from their main samples). The texts were downloaded from Chadwyck-Healey’s Literature Online and edited to regularize hyphens and to remove prose sections and non-authorial text, such as publication information, notes, section numbers, epigrams and other quotations. Delta tests were used to determine which of the poems and poets are most difficult to attribute, and these were analyzed using Zeta and Iota.My head-to-head tests of Wallace Stevens vs Archibald MacLeish and Edwin Arlington Robinson vs Robert Frost give even more definitive results than Burrows achieves for Marvell vs Waller, though my much larger samples require minor adjustments in technique). The new measures have no difficulty distinguishing the two poets, whichever poet’s word list is used.When Burrows tests Marvell and Waller (using each poet’s own primary word list) against the samples and twenty-four independent poems by twenty-four other main authors and twenty-one poems by other authors, Iota works very well for both authors, and Zeta works based on Waller’s list. Marvell’s list produces a group of failures which Burrows suggests are likely to be result from the contrast between the political satires being tested and the largely pastoral nature of most of Marvell’s poetry.My tests using the primary word lists of eight different authors yields strong results for Zeta on James Dickey, Vachel Lindsay, Robert Frost, and Wallace Stevens, with all of their individual poems ranking higher than any poem or sample by any other author. Zeta is very successful on two of William Vaughn Moody’s independent poems, but the third ranks far below other author samples and individual poems, and it fails badly when based on the word lists of Edwin Arlington Robinson, Kenneth Rexroth, and Archibald MacLeish. For Iota, only the lists of Stevens and Lindsay produce completely correct results, though those of Moody and Rexroth produce good results except for a single poem by each author. Further research into the causes of these poorer results is underway, but the problems with Iota may be related to my larger samples. (The definition of “rare” is obviously very different for 20,000-word samples and 120,000-word samples.)One alteration of Zeta that produces perfect results using Robinson’s word list not only limits the word list to words that appear in at least three of the author’s five sections, but also sets a lower limit on the word’s frequency in the main set and limits the total frequency of the word in the twenty-five counter-sets. Another, still under investigation, calculates the standard deviation of the word’s frequency in the five base sections and divides it by the mean frequency. Sorting the word list on the resulting Coefficient of Variation (or Relative Standard Deviation) makes it easy to limit the list to words that appear at relatively consistent frequencies in the five sections, besides appearing in at least three of them and in a limited number of the counter-set samples. A word appearing five times in each of a poet’s five sections seems intuitively more “characteristic” of the author than one appearing twenty-three times in one section and once in each of two others.Whatever the outcome of further testing and modification may be, Zeta, especially, is very effective in focusing attention on a poet’s characteristic words, a useful task in its own right. In head-to-head tests on MacLeish and Stevens, much more stringent stipulations than Burrows used produce fascinating results: the twenty-six words occurring in all five sections of MacLeish’s sample but with a frequency less than three in Stevens’s sample are good potential MacLeish authorship markers, and the same stipulations produce forty potential Stevens authorship markers. These words range in rank within their word lists from about the 200th to the 1,400th most frequent. These two sets of marker words return our attention to the texts:Characteristic Stevens words rare in MacLeish: reality, except, centre, element, colors, solitude, possible, ideas, hymns, essential, imagined, nothingness, crown, inhuman, motions, regard, sovereign, chaos, genius, glittering, lesser, singular, alike, archaic, luminous, phrases, casual, voluble, universal, autumnal, café, inner, reads, vivid, clearest, deeply, minor, perfection, relation, immaculateCharacteristic MacLeish words rare in Stevens: answered, knees, hope, ways, steep, pride, signs, lead, hurt, sea’s, sons, vanish, wife, earth’s, lifted, they’re, swing, valleys, fog, inland, catch, dragging, ragged, rope, strung, barkStevens’s words are longer and more abstract, especially the nouns, and his list is saturated with adjectives. MacLeish’s list has very few adjectives and more verbs and concrete nouns. Searching for marker words in each poet’s work yields a remarkable pair of short poems: MacLeish’s short poem “‘Dover Beach’–A Note to that Poem” (215 words) contains seven of his twenty-six marker words, including the three italicized in this brief passage:. . . It’s a fine and aWild smother to vanish in: pulling down---Tripping with outward ebb the urgent inward.Speaking alone for myself it’s the steep hill and theToppling lift of the young men I am toward now . . .In his even shorter poem, “From the Packet of Anacharsis” (144 words), six of Stevens’s forty marker words appear, including the three italicized in this brief passage (internal ellipsis in the original):And Bloom would see what Puvis did, protestAnd speak of the floridest reality . . .In the punctual centre of all circles whiteStands truly. The circles nearest to it shareIts color . . .Preventing the huge numbers of items being analyzed from masking any meaningful results is one of the most difficult challenges for quantitative analyses of literature. By selecting for examination words that are particularly characteristic of an author, Zeta and Iota are potentially very useful for literary analysis as well as authorship attribution, no matter what further refinements they may require.",
        "article_title": "Zeta and Iota and Twentieth-Century American Poetry",
        "authors": [
            {
                "given": "David L. ",
                "family": "Hoover",
                "affiliation": [
                    "New York University"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "Zeta",
            "Iota",
            "Delta",
            "poetry"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Abstract:This paper describes the results of research carried out during the LAIRAH (Log analysis of Internet Resources in the Arts and Humanities) project () which is based at UCL’s School of Library Archive and Information Studies. It was a fifteen month study (reporting in October 2006) to discover what influences the long-term sustainability and use of digital resources in the humanities through the analysis and evaluation of real-time use.At Digital Humanities 2006 we reported on the early stages of the project, in which we carried out deep log analysis of the AHDS and Humbul portals to determine the level of use of digital resources. (Warwick et al. 2006) This proposal will discuss the results of the final phase of the research in which we examined digital resources from the point of view of those who designed and built them. We aimed to discover whether there were common characteristics and elements of good practice linking resources that are well- used.Numerous studies have been carried out into the information needs and information seeking practices of humanities scholars (Barrett, (2005) Talja and Maula (2003), Herman (2001) and British Academy, (2005)). However, our research is original because it surveys the practices of those who produce digital humanities resources. We also based the selection of our projects on deep log analysis: a quantitative technique which has not previously been applied to digital humanities resources to ascertain real usage levels of online digital resources.Method:We selected a sample of twenty one publicly funded projects with varying levels of use, covering different subject disciplines, to be studied in greater depth. We classified projects as well-used if the server log data from the Arts and Humanities Data Service (AHDS) and Humbul portals showed that they had been repeatedly and frequently accessed by a variety of users. We also mounted a questionnaire on these sites and asked which digital resources respondents found most useful. Although most nominated information resources, such as libraries, archives and reference collections for example the eDNB, three UK publicly funded research resources were mentioned, and thus we added them to the study. We also asked representatives of each AHDS centre to specify which resources in their collections they believed were most used. In the case of Sheffield University the logs showed that a large number of digital projects accessed were based at the Humanities Research Institute. We therefore conducted interviews about the HRI and its role in fostering the creation of digital humanities resources.The selected projects were studied in detail, including any documentation and reports that could be found on the project’s website. We also interviewed a representative of the project, either the principal investigator or a research assistant.Results:Institutional context:The majority of projects that we interviewed had been well supported in technical terms, and this had undoubtedly aided the success of the project, especially where it was associated with a centre of digital humanities excellence such as the Centre for Computing in the Humanities at Kings College London or the HRI at Sheffield. Critical mass aided the spread of good practice in the construction and use of digital resources in the humanities. Where a university valued such activities highly they tended to proliferate. More junior members of staff were inspired to undertake digital humanities research by the success of senior colleagues and early adopters respected for their traditional and digital research. Unfortunately such critical mass is relatively rare in UK universities and some PIs reported that their digital resource was not understood or valued by their departments, and thus their success had not lead to further digital research.Staffing:PIs also stressed how vital it had been to recruit the ideal RAs. These were however relatively difficult to find, as they had to have both disciplinary research expertise and good knowledge of digital techniques. Most RAs therefore required training, which many PIs often found lacking or of poor quality. A further frustration was the difficulty of finding funding to continue research, this meant that an expert RA might leave, necessitating further training of a new employee if the project was granted future funding.Dissemination:The strongest correlation between well-used projects and a specific activity was in the area of dissemination. In all the projects studied, staff had made determined efforts to disseminate information as widely as possible. This was a new challenge for many humanities academics, who were more used to writing books, marketed by their publishers. This might include giving papers at seminars and conferences both within the subject community and the digital humanities domain; sending out printed material; running workshops, and in the most unusual instance, the production of a tea-towel!User contact:Very few projects maintained contact with their users or undertook any organised user testing, and many did not have a clear idea how popular the resource was or what users were doing with it. However, one of the few projects that had been obliged to undertake user surveys by its funders was very well-used, and its PI had been delighted at the unexpected amount and range of its use. Another project came to the belated realisation that if it had consulted users the process of designing the resource would have been simpler and less demanding.Documentation:Few of the projects kept organised documentation, with the exception of those in archaeology, linguistics and archival studies, where such a practice is the norm in all research. Most projects had kept only fragmentary, internal documents, many of which would not be comprehensible to someone from outside. Documentation could also be difficult to access, with only a small minority of projects making this information available from its website. This is an important omission since documentation aids reuse of resources, and also provides vital contextual information amount its contents and the rationale for its construction that users need to reassure them about the quality of the resource for academic research.Sustainability:Another area of concern was the issue of sustainability. Although the resources were offered for deposit with the AHDS, few PIs were aware that to remain usable, both the web interface and the contents of the resource would require regular updating and maintenance, since users tend to distrust a web page that looks outdated. Yet in most cases no resources were available to perform such maintenance, and we learnt of one ten year old resource whose functionality had already been significantly degraded as a result.Conclusion and recommendationsWell-used projects do therefore share common features that predispose them to success. The effect of institutional and disciplinary culture in the construction of digital humanities projects was significant. We found that critical mass was vital, as was prestige within a university or the acceptance of digital methods in a subject. The importance of good project staff and the availability of technical support also proved vital. If a project as to be well-used it was also essential that information about it should be disseminated as widely as possible.Even amongst well-used projects, however we found areas that might be improved, these included organised user testing, the provision of and easy access to documentation and the lack of updating and maintenance of many resources.Recommendations:Documentation:Projects should keep documentation and make it available from the project web site, making clear the extent, provenance and selection methods of materials for the resource.Funding bodies might consider making documentation a compulsory deliverable of a funded project.Discussions could be held between relevant stakeholders and the funding bodies, with the aim of producing an agreed documentation template. This should specify what should be documented and to what level of detail.Users:Projects should have a clear idea of whom the expected users might be; consult them as soon as possible and maintain contact through the project via a dedicated email list , website feedback or other appropriate methodThey should carry out formal user surveys, software and interface tests and integrate the results into project design.Applicants for funding should show that they have consulted documentation of other relevant projects and discuss what they have learnt from it in their case for support. The results of such contact could then be included in the final report as a condition of satisfactory progress.Management:Projects should have access to good technical support, ideally from a centre of excellence in digital humanities.Projects should recruit staff who have both subject expertise and knowledge of digital humanities techniques, then train them in other specialist techniques as necessary.Funding bodies might consider requiring universities to offer more training for graduate students and RAs in digital humanities techniques.Sustainability:Ideally projects should maintain and actively update the interface, content and functionality of the resource, and not simply archive it with a data archive such as the AHDS. However this is dependent on a funding model which makes this possible.Dissemination:Disseminate information about itself widely, both within its own subject domain and in digital humanities.Information should be disseminated widely about the reasons for user testing and its benefits, for example via AHRC/AHDS workshops. Projects should be encouraged to collaborate with experts on user behaviour.Acknowledgements:This project was funded by the Arts and Humanities Research Council ICT Strategy Scheme. We would also like to thank all of out interviewees for agreeing to talk to us.",
        "article_title": "The Master Builders: LAIRAH Research on Good Practice in the Construction of Digital Humanities Projects",
        "authors": [
            {
                "given": "Claire ",
                "family": "Warwick",
                "affiliation": [
                    "School of Library, Archive and Information StudiesUniversity College London"
                ]
            },
            {
                "given": "Melissa ",
                "family": "Terras",
                "affiliation": [
                    "School of Library, Archive and Information StudiesUniversity College London"
                ]
            },
            {
                "given": "Paul ",
                "family": "Huntington",
                "affiliation": [
                    "School of Library, Archive and Information StudiesUniversity College London"
                ]
            },
            {
                "given": "Nikoleta ",
                "family": "Pappa",
                "affiliation": [
                    "School of Library, Archive and Information StudiesUniversity College London"
                ]
            },
            {
                "given": "Isabel ",
                "family": "Galina",
                "affiliation": [
                    "School of Library, Archive and Information StudiesUniversity College London"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "good practice",
            "users",
            "producers",
            "log analysis",
            "recommendations"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "In the mid-1960s Joseph Weizenbaum created a stunning piece of software. Years before HAL 9000's screen debut in 2001: A Space Odyssey, this software, Eliza, made it possible to have a conversation with a computer. Eliza's most famous script, Doctor, caused the software to parody the conversational patterns of non-directive therapists during an initial visit. While Eliza/Doctor can seem quite smart at first blush, each script for Eliza is actually just a set of linguistic tricks. Most of these tricks use keyword-driven \"decomposition rules\" to take the user's last statement, divide it into pieces, and selectively reuse portions to rephrase it as a question. But when we interact with a piece of software we don't necessarily get a clear picture of how it actually operates internally. And many users of Eliza/Doctor initially developed very mistaken ideas about its internals. Weizenbaum (1976) discusses users who assumed that, since the surface appearance of an interaction with the program could resemble something like a coherent dialogue, internally the software must be very complex. Some at first thought it must be something close to the fictional HAL: a computer program intelligent enough to understand and produce arbitrary human language. This happened so often, and was so striking, that computer science circles developed a specific term for this kind of misunderstanding: \"the Eliza effect.\"  This paper is a brief look at the Eliza effect, and at two previously-unnamed effects that can arise in the relationship between the surface appearance of a digital system and its internal operations. More specifically, this paper looks where others haven't when exploring versions of this relationship: the area of play.While the initial experience of Eliza/Doctor can create the surface impression of an incredibly complex internal system, sustained interaction with the system, the verbal back-and-forth, invites play ... and linguistic play with Eliza/Doctor quickly begins to destroy the illusion. In other words, precisely the open-ended textual interaction that helped foster the illusion of internal complexity and intelligence enables play that draws attention to the system's rote simplicity, its distance from human interaction.On the other hand, a sort of inverse of the Eliza effect can be seen with James Meehan's 1976 Tale-Spin, the first major story generation program. Tale-Spin generates stories from rules for character behavior and a set of facts about the virtual world. When generating stories in interaction with an audience it asks questions to fill in details about locations, objects, relationships, and so on. In addition, internal Tale-Spin mechanisms draw \"inferences\" from the facts. For example, if it is asserted that a character is thirsty, then the inference mechanisms result in the character knowing she is thirsty, forming the goal of not being thirsty, forming a plan for reaching her goal, etc. Further, Tale-Spin characters can use its inference mechanisms to \"speculate\" about the results of different courses of action. Meehan's The Metanovel (1976) describes a story involving such speculation, in which a hungry Arthur Bear asks George Bird to tell him the location of some honey. We learn that George believes that Arthur trusts him, and that Arthur will believe whatever he says. So George begins to use the Tale-Spin inference mechanisms to \"imagine\" other possible worlds in which Arthur believes there is honey somewhere. George draws four inferences from this, and then he follows the inferences from each of those inferences, but he doesn't find what he's after. In none of the possible worlds about which he's speculated is he any happier or less happy than he is now. Seeing no advantage in the situation for himself, he decides, based on his fundamental personality, to answer. Specifically, he decides to lie. This is a relatively complex piece of psychological action, and certainly tells us something about George as a character. But the surface output of a Tale-Spin story never contains any information about this kind of action. No matter how creatively one plays with Tale-Spin, such hidden action cannot be deduced from its surface. This is probably why, though Tale-Spin is seen as a landmark in computer science circles, it is often treated with near-ridicule in literary circles. Janet Murray, Espen Aarseth, Jay David Bolter, and other critics have failed to see what makes Tale-Spin interesting, focusing instead on what its output looks like on the surface. Or, to put it another way, Tale-Spin fails to display its interesting internal processes in a manner that makes them visible to even the most careful of critics. This situation is far from uncommon in digital media, perhaps particularly in the digital arts, where fascinating processes  —  drawing on inspirations ranging from John Cage to the cutting edge of computer science  —  are often encased in an opaque surface. In fact, this effect is at least as common as the Eliza effect, though I know of no term that describes it. Given this, I propose \"the Tale-Spin effect\" as a term for works that appear, on their surface, significantly less complex than they are internally. An effect quite different from both of these can be seen in the case of Will Wright's 1989 game SimCity. The seed for this project was planted as Wright created a landscape editor for authoring his first game, an attack helicopter simulation. Working with the editor, he realized he was having more fun making virtual spaces than blowing them up. From this the idea for Wright's genre-defining SimCity was born. SimCity, of course, unlike a terrain editor, doesn't simply wait for a user to do something. Time begins passing the moment a new city is founded. A status bar tells the player what's needed next  —  starting with basic needs like a residential zone and a power plant and, if play succeeds for any period, ramping up to railroads, police stations, stadiums, and so on. As cities grow, areas respond differently. Some may be bustling while others empty out, or never attract interest. SimCity provides different map views that can help diagnose problems with abandoned areas. Players can try changing existing areas of the city (e.g., building additional roads) or create new areas with different characteristics. Observation and comparison offer insights, while answers are found by trying different approaches and considering the results. In other words, the process of play with SimCity is one of learning to understand the system's operations. Conversely, as Wright explains, the challenge of game design is to create a surface experience that will make it possible for audiences to build up an appropriate model of the system internals. Here, again, we lack a term for an experience. I propose \"the SimCity effect\" for this important phenomenon: a system that, through play, brings the player to an accurate understanding of the system's internal operations. Of course, the SimCity effect is named for cases where the system is complex, but the phenomenon can be observed generally. Pong works as well as it does because it effectively communicates at the surface level its quite simple internal operations. What is exciting about the SimCity effect, and about Wright's work generally, is that it helps us get at the new possibilities opened by working with computational media. Pong is very similar to games we play without computers, but SimCity is a more complex system than even the most die-hard Avalon Hill fan would want to play as a tabletop game. This ability to work with computational processes, to create complex computational systems, is the opportunity that digital media affords  —  and the SimCity effect points the way toward creating experiences of this sort that succeed for audiences.",
        "article_title": "Three Play Effects: Eliza, Tale-Spin, and SimCity",
        "authors": [
            {
                "given": "Noah ",
                "family": "Wardrip-Fruin",
                "affiliation": [
                    "University of California San Diego"
                ]
            }
        ],
        "publisher": null,
        "date": "2007-04",
        "keywords": [
            "games",
            "design",
            "play",
            "digital literature",
            "interfaces"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    }
]