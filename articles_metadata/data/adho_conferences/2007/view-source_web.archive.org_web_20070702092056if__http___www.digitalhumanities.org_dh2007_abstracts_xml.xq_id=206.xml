<?xml version="1.0" encoding="UTF-8"?>
<TEI.2 id="paper_206_mcgrath"><teiHeader><fileDesc><titleStmt><title>A Flexible System for Text Analysis with Semantic Networks</title><author><name reg="Auvil, Loretta">Loretta Auvil</name></author><author><name reg="Grois, Eugene">Eugene Grois</name></author><author><name reg="Llorà, Xavier">Xavier Llorà</name>name&gt;
                </author><author><name reg="Pape, Greg">Greg Pape</name></author><author><name reg="Goren, Vered">Vered Goren</name></author><author><name reg="Sanders, Barry">Barry Sanders</name></author><author><name reg="Acs, Bernie">Bernie Acs</name></author><author><name reg="McGrath, Robert Edward">Robert Edward McGrath</name></author><respStmt><resp>Marked up by</resp><name reg="Field, Hana S.">Hana S. Field</name></respStmt></titleStmt><publicationStmt><p>Marked up to be included in the Digital Humanities 2007 Conference Abstracts
                    book.</p></publicationStmt><sourceDesc><p>None</p></sourceDesc></fileDesc><profileDesc><textClass><classCode>paper</classCode><keywords><list type="simple"><item>text analysis</item><item>automated learning</item><item>analysis environments</item><item>data mining</item></list></keywords></textClass></profileDesc><revisionDesc><list><item>HSF: Created from Robert McGrath's doc<date value="2007-03">March
                        2007</date></item></list></revisionDesc></teiHeader><text><front><docTitle n="A Flexible System for Text Analysis with Semantic Network"><titlePart type="main">A Flexible System for Text Analysis with Semantic
                Networks</titlePart></docTitle><docAuthor><name reg="Auvil, Loretta">Loretta Auvil </name><address><addrLine>lauvil@ncsa.uiuc.edu</addrLine></address></docAuthor><titlePart type="affil">National Center for Supercomputing Applications<lb/>University
                of Illinois at Urbana-Champaign</titlePart><docAuthor><name reg="Grois, Eugene">Eugene Grois</name><address><addrLine>egrois@gmail.com</addrLine></address></docAuthor><titlePart type="affil">National Center for Supercomputing Applications<lb/>University
                of Illinois at Urbana-Champaign</titlePart><docAuthor><name reg="Llorà, Xavier">Xavier Llorà</name><address><addrLine>xllora@illigal.ge.uiuc.eduu</addrLine></address></docAuthor><titlePart type="affil">University of Illinois at Urbana-Champaign</titlePart><docAuthor><name reg="Pape, Greg">Greg Pape</name><address><addrLine>gpape@gpape.com</addrLine></address></docAuthor><titlePart type="affil">National Center for Supercomputing Applications<lb/>University
                of Illinois at Urbana-Champaign</titlePart><docAuthor><name reg="Goren, Vered">Vered Goren</name><address><addrLine>vered@ncsa.uiuc.edu</addrLine></address></docAuthor><titlePart type="affil">National Center for Supercomputing Applications<lb/>University
                of Illinois at Urbana-Champaign</titlePart><docAuthor><name reg="Sanders, Barry">Barry Sanders</name><address><addrLine/></address></docAuthor><titlePart type="affil">National Center for Supercomputing Applications<lb/>University
                of Illinois at Urbana-Champaign</titlePart><docAuthor><name reg="Acs, Bernie">Bernie Acs</name><address><addrLine/></address></docAuthor><titlePart type="affil">National Center for Supercomputing Applications<lb/>University
                of Illinois at Urbana-Champaign</titlePart></front><body><div0><head>1. Introduction</head><p>The explosive growth of digital and digitized text creates opportunities for
                    scholars and students to conduct new analyses and develop unique insights about
                    our written culture and heritage. To effectively use large collections of
                    textual data, scholars and students need flexible, easy to use tools that
                    provide powerful analysis and visualization.</p></div0><div0><head>1.1 Goals</head><p>The Automated Learning Group is developing interactive tools for mining large,
                    complex semantic networks, which were automatically extracted from a text
                    corpus. The corpus could be a collection of documents or a stream of messages.
                    The semantic network represents the concepts in the collection as entities and
                    relations. The visual environment allows the user to query the semantic network
                    to retrieve portions of the graph. The display allows the user to view and
                    navigate these networks to discover patterns in the collection.</p></div0><div0><head>1.2 Technical Approach</head><p>Our approach to the visual analysis of text documents implements extraction and
                    visualization of important entities and the relations among them. This is
                    accomplished in the following high-level steps:<list type="ordered"><item>Perform lexical analysis of textual document set</item><item>Extract key entities and relations from the text</item><item>Compose entity-relation triples into semantic network model
                            summarizing key information in document corpus</item><item>“Publish” the semantic network in a repository</item><item>Visualize semantic network model as graph</item><item>Search and prune visual graph through appropriately structured
                            queries against semantic network model.</item></list></p></div0><div0><head>2 Text Analysis</head><p>Textual processing is accomplished by passing documents through a series of
                    syntactic, semantic, and functional analysis tools. These tools primarily
                    consist of D2K <note n="1"><xptr to="http://alg.ncsa.uiuc.edu" from="ROOT" targOrder="U"/></note>and T2K, our own analytic software, working in concert with GATE <note n="2"><xptr to="http://gate.ac.uk" from="ROOT" targOrder="U"/></note>and MontyLingua <note n="3"><xptr to="http://web.media.mit.edu/~hugo/montylingua" from="ROOT" targOrder="U"/></note>. These tools are seamlessly connected together into the D2K visual
                    programming environment</p><p>The GATE toolkit is utilized for standard syntactic processes including
                    tokenization, sentence splitting, and part-of-speech tagging. Syntactically
                    annotated documents are passed to GATE’s Named Entity (NE) tagger. The GATE NE
                    tagger identifies proper nouns in the text that belong to relevant categories
                    such as “Person”, “Organization”, and “Location.” NE tagging passes through
                    three modules that perform different kinds of co-reference<note n="2"><xptr to="http://gate.ac.uk" from="ROOT" targOrder="U"/></note>. The final output of the NE tagging and co-referencing processes are a
                    set of annotations that identify references to “Person”, “Organization”, and
                    “Location” entities.</p><p>After NE extraction, we identify key relations in the text using a tool within
                    MontyLingua. This Jister tool extracts sentence structures called “jists,” which
                    carry thematic-role information such as verb, subject, and object. This is
                    similar to semantic role labeling <note n="2"><xptr to="http://gate.ac.uk" from="ROOT" targOrder="U"/></note>, but less formally structured. An example jist:</p><lg><l>Tiger Woods wrapped up the tournament at noon.</l><l>(Verb: ‘wrap up’, Subj: ‘Tiger Woods’,</l><l>Obj1: ‘tournament’,Obj2: ‘at noon’)</l></lg><p>The next step involves the normalization of references in the MontyLingua jists
                    with entities identified in the analysis. For example, “Tiger Woods” may have
                    been identified as a entity:PERSON, and “tournament” referenced back to “Masters
                    Golf Tournament” appearing earlier in the text. Tagging the verb “wrap up” as an
                    entity:ACTION, and “at noon” as a TIME relation to the verb, we can generate
                    triples as follows:
                    <hi rend="code">&lt;Tiger Woods&gt; &lt;is-a&gt; &lt;entity:PERSON&gt;
                        &lt;Masters Golf Tournament&gt; &lt;is-a&gt; &lt;entity:EVENT&gt;
                            &lt;wrap up&gt; &lt;is-a&gt; &lt;entity:ACTION&gt;
                                &lt;Tiger Woods&gt; &lt;actor&gt; &lt;wrap up&gt;
                                    
                    </hi></p><p>Figure 1 shows the D2K toolkit with a view of the text processing itinerary. The
                    itinerary is a dataflow graph: the nodes in the figure are D2K modules (major
                    processing blocks), connected by edges representing the flow of data during
                    execution. The itinerary can exploit both data parallelism (multiple documents
                    at the same time) and task parallelism (different modules in parallel). The D2K
                    itinerary can be run on a desktop or scaled up to multiple computers or High
                    Performance Computer systems.Figure 1 shows the D2K toolkit with a view of the
                    text processing itinerary. The itinerary is a dataflow graph: the nodes in the
                    figure are D2K modules (major processing blocks), connected by edges
                    representing the flow of data during execution. The itinerary can exploit both
                    data parallelism (multiple documents at the same time) and task parallelism
                    (different modules in parallel). The D2K itinerary can be run on a desktop or
                    scaled up to multiple computers or High Performance Computer systems.</p></div0><div0><head>3 Semantic Network Storage and Retrieval</head><p>The triples generated from the semantic extraction process are stored in an RDF
                        <note n="5"><xptr to="http://www.w3.org/RDF" from="ROOT" targOrder="U"/></note>metadata store. We use Kowari, developed by Tucana Technologies <note n="6"><xptr to="http://kowari.org" from="ROOT" targOrder="U"/></note>. Additional triples are generated to represent metadata in conformance
                    with a common vocabulary, and user annotations can be included as well. Queries
                    are coded in an SQL-like query language called iTQL. The result is a set of
                    triples, which represents a semantic graph <note n="6"><xptr to="http://kowari.org" from="ROOT" targOrder="U"/></note>. </p><p>This architecture demonstrates a key design principle for robust
                    Cyberinfrastructure. The analysis is decoupled from the visualization, so that a
                    large scale analysis can asynchronously update the triples as new results are
                    computed, while interactive tools will automatically pick up the new data by
                    refreshing the query. The triples generated from the semantic extraction process
                    can be combined with many other similar relation triples from many sources, and
                    additional triples are generated to represent a common vocabulary.</p></div0><div0><head>4 Visual Investigation of Semantic Networks</head><p>Using the visual environment, investigators can perform searches over the
                    semantic networks extracted from a text corpus. The familiar web browser
                    paradigm was employed in the user interface design. The user interface allows
                    one to construct more complex queries by incorporating multiple rules and
                    filters. Each user query is converted into iTQL and executed by the Kowari
                        server<note n="6"><xptr to="http://kowari.org" from="ROOT" targOrder="U"/></note> . The query history is available as a pull-down menu, just as query
                    histories are in a web browser. Investigators can directly observe semantic
                    relationships between entities in an interactive link-node graph visualization.
                    (Figure 2) </p><p>Relations between entities in the resulting semantic network graph are displayed
                    as a link-node graph visualized using Prefuse <note n="7"><xptr to="http://prefuse.org" from="ROOT" targOrder="U"/></note>, and also as a hierarchical tree of entities conforming to the common
                    vocabulary. The subject and object entities in the relations are displayed as
                    nodes in the graph visualization, predicates are displayed as links. This
                    simplification of the more complex semantic network, stored in Kowari, provides
                    a compact and usable abstraction of the important relations extracted from the
                    text streams. </p><p>The Entity pane at the upper left displays lists of entities. The Relations pane
                    at the lower left displays a list of relations, with additional options to
                    highlight synonyms, antonyms, hyponyms or hypernyms based on WordNet. Selecting
                    an entity in the left pane also highlights the corresponding node or edge in the
                    visualization.</p><p>Every entity in the graph maintains a link back to the original text document
                    from which it was extracted. By right-clicking on a node, and selecting View
                    Source Documents, the text of the original document will be retrieved from the
                    repository and displayed.</p></div0><div0><head>5 Collections</head><p>This tool can be applied for many different types of text, across one or many
                    collections. In addition, it can analyze evolving collections of text, such as
                    documents from one or more RSS feed.</p><p>This tool has been used as part of the Nora project, a multi-institution
                    collaboration to produce software for discovering, visualizing, and exploring
                    significant patterns across large collections of full-text humanities resources
                    in existing digital libraries.<note n="8"><xptr to="http://www.noraproject.org" from="ROOT" targOrder="U"/></note></p><p> For example, we are experimenting with the digitized text of the novel <emph rend="bold">Uncle Tom’s Cabin</emph>, available as part of the Early American
                    Fiction Collection from the University of Virginia<note n="9"><xptr to="http://etext.virginia.edu/eaf/overview.html" from="ROOT" targOrder="U"/></note>. The system performs
                    feature extraction in order to determine shared characteristics of the selected
                    documents, such as chapters of the novel. The resulting arc-node graph can be
                    viewed and navigated to discover patterns and relations within and among the
                    texts. Figure 2 illustrates an example analysis of text from nineteenth century
                    novels, showing the use of concepts related to “Mother” and “Man”.<note n="10">Tom Horton, Kristen Taylor, Bei Yu and Xin Xiang, <title level="a">Quite Right, Dear and Interesting”: Seeking the Sentimental in
                        Nineteenth Century American Fiction</title><title level="j">Digital
                            Humanities 2006</title>, pp. 81-82 </note></p></div0><div0><head>6 Conclusion</head><p>This paper described interactive tools for mining large, complex semantic
                    networks automatically extracted from a text corpus. These approaches can be
                    applied to news feeds, technical literature, or literary collections.</p><p>We believe this is a promising approach, although we are only beginning to
                    develop these tools for use by humanists, who will be the ultimate judges of the
                    utility and validity of this approach. The general purpose semantic analysis is
                    widely applicable to many types of text, though it is difficult to predict the
                    impact of such analysis on our understanding of texts.</p><p>The query interface and graphical displays are still under development. The
                    entity-relationship graphs may be quite complicated, so we must find new visual
                    methods and metaphors to enable scholars and students to understand the
                    information in the graphs, and to use them formulate hypotheses, and answer
                    questions.</p></div0><div0><head>7 Acknowledgements</head><p>The Nora project is funded in part by the Mellon Foundation. This work was funded
                    in part by the National Center for Advanced Secure Systems Research (NCASSR) at
                    the University of Illinois at Urbana-Champaign (UIUC), a multi-institutional
                    cybersecurity research team. NCASSR is led by the National Center for
                    Supercomputing Applications (NCSA) and supported by funding from the Office of
                    Naval Research (ONR). Substantial portions of the code were implemented by David
                    Clutter and Fang Guo. Thanks to Patricia S. Taylor.</p></div0><figure rend="ImageLink"><head/><p><xref to="DITTO" from="ROOT" targOrder="U">paper_206_mcgrath_1.jpg</xref></p><figDesc>Figure 1: D2K Itinerary showing text processing.</figDesc></figure><figure rend="ImageLink"><head/><p><xref to="DITTO" from="ROOT" targOrder="U">paper_206_mcgrath_2.jpg</xref></p><figDesc>Figure 2: Visual interface showing result of a query.</figDesc></figure></body><back/></text></TEI.2>