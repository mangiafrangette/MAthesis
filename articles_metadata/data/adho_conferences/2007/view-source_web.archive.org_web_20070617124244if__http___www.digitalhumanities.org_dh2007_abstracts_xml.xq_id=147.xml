<?xml version="1.0" encoding="UTF-8"?>
<TEI.2 id="paper_147_jockers.xml"><teiHeader><fileDesc><titleStmt><title>Macro Analysis (2.0)</title><author><name reg="Jockers, Matthew">Matthew Jockers</name></author><respStmt><resp>Marked up by</resp><name reg="Schmidt, Sara A.">Sara A. Schmidt</name></respStmt></titleStmt><publicationStmt><p>Marked up to be included in the Digital Humanities 2007 Conference Abstracts
                book</p></publicationStmt><sourceDesc><p>None</p></sourceDesc></fileDesc><profileDesc><textClass><classCode>paper</classCode><keywords><list type="simple"><item>text analysis</item><item>tools</item><item>corpus</item><item>literary history</item></list></keywords></textClass></profileDesc><revisionDesc><list type="simple"><item>SAS: Created from Matthew Jockers's text<date value="2007-05">May
                2007</date></item></list></revisionDesc></teiHeader><text><front><docTitle n="Macro Analysis (2.0)"><titlePart type="main">Macro Analysis (2.0)</titlePart></docTitle><docAuthor><name reg="Jockers, Matthew">Matthew Jockers</name><address><addrLine>mjockers@stanford.edu</addrLine></address></docAuthor><titlePart type="affil">Stanford</titlePart></front><body><div0><p>At the 2005 meeting in Victoria, I presented a paper with the revised title,
                    “A Macro-Economic Model for Literary Research.” That paper marked an early phase
                    in a project aimed at leveraging large, digital, text corpora for what I now
                    call “Macro-Analysis.” The work presented in Victoria was largely a
                    proof-of-concept using an electronic bibliography of 775 works of Irish-American
                    literature. For that paper I performed a dumbed down sort of macro-analytic text
                    analysis using metadata fields and the titles of works in the bibliography.
                    Today the software has improved significantly, and in place of the title
                    analysis of 2005, this paper offers results derived from a macro-analysis of the
                    full text of 1125 British and American novels from the 19th century. In the
                    presentation, I provide a general overview of the tool(s) and an explanation of
                    the methodology employed in the analysis.</p><p>The tools and techniques I have develop utilize both supervised and unsupervised
                    text-mining techniques. The supervised techniques allow for a focused analysis
                    in which a researcher probes the corpus for items meeting a specific research
                    criteria. A very simple example might involve tracing the behavior or
                    “frequency” of some “signal” (linguistic pattern, literary theme, or author
                    style) over the course of the corpus. I should note here that while this process
                    sounds similar in some ways to the supervised machine learning approach being
                    used by the NORA project, it is specifically not like NORA in that I am not
                    employing machine learning or utilizing previous identified, “marked,” training
                    data. Instead the “signal” is developed ad hoc by the researcher/user.</p><p>An easy way to understand the project is to visualize the tool being used: A user
                    is given an interface that allows for the usual sort of corpus searching. The
                    user performs a corpus wide search for some term (or other feature such as a
                    word cluster or syntactical pattern). The result page reports all of the texts
                    in the corpus in which the search term(s) is found, and then the user is given a
                    “toolbox” of macro-analytic tools with which to process and analyze the result
                    set. These tools are varied and perform diverse sorts of analysis.</p><p>A topic-modeling tool, for example, provides the ability to harvest the salient
                    themes from the text in the result set. The user is thus able to say, for
                    example, that works in the corpus that contain word “x” show a predominance of
                    the "n" topic. In my own work with ethnic American literature, I have found this
                    tool valuable in assessing and quantifying the dominant themes that occur in
                    works where ethnic markers (words denoting race or ethnicity) occur. This
                    technique is derived from the work of David Newman and his research team at the
                    University of California—Irvine.<note n="1">Newman’s work is profiled on line at
                        the following sites: <xptr to="http://arstechnica.com/news.ars/post/20060802-7408.html" from="ROOT" targOrder="U"/><lb/><xptr to="http://www.ics.uci.edu/community/news/press/view_press?id=51" from="ROOT" targOrder="U"/></note></p><p>Another tool offers a type of literary time series analysis. Figure 1 shows a
                    graph produced by my “timeline” tool.</p><figure rend="ImageLink"><head/><p><xref to="DITTO" from="ROOT" targOrder="U">paper_147_jockers_1.jpg</xref></p><figDesc>Figure 1</figDesc></figure><p>With the timeline tool, the results of any query of the corpus can be mapped over
                    time. Say, for example, that I am interested in how some textual feature evolves
                    over time. I perform the necessary query to isolate the occurrences of that
                    feature and then choose the timeline tool. The resulting graph provides a visual
                    time-series analysis of the frequency of the pattern. The graph shown here was
                    produced after a simple search for occurrences of the word “romance” in the
                    titles of 7300 novels from the 18th and 19th century. The raw counts are
                    displayed in red beneath each year. In addition to providing this timeline of
                    raw hits (figure 1), a second graph (figure 2) is also produced that shows a
                    normalized result in the form of “hits-per-100” texts.</p><figure rend="ImageLink"><head/><p><xref to="DITTO" from="ROOT" targOrder="U">paper_147_jockers_2.jpg</xref></p><figDesc>Figure 2</figDesc></figure><p>In this case, the normalized graph is particularly revealing because it shows
                    that the frequency of “romance” as a key word in titles is not especially
                    noteworthy. Aside from a very brief period (1800-1810) where the word appears in
                    2-3% of all titles, its use is steady at 1% or 1 occurrence per 100 titles in a
                    given year.</p><p>The macro-analytic tools developed in this research exist as both command line
                    applications and as a (beta) extension to the open-source eXtensible Text
                    Framework (XTF) application developed by Martin Haye and the California Digital
                    Library.<note n="2">See
                        <xptr to="http://www.cdlib.org/inside/projects/xtf/" from="ROOT" targOrder="U"/></note> The successful implementation of the tools into XTF has
                    been achieved with the assistance of Stanford Undergraduate digital humanities
                    major Jenny Loomis who will spend the final five minutes of this presentation
                    giving a live demonstration of the tool as implemented into the XTF application.
                </p></div0></body></text></TEI.2>