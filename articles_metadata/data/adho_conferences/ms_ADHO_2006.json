[
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "1. Introduction In this paper, we describe the approaches taken by two teams of researchers to the identification of spelling variants. Each team is working on a different language (English and German) but both are using historical texts from much the same time period (17th – 19th century). The approaches differ in a number of other respects, for example we can draw a distinction between two types of context rules: in the German system, context rules operate at the level of individual letters and represent constraints on candidate letter replacements or n-graphs; in the English system, contextual rules operate at the level of words and provide clues to detect real-word spelling variants i.e. ‘then’ used instead of ‘than’. However, we noticed an overlap between the types of issues that we need to address for both English and German and also a similarity between the letter replacement patterns found in the two languages. The aims of the research described in this paper are to compare manual and automatic techniques for the development of letter replacement heuristics in German and English, to determine the overlap between the heuristics and depending on the extent of the overlap, to assess whether it is possible to develop a generic spelling detection tool for Indo-European languages (of which English and German are examples). As a starting point, we have manually-built letter replacement rules for English and German. We will compare these as a means of highlighting the similarity between them. We will describe machine learning approaches developed by the German team and apply them to manually-derived ‘historical variant’-‘modern equivalent’ pairs (derived from existing corpora of English and German) to determine whether we can derive similar letter replacement heuristics. Using the manually-derived heuristics as a gold-standard we will evaluate the automatically derived rules. Our prediction is that if the technique works in both languages, it would suggest that we are able to develop generic letter-replacement heuristics for the identification of historical variants for Indo-European languages. 2. German spelling variation The interdisciplinary project “Rule based search in text databases with non-standard orthography” which is funded by the Deutsche Forschungsgemeinschaft [German Research Foundation] developed a rule-based fuzzy search-engine for historical texts (Pilz et al. 2005). Its aim of RSNSR is to provide means to perform reliable full text-search in documents written prior to the German unification of orthography in 1901. On basis of around 4,000 manually collected one-to-one word mappings between non-standard and modern spellings, RSNSR follows three different paths to come up with an efficient rule set. Those are manual rule derivation, trained string edit distance and automatic rule learning. Additional mappings will be collected to further enhance the quality of those approaches. The manual derivation uses an alphabet of 62 different sequences, in parts historical n-graphs (e.g. <a>, <äu>, <eau>), built from combinations of the 30 standard graphemes of the German language. Being built manually, the alphabet considers linguistic restraints. Neither in context nor at the position of substitution non-lingual n-graphs (i.e. grapheme sequences that directly correspond to phonemes) are allowed. The context may also feature regular expressions using the java.util.regex formalism. The manually derived gold standard features the most elaborate rules. However the design of a rule set for the period from 1803 to 1806, based on only 338 evidences took about three days to create. Furthermore, the manual derivation is prone to human-error. This is especially true as soon as the rule set exceeds certain limits where side effects become more and more likely. The algorithm used to calculate the edit costs was proposed in 1975 by Bahl and Jelinek and taken up 1997 by Ristad and Yianilos who extended the approach by machine learning abilities. The authors applied the algorithm to the problem of learning the pronunciation of words in conversational speech (Ristad and Yianilos 1997). In a comparison between 13 different edit distances, Ristad and Yianilos’ algorithm proofed to be the most efficient one. Its error rate on our list of evidences was 2.6 times lower than the standard Levenshtein distance measure and more than 6.7 times lower than Soundex (Kempken 2005). The automatic generation of transformation rules uses triplets containing the contemporary words, their historic spelling variant and the collection frequency of the spelling variant. First, we compare the two words and determine so called ‘rule cores’. We determine the necessary transformations for each training example and also identify the corresponding context. In a second step, we generate rule candidates that also consider the context information from the contemporary word. Finally, in the third step we select the useful rules by pruning the candidate set with a proprietary extension of the PRISM algorithm (Cendrowska 1987). For this paper, we compared the German gold standard, mentioned above, with the two different machine learning algorithms. The string learning algorithm produces a fixed amount of single letter replacement probabilities. It is not yet possible to gather contextual information. Bi- or tri-graph operations are reflected by subsequent application of letter replacements. Therefore they do not map directly onto the manual rules. However, the four most frequent replacements, excluding identities, correspond to the four most frequently used rules. For the period from 1800 to 1806 these are T→TH, Ä→AE, _→E and E→_. The manual and the automatic derived rules show obvious similarities, too. 12 of the 20 most frequently used rules from the automatic approach are also included in the manually built rules. For six other rules equivalent rules in the manual rule set exist. The rule T→ET from the automatic approach, for example, corresponds to the more generalised form _→E taken from the manual approach. And again do the first four rules match the four most frequent gold standard ones. The automatic approaches, rule generation as well as edit distance, could be enhanced by a manual checking. Nevertheless, even a semi-automatic algorithm allows us to save time and resources. It is furthermore obvious, that the machine learning is already able to provide with a highly capable rule set for historical documents of German language. 3. English spelling variation The existing English system called VARD (VARiant Detector) has three components. First, a list of 45,805 variant forms and their modern equivalents, built by hand. This provides a one-to-one mapping which VARD uses to insert a modern form alongside the historical variant which is preserved using an XML ‘reg’ tag. Secondly, a small set of contextual rules which take the form of templates of words and part-of-speech tags. The templates are applied to find real-word variants such as ‘then’ instead of ‘than’, ‘doe’ instead of ‘do’, ‘bee’ for ‘be’ and detection of the genitive when an apostrophe is missing. The third component consists of manually crafted letter replacement heuristics designed during the collection of the one-to-one mapping table and intended to reduce the manual overhead for detection of unseen variants in new corpora. The rationale behind the VARD tool is to detect and normalise spelling variants to their modern equivalent in running text. This will enable techniques from corpus linguistics to be applied more accurately (Rayson et al., 2005). Techniques such as frequency profiling, concordancing, annotation and collocation extraction will not perform well with multiple variants of each word type in a corpus. The English manual and automatically derived rules show a great deal of similarity. Nine of the twenty most frequent automatically derived rules are in the manual set. Eight other automatically derived rules have equivalents if we ignore context. Three automatically derived rules do not have a match in the manual version. 4. Conclusion The motivation behind the two approaches of VARD and RSNSR differs. This reflects on the overall structure of rules as well. While VARD is used to automatically normalise variants and thus takes more accurate aim to determine the correct modern equivalent, RSNSR focuses on finding and highlighting those historical spellings. Therefore its demands for precision are diminished while recall is the much more important factor. However, the approaches are highly capable of supporting each other and expanding their original field of application.",
        "article_title": "The Identification of Spelling Variants in English and German Historical Texts: Manual or Automatic?",
        "authors": [
            {
                "given": "Dawn",
                "family": "Archer",
                "affiliation": [
                    "Department of Humanities,     University of Central Lancashire"
                ]
            },
            {
                "given": "Andrea ",
                "family": "ERNST-GERLACH",
                "affiliation": [
                    "Universität Duisburg-Essen"
                ]
            },
            {
                "given": "Sebastian  ",
                "family": "KEMPKEN",
                "affiliation": [
                    "Universität Duisburg-Essen"
                ]
            },
            {
                "given": "Thomas  ",
                "family": "PILZ",
                "affiliation": [
                    "Universität Duisburg-Essen"
                ]
            },
            {
                "given": "Paul",
                "family": "RAYSON",
                "affiliation": [
                    "Department of Computing, Lancaster University"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Large digital libraries typically contain collections of heterogeneous resources intended to be delivered to a variety of user communities. A key challenge for these libraries is providing tight integration between resources both within a single collection and across multiple collections. Traditionally, efforts at developing digital collections for the humanities have tended toward two extremes [5]. On one side are huge collections such as the Making of America [14, 15], Project Gutenberg [18], and Christian Classics Ethereal Library [13] that have minimal tagging, annotation or commentary. On the other side are smaller projects that closely resemble traditional approaches to editorial work in which editors carefully work with each page and line providing markup and metadata of extremely high quality and detail, mostly by hand. Projects at this end of the spectrum include the William Blake Archive [21], the Canterbury Tales Project [11], the Rossetti Archive [19], and the Cervantes Project [12]. These extremes force library designers to choose between large collections that provide an impoverished set of services to the collection’s patrons on the one hand and relatively small, resource intensive projects on the other. Often, neither option is feasible. An alternative approach to digital humanities projects recasts the role of the editor to focus on customizing and skillfully applying automated techniques, targeting limited resources for hand coding to those areas of the collection that merit special attention [6]. The Perseus Project [16] exemplifies this class of projects. Elucidating the internal structure of the digital resources by automatically identifying important features (e.g., names, places, dates, key phrases) is a key approach to aid in the development of these “middle ground” projects. Once identified, the internal structure can be used to establish connections between the resources and to inform visualizations. This task is complicated by the heterogeneous nature of digital libraries and the diversity of user community needs. To address this challenge we have developed a framework based approach to developing feature identification systems that allows decisions about details of document representation and features identification to be deferred to domain specific implementations of the framework. These deferred decisions include details of the semantics and syntax of markup, the types of metadata to be attached to documents, the types of features to be identified, the feature identification algorithms to be applied, and the determination of which features are to be indexed. To achieve this generality, we represent a feature identification system as being composed of three layers, as diagramed in Figure 1. The core of the system is a “Feature Identification Framework” (FIF). This framework provides the major structural elements for working with documents, identifying features within documents, and building indices based on the identified features. Implementations customize components of the framework to interface with existing and new collections and to achieve domain specific functionality. Applications then use this framework, along with the appropriate set of customized modules, to implement visualizations, navigational linking strategies, and searching and filtering tools. Figure 1: Three layered approach to designing a feature identification system The document module implements the functionality needed to represent documents, manage storage and retrieval, provide an interface to searching mechanisms and facilitate automatic feature identification. It provides the following features:1. Multiple types of documents (e.g., XML, PDF, RTF, HTML, etc) can be supported without modifying the APIs with which the rest of the system will interact. 2. Arbitrary syntactical constraints can be associated with a document and documents tested to ensure their validity. Notably, this helps to ensure that the markup of identified features does not violate syntactic or semantic constraints. 3. Metadata conforming to arbitrary metadata standards can be attached to documents. 4. Storage and retrieval mechanisms are provided that allow documents persistence to be managed either directly by the framework or by external systems. 5. Large documents can be broken into smaller “chunks” for both indexing and linking. Work in this area is ongoing. The feature module builds on this base to provide the core toolset for identifying the internal structure of documents. Our design of this component reflects the highly contextualized nature of the feature identification task. The relevant features of a document can take many forms (e.g., a person or place, the greeting of a letter, a philosophical concept, or an argument against an idea) depending on both the type of document and the context in which that document is expected to be read. Equally contextualized are the algorithms used to identify features. Dictionary and statistically based methods are prevalent, though other techniques focusing on the semi-structured nature of specific documents have also yielded good results [3, 1, 4, 9, 2, 7]. Ultimately, which algorithm is selected will depend heavily on the choice of the corpus editor. Accordingly, our framework has been designed so that the only necessary property of a feature is that it can be identified within the text of a document and described within the structure provided by the document module. For applications using the framework to effectively access and present the informational content, an indexing system is needed. Given the open ended nature of both document representation and the features to be identified, the indexing tools must inter-operate with the other customized components of the framework. We accomplish this, by utilizing adapters that are implemented while customizing the system. These adapters work with the other customized components to specify the elements of each document to index. To demonstrate and test this framework, we have implemented a prototype for a collection of official records pertaining Miguel de Cervantes Saavedra (1547- 1616) originally assembled by Prof. Kris Sliwa [10]. This collection contains descriptions, summaries, and transcriptions in Spanish of nearly 1700 documents originally written from 1463 to 1681. These documents bear witness to the life of both Cervantes and his family and include inventory lists, birth and death certificates, and court testimonies. Our application provides two primary points of access to the collection; a timeline navigator and a browsing interface. Following Crane, et al. [7], we have utilized proper names (people and places) and time as the two primary dimensions for structuring the documents in this collection. The timeline navigator, shown in Figure 2, displays a bar chart showing the distribution of the documents over time. Selecting a bar takes the reader to a more detailed view of the time period. Once the chart displays documents as single years, clicking on the bar for a single year brings up a display listing all documents from that year. The browsing interface, shown in Figure 3, allows readers to browse lists of both the people and the places identified within the collection. Upon selecting an item to view, a page presenting the resources available for that person or place is displayed. Currently, this includes a list of all documents in which the specified person has appeared and a bar graph of all documents in which that individual has been found as shown in Figure 4. Figure 2: Timeline interface to the Sliwa collection Once the user has selected an individual document to view, through either the timeline or browsing interface, that document is presented with four types of features identified and highlighted. Identified people and places are used to automatically generate navigational links between documents and the pages presenting the resources for the people and places identified within a document. Dates and monetary units are identified and highlighted in the text. One challenge with any framework based system is to ensure that the framework is not so general that customizing it requires more time and effort than writing an equivalent application from scratch. Our experience developing the Sliwa collection prototype suggests that our framework offers significant benefits. With the framework in place, we were able to develop and integrate new features in days; sometimes hours. Moreover, as sophisticated, general purpose features (e.g., pattern matching, grammatical parsers, georeferenced locations) are implemented, it becomes possible to customize and apply these features in new collections via a web-based interface with no additional coding involved. Custom document formats are more complex to implement, but can serve in a wide variety of applications. The current implementation sufficient for most XML formats and work is underway to more fully support TEI encoded documents. Our approach provides strong support for the general components of a feature identification system thereby allowing individual projects to focus on details specific to the needs of particular collections and user communities. We are currently working to apply this framework to a number of other projects, including diaries written during early Spanish expeditions into southern Texas [8], scholarly comments about the life and art of Picasso from the Picasso Project [17], and the Stanford Encyclopedia of Philosophy [20]. This will include further enhancements to the framework itself including support for feature identification that utilizes the structure of the document (including other identified features) in addition to the text and better support for accessing “chunks” within document in addition to the document as a whole. For the long term, we also plan to explore ways in which this framework can be used assist and shape editorial practices.",
        "article_title": "A General Framework for Feature Identification",
        "authors": [
            {
                "given": "Neal",
                "family": "AUDENAERT",
                "affiliation": [
                    "Texas A&M University"
                ]
            },
            {
                "given": "Richard",
                "family": "FURUTA",
                "affiliation": [
                    "Texas A&M University"
                ]
            },
            {
                "given": "Eduardo",
                "family": "URBINA",
                "affiliation": [
                    "Texas A&M University"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "1. Introduction Even the largest libraries struggle to maintain a comprehensive journal collection. In 2003 Australian universities subscribed to over 1,300,000 journals of which of which 974,000 were in aggregate digital collections. This represented over 273,000 new serial titles and over 150,000 cancellations (Council of Australian University Libraries, 2005). One emerging approach is for libraries to form consortia that take a joint subscription to digital resource collections. The consolidation of substantial collections with direct delivery has seen the gradual attrition of subscriptions to the traditional print format (Fox & Marchionini, 1998; Weiderhold, 1995), but this cost saving is offset by the substantial increase in digital resources. The wealth of international research resources presents an even greater dilemma for small research institutions: how to effectively and economically access such a wide base of information resources within sometimes highly constrained budgets. The cost reductions obtained through aggregate subscriptions and consortia do not necessarily offset the net growth of fee-for-use published resources, and may have the consequence of centralizing subscriptions through a few large distributors – with the long-term collection risk that this centralization presents. Small research libraries that cannot afford participation in national inter-library loan networks have formed fee-free networks of collaborating libraries that share their journal resources. While a fee-free Inter-Library Loan (ILL) service offers obvious attractions to smaller participating libraries, alternative economic approaches are needed to avoid excessive demand on resource-rich members, and to avoid the phenomenon of “free-riders”. This paper presents the resource distribution approaches that have been used to balance resource demand in GratisNet, an Australian network of 250+ health research libraries, where collaboration is fee-free but resource holdings among member are unequal. Dynamic ranking resource-based approaches are used to encourage the equitable distribution of resource load. 2. Economics of demand balancing in a fee-free network Hooke (1999) highlighted the need for evidence -based approaches in the management of information services. In a fee-based environment, the metrics for efficiency may centre of cost versus speed of supply. Fee-free collaboration does not have the same economic driver for equilibrium between demand and resource supply that emerges in the long term in a fee-based service. Furthermore, resource sharing networks operating in a fee-free environment face several risks that are common to voluntary online communities. In Gaming Theory “outcomes” and “payoffs” are differentiated (Shubik, 1975). In the case of ILL collaboration, the payoffs are the supply of particular ILL requests in exchange for the provision of requests raised by other libraries at the risk of absorbing the costs of supplying requests raised by other libraries. The outcomes include access to a wider base of research resources than would otherwise be available to the library, and the potential for requests to exceed loans and constraints on the limit of demands based on membership of a closed community. One of the risks is the “free-rider” phenomenon, or those who take the benefit of membership of a collaborating community but provide no net contribution of resources. “Free-riders” can be managed in a number of ways: through “closed shops” (the example of unions that limit benefits to those who are members only), or through adjustment of the payoffs(Hamburger, 1979). 3. Demand balancing Imbalanced distribution of workload in a fee-free environment, if unmanaged, can create imperfect resource management through inequitable distribution of demand over time. These imperfections may be expressed in terms of a reluctance to declare resources or supply requests (a form of compliance failure), or through inequitable distribution of demand resulting in a delay in supply (a queuing problem). This reduces the payoff potential for the larger members of the network. While there is a risk that libraries may reduce their own collections through reliance on wider networks, the trade-off is the delay in fulfilling requests when they are completed through an ILL rather than directly out of their own collection. The rational choice in fulfilment of an individual ILL request in a fee-free environment is the selection of the nearest library that has the highest probability of fulfilling the request. This provides the best payoff in probability of fulfilment and timeliness of supply. However, aggregated over time this choice is likely to place a larger burden on those participating libraries with the largest collection of resources in a given region. Where staff represents as much as 80% of document supply cost (Morris, 2004), this can be a considerable burden on larger participating libraries. In the GratisNet network, search results for resources held by members of the GratisNet network are inversely ranked based on historical workload contribution. Participating libraries are requested to select from resources in the top-ranked selections presented, but compliance is voluntary. Participating libraries supply ILL requests at no charge to members and with no specific reciprocity. The objective of the ranking process is to adjust the payoff implied by a ration selection of the largest, nearest library by tempering this choice through ranking of search results based on previous workload of participating libraries. Libraries with a higher historical workload are ranked lower in search results. Libraries are encouraged to select from one of the first three listed libraries that have holdings in the journal they are requesting. Table 1 shows the percentage of libraries that by-passed the computer-recommended ranking when raising ILL requests for the years 2002, 2003 and 2004. Since compliance is voluntary, this change demonstrates increasing trust in the workload distribution mechanisms. While participating libraries do exercise a measure of discretion in selecting outside the recommended rankings, voluntary compliance to the ranking recommendations is generally good and has improved over time. Game-theoretic formulations can provide a useful approach to the design of co-operative IT systems(Mahajan, Rodrig, Wetherall, & Zahorjan, 2004). To illustrate the contrast between a time-efficient system for ILL delivery and one which distributes workload across the network, these same transactions were reprocessed under to a game scenario which simulated a rational select on the basis of proximity and breadth of holdings matching the request for the most recent two years. The objective of this scenario was to contrast the aggregate effect of load-based ranking with a utility-based approach to request ulfilment (see Table 2 below). In a time-efficient approach, larger libraries are consistently net providers, reducing their aggregate payoff from participation. Pure Egalitarianism takes the approach over time that yields the highest combined utility to participating libraries. The voluntary element of the ranking yields a “relative egalitarianism” which balances the result that yields utility achieved overall with the lowest level of frustration.(Moulin, 1988). 2003 2004 Transactions 150155 128365 Distribution of transactions to the top 20 largest libraries (demand-balanced) 39003 33212 Distribution of transactions to the top 20 largest libraries (utility-based) 56815 46760 Distribution of transactions to the 20 smallest libraries (demand-balanced) 1454 1154 Distribution of transactions to the 20 smallest libraries (utility-based) 779 754 Table 2 Contrasting load-based ranking to utility-based ranking The risk facing groups collaborating on a fee-free basis is that inequity of resource distribution could result in the resignation of members where their level of “frustration” exceeds the benefit they gain from participation. 4. Conclusion Participating libraries in the GratisNet network commit to supplying ILL requests at no charge and with no specific reciprocity, on the basis that they can be confident that an increase in demand on their library will be balanced progressively with a lower ranking in search results. Transactions for the period 2003 to 2005 are analysed to illustrate the ways in which a ranking-based approach to resource discovery improves workload distribution for participating members overall. Results from the GratisNet network illustrate the effectiveness of formal approaches to resource distribution in fee-free collaborative networks. This analysis also gives an insight into the ways in which service metrics can help in the management of workload in a fee-free environment.",
        "article_title": "Demand Balancing in Fee-free Resource Sharing Community Networks with Unequal Resource Distribution",
        "authors": [
            {
                "given": "Edmund",
                "family": "BALNAVES",
                "affiliation": [
                    "University of Sydney     Prosentient Systems Pty Ltd"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The Text Encoding Initiative Guidelines for Electronic Text Encoding and Interchange are a community based standard for text encoding that aim to “apply to texts in any natural language, of any date, in any literary genre or text type, without restriction on form or content.” The basic idea is not to tell scholars, librarians, and other encoders *what* to encode, but rather *how* to encode that which they choose to record. Of course the Guidelines cannot possibly anticipate every feature that users may wish to encode, and therefore the capability to extend the encoding scheme described by the Guidelines in a consistent, easily understood, and interchangeable manner is paramount. Over the past few years the Text Encoding Initiative Consortium (the organization charged with maintaining, developing, promulgating, and promoting the Guidelines) has been working steadily toward a new release of the Guidelines. This much anticipated version, referred to as “P5”, is significantly different from the current Guidelines (“P4”), and yet performs the same basic function of providing a community based standard for encoding literary and linguistic texts. In this presentation, the TEI editors will present P5 as of the latest release, with an emphasis on the ease with which P5 can be customized to particular uses. The talk will start with an overview of what P5 is, what it is good for, and why one would want to use it, and then progress to some of the detailed differences between P4 and P5. Topics addressed will include: * The general goal of the TEI Guidelines - TEI is a community initiative, driven by the needs of its members and users * How work gets done in the TEI - technical council and work groups - open source using Sourceforge - special interest groups * Why do this -- isn’t P4 good enough? - P4 is just P3 in and using XML - a lot has happened since P3 was released, including the creation of the W3C and the acceptance of Unicode - there are arenas P4 does not cover - lots of improvements, repairs, etc. are in order * What’s new and different - infrastructural + schemata + datatypes + classes + customization - attributes used wherever textual content allowed - major updates + manuscript description + character sets & language identification + feature structures (now an ISO standard) + pointing mechanism - less major updates + dictionaries + multiple hierarchies + support for graphics and multimedia + support for stand-off markup - new updates + personography + terminological databases + collation sequences * customization - customizations permit a project to select which parts of the TEI scheme they will use, and to add new bits if needed - in P5, all uses of TEI are customizations of one sort or another - customizations, and the user’s documentation for them, are written in a TEI file - thus customizations themselves can be interchanged or even shared - in theory, a customization can use another customization as its starting point - thus permitting customizations of customizations * The TEI universe - the TEI universe is one where all projects share a common base, but many use additional, local markup constructs - clusters of similar projects can share common subsets of additional markup constructs ",
        "article_title": "TEI P5: What’s in It for Me?",
        "authors": [
            {
                "given": "Syd",
                "family": "BAUMAN",
                "affiliation": [
                    "Women Writers Project, Brown University"
                ]
            },
            {
                "given": "Lou",
                "family": "BURNARD",
                "affiliation": [
                    "Oxford University Computing Services,     Oxford University"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Il est classique, en philologie, de dater un texte en utilisant comme « terminus post quem » la date d’attestation de certaines de ses formes. L’utilisation de grandes bases de données comme Frantext permet d’envisager l’automatisation de cette opération, et par conséquent son application à toutes les formes d’un texte, ce qui augmente la précision de la datation. Un certain nombre de précautions (choix du corpus de référence, prise en compte des variations graphiques,...) doivent être prises pour mener à bien cette opération, qui peut être améliorée par la prise en compte, au-delà de la seule date de première attestation, de la fréquence d’utilisation, tout au long de l’histoire de la langue, des formes du texte à dater. _La datation du vocabulaire d’un texte permet également d’apprécier le degré d’archaïsme et de néologie mis en oeuvre par l’auteur, ce qui a des applications en stylistique et en histoire des genres. _On envisagera aussi l’utilisation de dictionnaires d’attestations pour effectuer ces opérations de datation, dictionnaires existants ou à constituer dans ce but. _Les exemples porteront aussi bien, pour validation, sur des textes dont la date est connue de l’histoire littéraire que de textes dont la datation fait aujourd’hui l’objet de controverses. It is traditional, in philology, to date a text by using as a “terminus post quem” the date of first attestation of certain words. The use of a great data base as Frantext makes it possible to envisage the automation of this operation, and consequently its application to all the words of a text, which would increase the precision of the dating. A certain number of precautions (choice of the reference corpus, recognition of the graphic variations...) must be taken to perform this operation, which can be improved by the taking into account, beyond the only date of first attestation, of the frequency of use, throughout the history of the language, of the words of the text to be dated. The dating of the vocabulary of a text also makes it possible to appreciate the degree of archaism and neology implemented by the author, which has applications in stylistics and history of the genres. One will consider also the use of dictionaries of attestations to carry out these operations of dating, dictionaries existing or to be constituted to this end. The examples will relate as well, for validation, to texts whose date is known in literary history and to texts whose dating is the subject of controversies.",
        "article_title": "",
        "authors": [
            {
                "given": "Michel",
                "family": "BERNARD",
                "affiliation": [
                    "Centre de recherche Hubert de Phalèse.     Université Sorbonne Nouvelle"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The evaluation of the researchers is a problem we are facing at present times. Bibliometric methods use mainly statistical tools, consequently, this approach does not provide any tools of qualitative evaluation. Bibliometry is a quantitative evaluation of literature. Numerous studies contributed to the advance of this science and to the discovery of indicators allowing to estimate the productivity of a researcher, a country or an institution. So, we suggest to examine the specific details of the bibliographic references in the texts. Our hypothesis is that by locating the indicators in corpus we will provide sentences localization with linguistics clues. These specific linguistic units, using the method of contextual exploration, give us opportunities to annotate articles with information about citation. We uses the contextual exploration method, a linguistic approach, which allows us to annotate automatically the text. Contextual Exploration, proposed and developed by Jean-Pierre Desclés and LaLICC group, is based upon the observation that it is possible to identify specific semantic information contained in certain parts of text. This approach does not depend on the specific domain of articles. Furthermore, we will resolve the limitation of statistical method, related to a possible distortion resulting from the negative citation of authors.The informatic application of this study will be integrated into the platform EXCOM (EXploration COntexuel Multilingue). We demonstrated that it was possible to identify and to annotate the textual segments from bibliography. Furthermore, through this linguistic study of the textual segments, we identified and categorized linguistics clues. The annotation of these linguistics clues facilitates automated processing within the frame of the contextual exploration method implemented in the platform EXCOM. This phase allows us to qualify the relations between the author, the coauthors and also the bibliography. It is possible to classify citation according to a qualitative approach and it is offering a better use of the bibliography. 1. Les outils bibliométriques Si l’évaluation de la science et de la production scientifique des chercheurs est un débat récurrent, il est de plus en plus présent ces dernières années. L’approche bibliométrique est la plus développée pour ne pas dire la seule. On pourra citer les écrits de Bradford, Lotka, Zipf et des travaux portant sur l’unification de ces lois. Nous pouvons également présenter les calculs des distributions à travers les mesures de concentration ou l’entropie de Shannon. Ces méthodes sont principalement issues de l’univers des statistiques et des grands nombres. Au-delà de l’aspect théorique des distributions bibliométriques, nous pourrions citer un homme, Eugene Garfield, qui à travers son article « Citation indexes of science : a new dimension in documentation throught association of ideas » a proposer un outil dévaluation de la science connu sous le nom de Facteur d’Impact. Afin de montrer l’importance de disposer d’un tel outil, nous présenterons deux catégories d’indicateur et leur utilité: les indicateurs univariés et les indicateurs relationnels. Les indicateurs univariés permettent avant tout d’évaluer la productivité d’un chercheur, d’un laboratoire, d’un domaine ou bien d’un pays. Cependant, cet indicateur reste déconseillé au niveau de l’individu. Les quantités mesurables peuvent être le nombre de publications, le nombre de co-signatures et co-publications, le nombre de citations qui montrent l’impact des articles cités. Les liens scientifiques des citations montrent le rapport d’influence entre communautés scientifiques. Nous retrouvons ici le fameux facteur d’impact proposé par M. Garfield et qui est utilisé par l’Information Science Institute. Enfin, le nombre de brevets ainsi que les citations de brevets sont également des indicateurs d’inventivité, d’innovation et de capacité technologique montrant le résultat des ressources investies dans les activités de recherche et développement. Les indicateurs relationnels sont principalement les co-citations et les co-occurrences de mots. Les co-citations présentent les réseaux thématiques et l’influence des auteurs. L’indice d’affinité mesure les liens entre les pays en calculant le taux relatif des échanges scientifiques entre deux pays de masse scientifique comparable, pendant une période donnée par rapport à l’ensemble de la coopération nationale de ces deux pays. L’utilisation des citations ou des co-auteurs permet de proposer des relations entre auteurs sous forme matricielle afin d’obtenir des réseaux. 2. Les limitations de l’outil La bibliométrie apporte donc une mesure des activités de recherche, mais un ensemble de limites d’ordre technique et conceptuel ne permet pas l’utilisation à l’unanimité des indicateurs correspondants. Nous relèverons les biais suivants: actuellement, seul le premier auteur est pris en compte. Il faut également considérer les fautes de frappe et l’homonymie. Les domaines sont inégalement représentés et les indicateurs s’appliquent très difficilement pour les sciences humaines et sociales. Toutes les revues ne sont pas recensées et pour celles qui le sont, il peut y avoir sur ou sous-estimation de la revue et donc des travaux et des équipes. On notera que l’autocitation ou la citation d’un article controversé n’est pas abordé par l’approche statistique. De plus, les ouvrages ne sont pas pris en compte. Nous pouvons aussi constater que deux ans ne suffisent pas pour qu’un article se révèle or il s’agit de la durée retenue pour le calcul du facteur d’impact. Si les journaux en science de l’information s’intéressent naturellement à cette problématique, nous constaterons que cette question touche des domaines qui dépassent ce cadre. Garfield mentionnera par la suite : « I first mentioned the idea of an impact factor in 1955. At that time it did not occur to me that it would one day become the subject of widespread controversy. [...] I expected that it would be used constructively while recognizing that in the wrong hands it might be abused ». On peut non seulement affirmer que les moyens actuels ne permettent pas d’identifier la valeur d’un papier. Mais qu’elle conduit à des pratiques qui peuvent mettre en péril la qualité des articles. Les conséquences ne sont pas sans importance. Cela peut provoquer des comportements antiscientifiques comme le plagiat, la publication dans une revue où le FI est élevé plutôt que dans une revue adéquate ou bien encore de diviser les données en partie ridiculement petites. Nous sommes dans l’ère du « publier ou mourir ». De ce déclin de la diversité, nous risquons d’avoir à moyen terme une recherche homogène. Si les articles pointant du doigt les biais introduits par cette méthode d’évaluation sont de plus en plus nombreux, ils ne proposent cependant guère de solutions innovantes, seulement de nouvelles approches statistiques permettant de minimiser les biais introduits. Il est difficile de mesurer la qualité d’une production scientifique car la bibliométrie, et plus spécifiquement les indicateurs, caractérisent le contenant et non le contenu. Ils sont des mesures et non des signes précieux de la qualité de la recherche. Si la bibliométrie rajoute de la valeur à la vue des pairs, elle ne peut que difficilement les remplacer. Ce débordement, en dehors des canaux de communication classique n’est-il pas le signe précurseur que l’hégémonie du FI de ces cinquante dernières années a vécu et qu’il est désormais nécessaire de passer d’une évaluation quantitative à une évaluation qualitative de la publication scientifique. 3. Une approche qualitative Une nouvelle approche de cette problématique doit être envisagée. Nous devons désormais nous intéresser à l’auteur, à ces co-auteurs et également au contenu d’un article selon une approche qualitative. Pour cela, une réflexion sur l’étude des publications doit être entreprise. Sans prétendre fournir un traitement sémantique complet d’un article scientifique, nous pourrons dans un premier temps considérer les relations sémantiques entre l’auteur, les co-auteur et les références bibliographiques. Il serait tout à fait pertinent de savoir si un article est cité de façon positive ou négative. Une référence bibliographique citée en contre-exemple est tout à fait révélatrice des relations entre les travaux des chercheurs. Il peut s’agir entre autre d’une référence par rapport à une définition, une hypothèse ou bien une méthode, mais également d’un point de vue, d’une comparaison ou bien d’une appréciation. Cette approche permettra également de mettre en évidence l’autocitation. La méthode de l’Exploration Contextuelle va permettre, à l’aide d’une étude poussée des indices, une analyse plus fine des références bibliographiques. De l’importance des références bibliographiques. Nous nous proposons d’utiliser les renvois bibliographiques d’un article afin de déterminer des segments textuels sur lesquels nous pourrons appliquer la méthode d’exploration contextuelle. L’appel de citation dans un texte peut prendre différentes formes. Il peut s’agir principalement d’un renvoi numérique ou d’un renvoi par nom d’auteur. Pour cela, nous dresserons une classification des différentes familles numériques et alphanumériques des références bibliographiques. Afin de traiter automatiquement cette tâche d’identification et d’extraction, nous pourrons par exemple définir un alphabet adéquat permettant d’appliquer au corpus un automate fini déterministe. Cette extraction va nous permettre dans un premier temps d’étiqueter le corpus, puis de dresser des listes d’auteurs, de renvois ainsi qu’une bibliographie complète de l’auteur et de ces co-auteurs. Il sera également intéressant, dans notre approche qualitative, d’établir les relations entre les renvois bibliographiques et la bibliographie. Approche linguistique et Exploration Contextuelle. Suite à l’identification des appels bibliographiques, nous pourrons alors proposer une annotation de celles-ci avec une catégorie afin de définir comment l’auteur a été cité. Cette catégorisation est définie par l’étude d’indice que nous relèverons dans la phrase. Nous rechercherons les indices positifs/négatifs de citation d’un auteur, ainsi que les citations hypothèses/méthodes utilisées par un auteur. L’application des règles de l’Exploration Contextuelle permettra ainsi de lever les indéterminations sémantiques de l’unité linguistique analysée. On caractérisera ce point de vue comme étant une catégorisation sémantique des références de citation d’auteur. L’application informatique de cette étude s’effectue dans le cadre de la plateforme EXCOM (Exploration Contextuelle Multilingue) qui est en cours de réalisation au sein du Laboratoire LaLICC. Ce moteur d’annotation sémantique s’appuie sur la méthode de l’Exploration Contextuelle et permet d’étiqueter automatiquement un texte à partir de ressource linguistique. Nous serons alors en mesure d’apporter une information d’ordre sémantique et à terme de proposer une évaluation qualitative des renvois bibliographiques. Enfin cette approche proposera de dépasser le cadre bibliométrique pour analyser les sources d’un texte et détecter d’éventuelles cliques entre auteurs au sens de la théorie des graphes.",
        "article_title": "Critique de la bibliométrie comme outil d’évaluation; vers une approche qualitative.",
        "authors": [
            {
                "given": "Marc",
                "family": "BERTIN",
                "affiliation": [
                    "Laboratoire LaLICC (Langage, Logique, Informatique, Cognition et Communication), Université Paris-Sorbonne/CNRS UMR8139"
                ]
            },
            {
                "given": "Jean-Pierre",
                "family": "DESCLÉS",
                "affiliation": [
                    "Laboratoire LaLICC (Langage, Logique, Informatique, Cognition et Communication), Université Paris-Sorbonne/CNRS UMR8139"
                ]
            },
            {
                "given": "Yordan",
                "family": "KRUSHKOV",
                "affiliation": [
                    "Laboratoire LaLICC (Langage, Logique, Informatique, Cognition et Communication), Université Paris-Sorbonne/CNRS UMR8139"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "As discussed by McCarty, Beynon and Russ in a session organised at ACH/ALLC 2005, there is a remarkable convergence between McCarty’s concept of ‘model-building in the role of experimental end-maker’ (McCarty 2005:15) - a cornerstone in his vision for humanities computing (HC) - and the principles of Empirical Modelling (EM) (EMweb). More problematic is the tension between the pluralist conception of computing that is an essential ingredient of McCarty’s stance on HC, and the prominent emphasis on ‘dissolving dualities’ in McCarty, Beynon and Russ (ACH/ALLC 2005:138). Resolving this tension transforms the status of HC from one amongst many varieties of computing to that of first amongst equals. The plurality of computing In presenting his “rationale for a computing practice that is of and foras well as in the humanities”, McCarty (2005:14) emphasises the plurality of computing. Following Mahoney (2005), he calls into question the search for “the essential nature of computing” and appeals to history as evidence that ‘what people want computers to do’ and ‘how people design computers to do it’ determine many different computings. The audacity of McCarty’s vision in recommending his readers, as would-be practitioners of a variety of computing, to “[turn] their attention from working out principles to pursuing practice”, is striking. It is hard to imagine a reputable computer science department encouraging its students to see computing primarily in terms of its practice - congenial to the students themselves as this might be. In promoting computing as an academic subject, there is no recognised focus for developing ‘scientific’ principles other than the theory of computation at its historical core (Turing). McCarty instead sets out to characterise the practice of HC in such terms that it has its own integrity. When contemplating McCarty’s boldness, it is instructive to consider the alternatives. The problematic nature of the relationship between computer science and the humanities is notorious. Consider, for instance, Chesher’s observation (ACH/ALLC 2005:39) that - in teaching a course in Arts Informatics: “The Humanities critiques of science and technology (Heidegger, Virilio, Coyne) are difficult to reconcile with scientific conceptions of humanities practices (Holtzman). Each of these areas places quite different, and often clearly conflicting discourses, techniques and systems of value”. From this perspective, seeking to characterise HC as a unified entity seems to be the only plausible strategy, though it resembles conjuring a stable compound from an explosive combination of improbable ingredients. Invoking EM is helpful in critiquing McCarty’s treatment of this unification (2005: 195-8). To elaborate the chemistry metaphor, it illuminates the precise nature of the reaction and identifies it with a more general phenomenon. How modelling and computer science interact in humanities computing The semantic orientation of HC is crucial to understanding its chemistry. Where computer science emphasises prescribing and representing precise intended meanings, humanities is of its essence obliged to engage with meanings that are ambiguous and unintended. The authentic spirit of McCarty’s notion of HC is captured in Ramsay’s ‘placing visualisation in a rhetorical context’ (in his presentation at ACH/ALLC 2005:200) - the creative construction of an artefact as a subject for personal experience, whose interpretation is to be negotiated and potentially shared. This theme is amplified in many topical contributions to the proceedings of ACH/ALLC 20051. Interpreting such activities from an EM perspective obliges a more prominent shift in emphasis from the accepted view of computing than is acknowledged in (McCarty, 2005) - the rich diversity of HC activities cannot be attributed primarily to the versatility of the Turing Machine as a generator of functional relationships2. In EM, the focus is upon the role that observables, dependency and agency play in the modelling activity, and each of these concepts appeals to a personal experience of interaction with technology that defies merely functional characterisation. On this basis, EM trades first and foremost not in objective ‘formal’ interpretations, but in speculative constructions that cannot be realised or mediated without skillful and intelligent human interaction. Appreciation of observables, dependency relationships and potential agency is acquired through developing familiarity and evolving skills. This is in keeping with Polanyi’s account - cited by McCarty (2005:44) - of how awareness is transformed through skill acquisition. Functional abstraction can express transcendental computational relationships, but does not encompass such issues, which relate to what is given to the human interpreter in their immediate experience. A useful parallel may be drawn with musical performance. Though one and the same functional relationship between visual stimulus and tactile response is involved, a virtuoso pianist can perform an extended extract from a complex score in the time it takes a novice to identify the initial chord. In this context, it is significant that - in elaborating his vision for HC, McCarty (2005:53) drew upon his experience of making a specific model - the Onomasticon for Ovid’s Metamorphoses - whose construction and interpretation can be viewed as an EM archetype. Model-building in the Onomasticon, being based on spreadsheet principles, supplies the framework within which McCarty’s experimental ‘end-maker’ role can be played out most effectively. It is implausible that the same qualities can be realised on account of adopting other model-building principles, such as the use of object-orientation, since - in conventional use - their primary purpose is to rationalise the specification of complex functional abstractions. This challenges Galey’s - no doubt pragmatically most sensible! - contention (ACH/ALLC 2005:198) that “In order to bring electronic editing projects like the eNVS to the screen, humanists must think past documents to embrace the principles of object-oriented and standards-compliant programming and design.”. Humanities computing as the archetype for all varieties of computing Though McCarty (2005:14) first discusses plurality in computing in relation to communities of practice quite generally, his interest in a conceptual unification of HC and computer science (2005:195-8) acknowledges the plurality of HC itself. Where McCarty (2005:198) identifies “general-purpose DH.indb 18 6/06/06 10:55:14 DIGITAL HUMANITIES 2006 Single Sessions P. 19 modelling software, such as a spreadsheet or database” as one component within a more diverse unity, Beynon and Russ have a radically different conceptualisation in mind. Their account identifies EM as hybrid part- automated-part-human processing within a framework for generalised computation similar to that implicit in McCarty’s Onomasticon3. Within this framework, the functionality of the Turing Machine is subsumed by closely prescribed and highly automated modes of interaction, whilst modelling with the Onomasticon is a more open-ended human-centred form of processing - though by no means the most general activity of this nature. This places EM at the centre of a broader pragmatic discourse on programming that complements the conventional rational discourse (Beynon, Boyatt and Russ, 2005). The emphasis in (McCarty, Beynon and Russ, 2005) on dissolving dualities within the frame of Radical Empiricism (James, 1996) may still appear to be mismatched to the plurality of HC. Klein’s reaction to EM exemplifies the issues. In seeking a technology to support a world-wide collaborative creative venture4, he recognises the qualities of EM as supporting a concept of creativity that is expressed in the motto: “Build the camera while shooting the film” (cf. Lubart 1996, Klein 2002, METISweb). For Klein, this recognition calls to mind Joas’s concept of creative action, and the processes that shape the evolving meaning of context in Lévy’s ‘universe in perpetual creation’ (1997). The relevance of Radical Empiricism even where such diverse perspectives are being invoked stems from the subject-independent association it establishes between sense-making and the classification of relationships between experiences. For instance, whatever meaningful relationships inform the semiotics of Lévy’s Information Economy Meta Language (2005) should somewhere be ‘experiencable’ (cf. James, 1996:160), and in this manner be amenable to EM. Seen in this light, Radical Empiricism and EM relate to universal learning activities that are orthogonal to the subject of the learning (cf. Beynon and Roe, 2004). This accords with James’s monist view of experience and pluralist view of understanding (James, 1996:194). It is also resonates best with cultures where understanding through relationship has higher priority than objectification. In emphasising interaction and the interpretation of relationships, EM does not prescribe a rigid frame for understanding, but exhibits that positive quality of blandness5 (Jullien, 2004) that affords participation in many relationships. Even within the small community of EM practitioners, this potential for plurality can be seen in different nuances and idioms of elaboration, as in relation to analogue, phenomenological or ecological variants of computing. The aspiration of EM to connect computing decisively with modelling was also that of object-oriented (OO) modelling, as first conceived nearly forty years ago (Birtwistle, Dahl, Myhrhaug and Nygaard, 1982). As a young technology, EM cannot yet compete with OO in tackling technical challenges in HC, such as devising adaptive web interfaces for the ‘end-maker’. Perhaps, unlike OO, it can be more widely adopted and developed without in the process being conscripted to the cause of supporting functional abstraction. If so, it may yet demonstrate that the modelling activity McCarty has identified as characteristic of HC is in fact an integral and fundamental part of every computing practice: that all computings are humanities’ computings. Notes 1. For instance: acknowledging that there is no definitive digital representation (Galey, ACH/ALLC 2005:198); recognising the essential need for interactive playful visualisation (Ramsay, ibid: 200; Wolff, ibid: 273; Durnad and Wardrip-Fruin, ibid: 61); and appreciating the importance of collaborative modelling and role integration (Best et al, ibid: 13; van Zundert and Dalen-Oskam, ibid: 249). 2. For more background, see McCarty (2005) Figure 4.2 and the associated discussion on pages 195-8. 3. The framework alluded to here is that of the Abstract Definitive Machine, as described at (EMweb). 4. The Metis project (METISweb) is exploring collective creativity of global virtual teams of students and professionals in the movie industry. 5. The Chinese ‘dan’, which Jullien translates as ‘fadeur’: Varsano notes that she “would have liked to find an English word that signifies a lack of flavor and that at the same time benefits from the positive connotations supplied by a culture that honors the presence of absence” (see Schroeder 2005).",
        "article_title": "Humanities’ Computings",
        "authors": [
            {
                "given": "Meurig",
                "family": "BEYNON",
                "affiliation": [
                    "Computer Science, University of Warwick,     Coventry UK"
                ]
            },
            {
                "given": "Roderick",
                "family": "R. KLEIN",
                "affiliation": [
                    "Syscom Lab, University of Savoie,     Bourget du Lac, France"
                ]
            },
            {
                "given": "Steve",
                "family": "RUSS",
                "affiliation": [
                    "Computer Science, University of Warwick,     Coventry UK"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "This paper discusses the applicability of modelling methods originally meant for business applications, on the design of the complex markup vocabularies used for XML Web-content production. We are working on integrating these technologies to create a dynamic and interactive environment for the design of document markup schemes. This paper focuses on the analysis, design and maintenance of XML vocabularies based on UML. It considers the automatic generation of Schemas from a visual UML model of the markup vocabulary, as well as the generation of DTDs and also pieces of software, like input forms. INTRODUCTION Most authors that treated the relationship between UML and XML [5, 7] only targeted business applications and did not consider complex document modelling for massive and systematic production of XML contents for the Web. In a Web publishing project, we need to produce hundreds of XML documents for Web publication. Digital Library XML documents that model the structure of literary texts and include bibliographic information (metadata), plus processing and formatting instructions, are by far much more complex than the XML data we usually find in business applications. Figure 1 shows a small document model based on the TEI. Although it may seem complex, it is only a very small TEI subset. This type of markup is not as simple and homogeneous as conventional structured data. In these documents we usually find a wide variety of elements nested up to deep levels, and there are many exceptional cases that can lead to unexpected markup situations that also need to be covered. Complex markup schemes like TEI [9] and DocBook [1] are good examples of this versatility. However, no matter how heterogeneous and unpredictable the nature of humanities markup could get to be, software engineers have to deal with it in a systematic way, so that automatic processes can be applied to these texts in order to produce useful output for Web publishing, indexing and searching, pretty printing, and other end user facilities and services. There is also a need to reduce content production times and costs by automating and systematizing content production. For these, software, documentation and guides of good practice have to be developed. The building of all these automation, methods and procedures within the complexity of humanities content structuring can be called Document Engineering. The purpose is to reduce costs, and facilitate content production by setting constraints, rules, methods and implementing automation wherever and whenever is possible. XML, DTD or Schemas, XSL transforms, CSS stylesheets and Java programming are the usual tools to enforce the rules, constraints and transformations necessary to turn the document structuring problem to a systematic automated process that lead to useful Web services. But the wide variety of Schema types, and the individual limitations of each of them, make the task of setting a production environment like this very difficult. On one hand we need a markup vocabulary that can cover all document structuring requirements, even the most unusual and complex, but that is simple enough for our purposes. In other words, we need the simplest DTD/Schema that fits our needs. We previously treated the problem of DTD/Schema simplification in [2, 3]. But DTD/Schema simplification, although useful, doesn’t solve all the problems of Document Engineering, like building transformations to obtain useful output or assigning behaviour to certain structures (like popup notes, linking, and triggering services). This kind of environments are usually built incrementally. The design information, if any, is dispersed into many pieces of software (Schemas, transformation, Java applets and servlets), or does not exist at all. A system like this includes document design (DTD/Schemas), document production techniques and tools (XSL and Java), document exploitation tools (indexing, searching, metadata, dictionaries, concordances, etc.) and Web design altogether. UML modelling may be the answer to join all those bits and pieces into a coherent design that reduces design cost, improves the quality of the result, provides documentation and finally may even simplifies maintenance. UML modelling for massive Web content production may also lead to automatic generation of some of the tools mentioned. ADVANTAGES OF MODELING XML DOCUMENTS WITH UML Apart from modelling the structure of a class of documents (as DTDs and Schemas do), UML can capture other properties of elements: - Behaviour: this is related to event oriented functions (e.g. popup notes) - Additional powerful validation features (e.g. validating Fig. 1: a small document model based on the TEI. DH.indb 22 6/06/06 10:55:15 DIGITAL HUMANITIES 2006 Single Sessions P. 23 consistency of certain fields like author name against a database.) - Customization of document models to provide different views or subsets of the markup scheme to different users (e.g. DTDs for development of different types of news) We believe that the dynamic and interactive environment described here will be very useful to professionals responsible for designing and implementing markup schemes for Web documents and metadata. Although XML standards for text markup (like TEI and DocBook) and metadata markup (e.g. MODS, EAD) are readily available [8], tools and techniques for automating the process of customizing DTD/Schemas and addingpostprocessing functionality are not. PREVIOUS RELATED WORK As Kimber and Heintz define it [7], the problem is how do we integrate traditional system engineering modelling practice with traditional SGML and XML document analysis and modelling? According to David Carlson [5], eXtensible Markup Language (XML) and Unified Modelling Language (UML) are two of the most significant advances from the fields of Web application development and object- oriented modelling. DESCRIPTION OF THE PROJECT We are working on integrating these technologies to create a dynamic and interactive environment for the design of document markup schemes (see figure2). Our approach is to expand the capabilities of Visual Wade1 to obtain a tool that allows the visual analysis, design and maintenance of XML vocabularies based on UML. Among the targets we are working on the automatic generation of different types of DTD/Schemas from a visual UML model of the markup vocabulary, code generation when possible (like generating HTML forms or XSLT), documentation and special enhanced validators that can perform verifications beyond those allowed by DTDs or Schemas (like verification of certain element content or attribute values against a database). Fig. 2.: a environment for the design of document markup schemes. Carlson [5] suggests a method based on UML class diagrams and use case analysis for business applications which we adapted for modelling document markup vocabularies. A UML class diagram can be constructed to visually represent the elements, relationships, and constraints of an XML vocabulary (see figure 3 for a simplified example). Then all types of Schemas can be generated from the UML diagrams by means of simple XSLT transformations applied to the corresponding XMI representation of the UML model. Fig. 3.: Example of a UML class diagram (partial view). The UML model information can be stored in an XML document according to the XMI (XML Metadata Interchange) standard as described by Hayashi and Hatton [6]: “Adherence to the [XMI] standard allows other groups to easily use our modelling work and because the format DH.indb 23 6/06/06 10:55:15 P. 24 Single Sessions DIGITAL HUMANITIES 2006 is XML, we can derive a number of other useful documents using standard XSL transformations”. In our case, these documents are Schemas of various types as well as DTDs. Like Schemas, DTDs can be also generated from the XMI representation of the UML model (doted line), but as DTDs are simpler than Schemas, and all types of Schemas contain at least the same information as a DTD, DTDs can also be directly generated from them. POSTPROCESSING AND PRESENTATIONAL ISSUES In many cases, code generation from a high level model is also possible. Code generation may include JavaScript code to implement behaviour for certain elements like popup notes, hyperlinks, image display controls, etc. This is the case of input HTML forms that can be generated from Schemas as shown by Suleman [10]. We have successfully experimented on the generation of XSLT skeletons for XML transformation which save a lot of time. Usually XSL transforms produce fairly static output, like nicely formatted HTML with tables of contents and hyperlinks, but not much more. In exceptional cases we can find examples of more sophisticated interaction. This high level of flexible interactivity is the real payoff from the UML-XML-XSLT-browser chain. This sort of functionality is usually programmed specifically for individual projects, given that it’s highly dependent on the nature of the markup in any given document. We aim to provide the ability to specify this at the UML level. For instance, a note could be processed differently according to its type attribute and then be displayed as a footnote, a margin note, a popup note, etc. In certain cases it can be hooked to a JavaScript function to be popped up in a message window or in a new browser instance according to attribute values. In this sense, we could provide a set of generic JavaScript functions which could retrieve content from elements and display it in various ways (popup, insertions, etc.) or trigger events (like a dictionary lookup). We should look for document models that allow al kinds of presentation, navigation and cognitive metaphors. - Sequential reading - Text reuse (links and includes) - Non-sequential reading - Hyperlinks - Collapsible text - Foot notes, margin notes, popup notes - The folder metaphor - TOCs, indexes and menus All the elements in a structured document have an associated semantic and a behaviour or function (as in the above example, a popup note must appear on a popup window when a link to it is pressed). This is not reflected in conventional document models: a DTD/Schema may say that a note is a popup note: ... but the behaviour of this note is not stated at all. Some postprocessing must be implemented for the popup effect to happen. A UML based document model can incorporate the expected behaviour like methods in a class diagram. OTHER AUXILIARY TOOLS FOR DOCUMENT DESIGN AND OPTIMIZATION As additional aiding tools for this project we have incorporated two of our earlier developments: First the automatic simplification of DTDs based on sample sets of files [2, 3]. This tool can be applied to obtain simplified DTDs and Schemas customized to fit exactly a collection of documents. Second, automatic element names and attribute names translation can be applied when multilingual markup is required. A detailed explanation of the multilingual markup project can be found in [4]. See figure 2 for an idea of how these tools interact with the UML document modelling. The techniques described here can also be used for modelling metadata markup vocabularies. CONCLUSIONS Concerning the described set of DTD/Schema design tools, the integration of UML design with example based automatic simplification and multilingual vocabulary capabilities, is expected to be a very useful and practical design aid. However, we experienced some limitations in the use of UML. While commercial non UML products like XML Spy or TurboXML use custom graphical tree representation to handle XML schemas, comprising very handy collapsing and navigating capabilities, most general purpose UML design environments lack these specialized features. One of the downsides of UML is that it is less friendly when working with the low-level aspects of modelling [11]. For instance, it is easy to order the elements of a sequence in a tree, but it is very tricky to do so in UML. Although UML proves very useful for modelling document structures of small to medium complexity (metadata applications and simple documents), UML models for medium to big sized schemas (100 to 400 elements), like those used for complex DL documents, become practically unmanageable2. The diagrams become overloaded with too many class boxes and lines, which end up being unreadable. This problem could be solved, or at least mitigated, by enhancing the interfaces of UML design programs with newer and more powerful display functions. Facilities like intelligent collapsing or hiding of diagram parts or elements, overview maps (see figure 3), zooming, 3-D layouts, partial views, and other browsing capabilities would certainly help to solve the problem. Footnotes ♣ This work is part of the METASIGN project, and has been supported by the Ministry of Education and Science of Spain through the grant number: TIN2004-00779. 1 VisualWade is a tool for software development based on UML and extensions. It was developed by our research group, named IWAD (Ingeniería Web y Almacenes de Datos - Web Engineering and Data-Warehousing), at the University of Alicante. This group also developed the OOH Method (for more information see http://www.visualwade.com/) 2 The DTD used by the Miguel de Cervantes DL for its literary documents contains 139 different elements. The “teixlite” DTD, a simple and widely used XML-TEI DTD, contains 144 elements.",
        "article_title": "Using Software Modeling Techniques to Design Document and Metadata Structures ♣",
        "authors": [
            {
                "given": "Alejandro",
                "family": "BIA",
                "affiliation": [
                    "U. Miguel Hernández (Spain)"
                ]
            },
            {
                "given": "Jaime",
                "family": "GÓMEZ",
                "affiliation": [
                    "U. Alicante (Spain)"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "CTION Markup is based on mnemonics (i.e. element names, attribute names and attribute values). These mnemonics have meaning, being this one of the most interesting features of markup. Human understanding of this meaning is lost when the encoder doesn’t understand the language the mnemonics are based on. By “multilingual markup” we refer to the use of parallel sets of tags in various languages, and the ability to automatically switch from one to another. We started working with multilingual markup in 2001, within the Miguel de Cervantes Digital Library. By 2003, we have built a set of tools to automate the use of multilingual vocabularies [1]. This set of tools translates both XML document instances, and XML document validators (we first implemented DTD translation, and then Schemas [2]). First we translated the TEI tagset, and most recently the Dublin Core tagset [3] to Spanish, and Catalan. Other languages were added later1. Now we present a Multilingual Markup Website that provides this type of translation services for public use. PREVIOUS WORK At the time when we started this multilingual markup initiative in 2001 there were very few similar attempts to be found [4]. Today they are still scarce [5, 6]. Concerning document content, XML provides built-in support for multilingual documents: it provides the predefined lang attribute to identify the language used in any part of a document. However, in spite of allowing users to define their own tagsets, XML does not explicitly provide a mechanism for multilingual tagging. THE MAPPING STRUCTURE We started by defining the set of possible translations of element names, attribute names, and attribute values to a few target languages (Spanish, Catalan and French). We stored this information in an XML translation mapping document called “tagmap”, whose structure in DTD syntax is the following: <!ELEMENT tagmap (element)+ > <!ELEMENT element (attr)* > <!ATTLIST element en CDATA #REQUIRED es CDATA #REQUIRED fr CDATA #REQUIRED> <!ELEMENT attr (value)* > <!ATTLIST attr en CDATA #REQUIRED es CDATA #REQUIRED fr CDATA #REQUIRED> <!ELEMENT value EMPTY > <!ATTLIST value en CDATA #REQUIRED es CDATA #REQUIRED fr CDATA #REQUIRED > Fig. 1. Structure of the original tagmap.xml file This structure is pretty simple, and proved useful to support the mnemonic equivalences in various languages. It was meant to solve ambiguity problems, like having two attributes of the same name in English, who should be translated to different names in a given target language. For this purpose, this structure obliges us to include all the attribute names for each element and their translations. The problem with this is global attributes, which in this approach needed to be repeated, once for each element. This made the maintenance of this file cumbersome. Sebastian Rahtz then proposed another structured, under the assumption that an attribute name has the same meaning in all cases, no mater the element it is associated to, and accordingly it would have only one target translation to a given language. This is usually the case, and although theoretically there could be cases of double meaning, as above mentioned, they do not seem to appear within the TEI. So the currently available “teinames.xml” file follows Sabastian’s structure. Note that “element”, “attribute” and “value” appear at the same level, instead of nested: <!ELEMENT i18n (element | attribute | value)+ > <!ELEMENT element (equiv | desc)* > <!ATTLIST element ident CDATA #REQUIRED > <!ELEMENT attribute (equiv | desc)* > <!ATTLIST attribute ident CDATA #REQUIRED > <!ELEMENT value (equiv)* > <!ATTLIST value ident CDATA #REQUIRED > <!ELEMENT equiv EMPTY > <!ATTLIST equiv xml:lang CDATA #REQUIRED value CDATA #REQUIRED > In 2004, we discussed the idea of adding brief text descriptions to each element, the same brief descriptions of the TEI documentation, but now translated to all supported languages. This would allow the structure to provide help or documentation services in several languages, as another multilingual aid. This capability was then added to the “teinames.xml” file structure, although the translations of the all the descriptions still need to be completed: <!ELEMENT desc (#PCDATA) > <!ATTLIST desc xml:lang CDATA #REQUIRED > DH.indb 27 6/06/06 10:55:16 P. 28 Single Sessions DIGITAL HUMANITIES 2006 Fig. 2. Structure of the teinames.xml file. THE MULTILINGUAL MARKUP WEB SERVICE By means of a simple input form, the markup of a structured file can be automatically translated to the chosen target language. The user can choose a file to process (see figure 3) by means of a “Browse” button. Fig. 3. The Multilingual Markup Translator form. Currently, only TEI XML document instances are allowed. In the near future, the translation of TEI DTDs, W3C-Schemas and Relax-NG Schemas will be added, and later, other markup and metadata vocabularies will be supported, like Docbook and DublinCore. The system uses file extensions to identify the type of file submitted. Allowed file extensions are: .xml for document instances, .dtd for DTDs, .xsd for W3C Schemas, and .rng for RelaxNG schemas. The document to be uploaded must be valid and well-formed. If the document is not valid, the translation will not be completed successfully, and an error page will be issued. Once the source file has been chosen, the user must indicate the language of the markup of this source file, as well as the target language desired for the output. This is done by means of radio buttons. It would not be necessary to indicate the language of the markup of the source file if it was implicit in the file itself. We thought of three ways to do this: To use the name of the root tag to indicate the language of the vocabulary of the XML document. In this way, TEI.2 would be standard English based TEI, TEIes.2 would indicate that the document has been marked up using the Spanish tagset, and in the same way TEIfr.2, TEIde.2, TEIit.2 would indicate French, German, and Italian, for instance. To add an attribute to the root element, to indicate the language of the tagset, for instance: <TEI.2 markupLang = “it”> would indicate that the markup is in Italian. Use the name of the DTD to indicate the language of the tagset. TeiXLite.dtd would be English, while TeiXLiteFr.dtd would be the French equivalent. Option 3 is by far the worst method, since a document instance may lack a DOCTYPE declaration, and there may be lots of customized TEI DTDs everywhere with very different and unpredictable names. However, options 1 and 2 are reasonably good methods to identify the language of the markup. Consensus is needed to make one of them the common practice. IMPLEMENTATION DETAILS For the website pages we used JSP (dynamic pages) and HTML (static pages), and these are run under DH.indb 28 6/06/06 10:55:17 DIGITAL HUMANITIES 2006 Single Sessions P. 29 a Tomcat 5.5 web server. For the translations, we used XSLT, as described in [1, 2, 3] AUTOMATIC GENERATION OF MARKUP TRANSLATORS USING XSLT The XSLT model is thought to transform one input XML file into one output file (see figure 4), which could be XML, HTML, XHTML or plain text, and this includes program code. It does not allow the simultaneous processing of two input files. Fig. 4. The XSLT processing model. There are certain cases when we would like to process two input files altogether, like markup translation (see figure 5). Fig. 5. The ideal transformation required. As XSLT does not allow this, two alternatives occurred to us, both comprising two transformation steps. The first approach is to automatically generate translators. As Douglas Schmidt said: “I prefer to write code that writes code, than to write code” [7]. This is what we have done for the MMWebsite, i.e. to pre-process the translation map in order to generate an XSLT translation script which includes the translation knowledge embedded in its logic. Then this generated script can perform all the document-instance translations required. The mapping structure supports the language equivalences for various languages, so we should generate a translator for every possible pair of languages. Whenever the mapping structure is modified, a new set of translators must be generated. Fortunately, this is an automated process. Fig. 6. Pre-generation of a translating XSLT script, to then translate the document instance. The other alternative would be to merge the two input files into a new single XML structure, and then to process such file which would contain both the XML document instance, and the translation mapping information (see figure 7). This implies joining the two XML tree structures as branches of a higher level root. Fig. 7. Merging the two files before applying XSLT. Although this approach may prove useful for some problems, we did not use it for the MMWebsite, because the file merging preprocessing must be done for each file to translate, increasing the web service response time. Using preprocessed translators instead proved to be a faster solution. This limitation, which is proper of the XSLT processing model, could be avoided by using a standard programming language like Java instead. HOW WE ACTUALLY DO IT The mapping document which contains all the necessary structural information to develop the DH.indb 29 6/06/06 10:55:17 P. 30 Single Sessions DIGITAL HUMANITIES 2006 language converters is read by the transformations generator, which was built as an XSLT script. XSL can be used to process XML documents in order to produce other XML documents or a plain text document. As XSL stylesheets are XML, they can be generated as an XSL output. We used this feature to automatically generate both an English-to-local-language XSL transformation and a local-language to English XSL transformation for each of the languages contained in the multilingual translation mapping file. In this way we assured both ways convertibility for XML documents (see figure 8).Fig. 8. Schema translation using XSLT. For each target language we also generate a DTD or a Schema translator. In our first attempts, this took the form of a C++ and Lex parser. Later, we changed the approach. Now we first convert the DTD to a W3C Schema, then we translate the Schema to the local language, and finally we can (optionally) generate an equivalent translated DTD. This approach has the advantage of not using complex parsers (only XSLT) and also solves the translation of Schemas. In our latest implementation, the user can freely choose amongst DTD, W3C Schema and RelaxNG, both for input and output, allowing for a format conversion during the translation process. Many other markup translators can be built to other languages in the way described here. CONCLUSIONS Amongst the observed advantages of using markup in one’s own language are: reduced learning times, reduction of errors and higher production. It may also help spread the use of XML vocabularies like DC, TEI, DocBook, and many others, into non-English speaking countries. Cooperative multilingual projects may benefit from the possibility of easily translating the markup to each encoder’s language. Last, but not least, scholars of a given language feel more comfortable tagging their texts with mnemonics based on their own language. FUTURE WORK Multilingual Help Services: As already said, brief descriptions for elements and attributes in different languages have been added to the mapping structure. This allows for multilingual help services, like generating a glossary in the chosen language of the elements and attributes used in a given document, or a given DTD/Schema. We are working on adding this feature. Footnotes ♣ This work is part of the METASIGN project, and has been supported by the Ministry of Education and Science of Spain through the grant number: TIN2004-00779. 1 Translations of the TEI tagset by: Alex Bia and Manuel Sánchez (Spanish), Régis Déau (French), Francesca Mari (Catalan), Arno Mittelbach (German)",
        "article_title": "The Multilingual Markup Website ♣",
        "authors": [
            {
                "given": "Alejandro",
                "family": "BIA",
                "affiliation": [
                    "U. Miguel Hernández (Spain)"
                ]
            },
            {
                "given": "Jaime",
                "family": "GÓMEZ",
                "affiliation": [
                    "U. Alicante (Spain)"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Automatic summarization appears to be for the future an important domain in textual mining and information retrieval. The main purpose of automatic summarization is the extraction of the relevant information contained in a document. There are various approaches to make a summary, some approaches use a semantic representation of the text to generate a summary, and others find relevant parts of a text to extract them and to constitute the summary. In the first place, we introduce the domain of automatic summarization and then we present the two main approaches. Finally, we expose our method, ours choices, and the software application which proceed from it. 1. Présentation du résumé automatique Le résumé automatique a pour but de fournir à un utilisateur l’information pertinente et essentielle d’un document sous forme de rapport synthétique. Il doit au travers d’un résumé retranscrire le sens général de ce que le document original à voulu exprimer. Le parcours d’un document pour connaître son intérêt lors d’une recherche par un utilisateur peut être long et inutile, notamment s’il doit parcourir un grand nombre de textes. Un logiciel de résumé automatique permet ainsi à des utilisateurs de ne pas parcourir les textes dans leur totalité, en ne leur faisant lire que le résumé, ce qui produit ainsi pour eux un gain de temps important dans leurs recherches. L’intérêt du résumé automatique apparaît pour la consultation sélective de documents par des personnes dont la lecture entière de ceux-ci est impossible. Permettant une lecture synthétique, il aide le lecteur à filtrer les documents qui peuvent l’intéresser pour une lecture ultérieure alors normale du ou des documents choisis selon sa recherche. On note que la nature du résumé reste floue, il n’existe pas en effet de résumé standard. De nombreuses expériences sur des résumeurs professionnels montrent qu’il n’existe pas de résumé type. Le laboratoire LaLICC de l’université Paris4-Sorbonne a par ailleurs acquis une expérience dans le domaine du résumé automatique depuis plusieurs années. La réalisation de plusieurs projets tels que SERAPHIN, SAFIR et ContextO ont participé à la réflexion et à la mise en place d’applications concrètes dans ce domaine. Dans la phase actuelle, ces travaux entrepris sont repris en étant introduit dans la nouvelle plate-forme informatique EXCOM (EXploration COntextuelle Multilingue) qui a pour objectif principal l’annotation sémantique automatique de texte (dont la tâche de résumé automatique fait partie). 2. Les différentes approches du résumé automatique Nous allons présenter ici les deux grandes méthodes existantes dans le résumé automatiques afin d’introduire ensuite le projet EXCOM. La méthode par compréhension Cette méthode est issue essentiellement du domaine de l’intelligence artificielle. Elle considère la tâche de résumé automatique comme devant être calquée par l’activité résumante humaine. La constitution d’un résumé par un logiciel doit ainsi passer par la compréhension totale du texte. Le logiciel doit pouvoir construire une représentation du texte, qui éventuellement peut être modifier ensuite, afin de pouvoir générer à partir de celle-ci un résumé. L’avantage de cette méthode est de vouloir s’inspirer des processus cognitifs humains utilisés dans la compréhension de texte. Néanmoins en dehors de cet aspect, des problèmes surgissent. Premièrement, la compréhension de texte par l’homme est une tâche très loin d’être comprise, donc son implémentation informatique semble encore impossible. Deuxièmement, la représentation d’un texte est également très compliquée, et cette notion reste encore difficile pour les linguistes. Chaque méthode par compréhension propose une représentation propre, mais aucune n’arrive à représenter le texte correctement. La complexité d’un texte sous tous ses aspects (discursif, temporel, etc.) est toujours une barrière à la construction correcte d’une représentation. Enfin la génération du résumé qui apparaît comme étant l’étape finale est aussi difficile. Les travaux sur la production automatique de textes à partir de représentations sont encore très limitées dans leurs résultats. La méthode par extraction Cette méthode est issue essentiellement du domaine de la recherche d’information. L’objectif de cette méthode est de fournir rapidement un résumé simple à valeur informative pour l’utilisateur. Elle consiste par l’extraction des phrases les plus pertinentes du texte traité afin de constituer le résumé devant retransmettre l’essentielle de l’information pertinente générale qui se dégage du texte original. Le résumé est alors constitué des phrases extraites du document. Le travail principal se situe alors dans l’évaluation de la pertinence des phrases du texte suivant un ou plusieurs critères. On peut dissocier alors deux grandes façons de faire. Les techniques statistiques qui prennent comme critère de pertinence la présence de termes fortement représentatifs du texte traité (grâce à un calcul de fréquence). Une phrase est alors extraite ou non suivant la présence de ces termes représentatifs dans celle-ci. Ces techniques sont limitées et se trouvent confrontées à certains problèmes, comme la synonymie des termes par exemple. Les techniques plutôt linguistiques s’appuient sur la présence de marques linguistiques de surfaces pour établir l’importance ou non d’une phrase dans le texte. Certaines marques bien précises permettent d’attribuer une valeur sémantico-discursive à la phrase et ainsi de connaître sa pertinence ou non dans la structure discursive du texte. L’avantage de la méthode par extraction est de ne pas passer par des représentations complexes du texte, et de pouvoir fournir un résumé de façon assez simple (en comparaison d’une méthode par compréhension). Néanmoins les problèmes surviennent dans la qualité du résumé obtenu. Comme le résumé est le résultat de l’extraction d’un ensemble de phrases du texte que l’on a concaténée, la cohésion et la cohérence du résumé peuvent devenir médiocre. Il faut donc dans ces méthodes veiller à la qualité du résumé en sortie, notamment par des méthodes d’évaluations. 3. La plate forme EXCOM et le résumé automatique La plate-forme EXCOM est un moteur d’annotation sémantique travaillant à partir de ressources linguistiques préalablement rentrées par des linguistes. DH.indb 32 6/06/06 10:55:18 DIGITAL HUMANITIES 2006 Single Sessions P. 33 Le troisième critère est lié à la thématique présente dans le texte. Nous cherchons dans les phrases des termes de filtrage, c’est-à-dire qu’ils correspondent aux mots les plus représentatifs de l’univers thématique qui se trouve dans le texte. Enfin le dernier critère, qui est un critère négatif, est la présence ou non dans la phrase d’anaphores pronominales. La présence de pronoms personnels sans référent dans le résumé contribue à sa mauvaise lisibilité, et des stratégies discursives devront alors être étudiées pour la sélection ou non de ces phrases. Il existe donc trois étapes fondamentales dans la construction du résumé : • la première étant la phase d’annotation du texte selon les points de vue • la seconde étant la construction du résumé par la sélection des phrases disposant de la meilleure valeur de pertinence P. La valeur de pertinence P d’une phrase correspond à une valeur numérique qui est calculée en fonction des quatre critères qualitatifs qui sont affectés à chaque phrase • enfin la troisième étape étant la phase de nettoyage du résumé obtenu dans la seconde étape à l’aide de règles appropriées, afin d’assurer une cohésion et une cohérence meilleure. Nous montrerons donc dans la présentation des exemples de résumés que nous commenterons en expliquant l’avantage de notre stratégie. D’un point de vue technique EXCOM repose essentiellement sur les technologies XML. Par ailleurs la plate-forme propose une ouverture vers le multilinguisme en prenant en compte d’autres langues que le français tels que l’arabe et le coréen. La technique utilisée pour l’annotation est celle de la méthode d’exploration contextuelle constituée au sein du laboratoire. Cette méthode recherche à identifier des indicateurs linguistiques dans le texte, puis dans le cas où ils seraient présents, explorer le contexte textuel dans lequel ils se situent à la recherche d’autres indices linguistiques afin de pouvoir attribuer une annotation sémantique sur le segment textuel désigné par le linguiste. Ce traitement textuel repose sur deux hypothèses fondamentales : la première admet la présence dans un texte de marques discursives affectées à des points de vue, et la seconde affirmant l’invariance de ces points de vue suivant les domaines traités dans le document. Le choix de points de vue adaptés est ainsi en rapport avec la nature du texte traitée : articles scientifiques, articles de journaux, essais, etc… L’essentiel des ressources utilisées correspond donc à un ensemble de règles d’explorations préalablement construites par les linguistes. Il convient de remarquer que cette méthode d’exploration contextuelle ne fait pas appel à des ontologies externes mais que le système reste entièrement compatible avec celles-ci. La tâche de résumé automatique, actuellement en développement sous EXCOM, utilise pour la constitution de résumé une méthode par extraction de phrases basée sur quatre critères. A la suite de la segmentation préalable du texte en phrases, on attribue à chaque phrase quatre valeurs, correspondant aux quatre critères de pertinence. Le premier critère de pertinence que nous avons retenu pour une phrase est la valeur de son annotation sémantique qui est attribuée par une règle d’exploration contextuelle. Les principaux points de vue que nous retenons pour le résumé sont l’annonce thématique, la conclusion, la récapitulation et les soulignements de l’auteur. Le second critère correspond à la position de la phrase dans la structure textuelle. La position de certains types de phrases (comme les annonces thématiques ou les conclusions) par rapport à l’organisation des éléments constitutifs de l’argumentation de l’auteur, est déjà une information essentielle pour l’attribution du rôle de la phrase et de sa pertinence au niveau discursif. Ce second critère se trouve ainsi fortement lié au premier.",
        "article_title": "Le résumé automatique dans la plate - forme EXCOM",
        "authors": [
            {
                "given": "Antoine",
                "family": "BLAIS",
                "affiliation": [
                    "Laboratoire LaLICC UMR 8139,     Paris-Sorbonne (Paris IV)"
                ]
            },
            {
                "given": "Jean-Pierre",
                "family": "DESCLÉS",
                "affiliation": [
                    "Laboratoire LaLICC UMR 8139,     Paris-Sorbonne (Paris IV)"
                ]
            },
            {
                "given": "Brahim",
                "family": "DJIOUA",
                "affiliation": [
                    "Laboratoire LaLICC UMR 8139,     Paris-Sorbonne (Paris IV)"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Recent discussion about the scholarly digital edition has focussed on ways to change the edition from a passive text, only there to be consulted, into a dynamic research environment. Siemens in (Siemens 2005) asks why as yet we have seen no convincing integration between text analysis tools and the hypertext edition. Best in (Best 2005) speculates on the possibilities this vision offers for Shakespeare research. To some extent it seems to be what Mueller is realising in the Nameless Shakespeare (Mueller 2005). An essential step towards seamless integration of text analysis tools into the digital edition (TAML, the Text Analysis Mark-up Language) is suggested in (Sinclair 2005). The most visionary statement of the dynamic edition’s potential is no doubt given by Robinson in (Robinson 2003). A dynamic edition, in his view, while offering text analysis and other tools that may shed light on some aspect or another of the edited texts, would also be open to the addition of new content, of corrections, of many different types of annotations. Integrating third-party annotations into the edition is something that seems especially interesting, as it would open up the edition to the results of interpretive studies. The output of scholarly processes of textual analysis (as e.g. suggested in Bradley 2003) could be fed back into the digital edition, and made available for querying by other scholars. This paper will focus on a solution for adding third party annotations into a digital edition. It will propose a REST (Representational State Transfer, Fielding 2000) API for the exchange of annotation information between an edition and an annotation server. The edition display software (which transforms the edition XML source file into HTML) will ask the annotation server for the annotations that apply to text fragments that are being displayed to the edition user. Depending on the parameters the annotation server will return either annotations formatted for display or instructions for hyperlinking the text to the annotations. Thus, the digital edition will be able to include a display of external a nnotations without knowing about the annotations’ contents or even the annotation data model. The paper presentation will include a brief demonstration of a prototype implementation of the protocol. The demonstration will be based on a digital emblem book edition at the Emblem Project Utrecht (http://emblems.let.uu.nl) and use the EDITOR annotation toolset under development at the Huygens Institute (http://www.huygensinstituut.knaw.nl/ projects/editor, Boot 2005). EDITOR at present consists of an annotation input component that runs on the user’s workstation and an annotation display component that runs on a web server. The input component displays the CSS-styled edition XML to the user and facilitates the creation of multi-field user-typed annotations to arbitrary ranges of text in the edition. The display component, still at an early stage of development, shows the annotations in conjunction with the edition XML, has some facilities for filtering and sorting, and will offer, one day, advanced visualisation facilities. The EDITOR server component will serve up the annotations for display in other contexts, first and foremost, presumably, in the context of the digital edition that they annotate. As Robinson notes, one of the more complex issues in annotating the digital edition is the problem of concurrent hierarchies and the mark-up overlap problems to which this gives rise. The EDITOR annotation toolset assumes the edition and its annotations will be stored in separate locations. Each annotation stores information about the start and end locations of the text fragment to which it applies. There is no need to materialize the annotations into tagging interspersed between the basic edition mark-up, and the overlap issue therefore does not arise (the solution in that respect is similar to the Just In Time Markup described in Eggert 2005). Similarly, as the edition XML remains unmodified, there is no need to worry about potential corruptions during the annotation process. Making available third-party annotations from within the digital edition will go a long way towards establishing a ‘distributed edition fashioned collaboratively’, to borrow Robinson’s words. My paper will briefly look at some of the wider issues the integration of third-party scholarship into the digital edition raises. How will the presence of DH.indb 34 6/06/06 10:55:18 DIGITAL HUMANITIES 2006 Single Sessions P. 35 Edition’, CH Working Papers, http://www.chass.utoronto.ca/epc/chwp/Casta02/Siemens_casta02.htm, accessed 2005-11-13. Sinclair, Stéfan (2005), ‘Toward Next Generation Text Analysis Tools: The Text Analysis Markup Language (TAML)’, CH Working Papers, http://www.chass.utoronto.ca/epc/chwp/Casta02/ Sinclair_casta02.htm, accessed 2005-11-13. third-party material influence the edition’s status? Should there be a review process for third-party contributions? Or is it old-fashioned to even think in terms of ‘third parties’? Robinson speaks of the edition as a ‘mutual enterprise’. Editorial institutes, such as the Huygens Institute, will need to rethink their role, as scholarly editions evolve into centrepieces of ever-expanding repositories of text-related scholarship. References Best, Michael (2005), ‘Is this a vision? is this a dream?’: Finding New Dimensions in Shakespeare’s Texts’, CH Working Papers, http://www.chass.utoronto.ca/epc/chwp/Casta02/Best_casta02.htm, accessed 2005-11-13. Boot, Peter (2005), ‘Advancing digital scholarship using EDITOR’, Humanities, Computers and Cultural Heritage. Proceedings of the XVI international conference of the Association for History and Computing 14-17 September 2005 (Amsterdam: Royal Netherlands Academy of Arts and Sciences). Bradley, John (2003), ‘Finding a Middle Ground between ‘Determinism’ and ‘Aesthetic Indeterminacy’: a Model for Text Analysis Tools’, Lit Linguist Computing, 18 (2), 185-207. Eggert, Paul (2005), ‘Text-encoding, Theories of the Text, and the ‘Work-Site’, Lit Linguist Computing, 20 (4), 425-35. Fielding, Roy Thomas (2000), ‘Architectural Styles and the Design of Network-based Software Architectures’, (University of California). Mueller, Martin (2005), ‘The Nameless Shakespeare’, CH Working Papers, http://www.chass.utoronto.ca/epc/chwp/Casta02/Forest_casta02.htm, accessed 2005-11-13. Robinson, Peter (2003), ‘Where we are with electronic scholarly editions, and where we want to be’, Jahrbuch für Computerphilologie, http:// computerphilologie.uni-muenchen.de/jg03/ robinson.html, accessed 1005-11-13. Siemens, Ray (with the TAPoR community) (2005), ‘Text Analysis and the Dynamic Edition? A Working Paper, Briefly Articulating Some Concerns with an Algorithmic Approach to the Electronic Scholarly",
        "article_title": "Third-Party Annotations in the Digital Edition Using EDITOR",
        "authors": [
            {
                "given": "Peter",
                "family": "BOOT",
                "affiliation": [
                    "Huygens Instituut, Department of e-Research"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The Orlando Project, which has regularly reported on its work in progress at meetings of ACH/ALLC, is due for publication in early 2006. In this paper the three originating literary scholars on the project will look back at the its original goals, consider significant turning-points in the process, and reflect on what the project ended up producing for the first release. The project takes its name from Virginia Woolf’s fantastic narrative of an aspiring English writer who begins life as an Elizabethan male and is transformed in the course of the novel’s romp through history into a woman who lives up to the year of the novel’s publication in1928. The transformation occurs while Orlando is abroad as ambassador extraordinary for King Charles II in Turkey. Woolf’s narrator has much to say about the difficulties this poses for the historian, lamenting that “the revolution which broke out during his period of office, and the fire which followed, have so damaged or destroyed all those papers from which any trustworthy record could be drawn, that what we can give is lamentably incomplete.” The charred fragments offer some clues, but “often it has been necessary to speculate, to surmise, and even to use the imagination.” We would locate the electronic Orlando, like Woolf’s protagonist, as the site of an extraordinary transformation associated with the challenges of moving between cultures, the limitations of paper, and the necessity for speculation, imagination, and new approaches to scholarship. None of us were experienced in humanities computing when the project was begun; we set out to write a feminist literary history. In the process of trying to figure out how to do it, we decided to use computers. Ten years later, we have produced an extensively tagged XML textbase comprising a history of women’s writing in the British Isles in a form quite unlike any previous history of writing. At a glance, it may look like another translation into electronic form of the genre of alphabetical companion or literary encyclopedia, and it is indeed deeply indebted to that flexible and enduring form. However, our custom markup system has been used to tag a wide range of semantic content in the textbase, and a backend indexing and delivery system allows that markup to be exploited to produce on-the-fly documents composed of portions of the project’s digitally original source documents. The result is a very flexible and dynamic approach to literary history that challenges users to harness the power of the encoding to pursue their own interests. Recent argument about the crisis in scholarly publishing, such as that marshaled by Jerome McGann in support of the NINES initiative, has focused on the need to draw a larger community of scholars into best-practice methods of electronic markup and publication of texts. This is both crucial as a means of addressing the crisis, and indispensable to the continued development of electronic tools to serve the humanities research community. We offer ourselves as a kind of case study in such a process, given that our project did not originate as humanities computing endeavour but was completely transformed in the course of becoming one. In going electronic, we became radically experimental, tackling problems and producing results that we could not have foreseen at the outset. We will reflect on the results of taking an already very ambitious project electronic in relation to a range of factors including: o the impact on the intellectual trajectory of the project of engaging with, in addition to our disciplinary subject matter, a whole new field of inquiry and undertaking what became an interdisciplinary experiment in knowledge representation; o the impact on the project’s temporal trajectory; o funding, and its relationship to funding structures and opportunities; o the intensification of collaboration, increase in project personnel, and transformation of roles and responsibilities; o the impact on research and writing methods of composing an extensive scholarly text in XML; o the shaping of the research itself by the use of XML; o the development a delivery system that aimed at once to be reassuringly accessible and to challenge users to employ the system in new ways; o dilemmas regarding modes of publication While the paper will, given the constraints of time, necessarily touch briefly on some of these various areas, these reflections will be framed as an inquiry into what it means to bridge the gap between the community of researchers deeply invested in humanities computing and the wider scholarly community. We have come to see Orlando as a kind of emissary of humanities computing, in that we hope it will prove to be a major step towards establishing methods for encoding critical scholarly materials. It provides a test case of the feasibility and benefits of employing XML to encode large semantic units of critical discourse. It offers a model which we hope will be employed and adapted by other projects, and we will indicate the direction we would like to take the project in the future. But perhaps most importantly, the Orlando Project offers a substantial resource that in its design will, we hope, alert scholars beyond the humanities computing community to the potential of encoding as a means of scholarly inquiry and a tool of critical expression. The proof of that will be in the pudding, of course, so the paper will also report as far as possible on the initial reaction from the scholarly community in the field of English studies to the project’s public release.",
        "article_title": "Orlando Abroad : Scholarship and Transformation",
        "authors": [
            {
                "given": "Susan",
                "family": "BROWN",
                "affiliation": [
                    "School of English and Theatre Studies"
                ]
            },
            {
                "given": "Patricia",
                "family": "CLEMENTS",
                "affiliation": [
                    "Department of English studies     University of Guelph"
                ]
            },
            {
                "given": "Isobel",
                "family": "GRUNDY",
                "affiliation": [
                    "Department of English studies     University of Guelph"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Web applications are becoming more sophisticated and offer a rich user experience that is similar to native client applications. Examples of these applications include Google maps and Flickr. Interactive applications in the humanities can use some of these same design patterns to improve the user experience. The asynchronous update pattern is evaluated in this paper. Asynchronous update allows web applications to update without a complete page reload and goes by the popular name of Ajax. “Traditional web applications essentially submit forms, completed by a user, to a web server. The web server responds by sending a new web page back. Because the server must submit a new page each time, applications run more slowly and awkwardly than their native counterparts. Ajax applications, on the other hand, can send requests to the web server to retrieve only the data that is needed - usually using SOAP or some other XML-based web services dialect. On the client, JavaScript processes the web server response. The result is a more responsive interface, since the amount of data interchanged between the web browser and web server is vastly reduced. Web server processing time is also saved, since much of it is done on the client.” (from http://en.wikipedia.org/wiki/AJAX) This paper presents a case study of an ongoing project to create a digital library of Navajo language texts. After such texts are put into the database, the texts can be annotated with interlinear linguistic information using an interactive web application. The model for interlinear information that exists in TEI was determined to be inadequate for the present application and a different model is used. The design of the application for interactive and collaborative entry of interlinear linguistic information consists of a browser client using JavaScript and a server-side Exist native XML database that responds to XQueries. The acquisition and parsing methods for the texts are described in Canfield 2005. The Navajo language texts with annotated interlinear information are compliant with a Relaxng schema based on the XML from Bow 2003. Since this is a pilot application, it allows the schema to be tested and perhaps modified before finally adding the schema to TEI using an ODD specification. For example, the interlinear XML for the sentence “t’11’1ko [a’ sisi[“ is: <phrase> <item type=”txt”>t’11’1ko [a’ sisi[</item> <item type=”gls”>then one grabbed me</item> <words> <word> <item type=”txt”>t’11’1ko</item> <item type=”gls”>just then</item> <item type=”pos”>adv</item> </word> <word> <item type=”txt”>[a’</item> <item type=”gls”>one</item> <item type=”pos”>pro</item> </word> <word> <item type=”txt”>sisi[</item> <item type=”gls”>3p grabbed me</item> <item type=”pos”>verb</item> <morphemes> <morph> <item type=”txt”>shi</item> <item type=”gls”>me</item> <item type=”pos”>1st person obj </item> </morph> <morph> <item type=”txt”>yii</item> <item type=”gls”>he/she/it</item> <item type=”pos”>3p subj pro</item> </morph> <morph> <item type=”txt”>NULL</item> <item type=”pos”>classifier</item> </morph> <morph> <item type=”txt”>zi[</item> <item type=”gls”>grab</item> <item type=”root”>ZIID(1)</item> </morph> </morphemes> </word> </words> </phrase> All the XML in the database is transformed to XHTML for the user interface of the application. This page uses a tabular interface to allow the user to see and update the interlinear information for each sentence in the text. For example, if a word has already been annotated, it will appear with all the annotated information. If the word has not been annotated, the user double clicks on the word and the word appears in an editable html text input box. The user can then edit the detailed interlinear information for the text informed by an on-line Navajo lexicon. The display for each sentence appears as below, but the font for Navajo is not active so the characters display as the base ASCII. The sentence is « t’11’1ko [a’ sisi[ « which means «then one grabbed me.» All fields can be edited when double-clicked except for the base word in each sentence. Note that the last word is a verb «sisi[» and each underlying morpheme is annotated. t’11’1ko [a’ sisi[ t’11’1ko gls=just then pos=adverb [a’ gls=one pos=pronoun sisi[ gls=3rd person grabbed me pos=verb shiyiiNULLzi[mehe/she/itgrab1st person object3rd person subject pronounclassifierstem root=ZIID(1) gls= then one grabbed me DH The JavaScript code that updates each of the fields uses XML HTTP Request which allows the page to be updated without a page reload. Note that traditional web applications must reload the entire page for each update. This is time consuming and disruptive to the user experience. A sample of 25 chapter-length documents was used for this evaluation from the Navajo language digital library. The average document size was about 150kb, which is not very large for documents common in humanities applications. Each document update was timed (using JavaScript) in each mode - with asynchronous requests and with traditional page reloads. The average time for an update of a single field using XML HTTP Request was about 8 ms. The average time for a traditional (whole page reload) update of a field was about 400 ms. The traditional method shows a large update time that will cause unneeded user waiting. The traditional method also makes for a disruptive user experience where the page visually reloads while the user is trying to accomplish a task. Many interactive web applications in the humanities would benefit from this asynchronous update design pattern. Whenever a document is large and requires many small updates, the user experience will be improved with asynchronous requests due to shorter load times and a smoother experience with the user interface.",
        "article_title": "Asynchronous Requests for Interactive Applications in the Digital Humanities",
        "authors": [
            {
                "given": "Kip",
                "family": "CANFIELD",
                "affiliation": [
                    "Information Systems, University of Maryland"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "THE TEXTS This image markup project fits into the larger context of an electronic anthology, “Le marriage sous l’Ancien Régime: Une anthologie critique.” Since 1998, C. Carlin has been collecting texts about early modern marriage in France for her forthcoming book, L’imaginaire nuptial en France, 1545-1715. Given that the majority of documents studied for the book have not been republished since their original appearance in the sixteenth and seventeenth centuries, the idea of an electronic anthology should be appealing to scholars in several disciplines (history, literary studies, linguistics, cultural studies, art history, philosophy, religious studies). The proposed anthology is discussed in the article “Drawing Knowledge from Information: Early Modern Texts and Images on the TAPoR Platform” [1]. The radical changes undergone by the institution of marriage in France during and after the Counter Reformation generated texts of several different genres. Included in the anthology will be medical, legal, religious, satirical and literary documents and engravings, all heavily annotated. It is the engravings that interest us for this presentation. As part of a prototype for the anthology, several verse and prose polemics against marriage were encoded with XML markup in 2004 and early 2005. Most engravings of the period whose subject is marriage also fall into the polemical or satirical genre. Six from the collection of the Cabinet des Estampes of the Bibliothèque Nationale de France were requested on CD Rom, and are the test images for this project: Jacques Lagniet, “Il cherche ce qu’il voudrait ne pas trouver” Abraham Bosse, “La femme qui bât son mari” Jean Le Pautre, “Corrige, si tu peux, par un discours honneste” François Guérard, “Le Grand Bureau ou la confrèrie des martires” Nicolas Guérard, “Présage malheureux” Nicolas Guérard, “Argent fait tout” [2] THE SCHEMA Marking up annotations in XML required a framework that allowed for both the text of the annotations, and the image areas to which they correspond, to be encoded in a single document. Given that well-established tagsets exist for each of these functions, an XML model was developed based on a marriage of the Scalable Vector Graphics (SVG) 1.1 specification [3], and a subset of the Text Encoding Initiative (TEI) P5 guidelines [4]. This union allows TEI and SVG markup to operate concurrently. The TEI markup forms the overarching structure of a document, while elements belonging to the SVG vocabulary may appear in specific locations within the TEI encoding. Elements belonging to the SVG vocabulary are permitted within [div] tags and must be enclosed by the [svg] root element. Therefore, [svg] elements may appear anywhere with the TEI markup where [div] elements are permitted. Within an [svg] element may be any number of [rect] elements whose coordinates demarcate the area on an associated image to which an annotation applies. The texts of the annotations are enclosed in [div] blocks of their own, separate from the SVG encoding. This allows for annotation text to be encoded in any TEI-conformant way, presenting the possibility of integrating annotations with larger corpora. A [div] element containing an annotation text is associated explicitly with a set of annotation coordinates through references to the coordinates’ svg:id attribute. Markup validity is enforced through XML Schema or RELAX NG schema files which are bundled with the application. The schema which validates the TEI portion of the encoding has been generated by the ROMA suite of tools provided by the TEI for the purposes of specifying and documenting a customization. The TEI schema is supplemented by the addition of a schema describing the SVG 1.1 specification. The W3C provides the SVG 1.1 schema in either RELAX NG or DTD format, from which an XML Schema version may be derived using Trang [5]. Integrating schema from two different tagsets in this way is greatly facilitated by the modular construction inherent to both the TEI and SVG schema models. SVG may be ‘plugged-in’ to TEI by adding the [svg] root element to the list of allowable content in a particular context, and then associating the requisite schema documents with one another for the purposes of validation. Taking this approach to schema marriage has several advantages. The TEI guidelines for textual encoding provide a tagset whose usage rules are well-defined and understood, facilitating the portability of the encoding between projects, and easing the integration of corpora from different sources. An earlier method of encoding image annotations in XML, Image Markup Language [6], is based on a standalone markup structure which does not offer the same high degree of interoperability as the current model. The TEI encourages customization of its guidelines to accommodate for a wide range of implementations, an approach this project demonstrates. More generally, working with XML allows for the encoded material to be transformed into other formats as requirements dictate, such as XHTML, PDF, or OpenDocument format. THE IMAGE MARKUP TOOL Having decided on our approach to a schema, we then began to look at how we might create the markup. We wanted a straightforward tool for defining areas in an image and associating them with annotative markup, and we looked initially at two possible existing tools, INote [7] and the Edition Production Technology [8]. INote, from the University of Virginia, is a Java application for annotating images. It does not appear to have been updated since 1998. In some ways, INote is an ideal tool; it is cross-platform (written in Java), and covers most of our requirements. However, we rejected INote for several reasons. The program can load only GIF and JPEG images, and we wanted to be able to handle other common image formats such as BMP and PNG. INote also allows only limited zooming (actual size, double size, and half size). We required more flexible DH.indb 40 6/06/06 10:55:19 DIGITAL HUMANITIES 2006 Single Sessions P. 41 At the time of writing, the program is in at the «alpha» stage, and the first public version will be released under an open-source licence in December 2005. The program is written in Borland Delphi 2005 for Windows 2000 / XP. Development of the tool is guided by the following requirements: The Image Markup Tool should: - be simple for novices to use - load and display a wide variety of different image formats - allow the user to specify arbitrary rectangles on the image, and associate them with annotations - allow such rectangles to overlap if the user wishes - provide mechanisms for bringing overlapped rectangles to the front easily - require no significant knowledge of XML or TEI - allow the insertion of XML code if the user wishes - save data in an XML file which conforms to a TEI P5-based schema with embedded SVG - reload data from its own files - come packaged with an installer, Help file, and basic tutorial Using the Image Markup Tool, we have been able to perform several types of direct annotations, including the text within the engravings, commentary on that text, commentary on significant gestures depicted, and information about the engraver and the seal of the library at the time the engraving entered the library’s collection. The tool allows for distinction among types of annotation, and the use of a TEI-based file format allows us to link easily between the markup of the engravings the TEI-encoded polemical texts which are also included in the collection. We are now planning to use the program for a future project which involves marking up scans of historical architectural plans. One of the aims of this project will be to make the plans available to the public, so that (for example) the current owners of heritage buildings will be able to do renovation and restoration work with more detailed knowledge of the original building plan. zooming to handle larger images. Finally, INote uses a proprietary file format. However, INote does allow for polygonal and elliptical annotation areas, something not yet implemented in our own tool. The Edition Production Technology (EPT) platform is an Eclipse-based software suite developed by the ARCHway project [9]. Its ImagText plugin allows the association of rectangular areas of an image with sections of transcribed text. Although it promises to be a very powerful tool, especially for the specific job of associating document scans with transcription text, the interface of the program is complex and would be confusing for novice users. In addition, the tool developers expect and encourage the use of customized DTDs (“We do not provide support or guarantees for the DTDs included in the demo release - it is expected that users will provide their own DTDs and thus their own specific encoding practices.” [10]) The EPT also supports only JPEG, GIF, TIFF, and BMP files; other formats such as PNG are not supported ([http://rch01.rch.uky.edu/~ept/Tutorial/ preparing_files.htm#images]). We therefore decided to write our own markup program, which is called the Image Markup Tool [11]. Fig 1: scrshot_main_1.jpg, avalable at [http://mustard.tapor.uvic.ca/~mholmes/image_markup/scrshot_main_1.jpg]",
        "article_title": "Problems with Marriage: Annotating Seventeenth - Century French Engravings with TEI and SVG",
        "authors": [
            {
                "given": "Claire",
                "family": "CARLIN",
                "affiliation": [
                    "Dept. of French, University of Victoria"
                ]
            },
            {
                "given": "Eric",
                "family": "HASWELL",
                "affiliation": [
                    "HCMC, University of Victoria"
                ]
            },
            {
                "given": "Martin",
                "family": "HOLMES",
                "affiliation": [
                    "HCMC, University of Victoria"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "1 Overview Genre and its relation to textual style has long been studied, but only recently has it been a candidate for computational analysis. In this paper, we apply computational stylistics techniques to the study of genre, which allows us to analyze large amounts of text efficiently. Such techniques enable us to compare rhetorical styles between different genres; in particular, we are studying the communication of scientists through their publications in peer-reviewed journals. Our work examines possible genre/stylistic distinctions between articles in different fields of science, and seeks to relate them to methodological differences between the fields. We follow Cleland’s (2002) work in this area and divide the sciences broadly into Experimental and Historical sciences. According to this and other work in the philosophy of science, Experimental science attempts to formulate general predictive laws, and so relies on repeatable series of controlled experiments that test specific hypotheses (Diamond 2002), whereas Historical science deals more with contingent phenomena (Mayr 1976), studying unique events in the past in an attempt to find unifying explanations for their effects. We consider the four fundamental dimensions outlined by Diamond (2002, pp. 420-424): 1. Is the goal of the research to find general laws or statements or ultimate (and contingent) causes? 2. Is evidence gathered by manipulation or by observation? 3. Is research quality measured by accurate prediction or effective explanation? 4. Are the objects of study uniform entities (which are interchangeable) or are they complex entities (which are ultimately unique)? The present experiment was designed to see if language features support these philosophical points. These linguistic features should be topic independent and representative of the underlying methodology; we are seeking textual clues to the actual techniques used by the writers of these scientific papers. This paper is partially based on our previously presented results (Argamon, Chase & Dodick, 2005). 2 Methodology 2.1 The Corpus Our corpus for this study is a collection of recent (2003) articles drawn from twelve peer-reviewed journals in six fields, as given in Table 1. The journals were selected based both on their prominence in their respective fields as well as our ability to access them electronically, with two journals chosen per field and three fields chosen from each of Historical and Experimental sciences. Each article was prepared by automatically removing images, equations, titles, headings, captions, and references, converting each into a simple text file for further processing. 2.2 Systemic Functional Linguistics We base our analysis on the theory of Systemic Functional Linguistics (SFL; Halliday 1994), which construes language as a set of interlocking choices or systems for expressing meanings, with general choices constraining the possible more specific choices. SFL presents a large number of systems, each representing a certain type of functional meaning for a potential utterance. Each system has conditions constraining its use and several options; once within a system we can choose but one option. Specific utterances are constrained by all the systemic options they realize. This approach to language allows the following types of questions to be asked: In places where a meaning of general type A is to be expressed in a text, what sorts of more specific meanings are more likely to be expressed in different contexts? We focused on several systems for this study, chosen to correspond with the posited differences between the types of science we study: Expansion, Modality, and Comment (Matthiessen 1995). Expansion describes features linking clauses causally or logically, tying in to dimensions 1 and 4 above. Its three types are: Extension, linking different pieces of information; Elaboration, deepening a given meaning via clarification or exemplification; and Enhancement, qualifying previous information by spatial, temporal, or other circumstance. The second system, Modality, relates to how the likelihood, typicality, or necessity of an event is indicated, usually by a modal auxiliary verb or an adjunct adverbial group; as such it may serve to indicated differences on dimensions 2, 3, and 4. There are two main types of modality: Modalization, which quantifies levels of likelihood or frequency, and Modulation, which qualifies ability, possibility, obligation, or necessity of an action or event. Finally, the system of Comment is one of assessment, comprising a variety of types of ``comment” on a message, assessing the writer’s attitude towards it, its validity or its evidential status; this provides particular information related to dimensions 1 and 3. In our analysis, it will be most helpful to look at oppositions, in which an option in a particular system is strongly indicative of one article class (either Experimental or Historical science) while a different option of that same system is indicative of the other class. Such an opposition indicates a meaningful linguistic difference between the classes of articles, in that each prefers a distinctive way (its preferred option) of expressing the same general meaning. 2.3 Computational analysis Because hand analysis is impractical on large document sets the first analyses were done via computer. We built a collection of keywords and phrases indicating each option in the aforementioned systems. Each document is first represented by a numerical vector corresponding to the relative frequencies of each option within each system. From here, machine learning was applied in the form of the SMO (Platt 1998) algorithm as implemented on the Weka machine learning toolkit (Witten & Frank 1999), using 10-fold cross-validation in order to evaluate classification effectiveness. This method was chosen in part because it generates weights for each feature; a feature has high weight (either positive or negative) if it is strongly indicative for one or the other class. 2.4 Human annotation To measure the validity of our computational analysis, we are also performing hand tagging of systemic features on a subset of the corpus articles. Two articles from each journal have been chosen, each to be tagged by two trained raters. Part of the tagging process is to highlight key words or phrases indicating each option; we will compare these statistics to our previously generated feature lists in order to test and refine them. The tagging is currently under way; we will present results at the conference. 4 Results To determine the distinctiveness of Historical and Experimental scientific writing, the machine learning techniques described above were applied to pairs of journals, giving for each pair a classification accuracy indicating how distinguishable one journal was from the other. These results are shown in Figure 1, divided into four subsets: Same, where both journals are from the same science; Hist and Exper with pairs of journals from different sciences, but the same type; and Diff indicates pairings of Historical journals with Experimental ones. The thick black line indicates the mean for each set, and the outlined box represents the standard deviation. As we see, journal pairs become more distinguishable as their methodological differences increase. Interestingly, Historical journals appear more stylistically homogenous than the Experimental journals, which is a subject for further study. This shows that SFL is capable of discriminating between the different genres presented. We also examined the most important features across the 36 trials between different journals. The most consistently indicative-those features that are ranked highest for a class in at least 25 trials-are presented in Table 2. The table is arranged as a series of oppositions: the features on each row are in the same system, one side indicating Historical, the other Experimental. In the system of Expansion, we see an opposition of Extension and Enhancement for Historical and Experimental sciences, respectively. This implies more independent information units in Historical science, and more focused storylines within Experimental science. Furthermore, there are oppositions inside both systems, indicating a preference for contrasting information (Adversative) and contextualization (Matter) in Historical science and for supplementary Information (Additive) and time-space (Spatiotemporal) relations in Experimental science. DH.indb 44 6/06/06 10:55:20 DIGITAL HUMANITIES 2006 Single Sessions P. 45 Figure 1: Learning accuracy for distinguishing articles in different pairs of journals. ‘Same’ are pairs where both journals are in the same field, ‘Historical’ and ‘Experimental’ represent pairs of journals in different Historical and Experimental fields, and ‘Different’ pairs of journals where one journal is experimental and the other historical. Means and standard deviation ranges are shown. System Historical Experimental Expansion Extension(26) Enhancement(31) Elaboration Apposition(28) Extension Adversative(30) Additive(26) Enhancement Matter(29) Spatiotemporal(26) Comment Admissive(30) Validative(32) Predictive(36) Modality Type Modalization(36) Modulation(35) Modulation Obligation(29) Readiness(26) Modality Value High(27) Modility Orientation Objective(31) Subjective(31) Table 2. Consistent indicator features within each of the systems used in the study. Numbers in parentheses show in how many paired-classification tests the feature names was an indicator for the given class of documents. References Argamon, S., Chase, P., and Dodick, J.T. (2005). The Languages of Science: A Corpus-Based Study of The system of Comment also supports the posited differences in the sciences. The Experimental sciences’ preference for Predictive comments follows directly from their focus on predictive accuracy. On the Historical side, Admissive comments indicate opinions (as opposed to factual claims), similarly Validative comments show a concern with qualifying the validity of assertions, comprising more of strong evidence than rigid proofs. Finally in Modality we see interesting contrasted features. On the top level we have near-perfect opposition between Modalization and Modulation in general; Historical sciences speak of what is ‘normal’ or ‘likely’, while Experimental sciences assess what ‘must’ or ‘is able’ to happen. 5 Conclusion This work is the first step in developing new automated tools for genre analysis, which promises the possibility of automatically analyzing large corpora efficiently or stylistic aspects while giving human interpretable results. The specific research presented has implications for the understanding of the relationship between scientific methodology and its linguistic realizations, and may also have some impact on science education. Future work (beyond the hand annotation and analysis already in progress) includes looking into stylistic variation within different article sections, as well as other analysis techniques (such as principle components analysis). Journal #Art Avg. Words J. Geology 93 4891 J. Metamorphic Geol. 108 5024 Biol. J. Linnean Society 191 4895 Human Evolution 169 4223 Palaeontologia Electronica 111 4132 Quaternary Research 113 2939 Physics Letters A 132 2339 Physical Review Letters 114 2545 J. Physical Chemistry A 121 4865 J. Physical Chemistry B 71 5269 Heterocycles 231 3580 Tetrahedron 151 5057 Table 1: Journals used in the study; the top represents historical fields with experimental sciences below.",
        "article_title": "Methods for Genre Analysis Applied to Formal Scientific Writing",
        "authors": [
            {
                "given": "Paul",
                "family": "CHASE",
                "affiliation": [
                    "Linguistic Cognition Laboratory Dept.     of Computer Science Illinois Institute of     Technology 10 W 31st Street     Chicago, IL 60616, USA"
                ]
            },
            {
                "given": "Shlomo",
                "family": "ARGAMON",
                "affiliation": [
                    "Linguistic Cognition Laboratory Dept.     of Computer Science Illinois Institute of     Technology 10 W 31st Street     Chicago, IL 60616, USA"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Studies in the computational analysis of texts have been successful in distinguishing between authors and in linking anonymously published texts with their authors but computational tools have yet to be accepted as mainstream techniques in literary analysis. Criticisms are generally centred around the belief that computational analyses make false claims about scientific objectivity and are in fact no less subjective than any other critical approach. This is perhaps because computational projects are in conflict, at a fundamental level, with contemporary post-structuralist notions of subjectivity, meaning and the arbitrary nature of language. This paper will argue that these objections rest on assumptions about language that need to be examined in light of developments in linguistics and cognitive psychology and that cognitive linguistics has the potential to bring a more interpretive framework to computational stylistics, a practice that has traditionally been applied in fairly narrow, empirical way. Whilst computational analysis points to the possibility of subjectivity that is more coherent than some theoretical approaches imply, it does not necessarily diminish the role of culture and context in the formation of texts and subjectivity as highlighted by materialist readings. The application of cognitive linguistics in a computational study provides a model of syntax and semantics which is not independent of context but deeply bound up in context. Cognitive linguistics can explain the existence of computational results in a way that Saussurean based theories can not. It can offer a rich interpretive model that does not neglect the importance of author, reader, or context through its approach to language and literature as an expression of an innately constrained and embodied human mind. Computational stylistics, particularly in studies of attribution, generally makes use of function words in order to distinguish between texts. As Craig (2004) explains, independent variables like genre, are compared with counts of internal features, or dependent variables, like function words. Correlation of these two kinds of variables is the primary tool of computational stylistics (275-76). Critics of stylistics, most notably Stanley Fish, tend to privilege individual instances of particular words in interpretive communities over more general rules, and question the validity of the stylistic project. From a cognitive perspective, however, language possesses universal features because it emerges from the interaction of “inherent and experiential factors” that are “physical, biological, behavioural, psychological, social, cultural and communicative” (Langacker 1). Langacker claims that each language “represents a unique adaptation to common constraints and pressures as well as to the peculiarities of its own circumstances” (1). Computational stylistics of the kind undertaken in this study provides us with evidence of the peculiarities and creative adaptations of an individual user, and also highlights more general trends which can be used for comparative purposes. Our attitude to what we can say about a text depends largely on our account of language. Widely shared post-structuralist assumptions about language and indeterminacy have contributed to the lukewarm reception of computational stylistics in literary interpretation. In cognitive linguistics “Semantics is constrained by our models of ourselves and our worlds. We have models of up and down that are based on the way our bodies actually function. Once the word “up” is given its meaning relative to our experience with gravity, it is not free to “slip” into its opposite. “Up” means up and not down” (Turner 7). Cognitive stylistics views a text as the product of a human action and it therefore carries the mark of that action. The cognitive belief that language and conventional thought emerge from “our perception of a self within a body as it interacts with an environment” suggests that meaning is somewhat constrained and that “some form of agency is fundamental to language” (Crane 22). The idea of authorial agency is one that is rejected by structuralist and post-structuralist critics. In proclaiming the death of the author Barthes suggests that the text becomes an ‘open sea’, a space of ‘manifestly relative significations, no longer tricked out in the colors of an eternal nature’ (Barthes 170). The notion of the “transcendental signified” rejected by post-structuralist critiques is not, however, the notion of agency proposed by cognitive stylistics. The view of the “Author” rejected by Barthes, Derrida and Foucault is as, Seán Burke explains “a metaphysical abstraction, a Platonic type, a fiction of the absolute” (27). Cognitive stylistics tends not to deal in absolutes. Stylistics is one way of getting evidence and making sense of texts as human actions. Through its approach to thought and language, cognitive stylistics points to issues that are of concern to scholars of literature, such as “subject formation, language acquisition, agency and rhetoricity” (Richardson 157). Cognitive philosophy claims that the mind is embodied and that concepts are therefore created “as a result of the way the brain and body are structured and the way they function in interpersonal relations and in the physical world” (Lakoff and Johnson 37). The links between the brain and the body mean an objective reality is impossible given the role our sensorimotor system plays in perception. But as Lakoff and Johnson explain, it is our sensorimotor system’s role in shaping conceptual systems that keeps these systems in touch with the world (44). The embodied cognitivism of Lakoff and Johnson, also known as second generation cognitivism, argues that our access to the external world is mediated through cognitive processes. Cognitivism provides a framework in which we can still legitimately engage with psychoanalytic interpretations, gender focused readings, and the material conditions of production while using computational and cognitive techniques of analysis. Computers enable us to draw together instances of common forms, and other features of a text, in a way that would be simply impossible to an individual human reader. Cognitive linguistics provides a theoretical justification for paying attention to common forms in the first place, and reveals a way in which the features highlighted by computational approaches can contribute something of value to traditional literary analysis.",
        "article_title": "Combining Cognitive Stylistics and Computational Stylistics",
        "authors": [
            {
                "given": "Louisa ",
                "family": "CONNORS",
                "affiliation": [
                    "School of Humanities and Social Science     The University of Newcastle, Australia"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Introduction In earlier analyses of the introduction of word types in literary works authors came to contradictory conclusions. Some suggested, in accordance with reader’s intuition, that the launch of new chapters and a sudden increase in the number of the newly introduced word types (NWT) usually coincide. Others, on the other hand, found that there is no clear connection in the rise of NWT and the beginning of chapters, rather an increase in NWT appears at longish descriptions with rather stylistic reasons. Words do not occur randomly in texts, so the ultimate goal of building models based on word frequency distributions may not be the reproduction of the original text. Nevertheless, models based on the randomness assumption give reliable information about the structure of the texts (for review see Oakes, 1998; Baayen, 2001). To further our knowledge in this field the source of the bias between the original and the model-based texts should be examined. Baayen (1996; 2001) described a systematic overestimation for the expected vocabulary size and found that this bias disappears when the order of the sentences is randomized, indicating that the bias should not be attributed to constrains operating on sentence level. To prove that this misfit is due to significant changes on discourse level we introduced several new concepts during the process of building the model and analyzing the results (Csernoch, 2003). Among these the fundamental step was to scrutinize NWT in hundred- token-long intervals rather than examining the overall vocabulary size. Next, instead of eliminating the bias between these artificial texts and original works, the significant protuberances on the graphs of NWT were examined. First monolingual sets of works were processed then, to improve the comparison we also analyzed original English texts and their Hungarian translations together with English and German translations of a Hungarian text. Assuming that the changes occur on discourse level, the language in which the text is written should have no significance. In other words, neither syntactic nor semantic constrains on sentence or paragraph level should matter, and only events that occur on discourse level will provide substantial alterations in the flow of the text, and thus produce considerable protuberances on the graphs of NWT. Methods Building the model To analyze a text first the number of different word types was counted, the frequency of each was determined, and then based on these frequencies a dynamic model was built (Csernoch, 2003). The model generated an artificial text whose word types had the same frequencies as in the original text and was able to reproduce the trends of the original text. However, changes which are only seasonal – protuberances – did not appear in the artificial text. To locate these protuberances the difference between the original and the model text was calculated. We then determined the mean (M) and the standard deviation (SD) of the difference. Protuberances exceeding M±2SD were considered significant. The distribution of the hapax legomena was also examined. Assuming that they are binomially distributed their expected mean (Mh) and standard deviation (SDh) were calculated and again those points where considered significant which exceeded Mh+2SDh. texts compared to their translations Original texts were not only compared to the model-generated artificial texts but to their translations in other natural languages. In this study we analyzed the Hungarian novel, SORSTALANSÁG from Imre Kertész and its English (FATELESS) and German (ROMAN EINES SCHICKSALLOSEN) translations, Rudyard Kipling’s THE JUNGLE BOOKS and their Hungarian translations (A DZSUNGEL KÖNYVE), and Lewis Caroll’s ALICE ADVENTURES IN WONDERLAND and THROUGH THE LOOKING GLASS and their Hungarian translations (ALICE CSODAORSZÁGBAN and ALICE TÜKÖRORSZÁGBAN). These three languages were chosen because they are different in their morphological structures, it is hard to trace any common syntactic characteristic which all three share. Analyzing lemmatized texts To check whether the analyses of the raw, un-lemmatized texts give reliable information for the introduction of NWT the lemmatization of both the English and the Hungarian texts was carried out. The English texts were tagged and lemmatized by CLAWS (the Constituent Likelihood Automatic Word-tagging System) [1], while the morphological analysis of the Hungarian texts was carried out by Humor and the disambiguation was based on a TnT tagger [2]. Results Comparing the texts and their translations it was first found that the morphologically productive Hungarian texts had the smallest number of running words and lemmas while the largest number of hapax legomena both in the lemmatized and un-lemmatized versions. In contrast, the English texts contained the most running words but the smallest number of hapax legomena. To each text and language an individual model was created. Based on these models the positions of the significant protuberances were traced and compared to each other in the original texts and their translations. It was noticed that regardless of the actual language these protuberances occurred in most cases at the same position, that is, at the same event in the flow of the story. We could clearly establish that the protuberances were found at places where new, usually only marginally connected pieces of information were inserted into the text rather than at new chapters. This idea was strengthened by a peculiarity of the English translation of SORSTALANSÁG, namely that the boundaries of chapters are different from those of the Hungarian and German texts, which further substantiates that the protuberances do neither necessarily coincide with the beginning nor are hallmarks of a new chapter. Similarly, in the original Alice stories the boundaries of the chapters are eliminated by unusual typographic tools, while in the Hungarian translation these boundaries are set back to normal. Neither the English nor the Hungarian texts produced any protuberances at these places. In THE JUNGLE BOOKS we again found that the significant differences between the original text and the model are not necessarily at the beginning of a new tale, except for cases when a new setting is introduced. The fact that these descriptions have only a stylistic role in the text was further substantiated by examining the distribution of hapax legomena. The number of hapax legomena was found to be high exactly at the same positions of the text where protuberances in the number of the newly introduced word types occurred. To examine the lemmatized version of the texts carried some risk since loosing the affixes might eliminate the change in mode, time, style, etc., while, on the other hand, might reveal events lost in word types carrying the affixes. Since our dynamic model is capable of giving a relatively good estimation for the introduction of words, the question was whether using lemmas instead of word types would provide additional information gained by comparing the artificial texts and the translations to the original text. In the English texts the lemmatization did not reveal any additional information, the protuberances occurred at exactly the same places in the lemmatized as in the un-lemmatized versions. In un-lemmatized Hungarian texts the first protuberance usually occurred later than in corresponding English and German texts, although we were able to locate them by examining protuberances that were somewhat below the level of significance. In these cases lemmatization helped, and we got clear protuberances reaching the level of significance in lemmatized Hungarian texts. The comparison of the dynamic model built to lemmatized texts in different languages might also be used to analyze and compare the vocabulary of the original texts and their translations. It would, furthermore, enable the comparison of the stylistic tools used by the original author and the translator in the introduction of new words. Summary Using lexical statistical models for analyzing texts the explanation for the difference between the original and the model-based artificial text was examined. It was found that changes on discourse rather than on sentence or paragraph levels are responsible for these differences. Two methods were used to prove this. First, texts and their translations, both lemmatized and un-lemmatized versions, were analyzed and compared to a dynamic model built on the randomness assumption to find that the significant changes on the graphs of the newly introduced word types occurred at corresponding positions within the translations. Second, the distribution of hapax legomena was compared to a binomial distribution, again to find that the significant differences between the original and the predicted distributions occurred at descriptions, only in loose connection with the antecedents and what follows. More importantly, these coincided with the significant changes of the newly introduced word types.",
        "article_title": "The Introduction of Word Types and Lemmas in Novels, Short Stories and Their Translations",
        "authors": [
            {
                "given": "Mária ",
                "family": "CSERNOCH",
                "affiliation": [
                    "University of Debrecen, Hungary"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The Baedeker project, presented in this proposal, is initiated within the framework of the Austrian Academy Corpus, a research department at the Austrian Academy of Sciences. The sub corpus comprises first editions of German travel guidebooks brought out by the Baedeker publishing house between 1845 and 1914. The texts cover exclusively non-European destinations. Today they are rare, which is due to low print runs. The aim of the project is threefold: 1) it partly deals with the genre from a literary point of view, 2) it looks at travel guides as cultural historical resources as well as artefacts that represent various discourses on culture, and 3) it examines the capacities of digital resources. Travel guidebooks, not regarded as a literary form in their own right within the classical canon, played a minor role in comparison with travel narratives for a long time. Substantial contributions to the historical development of the genre and its specific language of expression are still few. Even postcolonial literary criticism cannot be regarded as an exception in this respect. Surprisingly enough, few approaches appreciate the importance and impact of this genre on the establishment and maintenance of Orientalist discourses and colonial practises. Linguistic accounts mostly concentrate on contemporary travel guides and exclude historical development and change. Thus, digital versions of early travel guidebooks can provide an incentive to improve both comparative linguistic and literary genre studies. Furthermore, research on tourism history, its influence on modern society, its bearing on social and cultural change has increased quantitatively in many fields of the humanities since the 1980s: history in general, art history, colonial studies, social and cultural anthropology, economics, geography and tourism studies, today an accepted sub branch of sociology. In these disciplines it goes without saying that travel guidebooks are valuable sources. Nonetheless, they are often dealt with as sources among many others. (The few exceptions are e.g. James Buzard 1993; Kathleen R. Epelde 2004, Sabine Gorsemann 1995, Rudy Koshar 2000 and the research group Türschau 16 1998.) The Baedeker, appearing from 1832 onwards, set the standard and defeated all competition both inside and outside the German-speaking countries. One cannot talk about travel guides without Baedeker coming to mind. During the first decades of the Baedekers, the focus was on Europe. However, they were issued for non-European destinations as well, an aspect missing from critical literature. The guides in the Baedeker-Corpus cover a variety of regions such as Palestine and Syria (1875), Lower and Upper Egypt (1877, 1891), North America and Mexico (1883), Asia Minor (1905), the Mediterranean coastline of Africa (1909) as well as India (1914). Dealing with a wide range of cultural environments the “Tourist Gaze” upon the “Other” has to be scrutinized in greater depth. Assuming that images of the “Other” reflect cultural self-perception to a great extent, travel guidebooks tell at least as much about the “Self” as about the “Other”. For well known reasons, none of the components involved here can be taken for granted as precise, unambiguous or fixed and independent entities. Taking this argument seriously, self-images - like all the other components - are to be understood as flexible phenomena. Moving away from the very frequent restriction on one region or country the Baedeker project turns towards a wider geographical diversification to explore the German repertoire of how one used to speak at the turn of the 19th century about one’s own culture, and at the same time, that of others. As concerns the digital methods by which these phenomena are to be investigated, the following has to be pointed out: while XML is now an accepted standard for the creation and exchange of digital data, it is necessary to move towards a closer consideration of domain-specific XML vocabularies. All Baedekers have undergone scanning, OCR and basic XML annotation as usual with all AAC projects. The task at hand is to devise a schema to markup the features of the Baedekers, relevant for their role in travel history. Existing standards like AnthML and Archaeological Markup Language are focused on material artefacts. A markup language which considers immaterial cultural aspects is missing so far. As a matter of course the development of a standardized language needs the expertise of the wider scholarly community and has to be a team effort - a requirement not feasible within one institution. Thus, the Baedeker project should be seen as a small contribution in preparing such a markup language, designing a sub-set of tags focusing on a well defined segment of cultural life. The main challenge is encoding what travel guides are essentially supposed to do, namely introducing foreign cultures and people/s, recommending an itinerary, assessing sites and festivals, cultural and social conditions, suggesting modes and attitudes of behaviour to adopt in these places and on these occasions. Since cultural knowledge as well as recommendations, valuations, stereotypes or comparisons often is articulated in an implicit manner, which is difficult to encode, the project targets subject-matters such as people/s, languages and religions, social, political, and other cultural concepts as well as sights being recommended, valuated, stereotyped, and compared in the travel guides. As an example I will refer to the repertoire of “group designations”, be it ethnic, national, social, religious, political, and occupational, showing how they relate to the historical discourse on culture. The paper will demonstrate that subject-matters can be easily marked up, they allow for an appropriate access to context - i.e. different routes to topics and explicit as well as implicit knowledge - and they provide a basis for comparative analysis. In addition, this strategy separates annotation from interpretation and limits the risk of encoding preconceived assumptions. Detailed domain-specific markup can be applied to other texts dealing with similar topics - to primary and secondary sources, historical as well as contemporary. In this respect the Baedeker can be seen as a starting point. Using open standards allows for ongoing quality enhancement and adjustment. XML annotation, in this sense, is not a single-serving tool, but a permanent enrichment, accessible and shareable with the wider scholarly community. Retrieval results can be reviewed by different scholars, paving the way for reinterpretation and new questions. It is expected that differing results will come from the same markup.",
        "article_title": "Digitizing Cultural Discourses. Baedeker Travel Guides 1875 - 1914.",
        "authors": [
            {
                "given": "Ulrike ",
                "family": "CZEITSCHNER",
                "affiliation": [
                    "Ulrike Czeitschner, AAC-Austrian Academy Corpus"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The Berlin-Brandenburg Academy of Science and Humanities (BBAW) [1] is an international and interdisciplinary association of excellent scholars with a distinctive humanities profile. The Academy hosts about 30 long term projects in the humanities with a project runtime often longer than 100 years. Examples of these projects include work on academic dictionaries, printed and manuscript editions, documentations, bibliographies, archival and publishing projects and more. These project groups have access to information and data made by outstanding scientists like Gottfried Wilhelm Leibniz, Johann Wolfgang von Goethe, Immanuel Kant, Albert Einstein, Jacob and Wilhelm Grimm, Alexander von Humboldt and many more who were members of the Academy during the last 300 years. Throughout its history the Society could rank 76 Nobel Laureates among its members. According to the “Berlin Declaration on Open Access in the Sciences and Humanities” [2], which was signed by the President of the Academy, an initiative was founded to provide a sustainable, interactive and transparent way to inspire activities in supporting research, communication and presentation with electronic media or in other words to “electrify” the long term humanities projects hosted by the Academy. This initiative is called “TELOTA – the electronic life of the academy”. [3] One part of TELOTA is the so called “Project of the Month” (POM) [4] working group which started work in January 2005. The main task of this working group is to provide solutions for the mentioned issues which are cost efficient, future proven (especially to be independent from commercial vendors) and freely extensible. Every month the data of a selected academy project is processed, (re-)structured and presented on the web to give researchers in the humanities, scholars and the interested public a new view in the extensive knowledge inventory of the academy. To identify and process information from the long term projects is one of the central tasks for POM. Further goals are: 1. To replace older, cost-intensive proprietary tools which are often not very suitable for presentations in the World Wide Web: • Improve the unification of the developed solutions for the different projects with respect for already existing solutions and open standards as well as concentration on third party open source software. • Production of reusable software modules. 2. To offer to the interested public an overview and provide an insight into the work done by the projects hosted by the academy: • To make new information and resources available on the World Wide Web. • Adaptation, unification and customization of existing data to offer a new point of view on a certain project. • Give access to the raw data which can be queried by arbitrary applications using XQuery. [5] 3. To benefit each humanities project hosted by the academy: • The projects should be able to access and administrate the data they produce on their own for a gradual extension of their web presence and research material. • Guarantee of long term accessibility and preservation as a result of consistent data and coherent administration. • Real time accessibility to the projects’ results in the World Wide Web. • Support of the project’s work flow with tools especially developed for their needs. All the applied technologies and third party tools are reused, like all the gained experience is transferred from one project to the next. In addition new technologies are adopted to the working group’s portfolio so it is able to react properly to the monthly changing requirements. This paper will introduce the work of the “Project of the Month” working group and exemplary present two systems for humanities projects from the viewpoint of an “in-house” working group. It shows the possibilities of developing electronic resources of long term projects in a very short time period. Additionally it demonstrates a way how the mentioned technologies can be combined as flexible as possible. The first system, the “scalable architecture for electronic editions”, uses the opportunities of web services applied on critical text editions and was developed while processing prominent projects such as the “Corpus Medicorum Graecorum/Latinorum” or the “Marx- Engels Gesamtausgabe”. The main component is a native XML-Database [6] which is able to interpret XQuery-scripts to form the web application. The user, according to his needs, dynamically decides on the view of the presented texts and translations and the information which is displayed like line numbers or links to the apparatus. So he can customize the electronic edition depending on his scientific position or interests. If possible, facsimiles are linked to text, translation and apparatus and if needed it is possible to search the electronic edition in different scripts, like ancient Greek. The second system, an approach for digital dictionaries, shows the development stages of an interactive on-line dictionary which currently is work in progress and could contain the digital versions of dictionary projects of the academy in the future. Such a system is necessary for the real time digital presentation of dictionary project results. Examples are the “Dictionary of contemporary German language”, the “Dictionary of Goethe” or the “German Dictionary of Jacob Grimm and Wilhelm Grimm”. One main feature besides arbitrary querying the database using XQuery is the possibility to add and edit own dictionary articles, if the user is authorized to do so. The search results than can be displayed in HTML or PDF. Both systems are conceptually designed with a general attempt but currently serve as sample applications. The use of a more technically matured version of this systems should not be limited to one project or just the academy long term projects rather than potentially be open to any kind of critical text edition or dictionary.",
        "article_title": "TELOTA - the Electronic Life of the Academy. New Approaches for the Digitization of Long Term Projects in the Humanities.",
        "authors": [
            {
                "given": "Alexander ",
                "family": "CZMIEL",
                "affiliation": [
                    "Berlin-Brandenburg Academy of Sciences and Humanities"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Laurence Sterne (1713-1768) is better-known nowadays as a novelist, but he was also an Anglican minister whose literary style was initially developed in the pulpit. He wrote many sermons before he turned to novel-writing. Indeed, his first publications were single sermons. Melvyn New, who cannot be praised enough for publishing the first scholarly edition of his sermons, claims that it is “foolish” to “argue Sterne’s uniqueness as a sermon-writer”. Others have followed suit (e.g. Elizabeth Kraft). However, Paul Goring and Judith Hawley have cogently argued that Sterne’s contemporaries often commented on the distinctiveness of Mr Yorick’s sermons. As a scholar who has devoted much of her research to eighteenth-century English pulpit literature, I have long had the impression that some of Sterne’s sermons stand out above all the rest. I therefore propose to test this intuition/assumption through a comparison between Sterne’s homiletic discourse and a corpus of contemporary sermons, in order to assess whether it is possible to reconcile the viewpoints of New and Goring. My working hypothesis is that Sterne as a preacher may have dealt with typical homiletic ideas in a very original (hence ‘eccentric’) idiolect. His innovation may be more stylistic than doctrinal. Besides, eccentricity seems to be the characteristic feature of only a relatively small number of Sterne’s sermons, which strike the reader as being more narrative, imaginative or even novelistic texts than standard post-Restoration pulpit oratory. Most of his other homilies sound much more conventional. Whether this is due to extensive plagiarism -- as first systematically analysed by Lansing Van der Heyden Hammond -- or to typically Latitudinarian theology, as argued by New, remains to be seen. Sterne’s printed sermons will be compared with a full-text corpus of eighteenth-century English sermons, comprising works by Jonathan Swift, John Wesley, perhaps George Whitefield, the manuscripts of John Sharp (1723-92), and a subset of political sermons published in the first two decades of the century. Furthermore, internal comparisons between the collections of sermons which Sterne himself prepared for publication after the first instalment of Tristram Shandy and the three posthumous volumes published by his daughter are also necessary. This paper will be based on the approach developed by the predominantly French school of lexicometry and stylometry, which emphasizes the use of exploratory multivariate statistical methods such as correspondence analysis and cluster analysis. For lack of a single software program that would ideally carry out all the necessary tasks, several packages will be used, especially Hyperbase, Lexico3, Weblex, Wordmapper, maybe Wordsmith. The linguistic and stylistic features identified by Douglas Biber as underlying different text types, especially the dimensions labelled by him as “narrative versus non-narrative concerns,” “informational versus involved production,” and “persuasion” will be explored.",
        "article_title": "Eccentricity and / or Typicality in Sterne’s Sermons ? Towards a Computer - Assisted Tentative Answer",
        "authors": [
            {
                "given": "Françoise ",
                "family": "DECONINCK-BROSSARD",
                "affiliation": [
                    "Université Paris X, U.F.R.     d’Etudes anglo-américaines"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Background In a paper presented at ACH/ALLC 2005, Allen H. Renear et.al. describe a problem of potentially great significance (Renear 2005). They argue that: “In ordinary linguistic communication we often use a name to refer to something in order to then go on to attribute some property to that thing. However when we do this we do not naturally construe our linguistic behavior as being at the same time an assertion that the thing in question has that name. (Ibid, p. 176)” Further, they claim that this distinction is over-looked when conceptual models based on encoded texts are developed. In our work at the Unit for Digital Documentation at the University of Oslo, we have used XML encoded material as sources for several of our databases (Holmen 1996, Holmen forthcoming). The way this is done is by marking up texts both descriptively and interpretatively, followed by the use of software to extract information which is included in the databases. If Renear’s argument is correct, we may infer that the databases include assertions which are based on information in the source texts that is, strictly speaking, not grounded in these texts. For example, we could be using a text as the source of a naming in the database while the naming is merely exhibited, and not asserted, in the text. The false resolutions Renear et.al. propose three possible resolutions to this problem, but they also state that all of these are false. Their resolutions are the following: 1. TEI encoding represents features of the text only. 2. The use of two arcs, i.e. “The Semantic Web community solution”, which will be discussed below. 3. Exhibition is a special case of presupposition. Based on the description of our work above, it should be obvious that resolution no. 1 is not an alternative for us. Semantic modelling of the real world on the basis of descriptions in texts is part of our work. I find it difficult to understand how resolution no. 3 may represent a possible solution. Whether exhibition is a type of presupposition or not does not change the basic problem; i.e. in our case, the use of a text as the source of a naming which is merely exhibited in the text. The problem remains the same if the naming is also presupposed in the text, as long as it is not asserted. I claim that resolution no. 2 is not false after all, and below I will demonstrate how the Conceptual Reference Model (CIDOC-CRM) will solve a similar problem in my example text. The CIDOC-CRM is a ontology developed in the museum community to be used for cultural heritage documentation. My example text In this paper, no general solution to the problems identified above will be proposed. However, I believe that the special solution that I propose could easily be generalized. The text used in my example is based on the work of Major Peter Schnitler. In the 1740s, Major Schnitler was appointed by the Danish government to explore the border area between the northern parts of Norway and Sweden/Finland. Significant parts of the text in the manuscript that he handed over to the Danish government consist of transcripts of local court interviews which were carried out by Schnitler in order to gather information about the local population as well as what they had to say about the border areas. The material includes information directly relevant to the border question, as well as general information of the areas in question, which corresponds to similar material collected through work carried out in Europe at the time (Burke 2000, pp. 128 f.). The text fragments below are taken from the very first meeting described in the text (English translation from Danish by me): [1] Of the Witnesses, supposed to be the most Cunning on the border issue, Were and stood up in the court 1: Ole Larsen Riise. [2] For these the Kingly order was read out loud [...] and they gave their Bodily Oath [3] Question: 1: What his name is? Answer: Ole Larsen Riise (Schnitler 1962, p. 1) In these quotes, we find that several facts are asserted by the text. Excerpt 1 claims the existence of a witness. We will call this witness x. Being a witness implies being a person. Thus, x is a person. We may also note that x is referred to by using the name “Ole Larsen Riise.”, abbreviated “OLR” below. Excerpt 2: Person x gave an oath to speak the truth. Excerpt 3: Person x, according to the text, claims that his name is OLR. The source of the naming is person x, as spoken out loud at a specified place at a specified date in 1742. The text puts forward an assertion by person x that he is named OLR. Modelling the semantic content from our perspective My semantic model of these facts will include the following information: It is easy to describe the source of the three first assertions through CIDOC-CRM, by stating that they are documented in Schnitler’s text: In this figure, as well as in the next one, the boxes with names starting with E represents entities, while the boxes with names starting with P represents the properties linking them together. But how do we describe the source of the naming event? We start with the event in which the attribute was assigned (the naming event, a speech act), which is an E13 Attribute assignment which states that x carried out this particular speech act: Figure 2 When looking at these two model figures, it is striking to what extent the modelling of the giving of the oath in Figure 1 compares to the naming of x in Figure 2. The explanation is that those are similar situations. Our traditional way of reading made us structure them differently in the table above, whereas represented in the CIDOC-CRM structure they came out the same in Figure 1 and 2. In order to show clearly in what way they correspond, note that line 4 in the table above could be rewritten as follows: Assertion Source 4) x named himself ORL The text This is a good example of the way modelling may help us understand a text better. What we have done is to rethink the difference between an event (x gave an oath) and a fact (ORL is the name of x). In order to model the fact correctly, i.e. to show that it was exhibited rather than asserted in the text, we had to consider it as a naming event. Considering it as an event is more feasible in that an event typically has actors who are responsible for the outcome. Further, this makes more sense in that both expressions are speech acts. When it is considered as a speech act, the naming event is the same kind of event as the giving of an oath. Why solution 2 is not false after all In order to be able to see the problem with Renear’s solution no. 2, or to realize that the problem is not really there, we have to quote his text in extensio: “Another approach, this one anticipated from the Semantic Web community, is simply to insist on an unambiguous corrected conceptual representation: one arc for being named “Herman Melville”, one for authoring Moby Dick. But this resolution fails for the reasons presented in the preceding section. Although this model would be in some sense an accurate representation of “how the world is” according to the document, it would not represent what is asserted by the document. The authorship arc in the corrected RDF graph model will correspond to relationships of exhibition, not assertion; and there is no accommodation for this distinction in the modelling language. (Renear, p. 178)” In the first couple of sentences in this paragraph, the resolution of using an “unambiguous corrected conceptual representation” is said to have failed. The next couple of sentences weakens his statement by saying that only RDF does not accommodate this; “there is no accommodation for this distinction in the modelling language” (my emphasis). There are no arguments to support why a different modelling language could not solve the problem. In fact, the CIDOC-CRM does solve this, by giving the modeller an opportunity to state explicitly who is the source of an assertion, as demonstrated in Figure 2. In the example above, we knew who made the assertion exhibited in the text. But even if we did not know, we could still make a similar model as long as we accept that it was made by somebody. In CIDOC-CRM, the modelling of entities we infer to exist without knowing who or what they are is quite possible. Generalization The example described above is quite special, as it includes an explicit naming. But it can be argued that all person names, at least in 18th century Scandinavia, are based on naming events, as people are baptised. As long as we believe that this is the case, we can include in the model an explicit attribute assignment event as the one in Figure 2 for each name used in the text. This will be an event of which we do not know who carried it out or when it took place, but that is not necessarily a problem. The will always be things we do not know in historical texts. The naming event we model this way will also be an event that is not documented in the text we are basing the model on. Whether this is acceptable is a decision one has to take when building up such a model. Conclusion There is reason to believe that the problem described in Renear’s paper is an important one. But a solution to the problem has been identified. I have shown that for one specific type of text, the problem may be solved by using CIDOC-CRM modelling including explicit statements of the sources of the assertions exhibited in the text. Further research may disclose whether this solution will work for other types of texts as well.",
        "article_title": "The Exhibition Problem. A Real Life Example with a Suggested Solution",
        "authors": [
            {
                "given": "Øyvind ",
                "family": "EIDE",
                "affiliation": [
                    "Unit for Digital Documentation at the Faculty of Arts, University of Oslo"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "In the work of the TEI Ontologies SIG there have been an interest in finding practical ways of combining TEI encoded documents with CIDOC-CRM models. One way of doing so is including CIDOC-CRM information in a TEI document and linking CIDOC-CRM elements to TEI elements where appropriate. In this paper, this method is described through an example, together with an outline of the additional elements necessary in the TEI DTD used. Background In projects at the Unit for Digital Documentation, University of Oslo, we have created SGML and later XML encoded versions of printed and hand-written museum documents, such as acquisition catalogues, for more than ten years (Holmen 1996). To be able to store such documents in a standard format, we are planning to use TEI. Much of our material are archaeological documents, and there have been a growing interest in the use of XML in general and TEI in specific in archaeological community the last few years (Falkingham 2005, sec. 3.3, cf. sec. 4.3 and 5.2.3). We also use CIDOC-CRM as a tool for modelling the contents of such tagged documents as they are read by museum professionals. We use this method to be able to include information from XML encoded documents in our museum inventory databases, with references back to the encoded documents (Holmen forthcoming). We would like to store CIDOC-CRM models in close relation to the TEI encoded document. This paper describes an example of how we try to define a syntax in which to store such datasets. Extension of a TEI DTD There are two different ways in which to extend a TEI DTD for inclusion of CIDOC-CRM models. We may include an element for each and every entity and property used in the model, or we may just include one TEI element for CIDOC-CRM entities and one for properties. We have chosen the latter method. This gives a limited and rather simple expansion of the DTD. This is similar to the way the XML version of the bibliographic standard MARC is designed (MARCXML). This method will make it possible to create one document storing both textual markup and semantic interpretations of a text, while keeping the two parts of the document separate, except for links between specific elements in the two parts. This means that the document can be published as a text as well as form the base of an import to a database of records based on the interpretation, keeping the links back to the original text. In this paper, we use a DTD fragment to show an outline of the extensions we need. The extensions is composed of a root crm element including a number of crmEntity elements and a number of crmProperty elements. Example of use A typical situation in which this approach could be used is in archaeological documents. We have created a short dummy document containing some of the informations types commonly existing in our museum documents, as shown in Example 1. The excavation in Wasteland in 2005 was performed by Dr. Diggey. He had the misfortune of breaking the beautiful sword in 30 pieces. There are many objects and relations of interest when modelling the archaeological world described in this text. A typical museum curator reading could include the elements shown in Table 1. 1. A place identified by a name documented in n1. 2. A person identified by a name documented by n2. 3. A time identified by a date documented in d1. 4. An event (the excavation) documented in e1. 5. An event (the breaking) documented in e2. 6. An object (sword) documented in o1. 7. Dr. Diggey participated in the excavation 8. Dr. Diggey and the sword participated in the breaking 9. The excavation took place at the place identified by a name documented in n1 and at a time identified by a date documented in d1. Table 1 A possible CIDOC-CRM representation of one of the entities in Table 1, the excavation in line 4, is shown in Example 3. Included are also references to lines 2, 3, 7 and 9. Note that Example 3 is only showing part of a model that would represent a normal archaeological reading of the paragraph above. E.g., the date should have a “is documented in” property such as the ones for the activity and the person, and the place (Wasteland) should be documented in a way similar to the person Dr. Diggey. 1) archaeological excavation 2) Dr. Diggey 3) the element identified by the id “n2” in the text of Example 2 above 4) 2005 5) the element identified by the id “e1” in the text of Example 2 above Example 3 Example 4 shows this using the TEI-CRM syntax outlined in the DTD addition above. The crm element holds the small CIDOC-CRM model we have expressed in a TEI syntax, while the link element holds connections between the CIDOC-CRM model and the TEI text from Example 2. In this example we see that although all the CIDOC-CRM information may be expressed in such a syntax, an XML validation of the document will only validate a part of the information. It will not check whether the model adheres to the rules for e.g. which CIDOC-CRM properties may be used in connection to which entities. Conclusion and further research While different uses of ontological models in connection to TEI documents will differ in their technical solutions, e.g. whether the ontological model rests in a separate document or not, and which syntax is chosen for the model, the three main elements shown here have to be present: • a TEI document • an ontological model expressed in some XML syntax • link elements to connect the specific elements from the two together We have described a way of expanding TEI that gives us the tools we need to include a CIDOC-CRM model in a TEI document, and connect specific CIDOC-CRM entities to specific TEI elements in the non-CRM part of the document. We would like to see research into similar methods of connecting informations in other ontological systems to TEI documents, to discover whether a similar method is feasible. It would also be interesting to see if it is possible to make a general addition to TEI for this use, or if each ontological system needs its own tag set. In our own research, we will write out an ODD to test this method on samples of our own data, and then continue to implement this model on real data, so that the usability of this method for complete documents and CIDOC-CRM models can be examined.",
        "article_title": "TEI, CIDOC - CRM and a Possible Interface between the Two",
        "authors": [
            {
                "given": "Øyvind ",
                "family": "EIDE",
                "affiliation": [
                    "Unit for Digital Documentation at the Faculty of Arts, University of Oslo"
                ]
            },
            {
                "given": "Christian-Emil ",
                "family": "ORE",
                "affiliation": [
                    "Unit for Digital Documentation at the Faculty of Arts, University of Oslo"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "At the Humanities Faculty of University of Pisa, we started a big project devoted to the study of the language and culture of the young people. The enquiries were initially held in the area of provenance of students of the University of Pisa, including the districts of Massa-Carrara, Lucca, Pistoia, Pisa, Livorno, Grosseto and the district of La Spezia in the Liguria region. We distributed a questionnaire among students of the last two years of secondary school (around 18 years old). The enquiries took place in towns that host secondary schools, i.e. towns and big villages of the urbanized country. The questionnaire includes: 1. a socio-linguistic section enquiring on the social condition, cultural preferences and style of life; usage of the dialect inside the family and usage of forms exclusive for young people; 2. a lexical section that counts 36 onomasiologic questions (“How do you say for…”) referred to the different spheres of the life of young people (family, school, external world, interpersonal relations, judgements, etc.) 3. open fields for spontaneous linguistics insertions. The enquiries involved 2.500 informants and produced over 70.000 forms. Because of this huge mass of data we have looked for a suitable storage solution that would have let us easily query the data. First of all, we wanted to query the forms themselves in a quick and simple way. Secondly, we wanted to group the results according to different parameters such as place of enquiry, sex of the informants, and socio-cultural divisions. Others goals were the possibility of measuring the dialectal or the mass-media language influence and focalizing any lines of development of the Italian language. It was immediately clear that the results of the enquiries could not be directly reversed into a digital format because of a lack of structured information. Furthermore, we felt the need of analyzing and classifying the produced forms from a linguistic point of view, for example tracing every form to the relevant lemma and recognizing the grammar category in order to enable sophisticated queries. Firstly we tried to mark data in the XML language, using the TEI Terminological Databases tag set, but this attempt showed some limitations from the very beginning. As we know, XML works at its best with semi-structured data such as texts, on the contrary our lexical entries are strongly structured data, and are connected with both linguistic information and personal data supplied by informants. Performing such links through XPath or ID-IDREFS system proved to be quite a farraginous mechanisms. Furthermore the TEI Terminological Database tag set offers just generic elements, providing poor description of lexical entries; the TEI Consortium itself has considered unsatisfactory such tag set that will be strongly revised in the forthcoming P5 version (see http://www.tei-c.org/P5/Guidelines/TE.html). In addition, reversing in XML the data pertinent to two points of enquiry, we obtained so large files that most common XML applications showed remarkable difficulties in managing them. For all these reasons, we decided to reverse the data in a MySQL relational database that would have let us to bypass such limitations. A relational DB is certainly a more suitable and performing solution for strongly structured data as we conceived our entries. We called our database BaDaLì, acrostic for Banca Dati Linguagiovanile, but also an expression that means ‘look over there’ or ‘mind you’ or even, in an antiphrastic sense very common in young people language, ‘never mind’. Figure 1 shows a simplified diagram of the BaDaLì database. BaDaLì can be ideally subdivided in four main modules: the informants module (green area), the questionnaires module (yellow area), the lemmas module (blue area) and the forms module (orange area); table lingue (‘languages’) is a lookup shared by lemmas and forms modules, while table inchieste (‘enquiries’) connects the informants module with the questionnaires module. Forms module The very central point of the database is the table forme (‘forms’) and contains the forms produced by informants. Every form is traced to its grammar category (categorie grammaticali table). In some cases the form is also related to a specific dialect (dialetti table), or to a language (lingue table) in case of foreignism. In this way an eventual existing distance between form and lemma (that we call gradiente ‘gradient’) is measured in terms of dialectal influence, foreign features or innovative traits on a graphical level. The forms module is connected to all other modules. Every form, in fact, is traced to its relevant lemma, is produced by an informant, under the stimulation of a question. Lemmas module Tracing forms to their relevant lemmas is a crucial point, especially for innovative forms not recorded by dictionaries. For this reason, we selected a reference dictionary and established a number of criterions to create the suitable lemma for the unattested occurrences. We decided to adopt the most complete dictionary of modern Italian, the Grande Dizionario Italiano dell’Uso (Gradit), edited by Tullio De Mauro. When a new lemma is inserted in the database, specific codes are added to mark its absence or a semantic innovation in regards of Gradit. Relevant lemmas (lemma table) have been categorized too, following an updated version of the classification of lexical components of the language of young people (componenti lessico table) proposed by Cortelazzo 1994. Informant module Table parlanti (‘informants’) collects all the information pertinent to the informants. A number of questions pointed out the need of typifying data collected by the enquiry. For example, the questionnaire asked the informer to declare his/her birthplace and residence. The result was a list of towns and villages that had little relevance in the case of a large scale enquiry. As the question about birthplace was included to retrieve information about the origin of the informant’s family in order to determine if an influence of a non local dialect can be assumed, we decide to group answers in macro-regional categories. The question on the residence was introduced to retrieve information about the commuting, to enquire on eventual differences between the language of towns and small villages. As the secondary schools where the enquiries were held are located in towns or in big villages, we decided to consider only if the informant lives in the same town where the school is located or elsewhere. In such cases a relative loss of information is compensated for the opportunity of comparing the results of different points of enquiry. Questionnaires module Questionnaires module collects data about questionnaires and questions. We took into account the possibility of inserting lexical entries produced by different questionnaires or by updating of our questionnaire. Therefore the term enquired by the questions (voce indagata table) is isolated. For example, in case of the question “How do you say for money?”, the word “money” is the enquired term; the comparison of forms produced by different questionnaires enquiring on the same term is easier by isolating the enquired term form the body of the question. Our database includes onomasiologic questions (‘How do you say for…’) and thematic questions (‘Which words do you know about…’), so a classification for different types of questions (tipi domanda table) was needed. BaDaLì public interface The database is currently freely available on the Web at the address http://dblg.humnet.unipi.it. [FIG. 2] BaDaLì home page The provided interface allows a number of possible queries: 1. starting from a lemma (Tipi lessicali), it is possible to retrieve all the forms traced to such a lemma; a further step allows grouping the results according to three parameters: place of the enquiry, sex, and kind of school. 2. starting from the enquired term (Voci Indagate), it is possible to retrieve all the forms produced under the stimulation of such a term. A further step allows grouping the results according to the same three parameters as in 1. 3. starting from a form (Forme), it is possible to retrieve forms grouped according to the three parameters as in 1. Modelling is certainly a crucial point in designing new projects, since the design will determine from the very beginning which requests a tool will be able to satisfy. In this frame we propose our experience, hoping to stimulate a reflection on such a topic.",
        "article_title": "Modelling Lexical Entries: a Database for the Analysis of the Language of Young People",
        "authors": [
            {
                "given": "Fabrizio ",
                "family": "FRANCESCHINI",
                "affiliation": [
                    "University of Pisa - Department of Italian Studies"
                ]
            },
            {
                "given": "Elena ",
                "family": "PIERAZZO",
                "affiliation": [
                    "University of Pisa - Department of Italian Studies"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "In this paper, we argue that, although collaborative indexing of cultural resources shows promise as a method of improving the quality of people’s interactions with those resources, several important questions about the level and nature of the warrant for basing access systems on collaborative indexing are yet to receive full consideration by researchers in cultural informatics. Specifically, we suggest that system designers look to three cognate fields---classification research, iconographical analysis, and annotation studies---for pointers to criteria that may be useful in any serious estimation of the potential value of collaborative indexing to cultural institutions. Collaborative indexing (CI) is the process by which the resources in a collection are indexed by multiple people over an ongoing period, with the potential result that any given resource will come to be represented by a set of descriptors that have been generated by different people. One community of researchers that has demonstrated heightened, ongoing interest in collaborative indexing is that which is active in cultural informatics, and specifically in the design and development of systems that provide patrons of cultural institutions such as libraries, archives, and museums with networked access to digital representations of the objects in institutions’ collections (see, e.g., Bearman & Trant, 2005). Justifying the collaborative-indexing approach At a simple level, the quality of any cultural information system (or any component of a system such as an indexing mechanism) may be evaluated by considering its performance in relation to three imperatives, each of which corresponds to a separate aspect---cultural, political, economic---of the complex mission of contemporary cultural institutions. 1. How effectively does the system allow its users to find the resources in which they have an interest, and to derive optimal value from those resources once found? In order that patrons derive positive value from their experience of interacting with the objects preserved in institutions’ collections, they should be actively supported in their efforts to develop an interpretive understanding of those objects and the contexts in which they were produced---both by being given high-quality access to information about (including visual images of) objects, and by being encouraged to express their understanding and share their interpretations with others. 2. How broadly and inclusively does the system serve all sections of its parent institution’s public? Managers of cultural institutions commonly express a concern that the opportunity to derive positive value from the services offered by those institutions should be distributed justly among, and enjoyed equally by, members of all social groups. 3. How well does it do at delivering maximal quality at minimal cost? The institution which consistently allows the costs incurred in the collection, preservation, interpretation, and provision of access to its resources to exceed the value of the benefits enjoyed by its public will not survive for long. Justifications of the CI approach tend to proceed by drawing attention to the ways in which it can be viewed as responding to one or other of these three imperatives. Proponents commonly highlight several distinctive characteristics of CI in this regard: (a) CI is distributed: No single person is required to index all resources; no single resource needs to be indexed by all people. (b) CI is cheap: Indexers typically volunteer their efforts at no or low cost to collection managers. (c) CI is democratic: Indexers are not selected for their expertise by collection managers, but are self-selected according to indexers’ own interests and goals. (d) CI is empowering: People who might in the past have been accustomed to searching databases by attempting to predict the descriptors used by “experts” are now given the opportunity to record their own knowledge about resources. (e) CI is collaborative: Any given record is potentially representative of the work of multiple people. (f) CI is dynamic: The description of a given resource may change over time, as different people come to make their own judgments of its nature and importance. All of these characteristics are relevant, in various combinations and to various degrees, to any estimation of the success with which CI-based systems are likely to meet the cultural, political, and economic imperatives described above. But each additionally raises issues of a more problematic nature than is typically admitted. Given the distributed nature of CI, for example, how can it be ensured that every resource attracts a “critical mass” of index terms, rather than just the potentially-quirky choices of a small number of volunteers? Given the self-selection of indexers, how can it be ensured that they are motivated to supply terms that they would expect other searchers to use? Empirical, comparative testing of the utility of different prototypes---focusing, for example, on forms of interface for elicitation of terms, or on algorithms for the ranking of resources---is undoubtedly an essential prerequisite for the future development of successful CI-based systems (Bearman & Trant, 2005). But it is also important, we argue, that the results of prior research in a variety of cognate fields be taken into account when addressing some of the more problematic issues that we have identified. Classification research In classification research, for example, it has long been argued that indexers and searchers benefit from having the opportunity to browse or navigate for the terms or class labels that correspond most closely to the concepts they have in mind, rather than being required to specify terms from memory (see, e.g., Svenonius, 2000). Indexer--searcher consistency, and thus retrieval effectiveness, can be improved to the extent that a system allows indexers and searchers to identify descriptors by making selections from a display of the descriptors that are available to them, categorized by facet or field, and arranged in a hierarchy of broader and narrower terms so that the user can converge on the terms that they judge to be of the most appropriate level of specificity. Current implementations of CI-based systems shy away from imposing the kind of vocabulary control on which classification schemes and thesauri are conventionally founded: the justification usually proceeds along the lines that indexers should be free, as far as possible, to supply precisely those terms that they believe will be useful to searchers in the future, whether or not those terms have proven useful in the past. Yet it remains an open question as to whether the advantages potentially to be gained from allowing indexers free rein in the choice of terms outweigh those that are known to be obtainable by imposing some form of vocabulary and authority control, by offering browsing-based interfaces to vocabularies, by establishing and complying with policies for the specificity and exhaustivity of indexing, and by other devices that are designed to improve indexer--searcher consistency. Theories of iconographical interpretation Another related subfield of library and information science is that which is concerned with the effective provision of subject access to art images (see, e.g., Layne, 1994), and commonly invokes the theory of iconographical interpretation developed by the art historian Erwin Panofsky (Panofsky, 1955). Current implementations of CI-based systems for art museums focus on eliciting generic terms for (what Panofsky calls) pre-iconographic elements, i.e., pictured objects, events, locations, people, and simple emotions---the assumption apparently being made that such terms are those that will be most useful to searchers (Jörgensen, 2003). There is very little evidence supplied by studies of the use of art image retrieval systems, however, to suggest either that pre-iconographic elements are indeed what non- specialists typically search for, or that generic terms lead non-specialist searchers to what they want. We do know from analyses of questions that visitors ask in museums that non-specialists typically do not have the specialist vocabulary to specify precisely what they are looking for (see, e.g., Sledge, 1995). This does not necessarily mean, however, that searchers always default to using pre-iconographic terms whenever they wish to get at more complex themes and ideas, nor that searches for higher-level elements using pre-iconographic terms will be successful. Further studies of the question- formulating and searching behavior of non-specialist art viewers and learners are clearly necessary. Annotation studies Researchers in the human--computer interaction (HCI) community are continuing to develop an agenda for work in the emerging subfield of annotation studies (see, e.g., Marshall, 1998), focusing on ways to improve interfaces that support annotation behavior of a variety of kinds, in a variety of domains. In this research, an annotation is commonly considered as evidence of a reader’s personal, interpretive engagement with a primary document---a form of engagement that is not so different from that which cultural institutions seek to encourage in their patrons. A cultural annotation system that allowed patrons not only to supply their own descriptions of an institution’s resources, but also to add comments and to build communities around personal collections, could be envisaged as a vital service that would help patrons interact with and interpret those resources, largely outside the authority and control of curators and other specialists. It remains an open question as to whether a system that allows patrons to supply their own descriptions of institutions’ resources is most appropriately evaluated as a tool for creating and accessing personal annotations, as a tool for sharing and accessing collaborative descriptions, as a retrieval tool pure and simple, or some combination of all three. Unfortunately, our understanding of the purposes and intentions of users of CI-based systems is still spotty, and further research in this area is necessary. Conclusion In general, we suggest that particular care needs to be taken by cultural institutions in examining and adjudicating between potentially conflicting motives for inviting patrons to provide basic-level descriptions of resources. Classification research shows us that simple assignment of single-word descriptors unsupported by vocabulary control or browsable displays of the semantic relationships among descriptors is not enough to guarantee effective access; theories of iconographical interpretation demonstrate how important it is that non-specialist indexers should not be led to assume that listing what one sees is somehow all that art-viewing and meaning-making involves; and annotation studies encourage us to consider how cultural institutions may go beyond simple systems for collaborative description, and develop more-sophisticated systems for truly collaborative annotation that support deeper levels of interpretation and learning.",
        "article_title": "Collaborative Indexing of Cultural Resources: Some Outstanding Issues",
        "authors": [
            {
                "given": "Jonathan ",
                "family": "FURNER",
                "affiliation": [
                    "OCLC Online Computer Library Center, Inc., Dublin, OH, USA"
                ]
            },
            {
                "given": "Martha ",
                "family": "SMITH",
                "affiliation": [
                    "Information School, University of Washington, Seattle, WA, USA"
                ]
            },
            {
                "given": "Megan ",
                "family": "WINGET",
                "affiliation": [
                    "School of Information and Library Science, University of North Carolina at Chapel Hill, NC, USA"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The Septuagint is in many ways a remarkable collection of texts. It represents the first known attempt to translate the Hebrew Bible into an Indo-European language, namely Hellenistic Greek. As such, it functions as an invaluable source for understanding pertinent linguistic, translational, text-critical, socio-cultural, and philosophical-theological issues that led to its creation and reception. Spanning in its inception from the first translations in the early- to mid-third century BCE to the later revisions in the second century CE, it gives scholars an insight not only into the development of the Greek language, but also into the influence of a Semitic language on its vocabulary and possibly even its syntax. Furthermore, being one of the rare cases where both a translated Greek text and the Semitic source text are extant, it also offers a rich source of insight into contemporary translation techniques and philosophies, albeit influenced by its hagiographic status, and helps in establishing the possibility of a clearer understanding of other Greek texts that are generally deemed to be translations from Semitic originals. Last, but not least, is its reflection of the culture and ideology of diaspora communities in the eastern Mediterranean metropoles, which led to the emergence and shaping of two important religious groupings. The Septuagint is also amongst the ancient texts to receive early concerted applications of Humanities Computing approaches. The most prominent project in this line is the Computer Assisted Tools for Septuagint Studies (CATSS) project, which was called into life through the initiative of the International Organization for Septuagint and Cognate Studies in the early 1970s. Located at the University of Pennsylvania’s emerging Center of Computer Analysis of Texts (CCAT), this project sought the use of computing resources towards three goals: (1) a morphological analysis of the Greek text, resulting in a tagged electronic text, (2) the comparison of Hebrew and Greek texts, resulting in an electronic parallel aligned Hebrew and Greek text, and the recording of published critical variants, resulting in an electronic Greek texts, with variants encoded. All these texts are now freely available (upon the signing of a user agreement/declaration) at the CCAT’s gopher and form the basis of most, if not all, current Septuagint projects and studies making use of computing. Departing from this pioneering, indispensable, and foundational application of Humanities Computing approaches to the study of the Septuagint, this paper will present an appraisal of the Humanities research questions presently asked of these texts, on the one hand, and of the potential of applying Humanities Computing in answering them, on the other. Beyond the widely agreed proposals of transforming such resources into established formats, e.g. Unicode for character encoding and TEI XML for text encoding, I will seek to discuss concrete problems and possibilities in pursuing Humanities Computing applications to the Septuagint, while generalising some of the insights within a wider context. The wider question will be: What should be done to and with electronic text(s) of the Septuagint in order to enrich it as a resource for answering the philological, historical, socio-cultural, and theological questions currently asked about it? One of the aims of this paper will be to tease out the current disciplinary boundaries between traditional Humanities approaches and emerging Humanities Computing ones and to identify important developments in their relationship. An important presupposition in this discussion will be the understanding that Humanities Computing, as a hybrid discipline, will only be truly successful if it reflects a thorough understanding of both Humanities research questions and Computing approaches and develops a balanced negotiation of models and concepts that successfully bridge between the two. The direction of proposing research questions has to be pursuit in both directions – both ‘how can one exploit Computing approaches to answer Humanities questions?’ and ‘How do Computing approaches alter the Humanities questions we ask about a research object?’ have to be asked. To push further the metaphor in the name of the aforementioned CATSS project: It is a matter of using computing approaches as collaborators, rather than as mere assistants. There are a number of issues in the case of the Septuagint that complicate straightforward conceptual models. To choose but one illustration: Both the textual bases of Hebrew source text and Greek translation text are disturbed not only by the textual variants on each side, but also by the fact that the Septuagint texts soon encountered rival Greek translations, in some cases clearly influencing later revisions. Furthermore, the Hebrew source text for the Septuagint clearly departs occasionally from the Hebrew Masoretic Text. Moreover, the Septuagint also includes a number of apocryphal books, some of which were probably written directly in Greek. It is evident that the conceptual model to deal with this scenario cannot just consist of the juxtaposition of two clearly delimited texts. But how does one model such a complicated picture and how does one approach such a picture by computational means? This paper will attempt to propose some answers to this question. It will seek to do so by sketching an ontology and incipit model to accommodate the complication. As an example of the wider context dealt with in this paper, I will discuss another, more general current development in Humanities research projects, not least influenced by contemporary communication technologies: the collaborative nature of the research undertaken. While the negotiation of consensus remains a crucial achievement and necessity in such endeavours, what are we to do with disagreement? How do we encode minority opinions and use them as a resource in computational approaches? If the underlying arguments are important: How do we record them for both agreements and disagreements?",
        "article_title": "The Septuagint and the Possibilities of Humanities Computing: from Assistant to Collaborator",
        "authors": [
            {
                "given": "Juan ",
                "family": "GARCES",
                "affiliation": [
                    "Centre for Computing in the Humanities"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Introduction The Digital Songlines project differs from the approach taken by most others in the field of virtual heritage. While there are many examples of recreated cultural sites, most of them are of a built form, such as temples, monuments, cities and townships. They are frequently re-created in 3-dimensions with a high level of realism. On the other hand, the Digital Songlines project’s focus is on more than simple visualisation, rather its mission is to recreate an experience; a way of interacting with the simulated environment by identifying the key elements give to each place and its special cultural significance that an Aboriginal group identifies as being within their own tribal boundaries. Integrating the key cultural elements in a synthetic environment goes some way towards providing a setting for exploring otherwise inaccessible or previously destroyed significant sites. While traditional virtual heritage reconstructions frequently depend on technological solutions, Digital Songlines depends more on an understanding of the traditional cultural values attached to specific landscape by the participating Aboriginal people and then on a methodology and process for integrating those values in the digital environment with a focus on cultural relevance independent of its level of visual realism. The continuing traditional culture of Australian Aborigines is one of the most ancient in the world. Recent research suggests that it is at least 40000 years old. Europeancolonisation of Australia since the late eighteenth-century, and farming, mining, tourism and social impacts of modern civilisation have since threatened this most remarkable cultural heritage. Aboriginal cultural custodians realise the urgent need to preserve the evidence of Australian Aboriginal heritage and culture to give young and future generations of Australian Aborigines a chance to identify with their aboriginal roots. However, creating an Australian Indigenous cultural heritage environment causes some difficulties. Australian Aboriginal people perceive, in the landscape, details that non-indigenous people often fail to appreciate. Such details are very much a part of Aboriginal knowledge, spirituality and survival as well as cultural heritage. The landscape is perceived as a cultural entity, and needs to be recognised in a synthetic environment by Aboriginal people if the cultural heritage environment is to be perceived as authentic. One of the difficulties in undertaking such a task is the re-presentation of Aboriginal knowledge. There are few written records, hence it is mostly through the process of interviewing of cultural custodians that information can be gathered. This poses another problem: Aboriginal cultural custodians are not always comfortable with the traditional western research methods of interviewing and recording (AITSIS, 2000); they may prefer to tell their stories in a location which relates to the cultural context of the story, “Aboriginal reading comes out of the land, each place is a repository for information that is rarely commented upon elsewhere in the abstract but is released or stimulated by the place itself” (Strang, 2003, p200) (see figure 1). It is in this context that the Digital Songlines project has set itself the task of collecting cultural knowledge from Australian Aboriginal cultural custodians and Knowledge keepers, and, to provide a means of sharing that knowledge with future generations of Australian Aboriginal people through a virtual environment. Collecting Indigenous Cultural Knowledge The need to ‘locate’ the telling of a story by an Aboriginal cultural custodian where access to the original environment is not possible involved reconstructing some locations using computer generated 3D models. This offers the advantage of portability and flexibility. However, the success of this method relies on its acceptance by the cultural custodians of the synthetic environment as a valid context for sharing their cultural knowledge. Two steps were implemented to try to achieve this. The first step was to identify the elements of the natural landscape that gave it a cultural meaning in the eyes of Indigenous people. The second step was to define a methodology to recreate these elements in a synthetic environment in a way that cultural custodians could recognise the cultural elements. Within the context of this paper the following describes the approach used to identify the elements of the landscape; it explains the protocols for approaching Australian Aboriginal people and a way of enlisting their trust. Understanding the cultural elements of the landscape. To-date some research with Australian Aboriginal people has resulted in suspicion and mistrust. As such , it is essential to inculcate participants in any study at every stage. The Cultural Custodians are the elders of their communities and not generally familiar with virtual reality and multi-media technologies. Therefore, it is necessary to demonstrate the potential of the technology. In this report we discuss an initial study that used a virtual environment showing landscape and approximately 10,000 year-old rock art from Mt Moffat in the Carnavon Gorges National Park in Central Queensland, Australia (see figure 2). It was shown to a group of Australian Aborigines from central Queensland and their reactions observed. As the initial goal was to gain the trust of the community, no recording or formal interview took place. Initially the community was suspicious of the researchers and of the technology but after three hours of community consultation, the community gained a better understanding of the technology and, most importantly, the intentions of the researchers. Following the success of the initial contact, a cultural tour of the region was organised by two of the cultural custodians of the community. This allowed for more observation even though there could still not be any formal recording of the event. One example of an observation related to the tradition of collecting food. Gathering bush food is a cultural activity. Australian Aboriginal people don’t find food in the wild, they believe food is provided to them by Country (a Eurocentric analogy might be a form of benevolent land genie). However, there are conditions. Aboriginal people believe that one has to look after Country if one expects Country to provide for one. That is why, in their view, so many white people died of thirst and starvation in the early white settlement days, because they did not respect the Aboriginal Country law. Looking after Country means more than caring for it, it includes respecting the rituals taught by the ancestors Converting the Observations to VR The most important thing to come out of these observations was the attention to detail leading to a “contextual accuracy” that is very important to all the Aboriginal people encountered. Hence, in re-presenting a specific landscape in VR the following information should be gathered from the site. The importance of flora and fauna to a culturally recognisable landscape means more than realism alone. Realism has only a marginal effect on immersion in a virtual environment (Gower, 2003; Lee, 2004; Lombard and Ditton, 1997; Slater, et al, 1996). Hence, minimal realism yet culturally accurate virtual environments allows people without access to advanced technology, including Indigenous people in remote communities, to gain better access to the Digital Songlines Project and satisfies the needs of those communities for cultural sensitivity to representation of their stories about the land. Conclusion There are at least two issues facing the design of Aboriginal virtual heritage environments, contextual and cultural accuracy and realism. As the context is one of the most important aspects of culture sharing within Australian Aboriginal society then we need to know more about how to re-create a meaningful context in a virtual environment. It may be easier to evoke this by using reduced detail than highly realistic environments. In order to achieve this we need to learn from Aboriginal people what are the best ‘signs’ that can be used to identify their environments as unique and culturally significant. Acknowledgment This work is supported by ACID (the Australasian CRC for Interaction Design) established and supported under the Cooperative Research Centres Programme through the Australian Government’s Department of Education, Science and Training.",
        "article_title": "Capturing Australian Indigenous Perceptions of the Landscape Using a Virtual Environment",
        "authors": [
            {
                "given": "Stef ",
                "family": "GARD",
                "affiliation": [
                    "School of Design, Queensland University     of Technology Information Environments     Program, ITEE, University of Queensland     Australasian CRC for Interaction Design (ACID), Brisbane, Australia."
                ]
            },
            {
                "given": "Sam ",
                "family": "BUCOLO",
                "affiliation": [
                    "School of Design, Queensland University     of Technology Information Environments     Program, ITEE, University of Queensland     Australasian CRC for Interaction Design (ACID), Brisbane, Australia."
                ]
            },
            {
                "given": "Theodor ",
                "family": "WYELD",
                "affiliation": [
                    "School of Design, Queensland University     of Technology Information Environments     Program, ITEE, University of Queensland     Australasian CRC for Interaction Design (ACID), Brisbane, Australia."
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "This presentation proposes a procedure for using frequency comparisons to help resolve challenging word choices in French-to-English literary translations. It explores a computer-assisted approach for enhancing the human translation of literary texts by two principal and complementary means: 1) through the comparison of existing translations, when available, as metatranslational, cognitive choices; 2) through the interlinguistic comparison by word frequency of cognitively admissible word choices ostensibly available to the source-language (SL) author and the chronologically distanced target- language (TL) translator. The methodology explored here does not purport to innovate in regards to machine translation but rather attempts to show how techniques in part developed by researchers in that field can assist human translators working with literature, an area where machine translation would not normally be used. In translating “L’Illustre Magicien” (The Illustrious Magician) and “L’Histoire de Gambèr-Aly” (The Story of Gamber Aly), of Arthur de Gobineau (1816-1882), the presenter compares word frequencies in both languages—French and English—to help determine the most suitable choice where several reasonable ones exist. This procedure consists of what he calls interlingual and intralingual frequency comparisons which expand on the concept of componential analysis (CA) proposed by Newmark (Approaches to Translation, 1981) which the latter proposed as an improvement on the matrix method, also addressed by Hervey and Higgins (Thinking French Translation, 2nd ed., 2002). In one example of his CA, Newmark develops an “...‘open’ series of words...and the use and choice of such words is determined as often by appropriate collocation as by intrinsic meaning (i.e. componential analysis): this particularly applies to generic terms or head-words such as ‘big’ and ‘large’, which are difficult to analyze” (29). Newmark also notes that the word-series he chooses (bawdy, ribald, smutty, lewd, coarse, etc.) creates a problem in that it is particularly “...closely linked to any SL and TL culture in period of time and social class....” While the approach proposed here does not distinguish social class, the interlingual frequency comparisons can usually rely on corpora and subsets established from literature written in the same time period as the works being analyzed and as early translations. In this case, several different corpora are used. For French: ARTFL (American and French Research on a Treasury of the French Language) and other tools made available by Etienne Brunet and the University of Nice. For English: Bartleby; the British National Corpus (BNC), the British Women Writers Project and Chadwyk-Healy’s LION (Literature Online). Besides the two translations, the overall project includes critical essays which treat the tales’ major themes of love, death, and intellectual or emotional obsession. Honoré de Balzac’s La recherche de l’Absolu (1834) floats tempingly in the background of “The Illustrious Magician” since the latter’s author even published an essay on Balzac in 1844. While several different examples will be used for applying the frequency approach, one example of using interlinguistic frequencies occurs when translating the following French from “The Illustrious Magician”, a several pages before its conclusion: “En effet, en entrant dans une des grottes, après en avoir visité deux ou trois, il aperçut son maître assis sur une pierre, et traçant avec le bout de son bâton des lignes, dont les combinaisons savantes annonçaient un travail divinatoire” (Nouvelles asiatiques, 1876, p. 150). Focusing for the purposes of this abstract on the clause containing the word “divinatoire,” we can initially compare the two existing translations. Both were published in New York, the first by Appleton Press in 1878, the second by Helen Morganthau Fox with Harcourt Brace in 1926: 1) Appleton (259): “...the profound combination of which announced a work of divination.” 2) Fox (268): “...the learned combinations of which showed that it was a work of divination.” 3) Draft of presenter’s translation: “ whose learned combinations revealed a divinatory work.” One might wonder whether the word “divinatoire” for a French writer in the 1870’s is as recherché as the word “divinatory” in English. Would it be reasonable to substitute the collocation “divinatory work” for the earlier one, “work of divination”? The BNC reveals two (2) occurrences of «divinatory» in 100,000,000 words: 1) A6C Seeing in the dark. ed. Breakwell, Ian and Hammond, Paul. London: Serpent’s Tail, 1990 (32,621 words); 2) CS0 Social anthropology in perspective. Lewis, I M. Cambridge: Cambridge University Press, 1992, pp. 5-130, (35,946 words). In the British Women Writers Project (http://www.lib.ucdavis. edu/English/BWRP/) we find no occurrences for eighty (80) texts, 1789-1832. We should conclude that the word “divinatory” is a rare word in English. However, within the ARTFL database of French works for the years 1850-1874 alone, out of 11,214,324 words, there are four (4) occurrences. Out of the 9,548,198 words in the database for the period 1875-1899 there are ten (10) occurrences or a total of 14 in 20,762,522 words or about 67 occurrences per 100,000,000 words, about 30 times more than in the BNC. In the period 1875-1899, its rate of occurrence was about once per million words, the highest of the four quarters in the century (see http://www. lib.uchicago.edu/efts/ARTFL/databases/TLF/freqs/tlf.timerseries.html). We must conclude that the French use of “divinatoire” at the time when Gobineau was using it was significantly more common than is its English counterpart though both uses are nonetheless relatively rare. And the bibliography for the fourteen occurrences covering the period in which the Nouvelles asiatiques was published includes works by well-known authors such as Amiel, Bourget, Flaubert, Garnier, the Goncourts, Mallarmé and others, as well as Gobineau himself. The one occurrence of «divinatoire» from this last author’s work should of course be subtracted from the total before any comparison with the same publication. Intralinguistically for French, there are forty-two (42) occurrences of the word “divination,” which is exactly three times more frequent than “divinatoire” in the period 1850-1899, out of almost 21 million words. Forty-two versus fourteen in that number is almost a third of an order of magnitude and worthy of notice. We can preserve the rarer use of divinatoire from French in the translation although the usage appears to be even rarer in English. However, we should note that there are seven (7) occurrences of “divinatory” in the 20th-century poetry on Chadwyk Healy’s LION site, so the word can be well represented in the poetic genre. And in comparisons that will be made in the presentation in both vocabulary and themes between Gobineau and Balzac, the interest of both authors in divination will be highlighted. Brunet’s database shows seven occurrences of divination/divinations in the Comédie Humaine (CH) and four of divinatoire, a typical total of the two words (11) for the 4,242,038 words in that corpus (CH) and for his time, 1825-1849: 35 occurrences in the 12,352,370 words contained in the ARTFL database. However, these words are only about one-half as common in French literature for the last quarter of the nineteenth century as in the second quarter. There are 18 occurrences in 9,548,198 words for the ARTFL database during the period 1875-1899 when the Nouvelles asiatiques were published. Besides the few sample words above where the computer-assisted techniques have been applied by a human translator of French literature, the presentation will suggest how such frequency-based comparisons can assist in the translation of thematic groups of words representing the literary authors’ symbolic universes. Often such clusters of associated words can be determined from methodically searching the secondary literature: the thematic areas where critics have focused their interest over the centuries. Furthermore, a complementary technique for using advanced search engines on the Internet to aid in solving translation problems will be illustrated or demonstrated. As long as French and English databases remain available with tools that allow for the appropriate date-stamping, so to speak, of word usage, a methodology can be developed using frequencies and simple statistical tests such as z-scores for comparing the ranking of words across chronological gaps. Such resources offer new and useful tools to the translator both in aiding the development of metatranslations and justifying both them and final translations. Additionally, this process can facilitate greater detail and support for literary criticism that makes use of intertextual and intratextual linguistic materials.",
        "article_title": "French-English Literary Translation Aided by Frequency Comparisons from ARTFL and Other Corpora",
        "authors": [
            {
                "given": "Joel ",
                "family": "GOLDFIELD",
                "affiliation": [
                    "Modern Languages and Literatures,     Fairfield University"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Contemporary stylistic and stylometric studies usually focus on an author with a distinctive style and often characterize that style by comparing the author’s texts to those of other authors. When an author’s works display diverse styles, however, the style of one text rather than the style of the author becomes the appropriate focus. Because authorship attribution techniques are founded upon the premise that some elements of authorial style are so routinized and habitual as to be outside the author’s control, extreme style variation within the works of a single author seems to threaten the validity of the entire enterprise. This apparent contradiction is only apparent, however, for the tasks are quite different. Successful attribution of a diverse group of texts to their authors requires only that each author’s texts be more similar to each other than they are to texts by other authors, or, perhaps more accurately, that they be less different from each other than from the other texts. The successful separation of texts or sections of texts with distinctive styles from the rest of the works of an author takes for granted a pool of authorial similarities and isolates whatever differences remain. Recent work has shown that the same techniques that are able to attribute texts correctly to their authors even when some of the authors’ styles are quite diverse do a good job of distinguishing an unusual passage within a novel from the rest of the text (Hoover, 2003). Other quite subtle questions have also been approached using authorship attribution techniques. Nearly 20 years ago, Burrows showed that Jane Austen’s characters can be distinguished by the frequencies of very frequent words in their dialogue (1987). More recent studies have used authorship techniques to investigate the sub-genres and varied narrative styles within Joyce’s Ulysses (McKenna and Antonia, 2001), the styles of Charles Brockden Brown’s narrators (Stewart, 2003), a parody of Richardson’s Pamela (Burrows, 2005), and two translations of a Polish trilogy made a hundred years apart (Rybicki, 2005). Hugh Craig has investigated chronological changes in Ben Jonson’s style (1999a, 1999b), and Burrows has discussed chronological changes in the novel genre (1992a). I am using authorship attribution techniques to study the often-remarked differences between Henry James’s early and late styles.1 I begin by analyzing a corpus of 46 American novels of the late 19th and early 20th century (12 by James and 34 by eight other authors) to determine the extent to which multivariate authorship attribution techniques based on frequent words, such as principal components analysis, cluster analysis, Burrows’s Delta, and my own Delta Primes, successfully attribute James’s early and late novels to him and distinguish them from novels by eight of his contemporaries.2 Because all of these techniques are very effective in this task, all are appropriate for further investigation of the variation within James’s style, but DeltaLz produces especially accurate results, correctly attributing all 40 novels by members of the primary set in eleven analyses based on the 2000-4000 most frequent words. All of the results also reconfirm recent findings that large numbers of frequent words are more effective than the 50-100 that have been traditionally used, and that the most accurate results (for novel-sized texts) often occur with word lists of more than 1000 words (see Hoover, 2004a, 2004b). The PCA analysis in Fig. 1, based on the 1001-1990 most frequent words, clusters the novels quite well–better, in fact, than analyses that include the 1000 most frequent words. When cluster analysis, PCA, Delta, and Delta Prime techniques are applied to nineteen novels by Henry James, they show that the early (1971-1881) and late styles (1897-1904) are very distinct indeed, and that an “intermediate” style (1886-1890) can also be distinguished. DeltaLz again produces especially accurate results, correctly identifying all early, intermediate, and late novels in 24 analyses based on the 200-4000 most frequent words. These results paint a remarkable picture of an author whose style was constantly and consistently developing, a picture that is congruent with James’s reputation as a meticulous craftsman who self-consciously transformed his style over his long career. A comparison with Charles Dickens and Willa Cather shows that Dickens’s early and late novels tend to separate, but do not fall into such neat groups as James’s do, and that Cather’s novels form consistent groupings that are not chronological. These authors seem not to have experienced the kind of progressive development seen in James. It is dangerous, then, simply to assume chronological development of authorial style. Finally, these same techniques show that the heavily revised versions of The American (1877), Daisy Miller (1878), and The Portrait of a Lady (1881) that appear in the New York edition of James’s novels (1907-09) are consistently and dramatically closer to the style of the later novels. Yet even his notoriously detailed and extensive revisions do not allow PCA to group the revised early novels with the late novels. Instead, the revised versions fall at the border between the early and intermediate novels in PCA graphs (see Fig. 2), and consistently join with their original versions in cluster analyses. The results obtained using Delta show that even the errors make sense. In analyses that are not completely accurate, The Portrait of a Lady and Washington Square, the latest of the early novels (both 1881) are sometimes identified as intermediate. The other errors involve the identification of The Spoils of Pointon, the first of the late novels (1897), as intermediate; no analyses incorrectly identify an early novel as late or a late novel as early. In the analyses that are completely correct for the 19 unambiguously early, intermediate and late novels, the New York edition versions of earlier novels are identified as follows: the revised early novels The American and Daisy Miller are universally identified as early, the revised intermediate novel The Reverberator is universally labeled intermediate, and the revised early The Portrait of a Lady is usually labeled intermediate, but sometimes early. This is an intuitively plausible result, with the latest of the early novels pulled far enough toward the late novels to appear intermediate, but, interestingly enough, DeltaLz, which produces much more accurate results overall, labels all of the novels according to their original publication dates in eighteen analyses based on the 200-2800 mfw. In the remaining six analyses, the early Daisy Miller is identified as late, and in one analysis (based on the 4000mfw) The Portrait of a Lady is identified as intermediate. Further investigation of the implications of these results is ongoing. Authorship attribution techniques thus confirm the traditional distinction between early and late James, establish the existence of an intermediate style, and lay the groundwork for a fuller analysis of the linguistic and stylistic differences upon which they rest. Based as they are on a very large proportion of the text of the novels (the 4000 most frequent words typically comprise more than 94% of a novel), these results provide a wealth of material for stylistic analysis. The huge numbers of words involved will, however, require new methods of selection, analysis, and presentation if they are not to prove overwhelming and incomprehensible. Meeting these challenges will advance and refine authorship attribution techniques, and, at the same time, further illuminate the linguistic bases of James’s style and his process of revision.",
        "article_title": "Stylometry, Chronology and the Styles of Henry James",
        "authors": [
            {
                "given": "David L. ",
                "family": "HOOVER",
                "affiliation": [
                    "Department of English, New York University"
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "",
        "article_title": "",
        "authors": [
            {
                "given": "",
                "family": "",
                "affiliation": [
                    ""
                ]
            }
        ],
        "publisher": null,
        "date": "2006",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    }
]