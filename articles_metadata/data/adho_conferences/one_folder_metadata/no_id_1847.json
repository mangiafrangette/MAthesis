{"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "  Latent Dirichlet Allocation (LDA) topic modeling (Blei, 2012) is a statistical method that discovers hidden themes and topics from a text corpus, and it has been widely applied in digital humanities over the past several years. My survey on applications of topic modeling have found 53 studies from the books of abstracts of the annual international conference of the Alliance of Digital Humanities Organizations between 2011 and 2018   Collection of abstracts from the last decade was initially planned. Unfortunately, due to a broken link, the abstracts of DH2009 could not be obtained, and studies related to topic modeling could not be found in the abstracts of DH2010.  . Topic modeling-related approaches are increasing (Figure 1) and most of them are related to historical studies (18 approaches) and literary studies (17 approaches). It has also been used for religious studies, digital archaeology, etc.      Figure 1. Distribution of studies over time. The Problem  The standard use of LDA topic modeling is to browse corpora through topics and data visualizations, while it is more complex than just training and visualizing topics in practice. Results of topic modeling can be influenced by several factors such as the LDA hyperparameters, topic number, chunk-length of documents, number of iterations of model-updating as well as hyperparameter optimization (Schöch, 2017). As far as I know, a common understanding on handling these factors seems yet to be established. Using text chunking as an example, Jockers (2013) divided novels into chunks in order to capture transient themes which only appear at certain points of a novel. In contrast, Nichols et al. (2018) writes: “Following common practice using LDA on texts, we did not chunk or split the texts in our corpus for analysis.” Stop words removal is another example, where more coherent topics are able to be obtained in general by removing stop words with no contents. Most approaches remove stop words before training the model, however discussions regarding the effectiveness of removing stop words after the modeling process have surfaced (Schofield et al. 2017).   In order to provide a comprehensive overview of how the majority of humanities scholars understands and uses topic modeling, a survey on above mentioned 53 approaches in detail has been done. In this paper I therefore propose to look at these approaches in the following aspects:  Preprocessing: text processing procedures before topic modeling. What kinds of preprocessing is used and why? Modeling: what are the parameters, which control a topic modeling process. How can they influence the results and how are they been used in different approaches?  Postprocessing: What method has been used for the interpretation of topics? How were the quality of a topic model and the topics evaluated?     Preprocessing:  The common preprocessing procedures include lemmatization, part-of-speech (POS) tagging and document chunking. By transforming words to their base form, the topic model can become more concentrated on the semantic structure. Through POS tagging, words with less contents could be identified and removed from corpus, in order to get more coherent topics. Chunking allows us to capture topics which only appear at certain points. My survey pays particular attention to the reasons of applying (or not) a preprocessing procedure in practice. For example, lemmatization is often applied when the corpora are in highly inflected languages like German or French. Document chunking is very diverse: the chunk-size could be several hundred or several thousand words, or a page of a book, or to split a book into ten equal segments. But almost no approach explained the reason of their chunking choices.   Modeling:  The LDA hyperparameters, the number of topics, the number of iterations of model-updating, the hyperparameter optimization control the modeling process. As a matter of fact, no approaches have been reported on setting the hyperparameters, while two approaches reported their number of iterations (Schöch, 2015; Maryl & Eder, 2017) and two approaches applied the hyperparameter optimization (Goldstone, 2014; Falk, 2016). Other approaches are more focused on the visualization and analysis on topic models. In 23 approaches, the choice of topic number has been reported, but only 8 of them explained how the numbers were determined.    Postprocessing:  After the modeling process, it is important to evaluate the topic model and the topics. Only 5 approaches reported their evaluation methods. Although the topic quality can be evaluated by measuring topic coherence (Röder et al., 2015) and topics can also be automatically labeled (Magatti et al., 2009). Only one approach used topic coherence for the evaluation of the trained topics (Rhody, 2014). It is more common to label topics manually and to highlight the correlation between interesting topics and metadata. More than half of all approaches applied data visualization for exploration purposes.  Conclusion This survey intends to provide an overview regarding the common use of topic modeling in digital humanities. It presents the situation that DH-community not always report how topic modeling was applied: Within 53 approaches, around 74% didn't report how their corpora were prepared; more than 70% didn't report which tool was used to train their topic models; almost 57% didn't report how many topics were trained and about 90.5% didn't report how their topic model were evaluated. Without reporting the technical details, the scientific reproducibility and the stability of their research could be questionable.   In addition, the lack of interest (or knowledge) on the complexity of topic modeling itself may indicate non-optimal application of this method. The standard parametrization of the topic modeling tool could be used, but because texts in humanities (for example literary texts) are more complex, it is not always clear how much previous understanding of topic modeling from computer science can be beneficial. One good example of the variation is the subgenre classification using topic modeling in Schöch, 2017. The accuracy difference between the worst and the best models is 17%. To obtain a better understanding of topic modeling, series of systematic investigations into the impact of factors on topic modeling will be my next steps to proceed.   ", "article_title": "A Survey On LDA Topic Modeling In Digital Humanities", "authors": [{"given": "Keli", "family": "Du", "affiliation": [{"original_name": "Universität Würzburg, Germany", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}], "publisher": null, "date": "2019", "keywords": ["digital humanities (history", "corpus and text analysis", "content analysis", "data mining / text mining", "English", "theory and methodology)"], "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}