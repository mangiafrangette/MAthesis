{"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Word embeddings, such as those created by the word2vec family of algorithms (Mikolov et al., 2013), are the current state of the art for modeling lexical semantics in Computational Linguistics. They are also getting more and more popular in the Digital Humanities , especially for diachronic language research (see below). Yet the most common methods for creating word embeddings are ill-suited for deriving qualitative conclusions since they typically involve random processes that severely limit the reliability of results-repeated experiments differ in which words are deemed most similar with each other (Hellrich and Hahn, 2016a,b). We provide a short overview of different embedding methods and demonstrate how this lack of reliability might affect the outcome of experiments. We also recommend a more recent embedding method, SVDPPMI (Levy et al., 2015), which seems immune to these reliability problems and, thus, much better suited (not only) for the Digital Humanities (Hamilton et al., 2016). Word embeddings are a form of computational dis-tributional semantics for determining a word's meaning \"from the company it keeps\" (Firth, 1957, p. 11), i.e., the words it co-occurs with. The word2vec algorithms have their origin in heavily trimmed artificial neural networks. Their skip-gram negative sampling (SGNS) variant is widely used because of its high performance and robustness (Mikolov et al., 2013; Levy et al., 2015). Two other word embedding methods were inspired by word2vec: GloVe (Pennington et al., 2014) tries to avoid the opaqueness stemming from word2vec's neural network heritage through an explicit word co-occurrence table, while the more recent SVDPPMI (Levy et al., 2015) is built upon the classical pointwise mutual information co-occurrence metric (Church and Hanks, 1990) enhanced with pre-processing steps and hyper-parameters from the two aforementioned algorithms. There are two sources of randomness affecting the training of SGNS and GloVe embeddings: First, the random initialization of all word embedding vectors before any examples are processed. Second, the order in which these examples are processed. Both can be replaced by deterministic alternatives, yet this would simply replace a random distortion with a fixed one, thus providing faux reliability only useful for testing purposes. In contrast, SVDPPMI is conceptually not affected by such reliability problems, as neither random initialization takes place nor is a relevant processing order established. Word embeddings can be compared with each other to measure the similarity of words (typically by cosine)-an ability by which they are often assessed (see e.g., Baroni et al. (2014) for more details on their evaluation). In the Digital Humanities, they have already been used to directly track diachronic changes in word meaning by comparing representations of the same word at different points in time (Kim et al., 2014; Kulkarni et al., 2015; Hellrich and Hahn, 2016c; Ham-ilton et al., 2016). They can also be used to track clusters of similar words over time and, thus, model the evolution of topics (Kenter et al., 2015) or compare neighborhoods in embedding spaces for preselected words (Jo, 2016). Besides temporal variations, word embeddings are also suited for analyzing geographic ones, e.g., the distinction between US American and British English variants (Kulkarni et al., 2016). In most of these approaches, the local neighborhood of selected words in the resulting embedding spaces, i.e., words deemed to be most similar with a word in question , are used to approximate their meaning at a given point in time or in a specific domain. Yet the aforemen-tioned randomness leads to a lack of replicability, since repeated experiments using the same data set and algorithms result in different neighborhoods and might thus mislead researchers. To investigate this problem, we trained three models each with three embedding methods, i.e., GloVe and SVDPPMI, on the same data set and measured how they differ in their outcomes on word neighborhoods. Our data set consists of 645 German texts from the 19 th century that are part of the Deutsches Textarchiv Kern-korpus (DTA) [German text archive core corpus] (Gey-ken, 2013; Jurish, 2013). The DTA contains manually transcribed texts selected for their representativeness and cultural importance; we use the orthographically normalized and lemmatized version, with casefolding. We evaluate the word embedding methods by calculating the percentage of neighbors for the most frequent nouns in the DTA on which all three models of each method agree. Overall, SVDPPMI provides perfect reliability, while the other two embedding methods lack reliability, SGNS dramatically so, which is consistent with our prior studies on word2vec (Hellrich and Hahn, 2016a,b). Figure 1 shows the reliability for each model evaluated against the 1000 most frequent nouns in the DTA when their first ten closest neighbors (from one up to ten) are compared. Larger neighborhood size had a small positive effect on the reliability of SGNS and GloVe, yet is clearly unable to mitigate the inherent un-reliability of these methods. A small inverse effect can be observed when the number of the most frequent nouns is modified while keeping a constant neighborhood size of five, as displayed in Figure 2. Finally, Table 1 provides differing neighborhoods for Herz [heart] as a qualitative example. In this case, though not necessarily in general, SGNS models featured a more anatomical view (e.g., bluten [to bleed]), whereas GloVe models uncovered metaphorical meaning (e.g., gem√ºt [mind]) and SVDPPMI came out with a mix thereof. Using SGNS or GloVe models to assess a word's meaning can be strongly misleading, as evi-denced by e.g., three SGNS models representing three different runs under the same experimental setup. They lead to completely different semantic characterizations of Herz [heart], since two provide negatively connotated words (e.g., schmerzen [pain]) as closest neighbors, whereas the third provides a more positive impression (e.g., herzen [to caress]).", "article_title": "Don't Get Fooled by Word Embeddings- Better Watch their Neighborhood", "authors": [{"given": "Johannes", "family": "Hellrich", "affiliation": [{"original_name": "Friedrich Schiller University Jena", "normalized_name": "Friedrich Schiller University Jena", "country": "Germany", "identifiers": {"ror": "https://ror.org/05qpz1x62", "GRID": "grid.9613.d"}}]}, {"given": "Friedrich", "family": "Schiller", "affiliation": [{"original_name": "Friedrich Schiller University Jena", "normalized_name": "Friedrich Schiller University Jena", "country": "Germany", "identifiers": {"ror": "https://ror.org/05qpz1x62", "GRID": "grid.9613.d"}}]}, {"given": "University", "family": "Jena", "affiliation": [{"original_name": "Friedrich Schiller University Jena", "normalized_name": "Friedrich Schiller University Jena", "country": "Germany", "identifiers": {"ror": "https://ror.org/05qpz1x62", "GRID": "grid.9613.d"}}]}, {"given": "Udo", "family": "Germany", "affiliation": [{"original_name": "Friedrich Schiller University Jena", "normalized_name": "Friedrich Schiller University Jena", "country": "Germany", "identifiers": {"ror": "https://ror.org/05qpz1x62", "GRID": "grid.9613.d"}}]}], "publisher": null, "date": "2017", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}