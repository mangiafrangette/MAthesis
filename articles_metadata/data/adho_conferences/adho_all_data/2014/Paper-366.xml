<?xml version="1.0" encoding="utf-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:exsl="http://exslt.org/common" xml:id="Paper-366">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>Encoding Metaknowledge for Historical Databases</title>
        <author>
          <name>
            <surname>Nuessli</surname>, <forename>Marc-Antoine</forename>
          </name>
          <affiliation>EPFL</affiliation>
          <email>nuessli.ma@gmail.com</email>
        </author>
        <author>
          <name>
            <surname>Kaplan</surname>, <forename>Frédéric</forename>
          </name>
          <affiliation>EPFL</affiliation>
          <email>frederic.kaplan@epfl.ch</email>
        </author>
      </titleStmt>
      <publicationStmt>
        <authority/>
        <publisher>EPFL, Switzerland</publisher>
        <distributor>
          <name>EPFL Digital Humanities Laboratory</name>
          <address>
            <addrLine>GC D2 386</addrLine>
            <addrLine>Station 18</addrLine>
            <addrLine>CH-1015 Lausanne</addrLine>
            <addrLine>frederic.kaplan@epfl.ch</addrLine>
          </address>
        </distributor>
        <pubPlace>Lausanne, Switzerland</pubPlace>
        <address>
          <addrLine>EPFL</addrLine>
          <addrLine>CH-1015 Lausanne</addrLine>
        </address>
        <availability>
          <p/>
        </availability>
      </publicationStmt>
      <notesStmt>
        <note type="abstract">This article presents a methodology for systematically documenting metahistorical information (or  paradata) for the encoding of historical data. The methodology allows us, to associate the following to a transcription or data extraction: the author of the process, the research project during which this operation was performed and the pipeline of methods that were used. This metaknowledge is crucial for aligning research data originating from different research groups, using different research approaches. These metadata are also important for the long-term evolution of projects, for which it is necessary to regularly revise data encoding strategies.</note>
      </notesStmt>
      <sourceDesc>
        <p>No source: created in electronic format.</p>
        <p>
          <date when="20140711"/>
          <time when="14:00:00"/>
        </p>
		<p n="session">8</p>
		<p n="room">321 - Amphipôle</p>
      </sourceDesc>
    </fileDesc>
    <profileDesc>
      <textClass>
        <keywords scheme="original" n="category">
          <term>Paper</term>
        </keywords>
        <keywords scheme="original" n="subcategory">
          <term>Long Paper</term>
        </keywords>
        <keywords scheme="original" n="keywords">
          <term>Semantic web</term>
          <term>RDF</term>
          <term>ontologies</term>
          <term>historical databases</term>
          <term></term>
        </keywords>
        <keywords scheme="original" n="topic">
          <term>Reifed RDF</term>
          <term></term>
        </keywords>
      </textClass>
    </profileDesc>
  </teiHeader>
  <text type="paper">
    <front>

      <div>
        <p></p>
        <p></p>
      </div>
    </front>
    <body>
      <div>
        <head>Motivation</head>
        <p>Historical knowledge is fundamentally uncertain. A given account of an historical event is typically based on a series of sources and on sequences of interpretation and reasoning based on these sources. Generally, the product of this historical research takes the form of a synthesis, like a narrative or a map, but does not give a precise account of the intellectual process that led to this result.</p>
        <p>
Our project consists of developing a methodology, based on semantic web technologies, to encode historical knowledge, while documenting, in detail, the intellectual sequences linking the historical sources with a given encoding, also know as<hi rend="italic"> paradata</hi><ref target="n1" rend="sup">1</ref>. More generally, the aim of this methodology is to build systems capable of representing multiple historical realities, as they are used to document the underlying processes in the construction of possible knowledge spaces.</p>
        <head>Overview of the Approach</head>
        <p>Semantic web technologies, with formal languages like <hi rend="italic">RDF</hi> and <hi rend="italic">OWL</hi>, offer relevant solutions for deploying sustainable, large-scale and collaborative historical databases (see for instance <ref target="n2" rend="sup">2</ref>). Compared to traditional relational databases, these technologies offer more flexibility and scalability, avoiding the painful problems of large schema migration. They are grounded in logic and thus permit us to easily conduct semantic inferences.  Some very stable semantic based ontologies like <hi rend="italic">CIDOC-CRM</hi>, now an ISO standard, have been used successfully in the cultural heritage domain for about 20 years <ref target="n3" rend="sup">3</ref>.</p>
        <p>
However, the languages used in the semantic web technologies have a major limitation that prevents their usage for encoding metahistorical information. Expressed knowledge is typically formalised with <hi rend="italic">RDF</hi> triplets which are not objects in the same order as the knowledge content (<hi rend="italic">RDF</hi> resources identified with <hi rend="italic">URIs</hi>) to which they link. For example, it is difficult to document the source, the author or the uncertainty of given <hi rend="italic">RDF</hi> statement.</p>
        <p>
One way to compensate for this flaw, while respecting the <hi rend="italic">W3C </hi>norms, consists of transforming each <hi rend="italic">RDF</hi> triplet (<hi rend="italic">subject predicate object</hi>) into three triplets (<hi rend="italic">statement rdf:subject subject</hi>), (<hi rend="italic">statement rdf:predicate predicate</hi>), (<hi rend="italic">statement rdf:object object</hi>). Using this approach, it becomes possible to add new triplets with a given statement as subject, documenting additional paradata about this statement. The resulting knowledge base can include metahistorical information, i.e. information about historical information creation processes. This metainformation can document the choice of sources, transcription phases, coding strategies, interpretation methods and whether these steps are realised by humans or machines. Thus, each historical database designed following this methodology integrates two levels of knowledge. The first level provides the documentation about the origin, the nature and the formalisation used to encode historical data, while the second level codes for the historical data itself. </p>
        <head>The Knowledge Construction Vocabulary (KCV)</head>
        <p>We are working on a specific <hi rend="italic">RDF </hi>vocabulary, called <hi rend="italic">Knowledge Construction Vocabulary</hi> (KCV), which will enable us to implement the two level organisation using the standards of the semantic web. <hi rend="italic">KCV RDF</hi> statements represent knowledge construction steps, while effective historical knowledge is only expressed through reified triplets. An important concept in this vocabulary is the notion of <hi rend="italic">knowledge spaces</hi>. A knowledge space designates a closed set of coherent knowledge, typically based on a defined set of sources and methods. Examples of knowledge spaces include documentary spaces (e.g. a defined corpus of sources) and fictional spaces (e.g a coherent world typically described in a book).</p>
        <p> 
Figure 1 shows an example of the kind of graphs that can be built using the KCV vocabulary.  In this example, two knowledge spaces have been defined: one documentary space <hi rend="italic">(</hi><hi rend="italic">DHLABDocuments)</hi> and one so-called fictional space <hi rend="italic">(HistoireVenise_S1)</hi>. Each of these two spaces is defined as a unique resource with an associated URI.  A statement <hi rend="italic">(Statement1)</hi> stands for a reified triplet defining that <hi rend="italic">(HistoireVenise)</hi> is a kind of <hi rend="italic">Book </hi>and<hi rend="italic"> </hi>is linked to the documentary space. The KCV vocabulary allows us to document who entered the information (<hi rend="italic">fournier</hi>) and the creation time of the statement (<hi rend="italic">May 06th</hi>). To formalise the fact that the book, <hi rend="italic">HistoireVenise,</hi> is used as a knowledge source, a specific resources <hi rend="italic">HistoireVenise_KS</hi> is created and linked with the <hi rend="italic">HistoireVenise</hi>, the book, and the general document space <hi rend="italic">DHLABDocuments</hi>. </p>
        <figure>
          <graphic url="DH2014_3_kcv-schema"/>
          <p>Fig. 1: A "toy" example of the use of the KCV vocabulary to code historical and metahistorical information</p>
        </figure>
        <p>In the fictional space <hi rend="italic">HistoireVenise_S1</hi>, a statement <hi rend="italic">(Statement2)</hi> codes for a reified triplet indicating that the reconstruction of the Rialto bridge occurred during the period of 1588-1591. Information about the author, the creation date and the reliability of <hi rend="italic">Statement2</hi> are documented using various <hi rend="italic">KCV</hi> triplets. The link between the document space and the fictional space is encoded by a link between the knowledge source <hi rend="italic">HistoireVenise_KS</hi> and a statement, <hi rend="italic">Statement2_origin</hi>, linked to <hi rend="italic">Statement2</hi> of type <hi rend="italic">interpretedtextknowledge</hi>. </p>
        <p>We can make three remarks:</p>
        <list type="ordered">
          <item>This is obviously a "toy" example (real graphs encoding historical data are typically much bigger), but it illustrates how historical and metahistorical information can be coded with a linked data approach. This allows us to envision queries mixing both historical and metahistorical requests, for instance reconstructing an historical context based only on certain kinds of sources or excluding information that was provided to the database by some authors.</item>
          <item>The kind of intellectual processes documented by <hi rend="italic">KCV</hi> can easily include algorithmic steps like digitisation, optical character recognition pipelines on documents,  text mining, semantic disambiguation, etc. The version and the author of the algorithms used can easily be included using <hi rend="italic">KCV</hi> statements. This kind of documentation permits us to exclude historical information linked with some processing using early versions of algorithms that may have "polluted" the data. This is an important prerequisite for building sustainable databases in the long term. </item>
          <item>Documenting metahistorical information using <hi rend="italic">KCV</hi> may look like a tedious process; however, in most cases, this information can be inserted automatically using a higher-level interface. A database interface in which the user is logged permits to easily produce historical data based on the<hi rend="italic"> KCV</hi> vocabulary, taking the form of reified <hi rend="italic">RDF </hi>triplets, while documenting the author, the data and the methods used. </item>
        </list>
        <head>Ontologies Matching</head>
        <p>The <hi rend="italic">KCV</hi> approach for encoding historical databases is also interesting from the perspective of ontologies alignment: a notoriously difficult issue <ref target="n4" rend="sup">4</ref>. Each research group tended to code historical data using their own local ontologies, adapted to their research approach. The metahistorical documentation provided by the KCV vocabulary enables us to envision strategies for mapping such ontologies to a pivot ontology. Figure 2 shows this general process in which several knowledge spaces are linked. Each group locally describes the source documents used (1), transcribes their content (2) and eventually codes/interprets this content (3). Throughout this process, two groups produced two independent custom ontologies (A and B). The alignment process proceed in two additional steps. First, both local ontologies are mapped onto a general content ontology (4) (for instance CIDOC-CRM, but not necessarily) and then, once expressed in this common conceptual model, the information contained in the graph is aligned and the content is merged (5). </p>
        <figure>
          <graphic url="DH2014_3_kcv-schema2"/>
          <p>Fig. 2: The general process of ontologies matching</p>
        </figure>
        <p>Figure 3 gives a more detailed account of the final step. First knowledge sources are mapped, then types are mapped and eventually predicates are mapped. In some cases, only a partial level of correspondence can be reached. These steps can be done manually or automatically and are, of course, subject to errors. It is therefore crucial to document the authors of these matching steps, whether they are humans or algorithms. This is why the authors are, linked all the other steps, described in the KCV vocabulary. </p>
        <figure>
          <graphic url="DH2014_3_kcv-schema3"/>
          <p>Fig. 3: Detail of the ontologies matching process</p>
        </figure>
        <head>Conclusion</head>
        <p>The approach briefly presented in this paper enables us to encode historical and metahistorical data in a unified framework. The method we describe is fully compliant with the current technologies and standards of the semantic web (RDF, SPARQL, etc.). It does impose a unified historical terminology but can also be used in conjunction with existing standards. For instance CIDOC-CRM can be used to describe historical knowledge extracted from archival documents (e.g events, people, places) using RDF triplets and KCV can be used to code information about the CIDOC-CRM triplets themselves, such as documenting who entered a particular triplet. The originality of our proposal comes from the introduction of the this second level (metahistorical) on top of the existing RDF ontologies.  This does not necessarily impose an additional burden on the person encoding the historical data. Using a dedicated web interface, the metahistorical information can be added automatically as the data is progressively entered. </p>
        <p>Coding metahistorical information by making explicit the many underlying modelling processes allows us to prepare for possible ontology evolution and enables easier ontology matching. More importantly, our approach does not impose the search for a global truth (a unique and common version of historical events) but pushes towards the explication of the intellectual and technical processes involved in historical research, thus giving the possibility of fully documented historical reconstructions. </p>
      </div>
    </body>
    <back>
      <div type="References">
        <listBibl>
          <bibl>1. <hi rend="bold">Bentkowska-Kafel A., Denard H., and Baker D.</hi> (2012). <hi rend="italic">Paradata and Transparency in Virtual Heritage</hi>. Ashgate Publishing, Ltd.
          </bibl>
          <bibl>2. <hi rend="bold">Ide, N., and D. Woolner</hi> (2007). <hi rend="italic">Historical Ontologies.</hi> Words and Intelligence II: 137–152.
          </bibl>
          <bibl>3. <hi rend="bold">Doerr, M.</hi> (2003) <hi rend="italic">The CIDOC CRM – an Ontological Approach to Semantic Interoperability of Metadata.</hi> AI Magazine 24, no. 3.
          </bibl>
          <bibl>4. <hi rend="bold">Shvaiko P. and J. Euzenat</hi> (2013). <hi rend="italic">Ontology matching: state of the art and future challenges</hi>. IEEE Transactions on Knowledge and Data Engineering, 25(1): 158-176.
          </bibl>
        </listBibl>
      </div>
    </back>
  </text>
</TEI>
