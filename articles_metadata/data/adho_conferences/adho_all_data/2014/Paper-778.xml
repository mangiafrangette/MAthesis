<?xml version="1.0" encoding="utf-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:exsl="http://exslt.org/common" xml:id="Paper-778">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>Towards an Archaeology of Text Analysis Tools</title>
        <author>
          <name>
            <surname>Sinclair</surname>, <forename>Stéfan</forename>
          </name>
          <affiliation>McGill University, Canada</affiliation>
          <email></email>
        </author>
        <author>
          <name>
            <surname>Rockwell</surname>, <forename>Geoffrey</forename>
          </name>
          <affiliation>University of Alberta, Canada </affiliation>
          <email></email>
        </author>
      </titleStmt>
      <publicationStmt>
        <authority/>
        <publisher>EPFL, Switzerland</publisher>
        <distributor>
          <name>EPFL Digital Humanities Laboratory</name>
          <address>
            <addrLine>GC D2 386</addrLine>
            <addrLine>Station 18</addrLine>
            <addrLine>CH-1015 Lausanne</addrLine>
            <addrLine>frederic.kaplan@epfl.ch</addrLine>
          </address>
        </distributor>
        <pubPlace>Lausanne, Switzerland</pubPlace>
        <address>
          <addrLine>EPFL</addrLine>
          <addrLine>CH-1015 Lausanne</addrLine>
        </address>
        <availability>
          <p/>
        </availability>
      </publicationStmt>
      <notesStmt>
        <note type="abstract">How have text analysis tools in the humanities been imagined in the past? What did humanities computing developers think they were addressing with now dated technologies like punch cards, printed concordances and verbose command languages? Whether the analytic functionality is at the surface, as with Voyant Tools, or embedded at deeper levels, as with the Lucene-powered searching and browsing capabilities of the Old Bailey, the web-based text analysis tools that we use today are very different from the first tentative technologies developed by computing humanists. Following Siegfried Zieliniski's exploration of forgotten media technologies, this paper will look at three forgotten text analysis technologies and how they were introduced by their developers at the time.</note>
      </notesStmt>
      <sourceDesc>
        <p>No source: created in electronic format.</p>
        <p>
          <date when="20140710"/>
          <time when="09:00:00"/>
        </p>
		<p n="session">4</p>
		<p n="room">321 - Amphipôle</p>
      </sourceDesc>
    </fileDesc>
    <profileDesc>
      <textClass>
        <keywords scheme="original" n="category">
          <term>Paper</term>
        </keywords>
        <keywords scheme="original" n="subcategory">
          <term>Long Paper</term>
        </keywords>
        <keywords scheme="original" n="keywords">
          <term>text analysis</term>
          <term></term>
        </keywords>
        <keywords scheme="original" n="topic">
          <term>text analysis</term>
          <term></term>
        </keywords>
      </textClass>
    </profileDesc>
  </teiHeader>
  <text type="paper">
    <front>
      
      <div>
        <p></p>
        <p></p>
      </div>
    </front>
    <body>
      <div>
        <p>How have text analysis tools in the humanities been imagined in the past? What did
humanities computing developers think they were addressing with now dated
technologies like punch cards, printed concordances and verbose command
languages? Whether the analytic functionality is at the surface, as with Voyant Tools,
or embedded at deeper levels, as with the Lucene-powered searching and browsing
capabilities of the Old Bailey, the web-based text analysis tools that we use today are
very different from the first tentative technologies developed by computing
humanists. Following Siegfried Zieliniski's exploration of forgotten media
technologies, this paper will look at three forgotten text analysis technologies and
how they were introduced by their developers at the time. Specifically we will:

					</p>
        <list type="unordered">
          <item>
            Discuss why is it important to recover forgotten tools and the discourse
around these instruments,

          </item>
          <item>
            Look at how punch cards were used in Roberto Busa’s Index Thomisticus
project as a way of understanding data entry,

          </item>
          <item>
            Look at Glickman’s ideas about custom card output from PRORA, as a way of
recovering the importance of output,

          </item>
          <item>
            Discuss the command language developed by John Smith for interacting with
ARRAS, and

          </item>
          <item>
            Conclude with a more general call for digital humanities archaeology. 
          </item>
        </list>
        <head>Zieliniski and Media Archaeology
</head>
        <p>Siegfried Zielinski, in <hi rend="italic">Deep Time of the Media</hi>, argues that technology does not evolve
smoothly and that we therefore need to look at periods of intense development and
then look at the dead ends that get overlooked to understand the history of media technology. In particular he shows how important it is to look at technologies that
are not in canonical histories as precursors to “successful” technologies, because
they provide insight into the thinking at the time. A study of forgotten technologies
can help us understand opportunities and challenges as they were perceived at the
time and on their own terms rather than imposing our prejudices. From the 1950s
until the early 1990s there was just such a period of technology development
around mainframe and personal computer text analysis tools. The tools developed,
the challenges they addressed, and the debates around these technologies have
largely been forgotten in an age of web-mediated digital humanities. For this reason
we recover three important mainframe projects that can help us understand how
differently data entry, output and interaction were thought through before born-
digital content, output to wall-sized screens, and interaction on a touchscreen. </p>
        <head>Busa and Tasman on Literary Data Processing </head>
        <p>The first case study we will present is about the methods that Father Busa and his
collaborator Paul Tasman developed for the <hi rend="italic">Index Thomisticus</hi> (Busa could hardly be
considered a forgotten figure, but he's often referred to metonymically as a founder
of the field, with relatively little attention paid to the specifics of his work and his
collaborations). Busa, when reflecting back on the project justified his technical
approach as supporting a philological method of research aimed at recapturing the
way a past author used words, much as we want to recapture past development. He
argued in 1980 that, “The reader should not simply attach to the words he reads the
significance they have in his mind, but should try to find out what significance they
had in the writer’s mind.” (Busa 1980, p. 83) Concordances could help redirect
readers towards the “verbal system of an author” or how the author used words in
their time and away from the temptation to interpret the text at hand using
contemporary conceptual categories. Concording creates a new text that shows the
verbal system, not the doctrine.
</p>
        <p>Busa’s collaborator Paul Tasman, however, presents a much more prosaic picture of
their methodology that focuses on data entry using punch cards so you can actually		
	
	
		
			
				
					get concordances of words. He published a paper in 1957 on “Literary Data
Processing” in the<hi rend="italic"> IBM Journal of Research and Development</hi> that focuses on how
they prepared their texts accounting for human error and other problems. Tasman
writes, “It is evident, of course, that the transcription of the documents in these
other fields necessitates special sets of ground rules and codes in order to provide
for information retrieval, and the results will depend entirely upon the degree and
refinement of coding and the variety of cross referencing desired.” (p. 256) This case
study takes us back to a forgotten set of problems (representing text using punch
cards) which led to more mature issues in text encoding. In the full presentation we
will look closely at the data entry challenges faced by Busa’s team and how they
were resolved with the card technology of the time. </p>
        <head>Glickman and Stallman on Printed Interfaces
</head>
        <p>The second case study we will look at is the development of the PRORA programs at
the University of Toronto in the 1960s. PRORA was reviewed in the first issue of
CHUM and with the publication of the<hi rend="italic"> Manual for the Printing of Literary Texts and
Concordances by Computer</hi> by the University of Toronto Press in 1966 is one of the
first academic analytical tools to be formally published in some fashion. What is
particularly interesting, for our purposes, is the discussion in the Manual of how
concordances might be printed. Glickman had idiosyncratic ideas about how
concordances could be printed as cards for 2-ring binders so that they could be
taken out and arranged on a table by users. He was combining binder technology
with computing to reimagine the concordance text. Today we no longer think about
output to paper as important to tools, and yet that is what the early tools were
designed to do as they were not interactive. We will use this case study to recover
what at the time was one of the most important features of a concording tool – how
it could output something that could be published for others to use. </p>
        <figure>
          <graphic url="DH2014_548_323-1"/>
          <p>Fig. 1: Example of PRORA output from the Manual</p>
        </figure>
        <head>Smith and Interaction
</head>
        <p>One of the first text analysis tools designed to support interactive research was John
Smith’s ARRAS. In ARRAS Smith developed a number of ideas about analysis that we
now take for granted. ARRAS was interactive in the sense that it was not a batch
program that you ran for output. It could generate visualizations and it was
explicitly designed to be part of a multi-tasking research environment where you
might be switching back and forth between analysis and word processing. Many of
these ideas influenced the interactive PC concordancing tools that followed like
TACT. In this paper, however, we are not going to focus on all the prescient features
of ARRAS, but look at the now rather dated command language which Smith was so
proud of. Almost no one uses a command language for text analysis any more; we
expect our tools to have graphical user interfaces that provide affordances for direct
manipulation. If you need to do something more than what Voyant, Tableau, Lucene,
Gephi or Weka let you do, then you learn to program in a language like R or Python.
John Smith by contrast, spent a lot of time trying to design a natural command
language for ARRAS that humanists would find easy to use and this comes through
in his publications on the tool (1984 &amp; 1985). Command languages were, for a while,
the way you interacted with such systems and attention to their design could make a
difference. Smith tried to develop a command language that was conversational so
humanists could learn to use it to explore “vast continents of literature or history or
other realms of information, much as our ancestors explored new lands.” (Smith
1984, p. 31) Close commanding for distant reading. </p>
        <head>Conclusions
</head>
        <p>In the 2013 Busa Award lecture Willard McCarty called us to look to our history and
specifically to look at the “incunabular” years before the web when humanists and
artists were imagining what could be done. One challenge we face in reanimating
this history is that so much of the story is in tools, standards and web sites –
instruments difficult to interrogate the way we do texts. This paper looks back at
one major thread of development - text analysis tools – not for the entertainment of
outdated technology, but recover a way of thinking about technology. We will
conclude by discussing other ways back including the need for better
documentation about past tools, along the lines of what TAPoR 2.0 is supporting,
and the need to preserve tools or at least a record of their usage. </p>
        
        
        
      </div>
    </body>
    <back>
      <div type="References">
        <listBibl>
          <bibl><hi rend="bold">Busa, R. </hi>(1980). <hi rend="italic">"The Annals of Humanities Computing: The Index Thomisticus."</hi>
            Computers and the Humanities. 14(2): 83-90.</bibl>
          <bibl><hi rend="bold">Glickman, Robert Jay, and Gerrit Joseph Staalman</hi>. <hi rend="italic">Manual for the Printing of Literary
            Texts and Concordances by Computer.</hi> Toronto: University of Toronto Press,
            1966.
          </bibl>
          <bibl><hi rend="bold">Liu, Alan</hi>. (2012) <hi rend="italic">“Where is Cultural Criticism in the Digital Humanities.”</hi> In Debates in the
            Digital Humanities. Ed. Matthew K. Gold. University of Minnesota Press.
            Liu’s essay is online at &lt;http://dhdebates.gc.cuny.edu/debates/part/11&gt;.</bibl>
          <bibl><hi rend="bold">Smith, J. B</hi>. (1978). <hi rend="italic">"Computer Criticism." </hi>STYLE XII(4): 326-356.</bibl>
          <bibl><hi rend="bold">Smith, J. B</hi>. (1984). <hi rend="italic">"A New Environment For Literary Analysis."</hi> Perspectives in
            Computing 4(2/3): 20-31.</bibl>
          <bibl><hi rend="bold">Smith, J. B. </hi>(1985).<hi rend="italic"> Arras User's Manual: TR85-036</hi>. Chapel Hill, NC, The University of
            North Carolina at Chapel Hill.</bibl>
          <bibl><hi rend="bold">Tasman, P.</hi> (1957). <hi rend="italic">"Literary Data Processing."</hi> IBM Journal of Research and
            Development 1(3): 249-256.
          </bibl>
          <bibl><hi rend="bold">Zieliniski, Siegfried.</hi> (2008) <hi rend="italic">Deep Time of the Media: Toward an Archaeology of Hearing and
            Seeing by Technical Means</hi>. Cambridge, Massachusetts: The MIT Press. </bibl>
        </listBibl>
      </div>
    </back>
  </text>
</TEI>
