<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../schema/xmod_web.rnc" type="compact"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0"
     xmlns:xmt="http://www.cch.kcl.ac.uk/xmod/tei/1.0" 
     xml:id="ab-245">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>HisDoc: Historical Document Analysis, Recognition, and Retrieval</title>
                <author>
                    <name>Baechler, Micheal</name>
                    <affiliation>University of Fribourg, Switzerland</affiliation>
                    <email>micheal.baechler@unifr.ch</email>
                </author>
                <author>
                    <name>Fischer, Andreas</name>
                    <affiliation>University of Bern, Switzerland</affiliation>
                    <email>afischer@iam.unibe.ch</email>
                </author>
                <author>
                    <name>Naji, Nada</name>
                    <affiliation>University of Neuchatel, Switzerland</affiliation>
                    <email>nada.naji@unine.ch</email>
                </author>
                <author>
                    <name>Ingold, Rolf</name>
                    <affiliation>University of Fribourg, Switzerland</affiliation>
                    <email>rolf.ingold@unifr.ch</email>
                </author>
                <author>
                    <name>Bunke, Horst</name>
                    <affiliation>University of Bern, Switzerland</affiliation>
                    <email>bunke@iam.unibe.ch</email>
                </author>
                <author>
                    <name>Savoy, Jacques</name>
                    <affiliation>University of Neuchatel, Switzerland</affiliation>
                    <email>jacques.savoy@unine.ch</email>
                </author>
            </titleStmt>
            <publicationStmt>
                <publisher>Jan Christoph Meister, Universität Hamburg</publisher>
                <address>
                   <addrLine>Von-Melle-Park 6, 20146 Hamburg, Tel. +4940 428 38 2972</addrLine>
                   <addrLine>www.dh2012.uni-hamburg.de</addrLine>
              </address>
            </publicationStmt>
            <sourceDesc>
                <p>No source: created in electronic format.</p>
            </sourceDesc>
        </fileDesc>
        <revisionDesc>
            <change>
                <date>2012-04-15</date>
                <name>DH</name>
                <desc>generate TEI-template with data from ConfTool-Export</desc>
            </change>
            <change>
                <date>2012-04-13</date>
                <name>LS</name>
                <desc>provide metadata for publicationStmt</desc>
            </change>
        </revisionDesc>
    </teiHeader>
    <text type="paper">
        <body>
            <div>
            <p>The HisDoc project aims at developing automatic generic tools to support cultural
                heritage. More precisely, we have designed and implemented a system that performs a
                fully automated conversion of manuscripts into the corresponding electronic format
                and provides effective search capabilities. To demonstrate the effectiveness of the
                proposed solution, we have conducted a large-scale experiment using medieval
                handwritten manuscripts written in Middle High German. The corpus used in our
                experiments is based on a well known medieval epic poem called Parzival and
                attributed to Wolfram von Eschenbach. The first version dates to the first quarter
                of the thirteenth century. Currently, we can find several versions (with variations)
                but the St. Gall collegiate library cod. 857 is the one used for experimental
                evaluation.  </p>
            <p>The complete system does not make strong assumptions about the current corpus, and it
                can be adapted with little effort to handle other types of documents and languages.
                The proposed HisDoc system is subdivided into three main components, namely: image
                analysis, text recognition and information retrieval. </p>
            <p>The image analysis module basically has two essential goals, namely image
                enhancement, which aims at improving image quality in order to make handling all the
                subsequent processing steps an easier task. The second goal is to perform layout
                analysis that provides a structural description of the scanned document pages. This
                meta-information may correspond to the number of columns, the number of lines per
                column, the location of the headline, etc. It is known that these old manuscripts
                suffer from ink bleeding, holes and stitches on parchments in addition to the
                artifacts surrounding the non-uniform handwritten text which represents a difficult
                challenge at this phase as well as in the phases that follow.  </p>
            <p>The resulting digital images and their meta-information are then passed to the text
                recognition phase, the second module of HisDoc, in order to produce the
                corresponding digital transcription. Here, flexible and robust recognition systems
                based on Hidden Markov Models (Marti &amp; Bunke 2001) and Neural Networks (Graves
                et al. 2009) are used for the task of automating the transcription of historical
                texts (Fischer et al. 2009). In this context, flexibility means that the system can
                be adapted to new writing styles without great effort, and robustness means that the
                recognizer should attain a high rate of correctly recognized words. Besides the
                automatically generated version, an error-free transcription was created manually
                with the help of experts. This latter version forms our ground-truth text and is
                used to assess the performance levels in the different stages of our project.</p>
            <p>For the analysis and recognition of historical documents, only little work has been
                reported in the literature so far, (Bunke &amp; Varga 2007; Likforman-Sulem et al.
                2007) present surveys of approaches to off-line handwriting recognition and on
                different state-of-the-art text line extraction methods respectively. Commercial
                systems with a high recognition accuracy are available only for restricted domains
                with modern handwritten scripts, e.g., for postal address (Srihari et al. 1996) and
                bank check reading (Gorski et al. 2001). </p>
            <p>The HisDoc recognition module has evolved in terms of performance and achieved its
                current word-accuracy which is close to 94% thus a word-error rate of around 6%
                using Hidden Markov Models and character similarity features (Fischer et al. 2010).
                As can be seen, it is unfortunately impossible to obtain flawless recognition
                especially with the existence of all of the aging and decorative issues mentioned
                above. The level of the error rate depends on various factors such as the accuracy
                of the recognition system, the quality of the contrast between the background and
                the ink, and the regularity of the handwriting. Finally, the size of the training
                set also plays a role, but relatively good performance can be achieved with a
                relatively small training set (around 6,000 words).</p>
            <p>In order to reduce the human effort needed to generate training samples, HisDoc’s
                text recognition module also includes methods for efficient ground-truth creation
                and text-image alignment in case of an existing electronic text edition (Fischer et
                al. 2011).</p>
            <p>Performing effective searches on the resulting transcriptions is the goal of the
                third and last module in HisDoc, the information retrieval (IR) module. The
                cascading effects of manuscript quality, graphical characteristics and recognition
                error rate will have an impact on the performance level. But we must also consider
                additional factors particularly related to medieval writing. Our corpus was
                handwritten during the thirteenth century, when spelling was not standardized which
                introduces yet an additional challenge. Moreover, the co-existence of different
                spellings referring to the same entity in the same manuscript would also reduce the
                retrieval effectiveness. Besides that, one should also keep in mind the fact that
                grammar used in medieval languages was clearly different once compared to that of
                our modern days, which allowed more flexibility for the writer, thus varying from
                one region to another, or even from one writer to another residing in the same
                region. All these grammatical and orthography-related matters (spelling variations,
                punctuation, initials’ capitalization, etc.) in addition to the challenges faced in
                the first two modules of HisDoc would absolutely burden the retrieval process as
                they impose their negative impact on retrieval effectiveness causing the performance
                level to fall if the retrieval system is built the conventional way. To quantify
                this, the retrieval effectiveness is compared to that obtained using the error-free
                transcription.  </p>
            <p>In HisDoc’s retrieval module, certain techniques were introduced in order to
                integrate the possible variants to accordingly enhance the retrieval effectiveness
                by allowing some sort of soft-matching between the query terms representing the
                user’s information needs and four different text surrogates of the transcription,
                each of which incorporates a certain intensity level of variants’ inclusion. As a
                concrete example, the term ‘<hi rend="italic">Parzival</hi>’ appeared in the
                original manuscript as ‘<hi rend="italic">Parcifal</hi>’, ‘<hi rend="italic"
                    >Parcival</hi>’ and ‘<hi rend="italic">Parzifal</hi>’. All of these variants are
                possible and must be considered as correct spellings. At this lexical level, one
                might also consider the inflectional morphology where various suffixes were possibly
                added to nouns, adjectives and names to indicate their grammatical case. With this
                additional aspect, the proper name ‘<hi rend="italic">Parcival</hi>’ may also appear
                as ‘<hi rend="italic">Parcivale</hi>’, ‘<hi rend="italic">Parcivals</hi>’ or ‘<hi
                    rend="italic">Parcivalen</hi>’, increasing the number of potential spelling
                variants that we need to take into account. </p>
            <p>Regarding text representations, we have implemented whole words representation, short
                overlapping sequences of characters within each word (<hi rend="italic">n</hi>-gram)
                or the first <hi rend="italic">n</hi> characters of each word (trunc-<hi
                    rend="italic">n</hi>). During this indexing stage, we have also evaluated the
                effect of different word normalization procedures such as stemming. This procedure
                tries to extract the stem from a surface form by automatically removing its
                inflectional suffixes (number, gender) (light stemmer) or both inflectional and
                derivational suffixes (aggressive stemmer). As for search strategies, we have
                evaluated the classical vector-space model <hi rend="italic">tf idf</hi> and its
                variants as well as several probabilistic models.  </p>
            <p>Based on our experiments and using a well-known evaluation methodology (Voorhees
                &amp; Harman, 2005), using either short (single term) or long queries (three terms),
                we found that probabilistic IR models tend to produce better retrieval effectiveness
                (Naji &amp; Savoy 2011).  Regarding the stemming procedure, aggressive stemming
                tends to produce slightly better retrieval performance when facing with longer
                queries. On the other hand, ignoring the stemming normalization with short queries
                usually offers the best performance.  </p>
            <p>The presence of compound words was also analyzed in order to improve the quality of
                retrieval. The German language is known for this compound construction which occurs
                more frequently than in English. The main concern here is the fact that the same
                concept can be expressed with or without a compound form (e.g., <hi rend="italic"
                    >Bankpräsident</hi> or <hi rend="italic">Präsident der Bank</hi>). Applying an
                automatic decompounding strategy may provide some successful improvement,
                particularly for longer queries.  </p>
            <p>We have assessed the various recognition corpora against the ground-truth version.
                 Compared to this error-free version, the simplest transcription surrogate (the
                classical output of recognition system, i.e., including no variants) shows a mean
                degradation in retrieval performance around -10.24% for single-term queries.
                Considering systematically three or seven variants per input word usually tends to
                cause the retrieval effectiveness to decrease significantly (from -5.42% for 3-term
                queries to -64.34% with single-term queries). Including more possible terms per
                recognition is therefore not an effective solution. We have thus proposed a wiser
                strategy that includes word variants according to their likelihood probabilities
                obtained during the recognition stage. This approach produces clearly better
                retrieval performance. To somehow illustrate our achievement, we can compare the
                best results obtained using two versions of an English language corpus having
                character error rates of 5% and 20%. The retrieval performance degradation was
                around -17% and -46% respectively (Voorhees &amp; Harman, 2005). While our best
                results for the Parzival corpus, which has a word-error rate of around 6%, the
                retrieval performance degradation was only limited to around -5%.</p>
            <p>The best practices found and the conclusions drawn from our experiments can then be
                applied to further manuscripts, hence making them digitally accessible and
                effectively searchable with the least cost possible by partially or totally
                eliminating the need to having them manually transcribed which in turn will result
                in saving a lot of resources (time, human effort, finances, etc.). With these
                documents being completely searchable via digital means, more user’s information
                needs can thus be satisfied via the facilities provided by web-service-based search
                engine.</p>
            </div>
            
            <div>                
            <p><hi rend="bold">Acknowledgement</hi></p>
            <p>The authors wish to thank Prof. Michael Stoltz, Dr. Gabriel Viehhauser (University of
                Bern, Switzerland), Prof. Anton Näf and Mrs. Eva Wiedenkeller (University of
                Neuchatel) for their valuable support. This research is supported by the Swiss NSF
                under Grant CRSI22_125220. </p>
            </div>
            
        </body>
        <back>
            <div>
                <head>References</head>
       
            <p><hi rend="bold">Bunke, H., and T. Varga</hi> (2007). Off-line Roman Cursive
                Handwriting Recognition. In B. Chaudhuri (ed.), <hi rend="italic">Digital Document
                    Processing: Major Directions and Recent Advances</hi> Berlin: Springer, pp.
                165-173.</p>
            <p><hi rend="bold">Fischer, A., M. Wüthrich, M. Liwicki, V. Frinken, H. Bunke, G.
                    Viehhauser, and M. Stolz</hi> (2009). Automatic Transcription of Handwritten
                Medieval Documents. <hi rend="italic">Proceedings of the 15th International
                    Conference on Virtual Systems and Multimedia</hi>, pp. 137-142.</p>
            <p><hi rend="bold">Fischer, A., K. Riesen, and H. Bunke</hi> (2010). Graph Similarity
                    Features for HMM-Based Handwriting Recognition in Historical Documents. <hi
                        rend="italic">Proceedings of the 12th International Conference on Frontiers
                        in Handwriting Recognition</hi>, pp. 253-258.</p>
            <p><hi rend="bold">Fischer, A., E. Indermühle, V. Frinken, and H. Bunke</hi> (2011).
                HMM-Based Alignment of Inaccurate Transcriptions for Historical Documents. <hi
                    rend="italic">Proceedings of the 11th International Conference on Document
                    Analysis and Recognition</hi>, pp. 53-57.</p>
            <p><hi rend="bold">Gorski, N., V. Anisimov, E. Augustin, O. Baret, and S. Maximor</hi>
                (2001). Industrial Bank Check Processing: The A2iA Check Reader. <hi rend="italic"
                    >International Journal on Document Analysis and Recognition</hi> 3(4):
                196-206.</p>
            <p><hi rend="bold">Graves, A., M. Liwicki, S. Ferdnandez, R. Bertolami, H. Bunke, and J.
                        Schmidhuber</hi> (2009). A Novel Connectionist System for Unconstrained
                    Handwriting Recognition. <hi rend="italic">IEEE Transactions on Pattern Analysis
                        and Machine Intelligence</hi> 31(5): 855-868.</p>
            <p><hi rend="bold">Likforman-Sulem, L., A. Zahour, and B. Taconet</hi> (2007). Text Line
                Segmentation of Historical Documents: A Survey. <hi rend="italic">International
                    Journal on Document Analysis and Recognition</hi> 9(2): 123-138.</p>
            <p><hi rend="bold">Marti, U.-V., and H. Bunke</hi> (2001). Using a Statistical Language
                Model to Improve the Performance of an HMM-Based Cursive Handwriting Recognition
                System. <hi rend="italic">International Journal of Pattern Recognition and
                    Artificial Intelligence</hi> 15(1): 65-90.</p>
            <p><hi rend="bold">Naji, N., and J. Savoy</hi> (2011). Information Retrieval Strategies
                for Digitized Handwritten Medieval Documents. <hi rend="italic">Proceedings of the
                    Asian Information Retrieval Symposium</hi>, Dubai, December 2011, LNCS #7097.
                Berlin: Springer, pp. 103-114. </p>
            <p><hi rend="bold">Srihari, S. N., Y. Shin, and V. Ramanaprasad</hi> (1996). A System to
                Read Names and Addresses on Tax Forms. In <hi rend="italic">Proceedings of the
                    IEEE</hi> 84(7): 1038-1049.</p>
            <p><hi rend="bold">Voorhees, E. M., and D. K. Harman</hi>(2005). <hi rend="italic">TREC</hi>. <hi rend="italic">Experiments and Evaluation in
                    Information Retrieval</hi>. Cambridge, MA: MIT Press.</p>
        </div>
            
        </back>
    </text>
</TEI>