<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../schema/xmod_web.rnc" type="compact"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0"
     xmlns:xmt="http://www.cch.kcl.ac.uk/xmod/tei/1.0" 
     xml:id="ab-201">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Feeling the View: Reading Affective Orientation of Tagged Images</title>
                <author>
                    <name>Liu, Jyi-Shane</name>
                    <affiliation>Natioanl Chengchi University, Taiwan</affiliation>
                    <email>jyishane.liu@gmail.com</email>
                </author>
                <author>
                    <name>Peng, Sheng-Yang</name>
                    <affiliation>Natioanl Chengchi University, Taiwan</affiliation>
                    <email>young.orange@gmail.com</email>
                </author>
            </titleStmt>
            <publicationStmt>
                <publisher>Jan Christoph Meister, Universität Hamburg</publisher>
                <address>
                   <addrLine>Von-Melle-Park 6, 20146 Hamburg, Tel. +4940 428 38 2972</addrLine>
                   <addrLine>www.dh2012.uni-hamburg.de</addrLine>
              </address>
            </publicationStmt>
            <sourceDesc>
                <p>No source: created in electronic format.</p>
            </sourceDesc>
        </fileDesc>
        <revisionDesc>
            <change>
                <date>2012-04-15</date>
                <name>DH</name>
                <desc>generate TEI-template with data from ConfTool-Export</desc>
            </change>
            <change>
                <date>2012-04-13</date>
                <name>LS</name>
                <desc>provide metadata for publicationStmt</desc>
            </change>
        </revisionDesc>
    </teiHeader>
    <text type="paper">
        <body>
            
            <div>
            <head>Problems</head>
            <p>Images are visual representations of human experiences and the world (Molyneaux
                1997). An image captures a moment of life and provides a glimpse of human history.
                With the overwhelming popularity of social media and the convenience of personal
                digital devices, digital images on web platforms have exploded in astonishing
                numbers. According to a recent article by an independent online news site (Kessler
                2011), the number of photos on Facebook was 60 billion, compared to Photobucket’s 8
                billion, Picasa’s 7 billion and Flickr’s 5 billion in early 2011. These digital
                images embody certain aspects of human nature in a historical scale. Challenges and
                opportunities abound for new findings of humanities research on digital visual
                media.</p>
            <p>Many images (or photos) on social media are associated with tags
                that describe the content of the images. As opposed to controlled vocabularies in
                domain-specific taxonomy, tags are keywords generated freely without hierarchical
                structure and are characterized as folksonomy (Trant 2009). An image, with its
                visual representation of the world, can be seen as an anchor that links a set of
                tags. Mitchell (1994) discussed the relations of images and words and suggested the
                views that images and words are interactive and constitutive of representation and
                that all representations are heterogeneous. Therefore, images with tags are integral
                representations that encode fuller and richer content. This observation seems to
                support the assumption that, through an image anchor, meaningful linkage among tags
                exists.</p>
             <p>This research focuses on the affective aspect of human life in
                digital images and attempts to identify a subset of tags that may indicate strong
                affective orientation. This provides a possibility to read into viewers’ affective
                response to images with certain affect indicative tags that are strongly associated
                with known affect types. An affective magnitude computation method is also proposed
                to retrieve images with strong affective orientation.</p></div>
            
            <div>
            <head>Methodology</head>
            <p>Among the major photo web sites, Flickr provides several unique features that seem to
                be better fit to the purposes of this research. First and foremost, Flickr
                emphasizes the use of tags for image description and community activity. Flickr
                photos tend to be associated with more tags than other web site photos. Second, user
                activities on Flickr involve more direct interaction of images and words with
                comments and feedbacks. Third, Flickr provides APIs that are conducive to data
                acquisition.</p>
                <p>This research adopts an affective framework based on Russel’s
                circumplex model of affect (Posner 2005), in which 28 emotion words are evenly
                distributed around a circle centered on the origin of a two dimensional space of
                affective experiences. The horizontal dimension of valence ranges from highly
                negative (left) to highly positive (right), whereas the vertical dimension of
                arousal ranges from sleepy (bottom) to agitated (top). The set of emotion words were
                reduced from 28 to 12 based on significant occurrence in Flickr tags and equal
                representation of four affective quadrants. For each affective tag, a total of 1,000
                Flickr photos are retrieved based on a popularity index called interesting. The
                retrieving process also includes a check-and-replace step to make sure that the
                final sample of 12,000 photos were all unique and from unique users. This is to
                remove bias and achieve objective representation in data collection.</p>
                
                <p>Point-wise mutual information (PMI), developed in the fields of probability and
                information theory, is a measure of mutual dependenceof two random variables and has
                been successfully applied to provide effective word association measurement in text
                mining research (Church &amp; Hanks 1989; Recchia &amp; Jones 2009). For any two
                words X and Y, their PMI value is calculated by taking the base-2 logarithm of the
                ratio of the joint probability distribution p(X, Y) over the independent probability
                distributions p(X) and p(Y). Maximal positive PMI values indicate strong tendency of
                co-occurrence, whereas negative values suggest less chance of occurring together and
                zero means independence. In this research, two tags co-occur if they appear in the
                tag set of a photo.</p>
                <p>For the purpose of retrieving images with strong
                affective orientation, an affective magnitude computation method is developed. This
                method combines association strength, affect distribution in tag set, and community
                feedbacks. First, community feedback parameters such as views, favorites, and
                comments, are used to retrieve candidate photos with above average parameter values.
                Second, for each candidate photo, an affective weight is calculated by the ratio of
                the number of affect indicative tags in the photo’s tag set to the total number of
                tags in the photo’s tag set. Third, for each candidate photo, the affective
                magnitude of certain affective type or certain affective quadrant is computed by the
                weighted sum of PMI values of the pairs of the affect types and the affect
                indicative tags of the photo.</p></div>
            
            <div>
            <head>Results</head>
            <p>The total number of tags in the sample set of 12,000 photos is 258,293, in which
                70,066 tags are unique. The frequency distribution of these unique tags shows a long
                tail. A stemming process is applied to these unique tags to reduce the inflected
                words to their root form, e.g., girls to girl, beautiful to beauti. A frequency
                threshold of 1% is also set to filter out less meaningful tags. The stemming and
                filtering processes result in 386 word forms of tags. These 386 tags are called
                affect indicative tags because of their high co-occurrence with the 12 affective
                tags. By calculating PMI values of a pair of an affective tag and an affect
                indicative tag, a matrix of association strength among 12 affective tags and 386
                affect indicative tags are obtained. Table 1 shows a partial matrix of PMI values
                between affective tags and affect indicative tags.</p>
            
                <p><figure><graphic url="tabl201-1.jpg" rend="left" height="256px" width="341px" mimeType="image/jpeg"/><head>Table 1</head></figure></p>
            
            
                <p>The PMI values of an affect indicative tag are further grouped by each
                affect quadrant to provide distinct affective orientation. The association strength
                of an affect indicative tag with an affect quadrant is calculated by summing up
                positive PMI values of the three affective tags in the affect quadrant. Negative PMI
                values indicate unlikely co-occurrence and are taken as zero during quadrant
                grouping to avoid incorrect underestimation of co-occurrence union. Table 2 shows
                the lists of the top ten affect indicative tags that have the most association
                strength with one of the four affect quadrants.</p>
                
                <p><figure><graphic url="tabl201-2.jpg" rend="left" height="256px" width="341px" mimeType="image/jpeg"/><head>Table 2</head></figure></p>
                
               <p>These affect indicative tags can be used to retrieve images with predicted
                affective orientation. Given a set of images that include an affect indicative tag
                in their tag sets, the affective magnitude computation (AffMC) method is then used
                to determine an image’s affective magnitude in the affect quadrant, and thus
                provides a ranked list of images. The performance of the affective magnitude
                computation method is compared to that of Google and Flickr, which also include a
                ranking mechanism on retrieved images. Images retrieved by AffMC, Flickr, Google
                with top affect indicative tags of each affect quadrant are shown in Tables 3, 4, 5,
                and 6.</p>
                    
                    <table>
                    <head>Table 3</head>
                <row>
                    <cell cols="2"> Q1: positive &amp; more aroused affect indicative tag: <hi
                            rend="italic">joy</hi></cell>
                </row>
                <row>
                    <cell>AffMC</cell>
                    <cell><figure><graphic url="img201-1.jpg" rend="left" height="256px" width="341px" mimeType="image/jpeg"/></figure></cell>
                </row>
                <row>
                    <cell>Flickr</cell>
                    <cell><figure><graphic url="img201-2.jpg" rend="left" height="256px" width="341px" mimeType="image/jpeg"/></figure></cell>
                </row>
                <row>
                    <cell>Google</cell>
                    <cell><figure><graphic url="img201-3.jpg" rend="left" height="256px" width="341px" mimeType="image/jpeg"/></figure></cell>
                </row>
                </table>
                
            <p>*Copyrights of all images shown are reserved by original contributors.</p>
            
            
                <table>
                <head>Table 4</head>
                    
                <row>
                    <cell cols="2"> Q2: negative &amp; more aroused affect indicative tag: <hi
                            rend="italic">scary</hi></cell>
                </row>
                <row>
                    <cell>AffMC</cell>
                    <cell><figure><graphic url="img201-4.jpg" rend="left" height="256px" width="341px" mimeType="image/jpeg"/></figure></cell>
                </row>
                <row>
                    <cell>Flickr</cell>
                    <cell><figure><graphic url="img201-5.jpg" rend="left" height="256px" width="341px" mimeType="image/jpeg"/></figure></cell>
                </row>
                <row>
                    <cell>Google</cell>
                    <cell><figure><graphic url="img201-6.jpg" rend="left" height="256px" width="341px" mimeType="image/jpeg"/></figure></cell>
                </row>
                </table>
            
                <table><head>Table 5</head>
              <row>
                    <cell cols="2"> Q3: negative &amp; less aroused affect indicative tag: <hi
                            rend="italic">lonely</hi>
                    </cell>
                </row>
                <row>
                    <cell>AffMC</cell>
                    <cell><figure><graphic url="img201-7.jpg" rend="left" height="256px" width="341px" mimeType="image/jpeg"/></figure></cell>
                </row>
                <row>
                    <cell>Flickr</cell>
                    <cell><figure><graphic url="img201-8.jpg" rend="left" height="256px" width="341px" mimeType="image/jpeg"/></figure></cell>
                </row>
                <row>
                    <cell>Google</cell>
                    <cell><figure><graphic url="img201-9.jpg" rend="left" height="256px" width="341px" mimeType="image/jpeg"/></figure></cell>
                </row>
                </table>                   
            
            
            <table>                          
            <head>Table 6</head>
                 <row>
                    <cell cols="2"> Q4: positive &amp; less aroused affect indicative tag: <hi rend="italic">serene</hi></cell>
                    </row>
                    
                <row>
                    <cell>AffMC</cell>
                    <cell><figure><graphic url="img201-10.jpg" rend="left" height="256px" width="341px" mimeType="image/jpeg"/></figure></cell>
                </row>
                
                <row>
                      <cell>Flickr</cell>
                    <cell><figure><graphic url="img201-11.jpg" rend="left" height="256px" width="341px" mimeType="image/jpeg"/></figure></cell>
                </row>
                
                <row>
                      <cell>Google</cell>
                    <cell><figure><graphic url="img201-12.jpg" rend="left" height="256px" width="341px" mimeType="image/jpeg"/></figure></cell>
                </row>
            
            </table>
            
                <p>To evaluate the affective magnitude computation (AffMC) method, top three affect
                indicative tags for each affect quadrant are used to retrieve images. The number of
                images retrieved by each ranking method is twelve, with three for each affect
                quadrant. A total of thirty-six images are given to a group of 136 subjects for
                testing their emotional response. Survey results indicate that AffMC is better than
                Flickr and comparable to Google in retrieving images of strong affective
                orientation. Note that image sources of Google and Flick (and AffMC) are different and Google enjoys a much significant user feedback for effective ranking.</p>
            </div>
                
            <div>
                <head>Conclusions</head>
            <p>This research shows that image tags can be exploited to characterize affect
                indicative tags. Accordingly, images with strong affective orientation can be
                identified and retrieved, from which viewers’ emotional response can be mostly
                anticipated. This method can be further developed into an engine for sorting and
                grouping images into affective types based on the given tags. Potential applications
                include intelligent visual services for user emotion enhancement or compensation.
                Another research implication is that significant pairs can be revealed by
                substantial repetition among loosely associated items and the triangular cluster of
                a significant pair and the central item provides the basis for functional
                exploitation.</p>
                <p/>
            </div>
             
            <div>              
            <p><hi rend="bold">Acknowledgement</hi></p>
            <p>This work was partially supported by the National Science Council, Taiwan, [NSC
                100-2221-E-004-013]; and the National Chengchi University’s Top University
                Project.</p></div>
        
        </body>
        
        <back>
            <div>
                <head>References</head>
            <p><hi rend="bold">Church, K. W., and P. Hanks</hi> (1989). <hi rend="italic">Word
                        Association Norms, Mutual Information and Lexicography</hi>,<hi
                        rend="italic">Proceedings of the 27th Annual Conference of the Association
                        of Computational Linguistics</hi>, June 1989, Vancouver, British Columbia,
                    Canada, pp. 76-83.</p>
                <p><hi rend="bold">Kessler, S.</hi> (2011). Facebook photos by the numbers, <hi
                    rend="italic">Mashable</hi>. <ref target="http://mashable.com/2011/02/14/facebook-photo-infographic/" type="external">
                    http://mashable.com/2011/02/14/facebook-photo-infographic/</ref> (accessed 28 October
                    2011).</p>
                <p><hi rend="bold">Mitchell, W. J. T.</hi> (1994). <hi rend="italic">Picture Theory:
                        Essays on Verbal and Visual Representation</hi>. Chicago: U of Chicago
                    P.</p>
                <p><hi rend="bold">Molyneaux, B. L., ed. </hi>(1997). <hi rend="italic">The Cultural
                        Life of Images: Visual Representation in Archaeology</hi>. London:
                    Routledge.</p>
                <p><hi rend="bold">Posner, J., J. A. Russell, and B. S. Peterson</hi> (2005). The
                    Circumplex Model of Affect: An Integrative Approach to Affective Neuroscience,
                    Cognitive Development, and Psychopathology. <hi rend="italic">Development and
                        Psychopathology</hi> 17(3): 715-734.</p>
                <p><hi rend="bold">Recchia, G. L., and M. N. Jones</hi> (2009). More Data
                            Trumps Smarter Algorithms: Comparing Pointwise Mutual Information to
                            Latent Semantic Analysis <hi rend="italic">Behavior Research
                                Methods</hi> 41: 657–663.</p>
                <p><hi rend="bold">Trant, J.</hi> (2009). Studying Social Tagging and
                        Folksonomy: A Review and Framework. <hi rend="italic">Journal of Digital
                            Information</hi> 10(1): 1-42.</p>
               
            </div>
        </back>
    </text>
</TEI>