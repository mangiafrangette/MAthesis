<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><meta name="generator" content="ABBYY FineReader 14"/><title>Microsoft Word - 502. Bulert-Optical Character Recognition with a Neural Network Model-502.docx</title><link rel="stylesheet" href="502_files/502.css" type="text/css"/>
</head>
<body>
<p><span class="font9" style="font-weight:bold;">Optical Character Recognition with a Neural&nbsp;Network Model for Printed&nbsp;Coptic Texts</span></p>
<p><span class="font5" style="font-weight:bold;">Kirill Bulert</span></p>
<p><span class="font5"><a href="mailto:kirill.bulert@stud.uni-goettingen.de">kirill.bulert@stud.uni-goettingen.de</a> eTRAP Research Group&nbsp;University of Gottingen, Germany</span></p>
<p><span class="font5" style="font-weight:bold;">So Miyagawa</span></p>
<p><span class="font5"><a href="mailto:so.miyagawa@mail.uni-goettingen.de">so.miyagawa@mail.uni-goettingen.de</a> University of Gottingen, Germany</span></p>
<p><span class="font5" style="font-weight:bold;">Marco Buchler</span></p>
<p><span class="font5"><a href="mailto:mbuechler@etrap.eu">mbuechler@etrap.eu</a></span></p>
<p><span class="font5">eTRAP Research Group</span></p>
<p><span class="font5">University of Gottingen, Germany</span></p><h1><a name="caption1"></a><a name="bookmark0"></a><span class="font2" style="font-weight:bold;">Introduction</span></h1>
<p><span class="font5">Optical character recognition (OCR) is the process of extracting text from images. The final results are&nbsp;machine readable versions of the original images.&nbsp;Nowadays every modern scanner comes with some&nbsp;kind of OCR, but the results may not be satisfying when&nbsp;the OCR is applied to historical texts, that</span></p>
<p><span class="font5">1. &nbsp;&nbsp;&nbsp;do not use standard fonts,</span></p>
<p><span class="font5">2. &nbsp;&nbsp;&nbsp;are not printed by a machine,</span></p>
<p><span class="font5">3. &nbsp;&nbsp;&nbsp;have varying paper and font quality.</span></p>
<p><span class="font5">Furthermore, historical texts are not passed down through the centuries in their entirety but rather contain lacunae and fragmentary words. This makes automatic post-correction more difficult on historical texts&nbsp;than on modern ones.</span></p>
<p><span class="font5">We used two tools to create language- and even document- specific recognition patterns (or so-called&nbsp;models) to recognize printed Coptic texts. Coptic is the&nbsp;last stage of the pre-Arabic, indigenous Egyptian language. It was used to create a rich and unique body of&nbsp;literature: monastic, “Gnostic,” Manichaean, magical&nbsp;and medical texts, hagiographies, and biblical and patristic translations. We found that Coptic texts have&nbsp;properties which make them excellent candidates for&nbsp;reading by computers. The characters can easily be&nbsp;distinguished due to their limited number and the fact&nbsp;that almost all the hand-written texts exhibit characters with highly consistent forms.</span></p><h1><a name="bookmark1"></a><span class="font2" style="font-weight:bold;">Related Work</span></h1>
<p><span class="font5">The process of digitizing historical documents can be split up into at least three major steps: (1) pre-processing, (2) text prediction (OCR), and (3) post-processing or correction.</span></p>
<p><span class="font5">Although many works already tackled subproblems (He et al, 2005; Gupta et al, 2007; Kluzner et al, 2009), Springman et al.(2014) presented the first&nbsp;complete approach containing all major steps for historical Greek and Latin books.</span></p>
<p><span class="font5">The first OCR results for printed Coptic texts were achieved by Mekhaiel (see </span><span class="font5" style="text-decoration:underline;">Moheb's Coptic Pages</span><span class="font5">) by using </span><span class="font5" style="text-decoration:underline;">Tesseract</span><span class="font5"> to create a model for Coptic texts. Tesser-act assumes that the image was printed with a standardized font. Although it can be trained to use many&nbsp;different fonts, creating a general model that would&nbsp;satisfy scholars is not feasible. In the end, this model is&nbsp;sufficient for pure printed Coptic texts, but creates a&nbsp;lot of noise for texts with mixed languages or annotations. Such drawbacks can be easily overcome by&nbsp;checking against a dictionary, but historical languages&nbsp;often do not have a dictionary that could be considered&nbsp;complete, and the texts might only be fragments that&nbsp;require further analysis.</span></p>
<p><span class="font5">The recognition itself is performed by either </span><span class="font5" style="text-decoration:underline;">Ocropy</span><span class="font5"> (Breuel, 2008) or Tesseract. Potentially, all&nbsp;character-based texts can be recognized. However,&nbsp;even though Mekhaiel provided a Coptic model for&nbsp;Tesseract, we were never able to achieve satisfying results on images which were not pre-processed.</span></p><h1><a name="bookmark2"></a><span class="font2" style="font-weight:bold;">Data Used</span></h1>
<p><span class="font5">For training and testing, an expert on Coptic created a clean version and transcription of Kuhn’s 1956 edition “Letters and sermons of Besa.” This will also be&nbsp;made available to the interested public.</span></p>
<p><span class="font5">Besa is a fifth-century abbot of a monastery in Upper Egypt and Coptic writer, whose literary legacy consists mainly of letters to monks and nuns on questions of monastic life and discipline.</span></p>
<p><span class="font5">Simplified pages were created to find the limits of the trained models with optimal input data. Since creating simplified pages consumes a lot of time, we consider this task as impractical for real use scenarios.&nbsp;Nevertheless, the results on these simplified pages&nbsp;show the best possible prediction.</span></p>
<p><span class="font5">In Fig. 1 all characters and symbols that are going to be removed are marked red. The resulting simplified image can be seen in Fig. 2. By procedure, adjacent&nbsp;characters that are supposed to form one word are cut&nbsp;apart by gaps. Those gaps are going to be predicted differently by the two OCR engines.</span></p>
<p><span class="font6" style="font-weight:bold;">neTMANoyoy. Mne-e-ooy nakim an ¿flneqm <sup>14</sup> :</span></p>
<p><span class="font6" style="font-weight:bold;">xnx BHCA</span></p>
<p><span class="font7">[Fragment 35] </span><span class="font3" style="font-style:italic;">A DENUNCIATION OF AN ERRING NUN</span></p>
<p><span class="font6" style="font-weight:bold;">.... <sub>L</sub>MjA......iAjYU* ,Nj ...OH [HIM n] </span><span class="font6" style="font-weight:bold;font-variant:small-caps;"><sub>l</sub>6jTN[AA]qj-</span></p>
<p><span class="font6" style="font-weight:bold;font-variant:small-caps;">[a£o]m e^pAi e[jclo&gt;- h him ne[TN]AKToq ng [gJygiphnh •</span></p>
<p><span class="font1">Fig.1, Original Image (excerpt), red elements are missing in the simplified version</span></p>
<p><span class="font6" style="font-weight:bold;">neTNANoyoy Mne-aooy hakim an ¿MneqHi ATTA BHCA</span></p>
<p><span class="font6" style="font-weight:bold;">MA &nbsp;&nbsp;&nbsp;A yo&gt; N&nbsp;&nbsp;&nbsp;&nbsp;OH N1M FI G TN AA </span><span class="font6" style="font-weight:bold;font-style:italic;">Of</span></p>
<p><span class="font6" style="font-weight:bold;">A£O M ej&gt;pAi 6 A (I) H NIM ne TN AKTOq NG G yGipHNH</span></p>
<p><span class="font1">Fig. 2, Simplified version (excerpt)</span></p><h1><a name="bookmark3"></a><span class="font2" style="font-weight:bold;">Methodology</span></h1>
<p><span class="font5">There are two methods to train for Coptic texts:</span></p>
<p><span class="font5">(i) &nbsp;&nbsp;&nbsp;Tesseract needs a font as the baseline and&nbsp;matches the found letters against this font.&nbsp;This can be highly convenient since fonts do&nbsp;not show many variations within a single&nbsp;document. Additional fonts can be incorporated into the model with the drawback that&nbsp;the prediction requires more computational&nbsp;time. So far, we have used Mekhaiel's original model, and we are currently experimenting by adding document-specific characters&nbsp;to increase the accuracy of a single document.</span></p>
<p><span class="font5">(ii) &nbsp;&nbsp;&nbsp;Ocropy, on the other hand, does not require&nbsp;a font. For training, it requires only a partial&nbsp;transcription: the ground truth. This transcription is used to train a neural network&nbsp;that can recognize the characters. Ocropy's&nbsp;drawback is that the ground truth cannot&nbsp;just be the alphabet but requires multiple&nbsp;pages of transcribed text with a representative letter frequency. Ocropy's training process is measured in iterations. Springmann&nbsp;proposed working with at least 30,000 iterations (a comment made by Springmann in a</span></p>
<p><span class="font5">private conversation, based upon his own experience).</span></p>
<p><span class="font5">For this contribution, we created an Ocropy model with a training set containing approximately 5,000&nbsp;characters. This set includes superlinear strokes,&nbsp;braces and foreign characters which are not part of the&nbsp;Coptic alphabet.</span></p>
<p><span class="font5">Multilingual documents and documents containing foreign characters are considered complex. Stains on&nbsp;the document, bad image quality, and annotations like&nbsp;line numbers increase the complexity of documents as&nbsp;well. We, therefore, created special pages with reduced complexity. Our original pages were stripped&nbsp;offline numbering and footnote annotations. In the&nbsp;“clean” version, all foreign characters, punctuations&nbsp;and annotations inside the text were removed, leaving&nbsp;us with a pure Coptic text. We further stripped all&nbsp;clean versions of superlinear strokes, giving us the&nbsp;simplified version.</span></p>
<p><span class="font5">For testing, the selected pages were transcribed with corresponding 'original', 'clean' and 'clean without stroke or simplified' ground truths. All results&nbsp;were compared with '</span><span class="font5" style="text-decoration:underline;">Ocreval</span><span class="font5">' (Baumann 2014)[9]&nbsp;against the ground truth.</span></p><h1><a name="bookmark4"></a><span class="font2" style="font-weight:bold;">Results</span></h1>
<p><span class="font2">Prediction</span></p>
<p><span class="font5">Mekhaiel's original Tesseract model produced the best results on simplified pages with an accuracy of&nbsp;~95%, while our Ocropy model performed better on&nbsp;the more complex pages. On the other hand, the Tes-seract tends to produce predictable errors. Character&nbsp;will, for example, always be recognised as </span><span class="font8">□</span><span class="font5">; while,&nbsp;Ocropy produces unpredictable errors. Although our&nbsp;Ocropy model is less accurate on simplified pages, it</span></p>
<p><span class="font5">surpasses Tesseract on noisier pages.</span></p>
<p><span class="font0" style="font-weight:bold;">Comparison of Mekhaiels Tesseract model and our Ocropy model</span></p><img src="502_files/502-1.jpg" style="width:169pt;height:98pt;"/>
<p><span class="font1">Fig. 3, OCR accuracy on different complexity levels</span></p>
<p><span class="font2">Costs</span></p>
<p><span class="font5">We measured that a skilled person needs roughly 10 minutes for manual transcription and 5 additional&nbsp;minutes for proofreading per page. Ocropy's models&nbsp;are built on top of transcribed images. Therefore, an&nbsp;initial ground truth is always required. Training with&nbsp;Ocropy does not require further human interaction&nbsp;but consumes up to two days of CPU power (Core i3/5</span></p>
<p><span class="font5">2.4GHz/3.2GHz, 8GB RAM, SSD), training cannot be run in parallel. Tesseract's training process, on the&nbsp;other hand, depends on the font extraction. We do not&nbsp;have enough data to estimate the time required to extract a font from an image. Both predictions still have&nbsp;to be checked manually, which can take up to 5&nbsp;minutes. With clean pages and reduced proofreading&nbsp;time per page, Fig. 4 shows an optimal OCR workload&nbsp;reduction (red lines) in comparison to manual transcription (yellow line). A more realistic scenario is&nbsp;mentioned in the discussion.</span></p><img src="502_files/502-2.jpg" style="width:222pt;height:137pt;"/>
<p><span class="font1">Fig. 4, workload comparison</span></p><h1><a name="bookmark5"></a><span class="font2" style="font-weight:bold;">Discussion</span></h1>
<p><span class="font5">Our result shows that Tesseract outperforms Ocropy on simplified pages in terms of accuracy and&nbsp;amount of human work. Unfortunately, in a realistic&nbsp;scenario, the pictures will always contain some of the&nbsp;previously described complexities. Pre-processing of&nbsp;the data is, therefore, essential to obtain good results.&nbsp;In Figure 4, we also computed a more realistic scenario&nbsp;(blue lines) with a higher workload on pre-processing&nbsp;for Tesseract. It shows that creating an Ocropy model&nbsp;pays off for larger and more complex document sets.</span></p>
<p><span class="font5">Tesseract's overall acceptable performance is based on the fact that no model has to be trained. As&nbsp;creating and testing a model can consume more time&nbsp;than manual transcription and proofreading, the creation of clean images might still be less efficient than the&nbsp;manual approach even if a model can be reused.</span></p>
<p><span class="font5">As long as cleaned images images are one of the desired results, our works shows that the workload can be reduced by half. This applies especially to Ocropy,&nbsp;since ground truth creation and training fit into the&nbsp;normal transcription workflow.</span></p>
<p><span class="font5">Unicode ambiguities, which unfortunately result in encoding differences, require normalization and filtering. Otherwise, these encoding differences, which&nbsp;would not be seen as errors by humans, will be&nbsp;counted. Due to the same ambiguities, it is easy to mix&nbsp;characters from different code pages, especially on&nbsp;multilingual texts and text markings. It is, therefore,&nbsp;recommended that one use only corresponding code&nbsp;pages, especially with multilingual models. Tests with&nbsp;models containing multilingual fonts will be considered in further studies.</span></p><h1><a name="bookmark6"></a><span class="font2" style="font-weight:bold;">Conclusion</span></h1>
<p><span class="font5">OCR of historical documents continues to be a hard problem, but we showed that utilizing OCR for the&nbsp;transcription of Coptic texts can reduce the overall&nbsp;workload. Since even the simplest images could not be&nbsp;recognized with 100% accuracy, further gains can only&nbsp;be achieved by better pre- and post-processing techniques.</span></p>
<p><span class="font5">A bigger workload reduction can be achieved by model reuse. However, no Coptic OCR models have&nbsp;been published besides Mekhaiel's. Therefore, we&nbsp;highly recommend publishing models alongside the&nbsp;transcription and suggest that it is possible to predict&nbsp;almost all well-preserved texts.</span></p>
<p><span class="font5">Also, although our model was able to partially predict multilingual texts, further studies are required. Multilingual texts require a specialized training process to compensate for the small numbers of foreign&nbsp;words.</span></p><h1><a name="bookmark7"></a><span class="font2" style="font-weight:bold;">Bibliography</span></h1>
<p><span class="font4" style="font-weight:bold;">He, J., Do, Q. D. M., Downton, A. C., and Kim, J. H. </span><span class="font4">(2005) “A comparison of binarization methods for historical archive documents,” in </span><span class="font4" style="font-style:italic;">Eighth International Conference on&nbsp;Document Analysis and Recognition (ICDAR05),</span><span class="font4"> p. 538542 Vol. 1.</span></p>
<p><span class="font4" style="font-weight:bold;">Gupta, M. R., Jacobson, N. P., and Garcia, E. K. </span><span class="font4">(2007). “{OCR} binarization and image pre-processing for&nbsp;searching historical documents,” </span><span class="font4" style="font-style:italic;">Pattern Recognit.,</span><span class="font4"> vol.&nbsp;40, no. 2, pp. 389-397.</span></p>
<p><span class="font4" style="font-weight:bold;">Kluzner, V., Tzadok, A., Shimony, Y., Walach, E., and An-tonacopoulos, A. </span><span class="font4">(2009) “Word-Based Adaptive OCR for Historical Books,” in </span><span class="font4" style="font-style:italic;">2009 10th International Conference&nbsp;on Document Analysis and Recognition,</span><span class="font4"> pp. 501-505.</span></p>
<p><span class="font4" style="font-weight:bold;">Springmann, U., Najock, D., Morgenroth, H., Schmid, H., Gotscharek, A., and Fink, F. </span><span class="font4">(2014) “OCR of Historical&nbsp;Printings of Latin Texts: Problems, Prospects, Progress,”&nbsp;in </span><span class="font4" style="font-style:italic;">Proceedings of the First International Conference on&nbsp;Digital Access to Textual Cultural Heritage,</span><span class="font4"> pp. 71-75.</span></p>
<p><span class="font4" style="font-weight:bold;">Mekhaiel, M. S. </span><span class="font4">(n.d.) “Moheb’s Coptic Pages.” [Online]. Available: <a href="http://www.moheb.de/">http://www.moheb.de/</a>. [Accessed: 01-Nov-2016].</span></p>
<p><span class="font4" style="font-weight:bold;">“Tesseract &nbsp;&nbsp;&nbsp;OCR.”&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font4">[Online].&nbsp;&nbsp;&nbsp;&nbsp;Available:</span></p>
<p><span class="font4"><a href="https://github.com/tesseract-ocr">https://github.com/tesseract-ocr</a>. [Accessed: 01-Nov-2016].</span></p>
<p><span class="font4" style="font-weight:bold;">“Ocropy.” &nbsp;&nbsp;&nbsp;</span><span class="font4">[Online].&nbsp;&nbsp;&nbsp;&nbsp;Available:</span></p>
<p><span class="font4"><a href="https://github.com/tmbdev/ocropy">https://github.com/tmbdev/ocropy</a>. [Accessed: 13-Dec-2016].</span></p>
<p><span class="font4" style="font-weight:bold;">Breuel, T. M. </span><span class="font4">(2008) “The OCRopus open source OCR system,” </span><span class="font4" style="font-style:italic;">Proc. SPIE 6815, Doc. Recognit. Retr. XV,</span><span class="font4"> 2008.</span></p>
<p><span class="font4" style="font-weight:bold;">Baumann, R. </span><span class="font4">(2014) “OCR Evaluation Tools.” [Online]. Available: <a href="https://github.com/ryanfb/ancientgreekocr-ocr-evaluation-tools">https://github.com/ryanfb/ancientgreekocr-ocr-evaluation-tools</a>. [Accessed: 01-Nov-2016].</span></p>
</body>
</html>