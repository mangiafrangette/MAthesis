 Interest among Digital Humanities (DH) practitioners in semantic annotation of corpora continues to grow, while at the same time linguistic resources for niche domains or languages underperform, or are simply unavailable. Since DH research often involves multilingual and multi-domain corpora—drawing on different genres, periods and cultures, and ranging in type from unstructured prose to semi-structured text—annotation can require significant time and language skills that not all contributors in research groups have. Annotation in collaborative projects is a case in point (Dossin et al., 2016). Furthermore, off the shelf tools for annotation, despite being trained with language agnostic architectures, only process one language in one domain at a time (Al-Rfou et al., 2014; Lample et al., 2016). Since available tools do not perform well for niche domains (Boeten, 2015; de Naegel, 2015), new semi-automatic semantic annotation solutions must be sought for creating data for downstream DH tasks (de Wilde and Hengchen, 2017). For annotating diverse corpora, we propose a language-agnostic, robust and customizable named entity recognition (NER) resource. It can identify any type of user-specified entities in any textual domain, although here we have largely focused on the annotation of place names for use in the spatial humanities. Instead of relying on pre-existing NER taggers not tailored to the domain of interest, our resource enables humanists to build their own customized NER taggers. Such taggers require manually annotated data to learn to identify named entities automatically. However, since annotation is costly, we propose an active learning pipeline in which the most informative sentences in a corpus are identified prior to annotation. Our approach is to optimize performance while minimizing the time and energy of exclusively manual annotation (Kettunen et al., 2017).  To this end, we developed the Humanities Entity Recognizer (HER), which uses the conditional random field (CRF) machine learning architecture, both to identify sentences containing named entities most crucial for annotation and to identify named entities once sufficient manual annotation is completed (Nadeau and Sakine, 2007; Erdmann et al., 2016).  The system is designed to work with unstructured texts in any language, especially humanities texts where entities are unevenly distributed. It assumes no language resources beyond a tokenizer.  Our inspiration comes from related NER research that demonstrates that feature-based architectures like CRFs can, in fact, be language agnostic (Curran and Clark, 2003). However, such systems do not anticipate noisy DH domains. A promising active learning strategy for low resourced neural NER exists (Shen et al., 2018), although it requires computational resources we do not assume to be available to most humanities researchers. Finally, a transfer learning solution has been proposed, whereby a model for the under-resourced Uyghur language leverages better resourced NER models for two related languages (Bharadwaj et al., 2016). This pipeline however, requires access to multiple specific sources of data beyond the average humanities data scenario.  Our design merits particular attention for its careful consideration of the workflows of the humanist. Using limited entity lists (for example, placeographies or personographies built from annotated data on the fly), we delexicalize features and introduce other factors to encourage the algorithm to generalize rapidly in identifying new entities with high recall, sorting sentences based on likelihood to contain frequent, previously unannotated named entities. The system learns on its own that capitalization is important in languages where it is, but can ignore capitalization in other languages. Since the textual scholar working with the HER system is actively involved in the iterative annotation and correction, a choice was made to favor recall over precision for the simple reason that  it is easier to remove, or hand correct, an inaccurate entity than to lose one in the “black box.” The annotation process on the initial seed and successive batches resembles a close reading of the corpus from the perspective of its potential named entities.  At present the scripts work on the command line and subsequent annotation is carried out in a text editor. Integration into community-based, social annotation interfaces is a desirable next step, but beyond the scope of this paper.   Our choice of CRF machine learning stems from a commitment to under-resourced domains, since neural models are  notoriously data hungry. Nonetheless, we tested neural models (BiLSTM-CRF and CNN-BiLSTM) extensively both for active learning and/or for performing the final NER tagging, with the result that CRFs outperform them until the amount of manually annotated data exceeds about 30,000 words. Should the user exceed this threshold, HER also supports use of neural models. Additionally, the interpretability of CRFs allowed us to maximize multiple criteria (uncertainty, representativeness, and diversity) when choosing the best sentences to annotate, whereas the neural models performed poorly for active learning, only capable of predicting sentences’ uncertainty. We also introduce the notion of an “inclusive” evaluation framework whereby the accuracy of the model is determined by both manual and automatic annotations, rather than “exclusive” frameworks that look only at the final model’s prediction on a held out test set (Erdmann et al., 2019).     Figure 1: Inclusive and Exclusive Evaluation of Learning Architectures (shallow and deep) with the Humanities Entity Recognizer (HER) The code and documentation for the Humanities Entity Recognizer (HER) are freely available at http://github.com/alexerdmann/HER. Our research stems from a session of a 2018 NYU-PSL Global Alliance funded workshop devoted to named entities and spatial humanities research. We began with non-English materials already annotated for named entities: the FranText corpus (around 200 works of pre-1920s French prose). We chose six sample texts across very different domains—a travel narrative, a gastronomic treatise, novels, an autobiography and a memoir—of varying named entity densities and distributions. The texts’ proper names were further annotated to distinguish place and person names. Performance of models trained using the HER system for annotation increased significantly. After annotating just 40,000 words, error was reduced 68.6% as compared to annotating 40,000 words from randomly selected sentences. In addition to this previously annotated corpus—not the norm for most DH research—we worked with three other unannotated corpora of significant typological and structural difference:  A German corpus of approximately 1M words (novels, travel narratives and philosophical texts) from the Weimar period, partially sourced from Project Gutenberg; A medieval French corpus composed of 1.1M words in both prose and verse for which a pre-existing placeography was available (drawing on the Open Medieval French corpus); A Portuguese corpus consisting of approximately 250K words from catalogs of the São Paolo exhibitions (1951-1959) containing a significant amount of structured information (lists of artists’ names, artwork titles, dates, places, etc.) for which a personography and placeography were available from the Artl@s Global Exhibition Catalogues Database.  We discuss results using these corpora, as well as challenges encountered using the Humanities Entity Recognizer (HER) across diverse domains and different kinds of entities. We report performance over a learning curve of quantities of manual annotation and qualitatively evaluate predictions in the absence of gold-standard data (van Hooland et al., 2015). We conclude with our key contributions in the development of this system: “whiteboxing” the NER process, handling totally unresourced domains and messy corpora in a language agnostic manner, and designing complete flexibility to granularity and types of entities. 