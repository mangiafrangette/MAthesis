 Normalisation can be produced with various solutions (Baron and Rayson, 2008; Bollmann et al., 2011; Pettersson et al., 2013a; Pettersson et al., 2013b; Sánchez-Martínez et al., 2013; Porta et al., 2013; Scherrer and Erjavec, 2013; Pettersson et al., 2014; Bollmann and Søgaard, 2016; Ljubešic et al., 2016; Tjong Kim Sang et al., 2017; Domingo et al., 2017), but recent research have demonstrated that neural machine translation (NMT) is the most efficient (Korchagina, 2017; Domingo and Casacuberta, 2018a). However, moving from test to production of a working tool is not an easy task, because of the amount of training data required for machine learning. This paper present a solution to create a parallel corpus and deliver an NMT-based normaliser for early modern French.  A first test corpus A first test has been made with the 1668 edition of  Andromaque of Jean Racine (Racine, 1668) and the 1624 edition of the  Lettres of Jean-Louis Guez de Balzac (Guez de Balzac, 1624).     Author Text Date Lines Tokens Characters   Corpus Guez de Balzac Correspondance 1624 1723 49,589 298,486    Racine Andromaque 1664 1756 13,884 86,612   Total    3479 63,473 385,098   This proto-corpus is deliberately heterogeneous to test our workflow. Guez’s  Correspondance is a collection of short letters in prose using a graphic system from the first half of the 17 th c. Racine’s  Andromaque is a play in verse with a graphic system from the second half of the 17 th c.  Transcriptions have been produced directly from PDF files (Fig. 1) with a model specifically designed for 17 th c. prints (Gabay, 2019). It has been trained on both low-quality (72 DPI) and high-quality (400 DPI) images of books using various fonts and the extracted text preserves abbreviations ( ẽ…) and special characters ( ſ…) but not ligatures ( ﬁ…).     Fig. 1 Racine,  Andromaque, Paris, BNF, RES-YF-3206, p. 2    Pre-processing Following previous successful experiments (Bollmann, 2012), a rule-based system for pre-orthographic French has been developed (Riguet, 2019). It is based on two lexical resources: Morphalou, an open lexical database of inflected forms of contemporary French (Romary et al., 2004), and LGeRM, an open morphological lexicon for middle French (Souvay and Pierrel, 2009) now covering also 17 th c. French (Diwersy et al., 2017). Based on these two databases, the normaliser applies transformations on each token, before a manual correction of the result.  Normalisation consists of aligning 17 th c. graphic systems (source) to 21 st c. orthography (target)    Source Target    Sur tout ie redout ois cette Mélancolie   Surtout je redout ais cette Mélancolie    Où j’a y v eu ſi long -temps v oſtre  Ame ense uelie.  Où j’a i v u si longtemps v otre  Âme ensevelie.     Ie craign ois que le Ciel, par  vn cruel  ſecours,   Je craign ais que le Ciel, par  un cruel  secours,      First results with an NMT-based normaliser We have decided to use NMTPYTORCH (Caglayan et al., 2017). The baseline model is composed of a 2-layer bi-directional GRU (Cho et al., 2014) encoder and a 2-layer conditional GRU (Sennrich et al., 2017) decoder with MLP attention (Bahdanau et al., 2015). The encoder and the decoder both have 256 hidden units and their initial hidden state is initialised to 0. The embedding dimensionality is also set to 256. Two versions of the system have been trained. The first one is a word level system and the second one uses the byte pair encoding (BPE) (Sennrich et al., 2015) which operates at the subword level. The corpus has been divided into two parts: 90% of the lines have been used for training and 10% for testing.    Lines Tokens Characters   Train 3,133 5,6825 348,098   Test 346 5,959 37,000   Total 3,479 62,784 385,098   Five trainings have been made with different initialisations on the two different models: words and subwords ( i.e. BPE units). Accuracy of the result is calculated with BLEU scores (Papineni et al., 2002).    Model Average BLEU Best BLEU   Words 79.27 82.960   BPE 75.79 77.070   These BLEU scores still have to be used with extreme care considering the limited size of our corpus. They are however promising enough to engage in the production of a large-scale corpus for a NMT-based normaliser.   Future developments To be as universal as possible, our training data must reflect all the lexical and graphic variety of 17 th c. French. We are therefore engaging in the construction of a representative corpus of early modern French, including excerpts of literary (plays, novel, poems…) and non-literary texts (theology, medicine, law, science…), in verse and in prose, spread diachronically across the century, and taken from original editions, reprints or illegal prints. Along this compilation phase, the OCR model and the rule-based normalising solution will be regularly improved to increase their efficiency before a final open source release.  The final corpus, expanded with back translation (Domingo and Casacuberta, 2018b), will be used for the training of an NMT-based solution. On top of words and subwords, character-level NMT will also be tested to provide the most efficient tool. A special model, trained to normalise the results of the rule-based system rather than the raw OCRised text will be tested, to evaluate the efficiency of a hybrid system using both technologies.  