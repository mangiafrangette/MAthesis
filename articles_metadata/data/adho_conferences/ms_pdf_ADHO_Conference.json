[
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": "In theory, digital humanities projects should rely on standards for text and character encoding. For character encoding, the standard recommended by TEI P5 (TEI Consortium, eds. Guidelines for Electronic Text Encoding and Interchange [last modiﬁ ed: 03 Feb 2008], http://www.tei-c.org/P5/) is the Unicode Standard (http://www.unicode.org/standard/standard.html). The choices made by digital projects in character encoding can be critical, as they impact text analysis and language processing, as well as the creation, storage, and retrieval of such textual digital resources. This talk will discuss new characters and important features of Unicode 5.0 and 5.1 that could impact digital humanities projects, discuss the process of proposing characters into Unicode, and provide the theoretical underpinnings for acceptance of new characters by the standards committees. It will also give speciﬁ c case studies from recent Unicode proposals in which certain characters were not accepted, relaying the discussion in the standards committees on why they were not approved. This latter topic is important, because decisions made by the standards committees ultimately will affect text encoding. For those characters not in Unicode, the P5 version of the TEI Guidelines deftly describes what digital projects should do in Chapter 5 (TEI Consortium, eds. “Representation of Non- standard Characters and Glyphs,” Guidelines for Electronic Text Encoding and Interchange [last modiﬁ ed: 03 Feb 2008], http://www.tei-c.org/release/doc/tei-p5-doc/en/html/ WD.html [accessed: 24 March 2008]), but one needs to be aware of the new characters that are in the standards approval process. The presentation will brieﬂ y discuss where to go to look for the new characters on public websites, which are “in the pipeline.” The release of Unicode 5.0 in July 2007 has meant that an additional 1,369 new characters have been added to the standard, and Unicode 5.1, due to be released in April 2008, will add 1,624 more (http://www.unicode.org/versions/ Unicode5.1.0/) In order to create projects that take advantage of what Unicode and Unicode-compliant software offers, one must be kept abreast of developments in this standard and make appropriate changes to fonts and documents as needed. For projects involving medieval and historic texts, for example, the release of 5.1 will include a signiﬁ cant number of European medieval letters, as well as new Greek and Latin epigraphic letters, editorial brackets and half-brackets, Coptic combining marks, Roman weights and measures and coin symbols, Old Cyrillic letters and Old Slavonic combining letters. The Menota project (http://www.menota.org/guidelines-2/convertors/ convert_2-0-b.page), EMELD’s “School of Best Practice” (http://linguistlist.org/emeld/school/classroom/conversion/index.html), and SIL’s tools (http://scripts.sil.org/Conversion) all provide samples of conversion methods for upgrading digital projects to include new Unicode characters. Since Unicode is the accepted standard for character encoding, any critical assessment of Unicode made to the body in charge of Unicode, the Unicode Technical Committee, is generally limited to comments on whether a given character is missing in Unicode or--if proposed or currently included in Unicode--critiques of a character’s glyph and name, as well as its line-breaking properties and sorting position. In Chapter 5 of the TEI P5 Guidelines, mention is made of character properties, but it does not discuss line-breaking or sorting, which are now two components of Unicode proposals and are discussed in annexes and standards on the Unicode Consortium website (Unicode Standard Annex #14 “Line Breaking Properties,” Unicode Technical Standard #10, “Unicode Collation Algorithm,” both accessible from www. unicode.org). Users should pay close attention to these two features, for an incorrect assignment can account for peculiar layout and sorting features in software. Comments on missing characters, incorrect glyphs or names, and properties should all be directed to the Unicode online contact page (http:// www.unicode.org/reporting.html). It is recommended that an addition to Chapter 5 of P5 be made regarding word-breaking and collation when defi ning new characters. The Unicode Standard will, with Unicode 5.1, have over 100,000 characters encoded, and proposals are underway for several unencoded historic and modern minority scripts, many through the Script Encoding Initiative at UC Berkeley (http://www.linguistics.berkeley.edu/sei/alpha-script-list.html). Reviewing the glyphs, names, and character properties for this large number of characters is diffi cult. Assistance from the academic world is sought for (a) authoring and review of current proposals of unencoded character and scripts, and (b) proofi ng the beta versions of Unicode. With the participation of digital humanists, this character encoding standard can be made a reliable and useful standard for such projects.",
    "article_title": "Unicode 5.0 and 5.1 and Digital Humanities Projects",
    "date": "2008",
    "publisher": "",
    "authors": [
    {
      "given": "Deborah",
      "family": "Winthrop Anderson",
      "affiliation": [
        "UC Berkeley, USA"
      ]
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": "<Introduction> Among works, devoted to the quantitative study of style, an approach prevails which can be conventionally called as synchronic. Synchronic approach is aimed at solving various classifi cation problems (including those of attribution), making use of average (mean) values of characteristics, which refl ect the style of the whole creative activity of an author. This approach is based on the assumption that the features of an individual style are not changing during lifetime or vary in time very little, due to which the changes can be disregarded as linguistically irrelevant. This assumption can be tested in experiments, organised within a diachronic approach, whose purpose is to compare linguistic properties of texts, written by the same author at different periods of his life. This paper presents the results of such a diachronic study of the individual style of famous American romantic poet E.A.Poe. The study was aimed at fi nding out whether there were linguistically relevant differences in the style of the poet at various periods of his creative activity and if so, at revealing linguistic markers for the transition from one period to the other. <Material> The material includes iambic lyrics published by Poe in his 4 collections of poems. Lyrics were chosen because this genre expresses in the most vivid way the essential characteristics of a poet. In order to achieve a common basis for the comparison only iambic texts were taken, they usually did not exceed 60 lines. It should be noted that iamb was used by Poe in most of his verses. Sonnets were not taken for analysis because they possess specifi c structural organization. Poe’s life is divided into three periods: (1) from Poe’s fi rst attempts to write verses approximately in 1824 till 1829, (2) from 1830 till 1835 and (3) from 1836 till 1849. <Characteristics> For the analysis 27 characteristics were taken. They include morphological and syntactic parameters. Morphological characteristics are formulated in terms of traditional morphological classes (noun, verb, adjective, adverb and pronoun). We counted how many times each of them occurs in the fi rst and the fi nal strong (predominantly stressed) syllabic positions – ictuses. Most of syntactic characteristics are based on the use of traditional notions of the members of the sentence (subject, predicate, object, adverbial modifi er) in the fi rst and the fi nal strong positions in poems. Other syntactic parameters are the number of clauses in (a) complex and (b) compound sentences. There are also several characteristics which represent what can be called as poetical syntax. They are the number of enjambements, the number of lines divided by syntactic pauses and the number of lines, ending in exclamation or question marks. Enjambement takes place when a clause is continued on the next line (And what is not a dream by day / To him whose eyes are cast / On things around him <…>). Pause is a break in a line, caused by a subordinate clause or another sentence (I feel ye now – I feel ye in your strength – <…>). The values of the characteristics, which were obtained as a result of the analysis of lyrics, were normalised over the size of these texts in lines. <Method> One of multivariate methods of statistical analyses – discriminant analysis – was used. This method has been successfully used in the study of literary texts for authorship detection (Stamatatos, Fakatakis and Kokkinakis 2001; Baayen, Van Halteren, and Tweedie 1996, etc.), genre differentiation (Karlgen, Cutting 1994; Minori Murata 2000, etc.), gender categorization (Koppel et al. 2002; Olsen 2005), etc. Discriminant analysis is a procedure whose purpose is to fi nd characteristics, discriminating between naturally occurring (or a priori formed) classes, and to classify into these classes separate (unique) cases which are often doubtful and “borderline”. For this purpose linear functions are calculated in such a way as to provide the best differentiation between the classes. The variables of these functions are characteristics of objects, relevant for discrimination. Judging by the coeffi cients of these variables we can single out the parameters which possess maximum discriminating force. Besides, the procedure enables us to test the statistical signifi cance of the obtained results (Klecka, 1989). In this paper discriminant analysis is used to fi nd out if there is any difference between groups of texts written during Periods 1–3, reveal characteristics differentiating these text groups and establish their discriminating force. <Results> It would be natural to expect that due to Poe’s relatively short period of creative activity (his fi rst collection of poems was published in 1827, his last collection – in 1845) his individual style does not vary much, if at all. Nevertheless the results show that there are clearly marked linguistic differences between his texts written during these three periods. Out of 27 characteristics, used in the analysis, 14 proved to possess discriminating force, distinguishing between the verse texts of different periods of the author’s life. The strongest discriminating force was observed in morphological characteristics of words both in the fi rst and fi nal strong positions and syntactic characteristics of the initial part of verse lines. These parameters may be used for automatic classifi cation of Poe’s lyrics into three groups corresponding to three periods of his creative activity with 100% correctness. The transition from the fi rst to the second period is mainly characterised by changes in the number of verbs, nouns and pronouns in the fi rst and the last strong positions, as well as in the number of subordinate clauses in complex sentences, words in the function of adverbial modifi er in the initial position in the line. The development in Poe’s style from the second to the third period is also marked by changes in the number of morphological classes of words in the initial and fi nal strong positions of the line (nouns, adverbs and pronouns). It should be stressed that these changes refl ect general tendencies of variation of frequencies of certain elements and are not present in all the texts. In the following examples the shift of verbs from the fi nal part of the line, which is characteristic of the fi rst period, to the initial strong position of the line (i.e. second syllable) in the second period is observed. Period 1 But when within thy waves she looks – Which glistens then, and trembles – Why, then, the prettiest of brooks Her worshipper resembles – For in my heart – as in thy streem – Her image deeply lies <...> (To the River) Period 2 You know the most enormous fl ower – That rose – <...> I tore it from its pride of place And shook it into pieces <...> (Fairy Land) On the whole the results show that there are certain linguistic features which refl ect the changes in the style of E.A.Poe. Among important period markers are part of speech characteristics and several syntactic parameters.",
    "article_title": "Variation of Style: Diachronic Aspect",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "Vadim",
      "family": "Andreev",
      "affiliation": [
        "Smolensk State University, Russian Federation"
      ]
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": "The US Government Photos and Graphics Collection include some of the nation’s most precious historical documents. However the current federation is not effective for exploration. We propose an architecture that enables users to collaboratively construct a faceted classifi cation for this historical image collection, or any other large online multimedia collections. We have implemented a prototype for the American Political History multimedia collection from usa.gov, with a collaborative faceted classifi cation interface. In addition, the proposed architecture includes automated document classifi cation and facet schema enrichment techniques. <Introduction> It is diffi cult to explore a large historical multimedia humanities collection without a classifi cation scheme. Legacy items often lack textual description or other forms of metadata, which makes search very diffi cult. One common approach is to have librarians classify the documents in the collection. This approach is often time or cost prohibitive, especially for large, growing collections. Furthermore, the librarian approach cannot refl ect diverse and ever-changing needs and perspectives of users. As Sir Tim Berners-Lee commented: “the exciting thing [about Web] is serendipitous reuse of data: one person puts data up there for one thing, and another person uses it another way.” Recent social tagging systems such as del. icio.us permit individuals to assign free-form keywords (tags) to any documents in a collection. In other words, users can contribute metadata. These tagging systems, however, suffer from low quality of tags and lack of navigable structures. The system we are developing improves access to a large multimedia collection by supporting users collaboratively build a faceted classifi cation. Such a collaborative approach supports diverse and evolving user needs and perspectives. Faceted classifi cation has been shown to be effective for exploration and discovery in large collections [1]. Compared to search, it allows for recognition of category names instead of recalling of query keywords. Faceted classifi cation consists of two components: the facet schema containing facets and categories, and the association between each document and the categories in the facet schema. Our system allows users to collaboratively 1) evolve a schema with facets and categories, and 2) to classify documents into this schema. Through users’ manual efforts and aided by the system’s automated efforts, a faceted classifi cation evolves with the growing collection, the expanding user base, and the shifting user interests. Our fundamental belief is that a large, diverse group of people (students, teachers, etc.) can do better than a small team of librarians in classifying and enriching a large multimedia collection. <Related Research> Our research builds upon popular wiki and social tagging systems. Below we discuss several research projects closest to ours in spirit. The Flamenco project [1] has developed a good browsing interface based on faceted classifi cation, and has gone through extensive evaluation with digital humanities collections such as the fi ne art images at the museums in San Francisco. Flamenco, however, is a “read-only” system. The facet schema is predefi ned, and the classifi cation is pre-loaded. Users will not be able to change the way the documents are classifi ed. The Facetag project [2] guides users’ tagging by presenting a predetermined facet schema to users. While users participate in classifying the documents, the predetermined facet schema forces users to classify the documents from the system’s perspective. The rigid schema is insuffi cient in supporting diverse user perspectives. A few recent projects [4, 7] attempt to create classifi cation schemas from tags collected from social tagging systems. So far these projects have generated only single hierarchies, instead of multiple hierarchies as in faceted schemas. Also just as any other data mining systems, these automatic classifi cation approaches suffers from quality problems. So far, no one has combined user efforts and automated techniques to build a faceted classifi cation, both to build the schema and to classify documents into it, in a collaborative and interactive manner. <Architecture and Prototype Implementation> The architecture of our system is shown in Figure 1. Users can not only tag (assign free-form keywords to) documents but also collaboratively build a faceted classifi cation in a wiki fashion. Utilizing the metadata created by users’ tagging efforts and harvested from other sources, the system help improve the classifi cation. We focus on three novel features: 1) to allow users collaboratively build and maintain a faceted classifi cation, 2) to systematically enrich the user-created facet schema, 3) to automatically classify documents into the evolving facet schema. We have developed a Web-based interface that allows users create and edit facets/categories similar to managing directories in the Microsoft File Explorer. Simply by clicking and dragging documents into faceted categories, users can classify (or re-classify) historic documents. All the fi les and documents are stored in a MySQL database. For automatic classifi cation, we use a support vector machine method [5] utilizing users’ manual classifi cation as training input. For systematic facet enrichment, we are exploring methods that create new faceted categories from free-form tags based on a statistical co-occurrence model [6] and also WordNet [8]. Note that the architecture has an open design so that it can be integrated with existing websites or content management systems. As such the system can be readily deployed to enrich existing digital humanity collections. We have deployed a prototype on the American Political History (APH) sub-collection (http://teachpol.tcnj.edu/ amer_pol_hist) of the US Government Photos and Graphics Collection, a federated collection with millions of images (http://www.usa.gov/Topics/Graphics.shtml). The APH collection currently contains over 500 images, many of which are among the nation’s most valuable historical documents. On the usa.gov site, users can explore this collection only by two ways: either by era, such as 18th century and 19th century, or by special topics, such as “presidents” (Figure 2). There are only four special topics manually maintained by the collection administrator, which do not cover most items in the collection. This collection is poor with metadata and tools, which is common to many digital humanity collections that contain legacy items that have little pre-existing metadata, or lack resources for maintenance. The prototype focused on the collaborative classifi cation interface. After deploying our prototype, the collection has been collaboratively classifi ed into categories along several facets. To prove the openness of system architecture, the prototype has been integrated with different existing systems. (Figure 3) As users explore the system (such as by exploring faceted categories or through a keyword search), besides each item there is a “classify” button which leads to the classifi cation interface. The classifi cation interface shows the currently assigned categories in various facets for the selected item. It allows user to drag and drop an item into a new category. At this level user can also add or remove categories from a facet, or add or remove a facet. <Evaluation and Future Steps> Initial evaluation results in a controlled environment show great promise. The prototype was tested by university students interested in American political history. The collection was collaboratively categorized into facets such as Artifact (map, photo, etc.), Location, Year, and Topics (Buildings, Presidents, etc.) The prototype is found to be more effective than the original website in supporting user’s retrieval tasks, in terms of both recall and precision. At this time, our prototype does not have all the necessary support to be deployed on public Internet for a large number of users. For this we need to work on the concept of hardening a newly added category or facet. The key idea behind hardening is to accept a new category or facet only after reinforcement from multiple users. In absence of hardening support our system will be overwhelmed by the number of new facets and categories. We are also exploring automated document classifi cation and facet schema enrichment techniques. We believe that collaborative faceted classifi cation can improve access to many digital humanities collections.",
    "article_title": "Exploring Historical Image Collections with Collaborative Faceted Classifi cation",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "Georges",
      "family": "Arnaout",
      "affiliation": [
        "Old Dominion University, USA"
      ]
    },
    {
      "given": "Georges",
      "family": "Arnaout",
      "affiliation": [
        "Old Dominion University, USA"
      ]
    },
    {
      "given": "Kurt",
      "family": "Maly",
      "affiliation": [
        "Old Dominion University, USA"
      ]
    },
    {
      "given": "Milena",
      "family": "Mektesheva",
      "affiliation": [
        "Old Dominion University, USA"
      ]
    },
    {
      "given": "Harris",
      "family": "Wu",
      "affiliation": [
        "Old Dominion University, USA"
      ]
    },
    {
      "given": "Mohammad",
      "family": "Zubair",
      "affiliation": [
        "Old Dominion University, USA"
      ]
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": "<Introduction> Facsimile images form a major component in many digital editing projects. Well-known projects such as the Blake Archive [Eaves 2007] and the Rossetti Archive [McGann 2007] use facsimile images as the primary entry point to accessing the visually rich texts in their collections. Even for projects focused on transcribed electronic editions, it is now standard practice to include high-resolution facsimile. Encoding standards and text processing toolkits have been the focus of signifi cant research. Tools, standards, and formal models for encoding information in image-based editions have only recently begun to receive attention. Most work in this area has centered on the digitization and presentation of visual materials [Viscomi 2002] or detailed markup and encoding of information within a single image [Lecolinet 2002, Kiernan 2004, Dekhtyar 2006]. Comparatively little has been work has been done on modeling the large-scale structure of facsimile editions. Typically, the reading interface that presents a facsimile determines its structure. Separating the software used to model data from that used to build user interfaces has well-known advantages for both engineering and digital humanities practices. To achieve this separation, it is necessary to develop a model of a facsimile edition that is independent of the interface used to present that edition. In this paper, we present a unifi ed approach for representing linguistic, structural, and graphical content of a text as an Annotated Facsimile Edition (AFED). This model grows out of our experience with several digital facsimile edition projects over more than a decade, including the Cervantes Project [Furuta 2001], the Digital Donne [Monroy 2007a], and the Nautical Archaeology Digital Library [Monroy 2007b]. Our work on these projects has emphasized the need for an intuitive conceptual model of a digital facsimile. This model can then serve as the basis for a core software module that can be used across projects without requiring extensive modifi cation by software developers. Drawing on our prior work we have distilled fi ve primary goals for such a model: • Openness: Scholars’ focused research needs are highly specifi c, vary widely between disciplines, and change over time. The model must accommodate new information needs as they arise. • Non-hierarchical: Facsimile editions contain some information that should be presented hierarchically, but they cannot be adequately represented as a single, properly nested hierarchy. • Restructuring: A facsimile is a representation of the physical form of a document, but the model should enable applications to restructure the original form to meet specifi c needs. • Alignment: Comparison between varying representations of the same work is a fundamental task of humanities research. The model must support alignment between facsimiles of different copies of a work. <Annotated Facsimile Editions> The Annotated Facsimile Edition (AFED) models the macro level structure of facsimile editions, representing them as a stream of images with annotations over that stream. Figure 1 shows a simplifi ed diagram illustrating a two-volume edition of collected poems. Annotations encode the structure of the document and properties of the structural elements they represent. Separate annotation streams encode multiple analytical perspectives. For example, in fi gure 1, the annotations shown below the image stream describe the physical structure of the edition (volumes, pages, and lines) while the annotations shown above the image stream describe the poetic structure (poems, titles, epigraphs, stanzas). Annotations within a single analytical perspective—but not those from different perspectives—follow a hierarchical structure. <The Image Stream> The image stream intuitively corresponds to the sequential ordering of page images in a traditional printed book. These images, however, need not represent actual “pages.” An image might show a variety of artifacts including an opening of a book, a fragment of a scroll, or an unbound leaf of manuscript notes. While it is natural to treat facsimile images sequentially, any particular linear sequence represents an implementation decision—a decision that may not be implied by the physical document. For example, an editor may choose to arrange an edition of letters according to the date written, recipient, or thematic content. The image stream, therefore, is an implementation detail of the model. The structure of the edition is specifi ed explicitly by the annotations. Many historical texts exist only as fragments. Many more have suffered damage that results in the lost of a portion of the original text. Despite this damage, the general content and characteristics of the text may be known or hypothesized based on other sources. In other cases, while the original artifact may exist, a digital representation of all or part of the artifact may unavailable initially. To enable scholars to work with missing or unavailable portions of a facsimile, we introduce the notion of an abstract image. An abstract image is simply a placeholder for a known or hypothesized artifact of the text for which no image is available. Annotations attach to abstract images in the same way they attach to existing images. <Annotations> Annotations are the primary means for representing structural and linguistic content in the AFED. An annotation identifi es a range of images and specifi es properties about those images. Table 1 lists the information specifi ed by each annotation. Properties in italics are optional. As shown in this table, annotations support three main categories of information: annotation management, content, and structural information. The annotation management and structural information categories contain record keeping information. Structural information describes the hierarchical structure of annotation within an analytical perspective. The annotation management category specifi es the annotation type and identifi es the image content referenced by the annotation. The sequence number is an identifi er used by AFED to determine the relative ordering of multiple annotations that have the same starting index. AFED is agnostic to the precise semantics of this value. The annotation type determines these semantics. For example, a paragraph annotation may refer to the paragraph number relative to a page, chapter, or other structural unit. The content category describes the item referenced by the annotation. Annotations support two naming conventions. To facilitate comparison between documents, an annotation may specify a canonical name according to a domain specifi c naming convention. Canonical names usually do not match the name given to the referenced item by the artifact itself and are rarely appropriate for display to a general audience. Accordingly, the annotation requires the specifi cation of a name suitable for display. Descriptive metadata can be specifi ed as a set of key/value properties. In addition to descriptive metadata, annotations support multiple transcriptions. Multiple transcriptions allow alternate perspectives of the text; for example, a paleographic transcription to support detailed linguistic analysis and a normalized transcription to facilitate reading. Transcriptions may also include translations. AFED’s annotation mechanism defi nes a high-level syntactical structure that is suffi cient to support the basic navigational needs of most facsimile projects. By remaining agnostic to semantic details, it allows for fl exible, project specifi c customization. Where projects need to support user interactions that go beyond typical navigation scenarios, these interactions can be integrated into the user interface without requiring changes to the lower-level tools used to access the facsimile. <Discussion> AFED has proven to be a useful model in our work. We have deployed a proof of concept prototype based on the AFED model. Several of the facsimile editions constructed by the Cervantes Project use this prototype behind the scenes. Given its success in these reader’s interfaces, we are working to develop a Web-based editing toolkit. This application will allow editors to quickly defi ne annotations and use those annotations to describe a facsimile edition. We anticipate completing this tool by the summer of 2008. By using multiple, hierarchical annotation streams, AFED’s expressive power falls under the well-studied class of document models, known as OHCO (ordered hierarchy of content objects). Specifi cally, it is an instance of a revised form of this generic model known as OHCO-3, [Renear 1996]. Whereas most prior research and development associated with the OHCO model has focused on XML-based, transcribed content, we have applied this model to the task of representing macro-level structures in facsimile editions. Focusing on macro-level document structure partially isolates the AFED model from the non-hierarchical nature of documents both in terms of the complexity of the required data structures, and in terms of providing simplifi ed model to facilitate system implementation. If warranted by future applications, we can relax AFED’s hierarchical constraint. Relaxing this constraint poses no problems with the current prototype; however, further investigation is needed to determine potential benefi ts and drawbacks. In addition to macro-level structures, a document model that strives to represent the visual content of a document for scholarly purposes must also account for fi ne-grained structures present in individual images and provide support for encoded content at a higher level of detail. We envision using the AFED model in conjunction with models tailored for these low-level structures. We are working to develop a model for representing fi ne-grained structure in visually complex documents grounded in spatial hypermedia theory.",
    "article_title": "Annotated Facsimile Editions: Defi ning Macro-level Structure for Image-Based Electronic Editions",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "Neal",
      "family": "Audenaert",
      "affiliation": [
        "Texas A&M University, USA"
      ]
    },
    {
      "given": "Richard",
      "family": "Furuta",
      "affiliation": [
        "Texas A&M University, USA"
      ]
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": "In this paper, we present a Web-based interface for editing visually complex documents, such as modern authorial manuscripts. Applying spatial hypertext theory as the basis for designing this interface enables us to facilitate both interaction with the visually complex structure of these documents and integration of heterogeneous sources of external information. This represents a new paradigm for designing systems to support digital textual studies. Our approach emphasizes the visual nature of texts and provides powerful tools to support interpretation and creativity. In contrast to purely image-based systems, we are able to do this while retaining the benefi ts of traditional textual analysis tools. <Introduction> Documents express information as a combination of written words, graphical elements, and the arrangement of these content objects in a particular media. Digital representations of documents—and the applications built around them—typically divide information between primarily textual representations on the one hand (e.g., XML encoded documents) and primarily graphical based representations on the other (e.g., facsimiles). Image-based representations allow readers access to highquality facsimiles of the original document, but provide little support for explicitly encoded knowledge about the document. XML-based representations, by contrast, are able to specify detailed semantic knowledge embodied by encoding guidelines such as the TEI [Sperberg-McQueen 2003]. This knowledge forms the basis for building sophisticated analysis tools and developing rich hypertext interfaces to large document collections and supporting materials. This added power comes at a price. These approaches are limited by the need to specify all relevant content explicitly. This is, at best, a time consuming and expensive task and, at worst, an impossible one [Robinson 2000]. Furthermore, in typical systems, access to these texts mediated almost exclusively by the transcribed linguistic content, even when images alongside their transcriptions. By adopting spatial hypertext as a metaphor for representing document structure, we are able to design a system that emphasizes the visually construed contents of a document while retaining access to structured semantic information embodied in XML-based representations. Dominant hypertext systems, such as the Web, express document relationships via explicit links. In contrast, spatial hypertext expresses relationships by placing related content nodes near each other on a two-dimensional canvas [Marshall 1993]. In addition to spatial proximity, spatial hypertext systems express relationships through visual properties such as background color, border color and style, shape, and font style. Common features of spatial hypermedia systems include parsers capable of recognizing relationships between objects such as lists, list headings, and stacks, structured metadata attributes for objects, search capability, navigational linking, and the ability to follow the evolution of the information space via a history mechanism. The spatial hypertext model has an intuitive appeal for representing visually complex documents. According to this model, documents specify relationships between content objects (visual representations of words and graphical elements) based on their spatial proximity and visual similarity. This allows expression of informal, implicit, and ambiguous relationships— a key requirement for humanities scholarship. Unlike purely image-based representations, spatial hypertext enables users to add formal descriptions of content objects and document structure incrementally in the form of structured metadata (including transcriptions and markup). Hypermedia theorists refer to this process as “incremental formalism” [Shipman 1999]. Once added, these formal descriptions facilitate system support for text analysis and navigational hypertext. Another key advantage of spatial hypertext is its ability to support “information triage” [Marshall 1997]. Information triage is the process of locating, organizing, and prioritizing large amounts of heterogeneous information. This is particularly helpful in supporting information analysis and decision making in situations where the volume of information available makes detailed evaluation of it each resource impossible. By allowing users to rearrange objects freely in a two-dimensional workspace, spatial hypertext systems provide a lightweight interface for organizing large amounts of information. In addition to content taken directly from document images, this model encourages the inclusion of visual surrogates for information drawn from numerous sources. These include related photographs and artwork, editorial annotations, links to related documents, and bibliographical references. Editors/ readers can then arrange this related material as they interact with the document to refi ne their interpretive perspective. Editors/readers are also able to supply their own notes and graphical annotations to enrich the workspace further. <System Design> We are developing CritSpace as a proof of concept system using the spatial hypertext metaphor as a basis for supporting digital textual studies. Building this system from scratch, rather than using an existing application, allows us to tailor the design to meet needs specifi c to the textual studies domain (for example, by including document image analysis tools). We are also able to develop this application with a Web-based interface tightly integrated with a digital archive containing a large volume of supporting information (such as artwork, biographical information, and bibliographic references) as well as the primary documents. Initially, our focus is on a collection of manuscript documents written by Picasso [Audenaert 2007]. Picasso’s prominent use of visual elements, their tight integration with the linguistic content, and his reliance on ambiguity and spatial arrangement to convey meaning make this collection particularly attractive [Marin 1993, Michaël 2002]. These features also make his work particularly diffi cult to represent using XML-based approaches. More importantly, Picasso’s writings contain numerous features that exemplify the broader category of modern manuscripts including documents in multiple states, extensive authorial revisions, editorial drafts, interlinear and marginal scholia, and sketches and doodles. CritSpace provides an HTML based interface for accessing the collection maintained by the Picasso Project [Mallen 2007] that contains nearly 14,000 artworks (including documents) and 9,500 biographical entries. CritSpace allows users to arrange facsimile document images in a two dimensional workspace and resize and crop these images. Users may also search and browse the entire holdings of the digital library directly from CritSpace, adding related artworks and biographical information to the workspace as desired. In addition to content taken from the digital library, users may add links to other the workspace in the form of annotations. All of these items are displayed as content nodes that can be freely positioned and whose visual properties can be modifi ed. Figure 1 shows a screenshot of this application that displays a document and several related artworks. CritSpace also introduces several features tailored to support digital textual studies. A tab at the bottom of the display opens a window containing a transcription of the currently selected item. An accordion-style menu on the right hand side provides a clipboard for temporarily storing content while rearranging the workspace, an area for working with groups of images, and a panel for displaying metadata and browsing the collection based on this metadata. We also introduce a full document mode that allows users to view a high-resolution facsimile. This interface allows users to add annotations (both shapes and text) to the image and provides a zooming interface to facilitate close examination of details. <Future Work> CritSpace provides a solid foundation for understanding how to apply spatial hypertext as a metaphor for interacting with visually complex documents. This perspective opens numerous directions for further research. A key challenge is developing tools to help identify content objects within a document and then to extracted these object in a way that will allow users to manipulate them in the visual workspace. Along these lines, we are working to adapt existing techniques for foreground/background segmentation [Gatos 2004], word and line identifi cation [Manmatha 2005], and page segmentation [Shafait 2006]. We are investigating the use of Markov chains to align transcriptions to images semiautomatically [Rothfeder 2006] and expectation maximization to automatically recognize dominant colors for the purpose of separating information layers (for example, corrections made in red ink). Current implementations of these tools require extensive parameter tuning by individuals with a detailed understanding of the image processing algorithms. We plan to investigate interfaces that will allow non-experts to perform this parameter tuning interactively. Modern spatial hypertext applications include a representation of the history of the workspace [Shipman 2001]. We are interested in incorporating this notion, to represent documents, not as fi xed and fi nal objects, but rather objects that have changed over time. This history mechanism will enable editors to reconstruct hypothetical changes to the document as authors and annotators have modifi ed it. It can also be used to allowing readers to see the changes made by an editor while constructing a particular form of the document. While spatial hypertext provides a powerful model for representing a single workspace, textual scholars will need tools to support the higher-level structure found in documents, such as chapters, sections, books, volumes. Further work is needed to identify ways in which existing spatial hypertext models can be extended express relationships between these structures and support navigation, visualization, and editing. <Discussion> Spatial hypertext offers an alternative to the dominate view of text as an “ordered hierarchy of content objects” (OCHO) [DeRose 1990]. The OCHO model emphasizes the linear, linguistic content of a document and requires explicit formalization of structural and semantic relationships early in the encoding process. For documents characterized by visually constructed information or complex and ambiguous structures, OCHO may be overly restrictive. In these cases, the ability to represent content objects graphically in a two dimensional space provides scholars the fl exibility to represent both the visual aspects of the text they are studying and the ambiguous, multi-facetted relationships found in those texts. Furthermore, by including an incremental path toward the explicit encoding of document content, this model enables the incorporation of advanced textual analysis tools that can leverage both the formally specifi ed structure and the spatial arrangement of the content objects.",
    "article_title": "CritSpace: Using Spatial Hypertext to Model Visually Complex Documents",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "Neal",
      "family": "Audenaert",
      "affiliation": [
        "Texas A&M University, USA,"
      ]
    },
    {
      "given": "George",
      "family": "Lucchese",
      "affiliation": [
        "Texas A&M University, USA,"
      ]
    },
    {
      "given": "Grant",
      "family": "Sherrick",
      "affiliation": [
        "Texas A&M University, USA,"
      ]
    },
    {
      "given": "Richard",
      "family": "Furuta",
      "affiliation": [
        "Texas A&M University, USA,"
      ]
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": "This paper demonstrates a web-based, interactive data visualisation, allowing users to quickly inspect and browse the collocational relationships present in a corpus. The software is inspired by tag clouds, fi rst popularised by on-line photograph sharing website Flickr (www.fl ickr.com). A paper based on a prototype of this Collocate Cloud visualisation was given at Digital Resources for the Humanities and Arts 2007. The software has since matured, offering new ways of navigating and inspecting the source data. It has also been expanded to analyse additional corpora, such as the British National Corpus (http://www.natcorp.ox.ac.uk/), which will be the focus of this talk. Tag clouds allow the user to browse, rather than search for specifi c pieces of information. Flickr encourages its users to add tags (keywords) to each photograph uploaded. The tags associated with each individual photograph are aggregated; the most frequent go on to make the cloud. The cloud consists of these tags presented in alphabetical order, with their frequency displayed as variation in colour, or more commonly font size. Figure 1 is an example of the most popular tags at Flickr: The cloud offers two ways to access the information. If the user is looking for a specifi c term, the alphabetical ordering of the information allows it to be quickly located if present. More importantly, as a tool for browsing, frequent tags stand out visually, giving the user an immediate overview of the data. Clicking on a tag name will display all photographs which contain that tag. language. McMaster University’s TAPoR Tools (http://taporware. mcmaster.ca/) features a ‘Word Cloud’ module, currently in beta testing. WMatrix (http://ucrel.lancs.ac.uk/wmatrix/) can compare two corpora by showing log-likelihood results in cloud form. In addition to other linguistic metrics, internet book seller Amazon provides a word cloud, see fi gure 2. In this instance a word frequency list is the data source, showing the most frequent 100 words. As with the tag cloud, this list is alphabetically ordered, the font size being proportionate to its frequency of usage. It has all the benefi ts of a tag cloud; in this instance clicking on a word will produce a concordance of that term. This method of visualisation and interaction offers another tool for corpus linguists. As developer for an online corpus project, I have found that the usability and sophistication of our tools have been important to our success. Cloud-like displays of information would complement our other advanced features, such as geographic mapping and transcription synchronisation. The word clouds produced by TAPoR Tools, WMatrix and Amazon are, for browsing, an improvement over tabular statistical information. There is an opportunity for other corpus data to be enhanced by using a cloud. Linguists often use collocational information as a tool to examine language use. Figure 3 demonstrates a typical corpus tool output: The data contained in the table lends itself to visualisation as a cloud. As with the word cloud, the list of collocates can be displayed alphabetically. Co-occurrence frequency, like word frequency, can be mapped to font size. This would produce an output visually similar to the word cloud. Instead of showing all corpus words, they would be limited to those surrounding the chosen node word. Another valuable statistic obtainable via collocates is that of collocational strength, the likelihood of two words co-occurring, measured here by MI (Mutual Information). Accounting for this extra dimension requires an additional visual cue to be introduced, one which can convey the continuous data of an MI score. This can be solved by varying the colour, or brightness of the collocates forming the cloud. The end result is shown in fi gure 4: The collocate cloud inherits all the advantages of previous cloud visualisations: a collocate, if known, can be quickly located due to the alphabetical nature of the display. Frequently occurring collocates stand out, as they are shown in a larger typeface, with collocationally strong pairings highlighted using brighter formatting. Therefore bright, large collocates are likely to be of interest, whereas dark, small collocates perhaps less so. Hovering the mouse over a collocate will display statistical information, co-occurrence frequency and MI score, as one would fi nd from the tabular view. The use of collocational data also presents additional possibilities for interaction. A collocate can be clicked upon to produce a new cloud, with the previous collocate as the new node word. This gives endless possibilities for corpus exploration and the investigation of different domains. Occurrences of polysemy can be identifi ed and expanded upon by following the different collocates. Particular instances of usage are traditionally hidden from the user when viewing aggregated data, such as the collocate cloud. The solution is to allow the user to examine the underlying data by producing an optional concordance for each node/collocate pairing present. Additionally a KWIC concordance can be generated by examining the node word, visualising the collocational strength of the surrounding words. These concordance lines can even by reordered on the basis of collocational strength, in addition to the more traditional options of preceding or succeeding words. This visualisation may be appealing to members of the public, or those seeking a more practical introduction to corpus linguistics. In teaching use they not only provide analysis, but from user feedback, also act as stimulation in creative writing. Collocate searches across different corpora or document sets may be visualised side by side, facilitating quick identifi cation of differences. While the collocate cloud is not a substitute for raw data, it does provide a fast and convenient way to navigate language. The ability to generate new clouds from existing collocates extends this further. Both this iterative nature and the addition of collocational strength information gives these collocate clouds greater value for linguistic research than previous cloud visualisations.",
    "article_title": "Glimpses though the clouds: collocates in a new light",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "David",
      "family": "Beavan",
      "affiliation": [
        "University of Glasgow, UK"
      ]
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": "Historical manuscripts and printed maps of the pre-cadastral1 period show enormous differences in scale, precision and color. Less apparent are differences in reliability between maps, or between different parts of the same map. True, modern techniques in computer assisted cartography make it possible to measure very accurately such differences between maps and geographic space with very distinct levels of precision and accuracy. However differences in reliability between maps, or between different parts of the same map, are not only due to the accuracy measurement techniques, but also to their original function and context of (re-)use. Historical information about the original context of function and context of (re-)use can give us insight how to measure accuracy, how to choose the right points for geo-referencing and how to rectify digital maps. On the other hand computer assisted cartography enables us to trace and to visualize important information about mapmaking, especially when further historical evidence is missing, is hidden or is distorted, consciously or unconsciously. The proposed paper is embedded in the project: Paper and Virtual Cities, (subsidized by the Netherlands Organization for Scientifi c Research) that aims at developing methodologies that a) permit researchers to use historical maps and related sources more accurately in creating in digital maps and virtual reconstructions of cities and b) allow users to recognize better technical manipulations and distortions of truth used in the process of mapmaking.2 In this paper we present as one of the outcomes of this project a method that visualizes different levels of accuracy in and between designs and maps in relation to their original function to assess their quality for re-use today. This method is presented by analyzing different 17th century designs, manuscript and engraved maps of the city of Leiden, in particular of the land surveyor, mapmaker Jan Pietersz. Dou. The choice for Leiden and Dou is no coincidence. One of the reasons behind differences the accuracy of maps is the enormous variety in methods and measures used in land surveying and mapmaking in the Low Countries.3 This variety was the result of differences in the private training and the backgrounds of the surveyors, in the use of local measures and in the exam procedures that differed from province to province.4 These differences would last until the 19th Century. However, already by the end of the 16th Century we see in and around Leiden the fi rst signs of standardization in surveying techniques and the use of measures.5 First of all, the Rhineland rod (3.767 meter) the standard of the water-administration body around Leiden is used more and more, along local measures in the Low Countries and in Dutch expansion overseas. A second reason to look at Leiden in more detail is that in 1600 a practical training school in land surveying and fortifi cation would be founded in the buildings of its university, the so-called Duytsche Mathematique, that turned out to be very successful not only in the Low Countries, but also in other European countries. This not only contributed to the spread and reception of the Rhineland rod, but also to the dissemination of more standardized ways of land surveying and fortifi cation.6 The instructional material of the professors of this training school and the notes of their pupils are still preserved, which allows us to study the process of surveying and mapmaking in more detail. The reason to look into the work of Jan Pietersz. Dou is his enormous production of maps. Westra (1994) calculated that at least 1150 maps still exist.7 Of the object of our case study alone, the city of Leiden, Dou produced at least 120 maps between 1600 and 1635, ranging from property maps, designs for extensions of the city and studies for civil engineering works etc. We will focus on the maps that Dou made for the urban extension of the city of Leiden of 1611. Sometimes these (partial) maps were made for a specifi c purpose; in other cases Dou tried in comprehensive maps, combining property estimates and future designs, to tackle problems of illegal economic activities, pollution, housing and fortifi cation. Since these measurements were taken in his offi cial role of, sworn-in land surveyor we can assume that they were supposed to be accurate. This variety in designs and maps for the same area allows us to discuss accuracy in relation to function and (re- )use of maps. We will also explain that the differences between designs and maps require different methods of geo-referencing and analysis. In particular, we will give attention to one design map of Dou for the northern part of the city of Leiden RAL PV 1002- 06 (Regionaal Archief Leiden) to show how misinterpretation of features lead to unreliable or biased decisions when the historical context is not taken into account, even when we can consider the results, in terms of accuracy, satisfactory. Since Dou later also made maps for commercial purposes of the same northern extension of Leiden it is interesting to compare these maps. Conclusions are drawn addressing the question of whether Dou used the same measurements to produce a commercial map or that he settled for less accuracy given the different purpose of the later map compared to his designs and property maps. To answer this question, we use modern digital techniques of geo-processing8 to compare the old maps to modern cartographical resources and to the cadastral map of the 1800s in order to determine how accurate the various maps in question are. We do this, by using the American National Standard for Spatial Data Accuracy (NSSDA) to defi ne accuracy at a 95% confi dence level. By point-based analysis we link distributional errors to classifi ed features in order to fi nd a relationship between accuracy and map function.9",
    "article_title": "The function and accuracy of old Dutch urban designs and maps. A computer assisted analysis of the extension of Leiden (1611)",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "Jakeline",
      "family": "Benavides",
      "affiliation": [
        "University of Groningen, The Netherlands"
      ]
    },
    {
      "given": "Charles",
      "family": "van den Heuvel",
      "affiliation": [
        "Royal Netherlands Academy of Arts and Sciences, The Netherlands"
      ]
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": "In this paper two highly innovative digital editions will be presented. The digital editions of the historical literary journals “Die Fackel” (published by Karl Kraus in Vienna from 1899 to 1936) and “Der Brenner” (published by Ludwig Ficker in Innsbruck from 1910 to 1954) have been developed within the corpus research framework of the “AAC - Austrian Academy Corpus” at the Austrian Academy of Sciences in collaboration with other researchers and programmers in the AAC from Vienna together with the graphic designer Anne Burdick from Los Angeles. For the creation of these scholarly digital editions the AAC edition philosophy and principles have been made use of whereby new corpus research methods have been applied for questions of computational philology and textual studies in a digital environment. The examples of these online editions will give insights into the potentials and the benefi ts of making corpus research methods and techniques available for scholarly research into language and literature. <Introduction> The “AAC - Austrian Academy Corpus” is a corpus research unit at the Austrian Academy of Sciences concerned with establishing and exploring large electronic text corpora and with conducting scholarly research in the fi eld of corpora and digital text collections and editions. The texts integrated into the AAC are predominantly German language texts of historical and cultural signifi cance from the last 150 years. The AAC has collected thousands of texts from various authors, representing many different text types from all over the German speaking world. Among the sources, which systematically cover various domains, genres and types, are newspapers, literary journals, novels, dramas, poems, advertisements, essays on various subjects, travel literature, cookbooks, pamphlets, political speeches as well as a variety of scientifi c, legal, and religious texts, to name just a few forms. The AAC provides resources for investigations into the linguistic and textual properties of these texts and into their historical and cultural qualities. More than 350 million running words of text have been scanned, digitized, integrated and annotated. The selection of texts is made according to the AAC’s principles of text selection that are determined by specifi c research interests as well as by systematic historical, empirical and typological parameters. The annotation schemes of the AAC, based upon XML related standards, have in the phase of corpus build-up been concerned with the application of basic structural mark-up and selective thematic annotations. In the phase of application development specifi c thematic annotations are being made exploring questions of linguistic and textual scholarly research as well as experimental and exploratory mark-up. Journals are regarded as interesting sources for corpus research because they comprise a great variety of text types over a long period of time. Therefore, model digital editions of literary journals have been developed: The AAC-FACKEL was published on 1 January 2007 and BRENNER ONLINE followed in October 2007. The basic elements and features of our approach of corpus research in the fi eld of textual studies will be demonstrated in this paper. The digital edition of the journal “Die Fackel” (“The Torch”), published and almost entirely written by the satirist and language critic Karl Kraus in Vienna from 1899 until 1936, offers free online access to 37 volumes, 415 issues, 922 numbers, comprising more than 22.500 pages and 6 million tokens. It contains a fully searchable database of the journal with various indexes, search tools and navigation aids in an innovative and functional graphic design interface, where all pages of the original are available as digital texts and as facsimile images. The work of Karl Kraus in its many forms, of which the journal is the core, can be regarded as one of the most important, contributions to world literature. It is a source for the history of the time, for its language and its moral transgressions. Karl Kraus covers in a typical and idiosyncratic style in thousands of texts the themes of journalism and war, of politics and corruption, of literature and lying. His infl uential journal comprises a great variety of essays, notes, commentaries, aphorisms and poems. The electronic text, also used for the compilation of a text-dictionary of idioms (“Wörterbuch der Redensarten zu der von Karl Kraus 1899 bis 1936 herausgegebenen Zeitschrift ‘Die Fackel’”), has been corrected and enriched by the AAC with information. The digital edition allows new ways of philological research and analysis and offers new perspectives for literary studies. The literary journal “Der Brenner” was published between 1910 and 1954 in Innsbruck by Ludwig Ficker. The text of 18 volumes, 104 issues, which is a small segment of the AAC’s overall holdings, is 2 million tokens of corrected and annotated text, provided with additional information. “Die Fackel” had set an example for Ludwig Ficker and his own publication. Contrary to the more widely read satirical journal of Karl Kraus, the more quiet “Der Brenner” deals primarily with themes of religion, nature, literature and culture. The philosopher Ludwig Wittgenstein was affi liated with the group and participated in the debates launched by the journal. Among its contributors is the expressionist poet Georg Trakl, the writer Carl Dallago, the translator and cultural critic Theodor Haecker, translator of Søren Kierkegaard and Cardinal Newman into German, the moralist philosopher Ferdinand Ebner and many others. The journal covers a variety of subjects and is an important voice of Austrian culture in the pre and post second world war periods. The digital edition has been made by the AAC in collaboration with the Brenner-Archive of the University of Innsbruck. Both institutions have committed themselves to establish a valuable digital resource for the study of this literary journal. Conclusion The philological and technological principles of digital editions within the AAC are determined by the conviction that the methods of corpus research will enable us to produce valuable resources for scholars. The AAC has developed such model editions to meet these aims. These editions provide well structured and well designed access to the sources. All pages are accessible as electronic text and as facsimile images of the original. Various indexes and search facilities are provided so that word forms, personal names, foreign text passages, illustrations and other resources can be searched for in various ways. The search mechanisms for personal names have been implemented in the interface for BRENNER ONLINE and will be done for “Die Fackel”. The interface is designed to be easily accessible also to less experienced users of corpora. Multi-word queries are possible. The search engine supports left and right truncation. The interface of the AAC-FACKEL provides also search mechanisms for linguistic searches, allows to perform lemma queries and offers experimental features. Instead of searching for particular word forms, queries for all the word forms of a particular lemma are possible. The same goes for the POS annotation. The web-sites of both editions are entirely based on XML and cognate technologies. On the character level use of Unicode has been made throughout. All of the content displayed in the digital edition is dynamically created from XML data. Output is produced through means of XSLT style sheets. This holds for the text section, the contents overview and the result lists. We have adopted this approach to ensure the viability of our data for as long a period as possible. Both digital editions have been optimized for use with recent browser versions. One of the basic requirements is that the browser should be able to handle XML-Dom and the local system should be furnished with a Unicode font capable of displaying the necessary characters. The interface has synchronized fi ve individual frames within one single window, which can be alternatively expanded and closed as required. The “Paratext”-section situated within the fi rst frame provides background information and essays. The “Index”- section gives access to a variety of indexes, databases and full-text search mechanisms. The results are displayed in the adjacent section. The “Contents”-section has been developed, to show the reader the whole range of the journal ready to be explored and provides access to the whole run of issues in chronological order. The “Text”-section has a complex and powerful navigational bar so that the reader can easily navigate and read within the journals either in text-mode or in image-mode from page to page, from text to text, from issue to issue and with the help of hyperlinks. These digital editions will function as models for similar applications. The AAC’s scholarly editions of “Der Brenner” and “Die Fackel” will contribute to the development of digital resources for research into language and literature. ",
    "article_title": "AAC-FACKEL and BRENNER ONLINE. New Digital Editions of Two Literary Journals",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "Hanno",
      "family": "Biber",
      "affiliation": [
        "Austrian Academy of Sciences, Austria"
      ]
    },
    {
      "given": "Evelyn",
      "family": "Breiteneder",
      "affiliation": [
        "Austrian Academy of Sciences, Austria"
      ]
    },
    {
      "given": "Karlheinz",
      "family": "Mörth",
      "affiliation": [
        "Austrian Academy of Sciences, Austria"
      ]
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": "The aim of this paper is to provide an overview of e-Science and e-Research activities for the arts and humanities in the UK. It will focus on research projects and trends and will not cover the institutional infrastructure to support them. In particular, we shall explore the methodological discussions laid out in the Arts and Humanities e-Science Theme, jointly organised by the Arts and Humanities e-Science Support Centre and the e-Science Institute in Edinburgh (http://www.nesc.ac.uk/esi/ themes/theme_06/). The second focus of the paper will be the current and future activities within the Arts and Humanities e-Science Initiative in the UK and their methodological consequences (http://www.ahessc.ac.uk). The projects presented so far are all good indicators of what the future might deliver, as ‘grand challenges’ for the arts and humanities e-Science programme such as the emerging data deluge (Hey and Trefethen 2003). The Bush administration will have produced over 100 million emails by the end of its term (Unsworth 2006). These can provide the basis for new types of historical and socio-political research that will take advantage of computational methods to deal with digital data. However, for arts and humanities research an information is not just an information. Complicated semantics underlie the archives of human reports. As a simple example, it cannot be clear from the email alone which Bush administration or even which Iraq war are under consideration. Moreover, new retrieval methods for such data must be intuitive for the user and not based on complicated metadata schemes. They have to be specifi c in their return and deliver exactly that piece of information the researcher is interested in. This is fairly straightforward for structured information if it is correctly described, but highly complex for unstructured information. Arts and humanities additionally need the means to on-demand reconfi gure the retrieval process by using computational power that changes the set of information items available from texts, images, movies, etc. This paper argues that a specifi c methodological agenda in arts and humanities e-Science has been developing over the past two years and explores some of its main tenets. We offer a chronological discussion of two phases in the methodological debates about the applicability of e-science and e-research to arts and humanities. The fi rst phase concerns the methodological discussions that took place during the early activities of the Theme. A series of workshops and presentations about the role of e- Science for arts and humanities purported to make existing e-science methodologies applicable to this new fi eld and consider the challenges that might ensue (http://www.nesc. ac.uk/esi/themes/theme_06/community.htm). Several events brought together computer scientists and arts and humanities researchers. Further events (fi nished by the time of Digital Humanities 2008) will include training events for postgraduate students and architectural studies on building a national einfrastructure for the arts and humanities. Due to space limitations, we cannot cover all the early methodological discussions during the Theme here, but focus on two, which have been fruitful in uptake in arts and humanities: Access Grid and Ontological Engineering. A workshop discussed alternatives for video-conferencing in the arts and humanities in order to establish virtual research communities. The Access Grid has already proven to be of interest in arts and humanities research. This is not surprising, as researchers in these domains often need to collaborate with other researchers around the globe. Arts and humanities research often takes place in highly specialised domains and subdisciplines, niche subjects with expertise spread across universities. The Access Grid can provide a cheaper alternative to face-to-face meetings. However, online collaboration technologies like the Access Grid need to be better adapted to the specifi c needs of humanities researchers by e.g. including tools to collaboratively edit and annotate documents. The Access Grid might be a good substitute to some face-to-face meetings, but lacks innovative means of collaboration, which can be especially important in arts and humanities research. We should aim to realise real multicast interaction, as it has been done in VNC technology or basic wiki technology. These could support new models of collaboration in which the physical organisation of the Access Grid suite can be accommodated to specifi c needs that would e.g. allow participants to walk around. The procedure of Access Grid sessions could also be changed, away from static meetings towards more dynamic collaborations. Humanities scholars and performers have priorities and concerns that are often different from those of scientists and engineers (Nentwich 2003). With growing size of data resources the need arises to use recent methodological frameworks such as ontologies to increase the semantic interoperability of data. Building ontologies in the humanities is a challenge, which was the topic of the Theme workshop on ‘Ontologies and Semantic Interoperability for Humanities Data’. While semantic integration has been a hot topic in business and computing research, there are few existing examples for ontologies in the Humanities, and they are generally quite limited, lacking the richness that full-blown ontologies promise. The workshop clearly pointed at problems mapping highly contextual data as in the humanities to highly formalized conceptualization and specifi cations of domains. The activities within the UK’s arts and humanities e-Science community demonstrate the specifi c needs that have to be addressed to make e-Science work within these disciplines (Blanke and Dunn 2006). The early experimentation phase, which included the Theme events presented supra, delivered projects that were mostly trying out existing approaches in e- Science. They demonstrated the need for a new methodology to meet the requirements of humanities data that is particularly fuzzy and inconsistent, as it is not automatically produced, but is the result of human effort. It is fragile and its presentation often diffi cult, as e.g. data in performing arts that only exists as an event. The second phase of arts and humanities e-Science began in September 2007 with seven 3-4 years projects that are moving away from ad hoc experimentation towards a more systematic investigation of methodologies and technologies that could provide answers to grand challenges in arts and humanities research. This second phase could be put in a nutshell as e-science methodology-led innovative research in arts and humanity. Next to performance, music research e.g. plays an important vanguard function at adopting e-Science methodologies, mostly because many music resources are already available in digital formats. At Goldsmiths, University of London, the project ‘Purcell Plus’ e.g. will build upon the successful collaboration ‘Online Musical Recognition and Searching (OMRAS)’ (http://www.omras.org/), which has just achieved a second phase of funding by the EPSRC. With OMRAS, it will be possible to effi ciently search large-scale distributed digital music collections for related passages, etc. The project uses grid technologies to index the very large distributed music resources. ‘Purcell Plus’ will make use of the latest explosion in digital data for music research. It uses Purcell’s autograph MS of ‘Fantazies and In Nomines for instrumental ensemble’ and will investigate the methodology problems for using toolkits like OMRAS for musicology research. ‘Purcell Plus’ will adopt the new technologies emerging from music information retrieval, without the demand to change completely proven to be good methodologies in musicology. The aim is to suggest that new technologies can help existing research and open new research domains in terms of the quantity of music and new quantitative methods of evaluation. Building on the earlier investigations into the data deluge and how to deal with it, many of the second-phase projects look into the so-called ‘knowledge technologies’ that help with data and text mining as well as simulations in decision support for arts and humanities research. One example is the ‘Medieval Warfare on the Grid: The Case of Manzikert’ project in Birmingham, which will investigate the need for medieval states to sustain armies by organising and distributing resources. A grid-based framework shall virtually reenact the Battle of Manzikert in 1071, a key historic event in Byzantine history. Agent-based modelling technologies will attempt to fi nd out more about the reasons why the Byzantine army was so heavily defeated by the Seljurk Turks. Grid environments offer the chance to solve such complex human problems through distributed simultaneous computing. In all the new projects, we can identify a clear trend towards investigating new methodologies for arts and humanities research, possible only because grid technologies offer unknown data and computational resources. We could see how e-Science in the arts and humanities has matured towards the development of concrete tools that systematically investigate the use of e-Science for research. Whether it is simulation of past battles or musicology using state-of-the-art information retrieval techniques, this research would have not been possible before the shift in methodology towards e-Science and e-Research.",
    "article_title": "e-Science in the Arts and Humanities – A methodological perspective",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "Tobias",
      "family": "Blanke",
      "affiliation": [
        "King’s College London, UK"
      ]
    },
    {
      "given": "Stuart",
      "family": "Dunn",
      "affiliation": [
        "King’s College London, UK"
      ]
    },
    {
      "given": "Lorna",
      "family": "Hughes",
      "affiliation": [
        "King’s College London, UK"
      ]
    },
    {
      "given": "Mark",
      "family": "Hedges",
      "affiliation": [
        "King’s College London, UK"
      ]
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": "<Intro> This paper describes an index on metaphor in Otto Vaenius’ emblem book Amoris divini emblemata (Antwerp, 1615). The index should be interesting both for its contents (that is, for the information about the use of metaphor in the book) and as an example of modelling a complex literary phenomenon. Modelling a complex phenomenon creates the possibility to formulate complex queries on the descriptions that are based on the model. The article describes an application that uses this possibility. The application user can interrogate the metaphor data in multiple ways, ranging from canned queries to complex selections built in the application’s guided query interface. Unlike other emblem indices, the metaphor index is not meant to be a tool for resource discovery, a tool that helps emblem scholars fi nd emblems relevant to their own research. It presents research output rather than input. The modelling techniques that it exemplifi es should help a researcher formulate detailed observations or fi ndings about his research subject – in this case, metaphor – and make these fi ndings amenable to further processing. The result is an index, embedded in an overview or explanation of the data for the reader. I will argue that for research output data it is up to the researcher who uses these modelling techniques to integrate the presentation of data in a narrative or argument, and I describe one possible way of effecting this integration. The paper builds on the techniques developed in (Boot 2006). The emblem book is encoded using TEI; a model of metaphor (an ontology) is formulated in OWL; the observations about the occurrence of metaphors are stored as RDF statements. An essay about the more important metaphors in this book is encoded in TEI. This creates a complex and interlinked structure that may be explored in a number of ways. The essay is hyperlinked to (1) individual emblems, (2) the presentation of individual metaphors in emblems, (3) searches in the metaphor data, and (4) concepts in the metaphor ontology. From each of these locations, further exploration is possible. Besides these ready-made queries, the application also facilitates user-defi ned queries on the metaphor data. The queries are formulated using the SPARQL RDF query language, but the application’s guided query interface hides the actual syntax from the user. <Metaphor model> There is a number of aspects of metaphor and the texts where metaphors occur that are modelled in the metaphor index. A metaphor has a vehicle and a tenor, in the terminology of Richards (1936). When love, for its strength en endurance in adversity, is compared to a tree, the tree is the vehicle, love is the tenor. It is possible to defi ne hierarchies, both for the comparands (that is, vehicles and tenors) and for the metaphors: we can state that ‘love as a tree’ (love being fi rmly rooted) belongs to a wider class of ‘love as a plant’ (love bearing fruit) metaphors. We can also state that a tree is a plant, and that it (with roots, fruit, leaves and seeds) belongs to the vegetal kingdom (Lakoff and Johnson 1980). It often happens that an emblem contains references to an object invested with metaphorical meaning elsewhere in the book. The index can record these references without necessarily indicating something they are supposed to stand for. The index can also represent the locations in the emblem (the text and image fragments) that refer to the vehicles and tenors. The text fragments are stretches of emblem text, the image fragments are rectangular regions in the emblem pictures. The index uses the TEI-encoded text structure in order to relate occurrences of the comparands to locations in the text. The metaphor model is formulated using the Web Ontology Language OWL (McGuinness and Van Harmelen 2004). An ontology models the kind of objects that exist in a domain, their relationships and their properties; it provides a shared understanding of a domain. On a technical level, the ontology defi nes the vocabulary to be used in the RDF statements in our model. The ontology thus limits the things one can say; it provides, in McCarty’s words (McCarty 2005), the ‘explicit, delimited conception of the world’ that makes meaningful manipulation possible. The ontology is also what ‘drives’ the application built for consultation of the metaphor index. See for similar uses of OWL: (Ciula and Vieira 2007), (Zöllner- Weber 2005). The paper describes the classes and the relationships between them that the OWL model contains. Some of these relationships are hierarchical (‘trees belong to the vegetal kingdom’), others represent relations between objects (‘emblem 6 uses the metaphor of life as a journey’ or ‘metaphor 123 is a metaphor for justice’). The relationships are what makes it possible to query objects by their relations to other objects: to ask for all the metaphors based in an emblem picture, to ask for all of the metaphors for love, or to combine these criteria. <Application> In order to present the metaphor index to a reader, a web application has been developed that allows readers to consult and explore the index. The application is an example of an ontology-driven application as discussed in (Guarino 1998): the data model, the application logic and the user interface are all based on the metaphor ontology. The application was created using PHP and a MySQL database backend. RAP, the RDF API for PHP, is used for handling RDF. RDF and OWL fi les that contain the ontology and occurrences are stored in an RDF model in the database. RDF triples that represent the structure of the emblem book are created from the TEI XML fi le that contains the digital text of the emblem book. The application has to provide insight into three basic layers of information: our primary text (the emblems), the database-like collection of metaphor data, and a secondary text that should make these three layers into a coherent whole. The application organizes this in three perspectives: an overview perspective, an emblem perspective and an ontology perspective. Each of these perspectives offers one or more views on the data. These views are (1) a basic selection interface into the metaphor index; (2) an essay about the use and meaning of metaphor in this book; (3) a single emblem display; (4) information about metaphor use in the emblem; and (5) a display of the ontology defi ned for the metaphor index (built using the OWLDoc). The paper will discuss the ways in which the user can explore the metaphor data. <Discussion> The metaphor index is experimental, among other things in its modelling of metaphor and in its use of OWL and RDF in a humanities context. If Willard McCarty is right in some respects all humanities computing is experimental. There is, however, a certain tension between the experimental nature of this index and the need to collect a body of material and create a display application. If the aim is not to support resource discovery, but solely to provide insight, do we then need this large amount of data? Is all software meant to be discarded, as McCarty quotes Perlis? The need to introduce another aspect of metaphor into the model may confl ict with the need to create a body of material that it is worthwhile to explore. It is also true, however, that insight doesn’t come from subtlety alone. There is no insight without numbers. McCarty writes about the computer as ‘a rigorously disciplined means of implementing trial-and-error (…) to help the scholar refi ne an inevitable mismatch between a representation and reality (as he or she conceives it) to the point at which the epistemological yield of the representation has been realized’. It is true that the computer helps us be rigorous and disciplined, but perhaps for that very reason the representations that the computer helps us build may become a burden. Computing can slow us down. To clarify the conceptual structure of metaphor as it is used in the book we do not necessarily need a work of reference. The paper’s concluding paragraphs will address this tension. ",
    "article_title": "An OWL-based index of emblem metaphors",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "Peter",
      "family": "Boot",
      "affiliation": [
        "Huygens Institute"
      ]
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": "In the early days the Digital Humanities (DH) focused on the development of tools to support the individual scholar to perform original scholarship, and tools such as OCP and TACT emerged that were aimed at the individual scholar. Very little tool-building within the DH community is now aimed generally at individual scholarship. There are, I think, two reasons for this: • First, the advent of the Graphical User Interface (GUI) made tool building (in terms of software applications that ran on the scholar’s own machine) very expensive. Indeed, until recently, the technical demands it put upon developers have been beyond the resources of most tool developers within the Humanities. • Second, the advent of the WWW has shifted the focus of much of the DH community to the web. However, as a result, tool building has mostly focused on not the doing of scholarly research but on the publishing of resources that represent the result of this. DH’s tools to support the publishing of, say, primary sources, are of course highly useful to the researcher when his/her primary research interest is the preparation of a digital edition. They are not directly useful to the researcher using digital resources. The problem (discussed in detail in Bradley 2005) is that a signifi cant amount of the potential of digital materials to support individual research is lost in the representation in the browser, even when based on AJAX or Web 2.0 practices. The Pliny project (Pliny 2006-7) attempts to draw our attention as tool builders back to the user of digital resources rather than their creator, and is built on the assumption that the software application, and not the browser, is perhaps the best platform to give the user full benefi t of a digital resource. Pliny is not the only project that recognises this. The remarkable project Zotero (Zotero 2006-7) has developed an entire plugin to provide a substantial new set of functions that the user can do within their browser. Other tool builders have also recognised that the browser restricts the kind of interaction with their data too severely and have developed software applications that are not based on the web browser (e.g. Xaira (2005-7), WordHoard (2006-7), Juxta (2007), VLMA (2005-7)). Some of these also interact with the Internet, but they do it in ways outside of conventional browser capabilities. Further to the issue of tool building is the wish within the DH community to create tools that work well together. This problem has often been described as one of modularity – building separate components that, when put together, allow the user to combine them to accomplish a range of things perhaps beyond the capability of each tool separately. Furthermore, our community has a particularly powerful example of modularity in Wilhelm Ott’s splendid TuStep software (Ott 2000). TuStep is a toolkit containing a number of separate small programs that each perform a rather abstract function, but can be assembled in many ways to perform a very large number of very different text processing tasks. However, although TuStep is a substantial example of software designed as a toolkit, the main discussion of modularity in the DH (going back as far as the CETH meetings in the 1990s) has been in terms of collaboration – fi nding ways to support the development of tools by different developers that, in fact, can co-operate. This is a very different issue from the one TuStep models for us. There is as much or more design work employed to create TuStep’s framework in which the separate abstract components operate (the overall system) as there is in the design of each component itself. This approach simply does not apply when different groups are designing tools semi-independently. What is really wanted is a world where software tools such as WordHoard can be designed in ways that allow other tools (such as Juxta) to interact in a GUI, on the screen. Why is this so diffi cult? Part of the problem is that traditional software development focuses on a “stack” approach. Layers of ever-more specifi c software are built on top of moregeneral layers to create a specifi c application, and each layer in the stack is aimed more precisely at the ultimate functions the application was meant to provide. In the end each application runs in a separate window on the user’s screen and is focused specifi cally and exclusively on the functions the software was meant to do. Although software could be written to support interaction between different applications, it is in practice still rarely considered, and is diffi cult to achieve. Pliny, then, is about two issues: • First, Pliny focuses on digital annotation and note-taking in humanities scholarship, and shows how they can be used facilitate the development of an interpretation. This has been described in previous papers and is not presented here. • Second, Pliny models how one could be building GUIoriented software applications that, although developed separately, support a richer set of interactions and integration on the screen. This presentation focuses primarily on this second theme, and is a continuation of the issues raised at last year’s poster session on this subject for the DH2007 conference (Bradley 2007). It arises from a consideration of Pliny’s fi rst issue since note-taking is by its very nature an integrative activity – bringing together materials created in the context of a large range of resources and kinds of resources. Instead of the “stack” model of software design, Pliny is constructed on top of the Eclipse framework (Eclipse 2005- 7), and uses its contribution model based on Eclipse’s plugin approach (see a description of it in Birsan 2005). This approach promotes effective collaborative, yet independent, tool building and makes possible many different kinds of interaction between separately written applications. Annotation provides an excellent motivation for this. A user may wish to annotate something in, say, WordHoard. Later, this annotation will need to be shown with annotations attached to other objects from other pieces of software. If the traditional “stack” approach to software is applied, each application would build their own annotation component inside their software, and the user would not be able to bring notes from different tools together. Instead of writing separate little annotation components inside each application, Eclipse allows objects from one application to participate as “fi rst-class” objects in the operation of another. Annotations belong simultaneously to the application in which they were created, and to Pliny’s annotation-note-taking management system. Pliny’s plugins both support the manipulation of annotations while simultaneously allowing other (properly constructed) applications to create and display annotations that Pliny manages for them. Furthermore, Pliny is able to recognise and represent references to materials in other applications within its own displays. See Figures I and II for examples of this, in conjunction with the prototype VLMA (2005-7) plugin I created from the standalone application produced by the VLMA development team. In Figure I most of the screen is managed by the VLMA application, but Pliny annotations have been introduced and combined with VLMA materials. Similarly, in fi gure II, most of the screen is managed by Pliny and its various annotation tools, but I have labelled areas on the screen where aspects of the VLMA application still show through. This connecting of annotation to a digital object rather than merely to its display presents some new issues. What, for example, does it mean to link an annotation to a line of a KWIC display – should that annotation appear when the same KWIC display line appears in a different context generated as the result of a different query? Should it appear attached to the particular word token when the document it contains is displayed? If an annotation is attached to a headword, should it be displayed automatically in a different context when its word occurrences are displayed, or only in the context in which the headword itself is displayed? These are the kind of questions of annotation and context that can only really be explored in an integrated environment such as the one described here, and some of the discussion in this presentation will come from prototypes built to work with the RDF data application VLMA, with the beginnings of a TACT-like text analysis tool, and on a tool based on Google maps that allows one to annotate a map. Building our tools in contexts such as Pliny’s that allow for a complex interaction between components results in a much richer, and more appropriate, experience for our digital user. For the fi rst time, perhaps, s/he will be able to experience the kind of interaction between the materials that are made available through applications that expose, rather than hide, the true potential of digital objects. Pliny provides a framework in which objects from different tools are brought into close proximity and connected by the paradigm of annotation. Perhaps there are also paradigms other than annotation that are equally interesting for object linking?",
    "article_title": "Collaborative tool-building with Pliny: a progress report",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "John",
      "family": "Bradley",
      "affiliation": [
        "King's College London, UK"
      ]
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": "In this paper we discuss some of the problems that arise when searching for misspelled names. We suggest a solution to overcome these and to disambiguate the names found. <Introduction> The Nineteenth-Century Serials Edition (NCSE) is a digital edition of six nineteenth-century newspaper and periodical titles. It is a collaborative project between Birkbeck College, King’s College London, the British Library and Olive Software Inc. funded by the UK’s Arts and Humanities Research Council. The corpus consists of about 100,000 pages that were microfi lmed, scanned in and processed using optical character recognition software (OCR) to obtain images for display in the web application as well as the full text of the publications. In the course of this processing (by Olive Software) the text of each individual issue was also automatically segmented into its constituent parts (newspaper departments, articles, advertisements, etc.). The application of text mining techniques (named entity extraction, text categorisation, etc.) allowed names, institutions and places etc. to be extracted as well as individual articles to be classifi ed according to events, subjects and genres. Users of the digital edition can search for very specifi c information. The extent to which these searches are successful depends, of course, on the quality of the available data. The problem The quality of some of the text resulting from the OCR process varies from barely readable to illegible. This refl ects the poor print quality of the original paper copies of the publications. A simple search of the scanned and processed text for a person’s name, for example, would retrieve exact matches, but ignore incorrectly spelled or distorted variations. Misspellings of ordinary text could be checked against an authoritative electronic dictionary. An equivalent reference work for names does not exist. This paper describes the solutions that are being investigated to overcome these diffi culties. This theatrical notice on page 938 of the Leader from 17.11.1860 highlights the limitations of OCR alone. The actress Mrs. Billington is mentioned twice. OCR recognised the name once as Mrs. lUllinijton and then as Mrs. BIIMngton. A simple search for the name Billington would therefore be unsuccessful. By applying a combination of approximate string matching techniques and allowing for a certain amount of spelling errors (see below) our more refi ned approach successfully fi nds the two distorted spellings as Mrs. Billington. However, it also fi nds a number of other unrelated names (Wellington and Rivington among others). This additional problem is redressed by mapping misspelled names to correctly spelled names. Using a combination of string similarity and string distance algorithms we have developed an application to rank misspelled names according to their likelihood of representing a correctly spelled name. <The algorithm> As already briefl y mentioned above we are applying several well known pattern matching algorithms to perform approximate pattern matching, where the pattern is a given name (a surname normally), and the “text” is a (huge) list of names, obtained from the OCR of scanned documents. The novelty of this work comes from the fact that we are utilizing application-specifi c characteristics to achieve better results than are possible through general-purpose pattern matching techniques. Currently we are considering the pattern to be error-free (although our algorithms can easily be extended to deal with errors in the pattern too). Moreover, all the algorithms take as input the maximum “distance” that a name in the list may have from the pattern to be considered a match; this distance is given as a percentage. As one would expect, there is a tradeoff in distance - quality of matches: low distance threshold yields less false positives, but may miss some true matches; on the other hand, a high distance threshold has less chances of missing true matches, but will return many fake ones. At the heart of the algorithms lies a ranking system, the purpose of which is to sort the matching names according to how well they match. (Recall that the list of matching names can be huge, and thus what is more important is really the ranking of those matches, to distinguish the good ones from random matches.) Obviously, the ranking system depends on the distance-metric in use, but there is more to be taken into account. Notice that, if a name matches our pattern with some error, e, there are two cases: - either the name is a true match, and the error e is due to bad OCR, or - the name (misspelled or not, by OCR) does not correspond to our pattern, in which case it is a bad match and should receive a lower rank. It is therefore essential, given a possibly misspelled (due to OCR) name, s’, to identify the true name, s, that it corresponds to. Then, it is obvious that s’ is a good match if p = s, and a bad match otherwise, where p is the pattern. To identify whether a name s’ in our list is itself a true name, or has been mispelled we use two types of evaluation: - We count the occurrences of s’ in the list. A name that occurs many times, is likely to be a true name; if it had been misspelled, it is very unlikely that all its occurrences would have been misspelled in exactly the same manner, by the OCR software. - We have compiled a list L of valid names; these names are then used to decide whether s’ is a valid name (s’ ∈ L) or not (s’ ∉ L). In the case where s’ is indeed mispelled by OCR, and is thus not a true name, one must use distance metrics to identify how closely it matches the pattern p. Given that the pattern is considered to be error-free, if the distance of the name from the pattern is large then it is very unlikely (but not impossible) that too many of the symbols of the name have been mispelled by OCR; instead, most probably the name does not really match the pattern. Taking into account the nature of the errors that occur in our application, when computing the distance of a name in the list from our pattern, we consider optical similarities. That is, we drop the common tactic where one symbol is compared against another symbol, and either they match - so the distance is 0, or they don’t - so the distance is 1; instead, we consider a symbol (or a group of symbols) to have a low distance from another symbol (or group of symbols) if their shapes look similar. As an example, check that “m” is optically very similar to “rn”, and thus should be assigned a small distance, say 1, while “m” and “b” do not look similar to each other and therefore should have a big distance, say 10. The results of our efforts to date have been very promising. We look forward to investigating opportunities to improve the effectiveness of the algorithm in the future.",
    "article_title": "How to fi nd Mrs. Billington? Approximate string matching applied to misspelled names",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "Gerhard",
      "family": "Brey",
      "affiliation": [
        "King’s College London, UK"
      ]
    },
    {
      "given": "Manolis",
      "family": "Christodoulakis",
      "affiliation": [
        "University of East London, UK"
      ]
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": "Orlando: Women’s Writing in the British Isles from the Beginnings to the Present is a literary-historical textbase comprising more than 1,200 core entries on the lives and writing careers of British women writers, male writers, and international women writers; 13,000+ free-standing chronology entries providing context; 12,000+ bibliographical listings; and more than 2 million tags embedded in 6.5 million words of born-digital text. The XML tagset allows users to interrogate everything from writers’ relationships with publishers to involvement in political activities or their narrative techniques. The current interface allows users to access entries by name or via various criteria associated with authors; to create chronologies by searching on tags and/or contents of dated materials; and to search the textbase for tags, attributes, and text, or a combination. The XML serves primarily to structure the materials; to allow users to draw on particular tags to bring together results sets (of one or more paragraphs incorporating the actual ‘hit’) according to particular interests; and to provide automatic hyperlinking of names, places, organizations, and titles. Recognizing both that many in our target user community of literary students and scholars dislike tag searches, and that our current interface has not fully plumbed the potential of Orlando’s experimentation in structured text, we are exploring what other kinds of enquiry and interfaces the textbase can support. We report here on some investigations into new ways of probing and representing the links created by the markup. The current interface leverages the markup to provide contexts for hyperlinks. Each author entry includes a “Links” screen that provides hyperlinks to mentions of that author elsewhere in the textbase. These links are sorted into groups based on the semantic tags of which they are children, so that users can choose, for instance, from the more than 300 links on the George Eliot Links screen, between a link to Eliot in the Elizabeth Stuart Phelps entry that occurs in the context of Family or Intimate Relationships, and a link to Eliot in Simone de Beauvoir’s entry that occurs in the discussion of features of de Beauvoir’s writing. Contextualizing Links screens are provided not only for writers who have entries, but also for any person who is mentioned more than once in the textbase, and also for titles of texts, organizations, and places. It thus provides a unique means for users to pursue links in a less directed and more informed way than that provided by many interfaces. Building on this work, we have been investigating how Orlando might support queries into relationships and networking, and present not just a single relationship but the results of investigating an entire fi eld of interwoven relationships of the kind that strongly interest literary historians. Rather than beginning from a known set of networks or interconnections, how might we exploit our markup to analyze interconnections, reveal new links, or determine the points of densest interrelationship? Interface design in particular, if we start to think about visualizing relationships rather than delivering them entirely in textual form, poses considerable challenges. We started with the question of the degrees of separation between different people mentioned in disparate contexts within the textbase. Our hyperlinking tags allow us to conceptualize links between people not only in terms of direct contact, that is person-to-person linkages, but also in terms of linkages through other people, places, organizations, or texts that they have in common. Drawing on graph theory, we use the hyperlinking tags as key indices of linkages. Two hyperlinks coinciding within a single document—an entry or a chronology event--were treated as vertices that form an edge, and an algorithm was used to fi nd the shortest path between source and destination. So, for instance, you can get from Chaucer to Virginia Woolf in a single step in twelve different ways: eleven other writer entries (including Woolf’s own) bring their names together, as does the following event: 1 November 1907: The British Museum’s reading room reopened after being cleaned and redecorated; the dome was embellished with the names of canonical male writers, beginning with Chaucer and ending with Browning. Virginia Woolf’s A Room of One’s Own describes the experience of standing under the dome “as if one were a thought in the huge bald forehead which is so splendidly encircled by a band of famous names.” Julia Hege in Jacob’s Room complains that they did not leave room for an Eliot or a Brontë. It takes more steps to get from some writers to others: fi ve, for instance, to get from Frances Harper to Ella Baker. But this is very much the exception rather than the rule. Calculated according to the method described here, we have a vast number of links: the textbase contains 74,204 vertices with an average of 102 edges each (some, such as London at 101,936, have considerably more than others), meaning that there are 7.5 million links in a corpus of 6.5 million words. Working just with authors who have entries, we calculated the number of steps between them all, excluding some of the commonest links: the Bible, Shakespeare, England, and London. Nevertheless, the vast majority of authors (on average 88%) were linked by a single step (such as the example of Chaucer and Woolf, in which the link occurs within the same source document) or two steps (in which there is one intermediate document between the source and destination names). Moreover, there is a striking similarity in the distribution of the number of steps required to get from one person to another, regardless of whether one moves via personal names, places, organizations, or titles. 10.6% of entries are directly linked, that is the two authors are mentioned in the same source entry or event. Depending on the hyperlinking tag used, one can get to the destination author with just one intermediate step, or two degrees of separation, in 72.2% to 79.6% of cases. Instances of greater numbers of steps decline sharply, so that there are 5 degrees of separation in only 0.6% of name linkage pages, and none at all for places. Six degrees of separation does not exist in Orlando between authors with entries, although there are a few “islands”, in the order of from 1.6% to 3.2%, depending on the link involved, of authors who do not link to others. These results raise a number of questions. As Albert-Lászlo Barabási reported of social networks generally, one isn’t dealing with degrees of separation so much as degrees of proximity. However, in this case, dealing not with actual social relations but the partial representation in Orlando of a network of social relations from the past, what do particular patterns such as these mean? What do the outliers—people such as Ella Baker or Frances Harper who are not densely interlinked with others—and islands signify? They are at least partly related to the brevity of some entries, which can result either from paucity of information, or decisions about depth of treatment, or both. But might they sometimes also represent distance from literary and social establishments? Furthermore, some linkages are more meaningful, in a literary historical sense, than others. For instance, the Oxford Dictionary of National Biography is a common link because it is frequently cited by title, not because it indicates a historical link between people. Such incidental links can’t be weeded out automatically. So we are investigating the possibility of using the relative number of single- or double-step links between two authors to determine how linked they ‘really’ are. For instance, Elizabeth Gaskell is connected to William Makepeace Thackeray, Charles Dickens, and George Eliot by 25, 35, and 53 single-step links, respectively, but to Margaret Atwood, Gertrude Stein, and Toni Morrison by 2, 1, and 1. Such contrasts suggest the likely utility of such an approach to distinguishing meaningful from incidental associations. The biggest question invited by these inquiries into linkages is: how might new modes of inquiry into, or representation of, literary history, emerge from such investigations? One way to address this question is through interfaces. We have developed a prototype web application for querying degrees of separation in Orlando, for which we are developing an interface. Relationships or associations are conventionally represented by a network diagram, where the entities are shown as nodes and the relationships as lines connecting the nodes. Depending on the content, these kinds of fi gures are also referred to as directed graphs, link-node diagrams, entity-relationship (ER) diagrams, and topic maps. Such diagrams scale poorly, since the proliferation of items results in a tangle of intersecting lines. Many layout algorithms position the nodes to reduce the number of crisscrossing lines, resulting in images misleading to people who assume that location is meaningful. In the case of Orlando, two additional complexities must be addressed. First, many inter-linkages are dense: there are often 50 distinct routes between two people. A conventional ER diagram of this data would be too complex to be useful as an interactive tool, unless we can allow the user to simplify the diagram. Second, the Orlando data differs from the kind of data that would support “distant reading” (Moretti 1), so our readers will need to access the text that the diagram references. How, then, connect the diagram to a reading view? We will present our preliminary responses to these challenges in an interface for degree of separation queries and results. We are also experimenting with the Mandala browser (Cheypesh et al. 2006) for XML structures as a means of exploring embedded relationships. The current Mandala prototype cannot accommodate the amount of data and number of tags in Orlando, so we will present the results of experimenting with a subset of the hyperlinking tags as another means of visualizing the dense network of associations in Orlando’s representations of literary history.",
    "article_title": "Degrees of Connection: the close interlinkages of Orlando",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "Susan",
      "family": "Brown",
      "affiliation": [
        "University of Guelph, Canada"
      ]
    },
    {
      "given": "Patricia",
      "family": "Clements",
      "affiliation": [
        "University of Alberta, Canada"
      ]
    },
    {
      "given": "Isobel",
      "family": "Grundy",
      "affiliation": [
        "University of Alberta, Canada"
      ]
    },
    {
      "given": "Stan",
      "family": "Ruecker",
      "affiliation": [
        "University of Alberta, Canada"
      ]
    },
    {
      "given": "Jeffery",
      "family": "Antoniuk",
      "affiliation": [
        "University of Alberta, Canada"
      ]
    },
    {
      "given": "Sharon",
      "family": "Balazs",
      "affiliation": [
        "University of Alberta, Canada"
      ]
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": " ",
    "article_title": "The impact of digital interfaces on virtual gender images",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "Sandra",
      "family": "Buchmüller",
      "affiliation": [
        "Deutsche Telekom Laboratories, Germany"
      ]
    },
    {
      "given": "",
      "family": "",
      "affiliation": null
    },
    {
      "given": "",
      "family": "",
      "affiliation": null
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": " ",
    "article_title": "",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "",
      "family": "",
      "affiliation": null
    },
    {
      "given": "",
      "family": "",
      "affiliation": null
    },
    {
      "given": "",
      "family": "",
      "affiliation": null
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": " ",
    "article_title": "",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "",
      "family": "",
      "affiliation": null
    },
    {
      "given": "",
      "family": "",
      "affiliation": null
    },
    {
      "given": "",
      "family": "",
      "affiliation": null
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": " ",
    "article_title": "",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "",
      "family": "",
      "affiliation": null
    },
    {
      "given": "",
      "family": "",
      "affiliation": null
    },
    {
      "given": "",
      "family": "",
      "affiliation": null
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": " ",
    "article_title": "",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "",
      "family": "",
      "affiliation": null
    },
    {
      "given": "",
      "family": "",
      "affiliation": null
    },
    {
      "given": "",
      "family": "",
      "affiliation": null
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": " ",
    "article_title": "",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "",
      "family": "",
      "affiliation": null
    },
    {
      "given": "",
      "family": "",
      "affiliation": null
    },
    {
      "given": "",
      "family": "",
      "affiliation": null
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": " ",
    "article_title": "",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "",
      "family": "",
      "affiliation": null
    },
    {
      "given": "",
      "family": "",
      "affiliation": null
    },
    {
      "given": "",
      "family": "",
      "affiliation": null
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": " ",
    "article_title": "",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "",
      "family": "",
      "affiliation": null
    },
    {
      "given": "",
      "family": "",
      "affiliation": null
    },
    {
      "given": "",
      "family": "",
      "affiliation": null
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": " ",
    "article_title": "",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "",
      "family": "",
      "affiliation": null
    },
    {
      "given": "",
      "family": "",
      "affiliation": null
    },
    {
      "given": "",
      "family": "",
      "affiliation": null
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  },
  {
    "url": null,
    "identifier": {
    "string_id": null,
    "id_scheme": null
    },
    "abstract": " ",
    "article_title": "",
    "date": null,
    "publisher": "",
    "authors": [
    {
      "given": "",
      "family": "",
      "affiliation": null
    },
    {
      "given": "",
      "family": "",
      "affiliation": null
    },
    {
      "given": "",
      "family": "",
      "affiliation": null
    }
    ],
    "keywords": null,
    "journal_title": "ADHO Conference",
    "volume": null,
    "issue": null,
    "ISSN": [
    {
      "value": null,
      "type": null
    }
    ]
  }
]