[{"url": "https://hal.archives-ouvertes.fr/hal-02280013v2", "identifier": {"string_id": "hal-02280013", "id_scheme": "hal"}, "abstract": "While online crowdsourced text transcription projects have proliferated in the last decade, there is a need within the broader field to understand differences in project outcomes as they relate to task design, as well as to experiment with different models of online crowdsourced transcription that have not yet been explored. The experiment discussed in this paper involves the evaluation of newly-built tools on the Zooniverse.org crowdsourcing platform, attempting to answer the research question: \"Does the current Zooniverse methodology of multiple independent transcribers and aggregation of results render higher-quality outcomes than allowing volunteers to see previous transcriptions and/or markings by other users? How does each methodology impact the quality and depth of analysis and participation?\" To answer these questions, the Zooniverse team ran an A/B experiment on the project Anti-Slavery Manuscripts at the Boston Public Library. This paper will share results of this study, and also describe the process of designing the experiment and the metrics used to evaluate each transcription method. These include the comparison of aggregate transcription results with ground truth data; evaluation of annotation methods; the time it took for volunteers to complete transcribing each dataset; and the level of engagement with other project elements such as posting on the message board or reading supporting documentation. Particular focus will be given to the (at times) competing goals of data quality, efficiency, volunteer engagement, and user retention, all of which are of high importance for projects that focus on data from galleries, libraries, archives and museums. Ultimately, this paper aims to provide a model for impactful, intentional design and study of online crowdsourcing transcription methods, as well as shed light on the associations between project design, methodology and outcomes.", "article_title": "Individual vs. Collaborative Methods of Crowdsourced Transcription", "authors": [{"given": "Samantha", "family": "Blickhan", "affiliation": ["Adler Planetarium [Chicago]"]}, {"given": "Coleman", "family": "Krawczyk", "affiliation": ["University of Portsmouth"]}, {"given": "Daniel", "family": "Hanson", "affiliation": ["University of Minnesota [Twin Cities]"]}, {"given": "Amy", "family": "Boyer", "affiliation": ["The Adler Planetarium, USA"]}, {"given": "Andrea", "family": "Simenstad", "affiliation": ["University of Minnesota [Twin Cities]"]}, {"given": "Victoria", "family": "van Hyning", "affiliation": ["Library of Congress"]}], "publisher": null, "date": "2019-12-03", "keywords": [], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Collecting, Preserving, and Disseminating Endangered Cultural Heritage for New Understandings through Multilingual Approaches", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01281266v2", "identifier": {"string_id": "hal-01281266", "id_scheme": "hal"}, "abstract": "The ancient commentaries provide a large sample of quotations from classical or biblical texts for which Latin gramamrians developed a complex system of insertion of quoted texts. The paper examines how to encode these places using XML Tei, and focuses on difficult cases, such as inaccurate quotations, or quotations of partly or wholly lost texts.", "article_title": "Encoding (inter)textual insertions in Latin \"grammatical commentary\"", "authors": [{"given": "Bruno", "family": "Bureau", "affiliation": ["HiSoMA - Histoire et Sources des Mondes antiques"]}, {"given": "Christian", "family": "Nicolas", "affiliation": ["HiSoMA - Histoire et Sources des Mondes antiques"]}, {"given": "Ariane", "family": "Pinche", "affiliation": ["CIHAM - Histoire, Archéologie et Littératures des mondes chrétiens et musulmans médiévaux"]}], "publisher": null, "date": "2017-11-30", "keywords": ["Ancient commentary", "allusion", "fragmentary texts", "mentioned words", "Latin grammar", "XML Tei", "quotation"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01283638v2", "identifier": {"string_id": "hal-01283638", "id_scheme": "hal"}, "abstract": "Colophons of Armenian manuscripts are replete with yet untapped riches. Formulae are not the least among them: these recurrent stereotypical patterns conceal many clues as to the schools and networks of production and diffusion of books in Armenian communities. This paper proposes a methodology for exploiting these sources, as elaborated in the framework of a PhD research project about Armenian colophon formulae. Firstly, the reader is briefly introduced to the corpus of Armenian colophons and then, to the purposes of our project. In the third place, we describe our methodology, relying on lemmatization and modelling of patterns into automata. Fourthly and finally, the whole process is illustrated by a basic case study, the occasion of which is taken to outline the kind of results that can be achieved by combining this methodology with a philologico-historical approach to colophons.", "article_title": "Recurrent Pattern Modelling in a Corpus of Armenian Manuscript Colophons", "authors": [{"given": "Emmanuel", "family": "van Elverdinghe", "affiliation": ["Fonds de la Recherche Scientifique [FNRS]", "INCAL - Institut des Civilisations Arts et Lettres (INCAL)"]}], "publisher": null, "date": "2017-12-22", "keywords": ["codicology", "Unitex", "finite state transducers", "Armenian colophons", "automata", "colophon formula", "formulaic patterns", "lemmatization", "manuscript studies"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-02520508v3", "identifier": {"string_id": "hal-02520508", "id_scheme": "hal"}, "abstract": "This article proposes use the Transkribus software to report on a \"user experiment\" in a French-speaking context. It is based on the semi-automated transcription project using the diary of the jurist Eugène Wilhelm (1866-1951). This diary presents two main challenges. The first is related to the time covered by the writing process-66 years. This leads to variations in the form of the writing, which becomes increasingly \"unreadable\" with time. The second challenge is related to the concomitant use of two alphabets: Roman for everyday text and Greek for private issues. After presenting the project and the specificities related to the use of the tool, the experiment presented in this contribution is structured around two aspects. Firstly, I will summarise the main obstacles encountered and the solutions provided to overcome them. Secondly, I will come back to the collaborative transcription experiment carried out with students in the classroom, presenting the difficulties observed and the solutions found to overcome them. In conclusion, I will propose an assessment of the use of this Human Text Recognition software in a French-speaking context and in a teaching situation.", "article_title": "Optical Recognition Assisted Transcription with Transkribus: The Experiment concerning Eugène Wilhelm's Personal Diary (1885-1951)", "authors": [{"given": "Régis", "family": "Schlagdenhauffen", "affiliation": ["IRIS - Institut de Recherche Interdisciplinaire sur les enjeux Sociaux - sciences sociales, politique, santé"]}], "publisher": null, "date": "2020-07-14", "keywords": ["TEI", "User Experience", "Human Text Recognition", "Learning process", "OCR"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Atelier Digit_Hum", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01671592v1", "identifier": {"string_id": "hal-01671592", "id_scheme": "hal"}, "abstract": "This paper presents some computer tools and linguistic resources of the GREgORI project. These developments allow automated processing of texts written in the main languages of the Christian Middel East, such as Greek, Arabic, Syriac, Armenian and Georgian. The main goal is to provide scholars with tools (lemmatized indexes and concordances) making corpus-based linguistic information available. It focuses on the questions of text processing, lemmatization, information retrieval, and bitext alignment.", "article_title": "Processing Tools for Greek and Other Languages of the Christian Middle East", "authors": [{"given": "Bastien", "family": "Kindt", "affiliation": ["INCAL - Institut des Civilisations Arts et Lettres (INCAL)"]}], "publisher": null, "date": "2017-12-22", "keywords": ["Lemmatization", "Greek", "Syriac", "Arabic", "Armenian", "Georgian", "lexical tagging", "POS tagging", "concordances", "indexes", "bitext", "bilingual alignment", "translation memories", "mkAlign"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01913435v3", "identifier": {"string_id": "hal-01913435", "id_scheme": "hal"}, "abstract": "The Foucault Fiches de Lecture (FFL) project aims both to explore and to make available online a large set of Michel Foucault’s reading notes (organized citations, references and comments) held at the BnF since 2013. Therefore, the team is digitizing, describing and enriching the reading notes that the philosopher gathered while preparing his books and lectures, thus providing a new corpus that will allow a new approach to his work. In order to release the manuscripts online, and to collectively produce the data, the team is also developing a collaborative platform, based on RDF technologies, and designed to link together archival content and bibliographic data. This project is financed by the ANR (2017-2020) and coordinated by Michel Senellart, professor of philosophy at the ENS Lyon. It benefits from the partnerships of the ENS/PSL and the BnF. In addition, a collaboration with the European READ/Transkribus project has been started so as to produce automatic transcription of the reading notes.", "article_title": "Transcribing Foucault’s handwriting with Transkribus", "authors": [{"given": "Marie-Laure", "family": "Massot", "affiliation": ["CAPHÉS - Centre d’Archives en Philosophie, Histoire et Édition des Sciences"]}, {"given": "Arianna", "family": "Sforzini", "affiliation": ["TRIANGLE - Triangle : action, discours, pensée politique et économique"]}, {"given": "Vincent", "family": "Ventresque", "affiliation": ["TRIANGLE - Triangle : action, discours, pensée politique et économique"]}], "publisher": null, "date": "2019-02-18", "keywords": ["transkribus", "reading notes", "automatic transcription of manuscripts", "artificial intelligence"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Atelier Digit_Hum", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-02109972v2", "identifier": {"string_id": "hal-02109972", "id_scheme": "hal"}, "abstract": "The creation of the Artist Libraries Project was sparked by the observation that artist libraries are still not well known, yet many art historians are interested in this archive for the value it adds to understanding the person behind the artist and his or her creative process. The problem is that these libraries are rarely physically preserved. To remedy this dispersion, we built an online database and a website www.lesbibliothequesdartistes.org that house this valuable source in the form of lists of books and their electronic versions. First data on Monet's library have been made available, and several additional artist libraries from the 19 th and 20 th centuries are on the way for 2019. By gathering all these bibliographical data in a central database, it's possible to explore one library and to compare several. This article explains how we built the database and the website and how the implementation of those IT tools has raised questions about the use of this resource as an archive on the one hand, as well as its value for art history on the other.", "article_title": "The Artist Libraries Project in the Labex Les passés dans le présent", "authors": [{"given": "Félicie Faizand", "family": "de Maupeou", "affiliation": ["HAR - Histoire des Arts et des Représentations"]}, {"given": "Ségolène", "family": "Le Men", "affiliation": ["HAR - Histoire des Arts et des Représentations"]}], "publisher": null, "date": "2019-04-25", "keywords": [], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Atelier Digit_Hum", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01287195v4", "identifier": {"string_id": "hal-01287195", "id_scheme": "hal"}, "abstract": "This contribution to a special issue on “Computer-aided processing of intertextuality” in ancient texts will illustrate how using digital tools to interact with the Hebrew Bible offers new promising perspectives for visualizing the texts and for performing tasks in education and research. This contribution explores how the corpus of the Hebrew Bible created and maintained by the Eep Talstra Centre for Bible and Computer can support new methods for modern knowledge workers within the field of digital humanities and theology be applied to ancient texts, and how this can be envisioned as a new field of digital intertextuality. The article first describes how the corpus was used to develop the Bible Online Learner as a persuasive technology to enhance language learning with, in, and around a database that acts as the engine driving interactive tasks for learners. Intertextuality in this case is a matter of active exploration and ongoing practice. Furthermore, interactive corpus-technology has an important bearing on the task of textual criticism as a specialized area of research that depends increasingly on the availability of digital resources. Commercial solutions developed by software companies like Logos and Accordance offer a market-based intertextuality defined by the production of advanced digital resources for scholars and students as useful alternatives to often inaccessible and expensive printed versions. It is reasonable to expect that in the future interactive corpus technology will allow scholars to do innovative academic tasks in textual criticism and interpretation. We have already seen the emergence of promising tools for text categorization, analysis of translation shifts, and interpretation. Broadly speaking, interactive tools and tasks within the three areas of language learning, textual criticism, and Biblical studies illustrate a new kind of intertextuality emerging within digital humanities. ", "article_title": "Interactive Tools and Tasks for the Hebrew Bible: From Language Learning to Textual Criticism", "authors": [{"given": "Nicolai", "family": "Winther-Nielsen", "affiliation": ["FIUC-Dk - Fjellhaug International University College Denmark"]}], "publisher": null, "date": "2017-10-18", "keywords": ["Bible Online Learner", "Joshua 24", " Hebrew Bible", "Digital intertextuality", "Corpus", "Logos Bible software", "language learning", "Hebrew Bible", "ETCBC", "textual criticism", "Joshua"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01294158v2", "identifier": {"string_id": "hal-01294158", "id_scheme": "hal"}, "abstract": "The project is to develop a database, which is planned to include all available information on the use of the Bible in the patristic works of Migne's Patrologia Graeca. Utilization of the data will be available through a web page equipped with necessary tools for developing data mining techniques and other methods of analysis. The main aim of the project is to revive the catenae, the ancient exegetical tool for biblical interpretation.", "article_title": "Digital Greek Patristic Catena (DGPC). A brief presentation", "authors": [{"given": "Athanasios", "family": "Paparnakis", "affiliation": ["Department of Pastoral and Social Theology"]}, {"given": "Constantinos", "family": "Domouchtsis", "affiliation": ["Department of Pastoral and Social Theology"]}], "publisher": null, "date": "2017-07-24", "keywords": ["patristic authors", "Catena", "bible references", "biblical exegesis", "database", "Patrologia Graeca"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01915730v2", "identifier": {"string_id": "hal-01915730", "id_scheme": "hal"}, "abstract": "British philosopher and reformer Jeremy Bentham (1748-1832) left over 60,000 folios of unpublished manuscripts. The Bentham Project, at University College London, is creating a TEI version of the manuscripts, via crowdsourced transcription verified by experts. We present here an interface to navigate these largely unedited manuscripts, and the language technologies the corpus was enriched with to facilitate navigation, i.e Entity Linking against the DBpedia knowledge base and keyphrase extraction. The challenges of tagging a historical domain-specific corpus with a contemporary knowledge base are discussed. The concepts extracted were used to create interactive co-occurrence networks, that serve as a map for the corpus and help navigate it, along with a search index. These corpus representations were integrated in a user interface. The interface was evaluated by domain experts with satisfactory results , e.g. they found the distributional semantics methods exploited here applicable in order to assist in retrieving related passages for scholarly editing of the corpus.", "article_title": "Mapping the Bentham Corpus: Concept-based Navigation", "authors": [{"given": "Pablo", "family": "Ruiz", "affiliation": ["LILPA - Linguistique, Langues et Parole "]}, {"given": "Thierry", "family": "Poibeau", "affiliation": ["LattIce - Lattice - Langues, Textes, Traitements informatiques, Cognition - UMR 8094"]}], "publisher": null, "date": "2019-02-12", "keywords": ["Jeremy Bentham", "manuscripts", "corpus navigation", "entity linking", "keyphrase extraction"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue: Digital Humanities between knowledge and know-how (Atelier Digit_Hum)", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-02513038v2", "identifier": {"string_id": "hal-02513038", "id_scheme": "hal"}, "abstract": "The study of watermarks is a key step for archivists and historians as it enables them to reveal the origin of paper. Although highly practical, automatic watermark recognition comes with many difficulties and is still considered an unsolved challenge. Nonetheless, Shen et al. [2019] recently introduced a new approach for this specific task which showed promising results. Building upon this approach, this work proposes a new public web application dedicated to automatic watermark recognition entitled Filigranes pour tous. The application not only hosts a detailed catalog of more than 17k watermarks manually collected from the French National Archives (Minutier central) or extracted from existing online resources (Briquet database), but it also enables non-specialists to identify a watermark from a simple photograph in a few seconds. Moreover, additional watermarks can easily be added by the users making the enrichment of the existing catalog possible through crowdsourcing. Our Web application is available at http://filigranes.inria.fr/.", "article_title": "A Web Application for Watermark Recognition", "authors": [{"given": "Oumayma", "family": "Bounou", "affiliation": ["SED - Service Expérimentation et Développement  [Paris Rocquencourt]", "ENC - École nationale des chartes", "LIGM - Laboratoire d'Informatique Gaspard-Monge"]}, {"given": "Tom", "family": "Monnier", "affiliation": ["ENC - École nationale des chartes", "LIGM - Laboratoire d'Informatique Gaspard-Monge", "WILLOW - Models of visual object recognition and scene understanding"]}, {"given": "Ilaria", "family": "Pastrolin", "affiliation": ["ENC - École nationale des chartes"]}, {"given": "Xi", "family": "Shen", "affiliation": ["LIGM - Laboratoire d'Informatique Gaspard-Monge"]}, {"given": "Christine", "family": "Benevent", "affiliation": ["ENC - École nationale des chartes"]}, {"given": "Marie-Françoise", "family": "Limon-Bonnet", "affiliation": ["ENC - École nationale des chartes", "Archives nationales"]}, {"given": "François", "family": "Bougard", "affiliation": ["IRHT - Institut de recherche et d'histoire des textes"]}, {"given": "Mathieu", "family": "Aubry", "affiliation": ["LIGM - Laboratoire d'Informatique Gaspard-Monge"]}, {"given": "Marc H.", "family": "Smith", "affiliation": ["ENC - École nationale des chartes"]}, {"given": "Olivier", "family": "Poncet", "affiliation": ["ENC - École nationale des chartes"]}, {"given": "Pierre-Guillaume", "family": "Raverdy", "affiliation": ["SED - Service Expérimentation et Développement  [Paris Rocquencourt]"]}], "publisher": null, "date": "2020-06-16", "keywords": ["cross-domain recognition", "deep learning", "watermark recognition", "web application", "paper analysis"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "24", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/halshs-01543050v2", "identifier": {"string_id": "halshs-01543050", "id_scheme": "hal"}, "abstract": "This paper discusses markup strategies for the identification and description of text reuses in a corpus of patristic texts related to the BIBLINDEX Project, an online index of biblical references in Early Christian Literature. In addition to the development of a database that can be queried by canonical biblical or patristic references, a sample corpus of patristic texts has been encoded following the guidelines of the TEI (Text Encoding Initiative), in order to provide direct access to quoted and quoting text passages to the users of the https://www.biblindex.info platform.", "article_title": "TEI-encoding of text reuses in the BIBLINDEX Project", "authors": [{"given": "Elysabeth", "family": "Hue-Gay", "affiliation": ["HiSoMA - Histoire et Sources des Mondes antiques"]}, {"given": "Laurence", "family": "Mellerin", "affiliation": ["HiSoMA - Histoire et Sources des Mondes antiques"]}, {"given": "Emmanuelle", "family": "Morlock", "affiliation": ["HiSoMA - Histoire et Sources des Mondes antiques"]}], "publisher": null, "date": "2017-10-07", "keywords": ["patristics", " Bernard of Clairvaux", " Sources Chrétiennes", " text reuses", "BIBLINDEX", " quotations", " Biblia Patristica", "TEI", " Text Encoding Initiative", " text markup", " Bible", " Greek", " Latin", " Septuagint", " Vulgata"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01528092v2", "identifier": {"string_id": "hal-01528092", "id_scheme": "hal"}, "abstract": "The Text Alignment Network (TAN) is a suite of XML encoding formats intended to serve anyone who wishes to encode, exchange, and study multiple versions of texts (e.g., translations, paraphrases), and annotations on those texts (e.g., quotations, word-for-word correspondences). This article focuses on TAN’s innovative intertextual pointers, which, I argue, provide an unprecedented level of readability, interoperability, and semantic context. Because TAN is a new, experimental format, this article provides a brief introduction to the format and concludes with comments on progress and future prospects.", "article_title": "Intertextual Pointers in the Text Alignment Network", "authors": [{"given": "Joel", "family": "Kalvesmaki", "affiliation": ["DO - Dumbarton Oaks"]}], "publisher": null, "date": "2017-10-20", "keywords": ["alignment", " TEI", "XML", " TAN", " intertextuality", " canonical references"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01265297v2", "identifier": {"string_id": "hal-01265297", "id_scheme": "hal"}, "abstract": "The production of digital critical editions of texts using TEI is now a widely-adopted procedure within digital humanities. The work described in this paper extends this approach to the publication of gnomologia (anthologies of wise sayings) , which formed a widespread literary genre in many cultures of the medieval Mediterranean. These texts are challenging because they were rarely copied straightforwardly ; rather , sayings were selected , reorganised , modified or re-attributed between manuscripts , resulting in a highly interconnected corpus for which a standard approach to digital publication is insufficient. Focusing on Greek and Arabic collections , we address this challenge using semantic web techniques to create an ecosystem of texts , relationships and annotations , and consider a new model – organic , collaborative , interconnected , and open-ended – of what constitutes an edition. This semantic web-based approach allows scholars to add their own materials and annotations to the network of information and to explore the conceptual networks that arise from these interconnected sayings .", "article_title": "Computer - Assisted Processing of Intertextuality in Ancient Languages", "authors": [{"given": "Mark", "family": "Hedges", "affiliation": ["King‘s College London"]}, {"given": "Anna", "family": "Jordanous", "affiliation": ["University of Kent [Canterbury]"]}, {"given": "K. Faith", "family": "Lawrence", "affiliation": ["King‘s College London"]}, {"given": "Charlotte", "family": "Roueché", "affiliation": ["King‘s College London"]}, {"given": "Charlotte", "family": "Tupman", "affiliation": ["University of Exeter"]}], "publisher": null, "date": "2017-08-03", "keywords": ["TEI", "gnomologia", "RDF", "manuscripts", "ontology", "linked data", "semantic web", "digital edition", "anthologies"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://arxiv.org/abs/1912.05082v3", "identifier": {"string_id": "1912.05082", "id_scheme": "arXiv"}, "abstract": "Scholarship on underresourced languages bring with them a variety of challenges which make access to the full spectrum of source materials and their evaluation difficult. For Coptic in particular, large scale analyses and any kind of quantitative work become difficult due to the fragmentation of manuscripts, the highly fusional nature of an incorporational morphology, and the complications of dealing with influences from Hellenistic era Greek, among other concerns. Many of these challenges, however, can be addressed using Digital Humanities tools and standards. In this paper, we outline some of the latest developments in Coptic Scriptorium, a DH project dedicated to bringing Coptic resources online in uniform, machine readable, and openly available formats. Collaborative web-based tools create online 'virtual departments' in which scholars dispersed sparsely across the globe can collaborate, and natural language processing tools counterbalance the scarcity of trained editors by enabling machine processing of Coptic text to produce searchable, annotated corpora.      ", "article_title": "A Collaborative Ecosystem for Digital Coptic Studies", "authors": [{"given": "Caroline T.", "family": "Schroeder", "affiliation": [" University of Oklahoma, United States of America"]}, {"given": "Amir", "family": "Zeldes", "affiliation": ["Georgetown University, United States of America"]}], "publisher": null, "date": "2019-12-11", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01276243v3", "identifier": {"string_id": "hal-01276243", "id_scheme": "hal"}, "abstract": "The Project The literary tradition in the third and fourth centuries CE: Grammarians, rhetoricians and sophists as sources of Graeco-Roman literature (FFI2014-52808-C2-1-P) aims to trace and classify all types of quotations, both explicit (with or without mention of the author and/or title) and hidden, in a corpus comprising the Greek grammarians, rhetoricians and \" sophists \" of the third and fourth centuries CE. At the same time, we try to detect whether or not these are first-hand quotations, and if our quoting authors (28 in all) are, in turn, secondary sources for the same citations in later authors. We also study the philological (textual) aspects of the quotations in their context, and the problems of limits they sometimes pose. Finally, we are interested in the function of the quotation in the citing work. This is the first time that such a comprehensive study of this corpus is attempted. This paper explains our methodology, and how we store all these data in our electronic card-file. ", "article_title": "Dealing with all types of quotations (and their parallels) in a closed corpus: The methodology of the Project The literary tradition in the third and fourth centuries CE: Grammarians, rhetoricians and sophists as sources of Graeco-Roman literature", "authors": [{"given": "Lucía", "family": "Rodríguez-Noriega", "affiliation": ["Universidad de Oviedo [Oviedo]"]}], "publisher": null, "date": "2017-06-13", "keywords": ["Intertextuality", "Greco-Roman scholars of the Empire", "Fragmentary literature"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01279493v2", "identifier": {"string_id": "hal-01279493", "id_scheme": "hal"}, "abstract": "Greek documentary papyri form an important direct source for Ancient Greek. It has been exploited surprisingly little in Greek linguistics due to a lack of good tools for searching linguistic structures. This article presents a new tool and digital platform, “Sematia”, which enables transforming the digital texts available in TEI EpiDoc XML format to a format which can be morphologically and syntactically annotated (treebanked), and where the user can add new metadata concerning the text type, writer and handwriting of each act of writing. An important aspect in this process is to take into account the original surviving writing vs. the standardization of language and supplements made by the editors. This is performed by creating two different layers of the same text. The platform is in its early development phase. Ongoing and future developments, such as tagging linguistic variation phenomena as well as queries performed within Sematia, are discussed at the end of the article.", "article_title": "Preprocessing Greek Papyri for Linguistic Annotation", "authors": [{"given": "Marja", "family": "Vierros", "affiliation": ["Department of World Cultures"]}, {"given": "Erik", "family": "Henriksson", "affiliation": ["Department of World Cultures"]}], "publisher": null, "date": "2017-06-09", "keywords": ["JavaScript", "Python", "MySQL", "TEI EpiDoc XML", "Greek", "papyri", "linguistic annotation", "treebank", "dependency grammar"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://arxiv.org/abs/1602.08844v2", "identifier": {"string_id": "1602.08844", "id_scheme": "arXiv"}, "abstract": "This paper describes the Quantitative Criticism Lab, a collaborative initiative between classicists, quantitative biologists, and computer scientists to apply ideas and methods drawn from the sciences to the study of literature. A core goal of the project is the use of computational biology, natural language processing, and machine learning techniques to investigate authorial style, intertextuality, and related phenomena of literary significance. As a case study in our approach, here we review the use of sequence alignment, a common technique in genomics and computational linguistics, to detect intertextuality in Latin literature. Sequence alignment is distinguished by its ability to find inexact verbal similarities, which makes it ideal for identifying phonetic echoes in large corpora of Latin texts. Although especially suited to Latin, sequence alignment in principle can be extended to many other languages.      ", "article_title": "Bioinformatics and Classical Literary Study", "authors": [{"given": "Pramit", "family": "Chaudhuri", "affiliation": ["University of Texas at Austin, USA"]}, {"given": "Joseph P.", "family": "Dexter", "affiliation": ["Harvard University, USA"]}], "publisher": null, "date": "2016-02-29", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01371751v3", "identifier": {"string_id": "hal-01371751", "id_scheme": "hal"}, "abstract": "We describe the course of a hackathon dedicated to the development of linguistic tools for Tibetan Buddhist studies. Over a period of five days, a group of seventeen scholars, scientists, and students developed and compared algorithms for intertextual alignment and text classification, along with some basic language tools, including a stemmer and word segmenter.", "article_title": "A Hackathon for Classical Tibetan", "authors": [{"given": "Orna", "family": "Almogi", "affiliation": ["UHH - Universität Hamburg"]}, {"given": "Lena", "family": "Dankin", "affiliation": ["TAU-CS - School of Computer Science "]}, {"given": "Nachum", "family": "Dershowitz", "affiliation": ["TAU-CS - School of Computer Science "]}, {"given": "Lior", "family": "Wolf", "affiliation": ["TAU-CS - School of Computer Science "]}], "publisher": null, "date": "2018-12-30", "keywords": ["Tibetan", "Buddhist studies", "hackathon", "stemming", "segmentation", "intertextual alignment", "text classification"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://arxiv.org/abs/1603.01207v1", "identifier": {"string_id": "1603.01207", "id_scheme": "arXiv"}, "abstract": "Despite increasing interest in Syriac studies and growing digital availability of Syriac texts, there is currently no up-to-date infrastructure for discovering, identifying, classifying, and referencing works of Syriac literature. The standard reference work (Baumstark's Geschichte) is over ninety years old, and the perhaps 20,000 Syriac manuscripts extant worldwide can be accessed only through disparate catalogues and databases. The present article proposes a tentative data model for this http URL's New Handbook of Syriac Literature, an open-access digital publication that will serve as both an authority file for Syriac works and a guide to accessing their manuscript representations, editions, and translations. The authors hope that by publishing a draft data model they can receive feedback and incorporate suggestions into the next stage of the project.      ", "article_title": "From manuscript catalogues to a handbook of Syriac literature: Modeling an infrastructure for Syriaca.org", "authors": [{"given": "Nathan P.", "family": "Gibson", "affiliation": ["Vanderbilt University, USA"]}, {"given": "David A.", "family": "Michelson", "affiliation": ["Vanderbilt University, USA"]}, {"given": "Daniel L.", "family": "Schwartz", "affiliation": ["Texas A&M University, USA"]}], "publisher": null, "date": "2016-03-03", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://arxiv.org/abs/1603.01597v2", "identifier": {"string_id": "1603.01597", "id_scheme": "arXiv"}, "abstract": "In this paper we consider two sequence tagging tasks for medieval Latin: part-of-speech tagging and lemmatization. These are both basic, yet foundational preprocessing steps in applications such as text re-use detection. Nevertheless, they are generally complicated by the considerable orthographic variation which is typical of medieval Latin. In Digital Classics, these tasks are traditionally solved in a (i) cascaded and (ii) lexicon-dependent fashion. For example, a lexicon is used to generate all the potential lemma-tag pairs for a token, and next, a context-aware PoS-tagger is used to select the most appropriate tag-lemma pair. Apart from the problems with out-of-lexicon items, error percolation is a major downside of such approaches. In this paper we explore the possibility to elegantly solve these tasks using a single, integrated approach. For this, we make use of a layered neural network architecture from the field of deep representation learning.      ", "article_title": "Integrated Sequence Tagging for Medieval Latin Using Deep Representation Learning", "authors": [{"given": "Mike", "family": "Kestemont", "affiliation": ["University of Antwerp, Belgium"]}, {"given": "Jeroen", "family": "De Gussem", "affiliation": ["Ghent University, Belgium"]}], "publisher": null, "date": "2016-03-04", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/halshs-01532877v2", "identifier": {"string_id": "halshs-01532877", "id_scheme": "hal"}, "abstract": "The ‘Version Variation Visualization’ project has developed online tools to support comparative, algorithm-assisted investigations of a corpus of multiple versions of a text, e.g. variants, translations, adaptations (Cheesman, 2015, 2016; Cheesman et al., 2012, 2012-13, 2016; Thiel, 2014; links: www.tinyurl.com/vvvex). A segmenting and aligning tool allows users to 1) define arbitrary segment types, 2) define arbitrary text chunks as segments, and 3) align segments between a ‘base text’ (a version of the ‘original’ or translated text), and versions of it. The alignment tool can automatically align recurrent defined segment types in sequence. Several visual interfaces in the prototype installation enable exploratory access to parallel versions, to comparative visual representations of versions’ alignment with the base text, and to the base text visually annotated by an algorithmic analysis of variation among versions of segments. Data can be filtered, viewed and exported in diverse ways. Many more modes of access and analysis can be envisaged. The tool is language neutral. Experiments so far mostly use modern texts: German Shakespeare translations. Roos is working on a collection of approx. 100 distinct English-language translations of a Hebrew text with ancient Hebrew and Aramaic passages: the Haggadah (Roos, 2015)", "article_title": "Version Variation Visualization (VVV): Case Studies on the Hebrew Haggadah in English", "authors": [{"given": "Tom", "family": "Cheesman", "affiliation": ["Swansea University"]}, {"given": "Avraham", "family": "Roos", "affiliation": ["UvA - Universiteit van Amsterdam"]}], "publisher": null, "date": "2017-06-22", "keywords": ["digital humanities", "Social science", "translation", " corpus", " Judaism", " haggadah", " VVV", " visualization", " algorithm", " retranslation", " 4 sons", "humanities"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://arxiv.org/abs/1602.08657v2", "identifier": {"string_id": "1602.08657", "id_scheme": "arXiv"}, "abstract": "The software programs generally used with the TLG (Thesaurus Linguae Graecae) and the CLCLT (CETEDOC Library of Christian Latin Texts) CD-ROMs are not well suited for finding quotations and allusions. QuotationFinder uses more sophisticated criteria as it ranks search results based on how closely they match the source text, listing search results with literal quotations first and loose verbal parallels last.      ", "article_title": "QuotationFinder - Searching for Quotations and Allusions in Greek and Latin Texts and Establishing the Degree to Which a Quotation or Allusion Matches Its Source", "authors": [{"given": "Luc", "family": "Herren", "affiliation": ["University of Münster, Germany"]}], "publisher": null, "date": "2016-02-28", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://arxiv.org/abs/1602.08715v2", "identifier": {"string_id": "1602.08715", "id_scheme": "arXiv"}, "abstract": "We propose a method for efficiently finding all parallel passages in a large corpus, even if the passages are not quite identical due to rephrasing and orthographic variation. The key ideas are the representation of each word in the corpus by its two most infrequent letters, finding matched pairs of strings of four or five words that differ by at most one word and then identifying clusters of such matched pairs. Using this method, over 4600 parallel pairs of passages were identified in the Babylonian Talmud, a Hebrew-Aramaic corpus of over 1.8 million words, in just over 30 seconds. Empirical comparisons on sample data indicate that the coverage obtained by our method is essentially the same as that obtained using slow exhaustive methods.      ", "article_title": "Identification of Parallel Passages Across a Large Hebrew/Aramaic Corpus", "authors": [{"given": "Avi", "family": "Shmidman", "affiliation": ["Department of Computer Science, Bar-Ilan University, Israel", "Dicta: The Israel Center for Text Analysis"]}, {"given": "Moshe", "family": "Koppel", "affiliation": ["Department of Computer Science, Bar-Ilan University, Israel", "Dicta: The Israel Center for Text Analysis"]}, {"given": "Ely", "family": "Porat", "affiliation": ["Department of Computer Science, Bar-Ilan University, Israel", "Dicta: The Israel Center for Text Analysis"]}], "publisher": null, "date": "2016-02-28", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01280627v4", "identifier": {"string_id": "hal-01280627", "id_scheme": "hal"}, "abstract": "If one is convinced that \" quantitative research provides data not interpretation \" [Moretti, 2005, 9], close reading should thus be considered as not only the necessary bridge between big data and interpretation but also the core duty of the Humanities. To test its potential in a neglected field – the Arabic manuscripts of the Letters of Paul of Tarsus – an enhanced, digital edition has been in development as a progression of a Swiss National Fund project. This short paper presents the development of this edition and perspectives regarding a second project. Based on the Edition Visualization Technology tool, the digital edition provides a transcription of the Arabic text, a standardized and vocalized version, as well as French translation with all texts encoded in TEI XML. Thanks to another Swiss National Foundation subsidy, a new research project on the unique New Testament, trilingual (Greek-Latin-Arabic) manuscript, the Marciana Library Gr. Z. 11 (379), 12th century, is currently underway. This project includes new features such as \" Textlink \" , \" Hotspot \" and notes: HumaReC.", "article_title": "Editing New Testament Arabic Manuscripts in a TEI-base: fostering close reading in Digital Humanities", "authors": [{"given": "Claire", "family": "Clivaz", "affiliation": ["SIB - Swiss Institute of Bioinformatics [Lausanne]"]}, {"given": "Sara", "family": "Schulthess", "affiliation": ["SIB - Swiss Institute of Bioinformatics [Lausanne]"]}, {"given": "Martial", "family": "Sankar", "affiliation": ["SIB - Swiss Institute of Bioinformatics [Lausanne]"]}], "publisher": null, "date": "2017-06-03", "keywords": ["Paul of Tarsus", "Letters", "Digital edition", "Arabic TEI", "New Testament"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01282568v4", "identifier": {"string_id": "hal-01282568", "id_scheme": "hal"}, "abstract": "Most intertextuality in classical poetry is unmarked, that is, it lacks objective signposts to make readers aware of the presence of references to existing texts. Intergeneric relationships can pose a particular problem as scholarship has long privileged intertextual relationships between works of the same genre. This paper treats the influence of Latin love elegy on Lucan’s epic poem, Bellum Civile, by looking at two features of unmarked intertextuality: frequency and distribution. I use the Tesserae project to generate a dataset of potential intertexts between Lucan’s epic and the elegies of Tibullus, Propertius, and Ovid, which are then aggregrated and mapped in Lucan’s text. This study draws two conclusions: 1. measurement of intertextual frequency shows that the elegists contribute fewer intertexts than, for example, another epic poem (Virgil’s Aeneid), though far more than the scholarly record on elegiac influence in Lucan would suggest; and 2. mapping the distribution of intertexts confirms previous scholarship on the influence of elegy on the Bellum Civile by showing concentrations of matches, for example, in Pompey and Cornelia’s meeting before Pharsalus (5.722-815) or during the affair between Caesar and Cleopatra (10.53-106). By looking at both frequency and proportion, we can demonstrate systematically the generic enrichment of Lucan’s Bellum Civile with respect to Latin love elegy.", "article_title": "Measuring and Mapping Intergeneric Allusion in Latin Poetry using Tesserae", "authors": [{"given": "Patrick J.", "family": "Burns", "affiliation": [" Institute for the Study of the Ancient World"]}], "publisher": null, "date": "2017-07-31", "keywords": ["allusion", " Lucan", " Latin epic", " Latin love elegy", " intertextuality", " generic enrichment", " Tesserae"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01645124v2", "identifier": {"string_id": "hal-01645124", "id_scheme": "hal"}, "abstract": "This article explores whether and how network visualization can benefit philological and historical-linguistic study. This is illustrated with a corpus-based investigation of scribes' language use in a lemmatized and morphologically annotated corpus of documentary Latin (Late Latin Charter Treebank, LLCT2). We extract four continuous linguistic variables from LLCT2 and utilize a gradient colour palette in Gephi to visualize the variable values as node attributes in a trimodal network which consists of the documents, writers, and writing locations underlying the same corpus. We call this network the \"LLCT2 network\". The geographical coordinates of the location nodes form an approximate map, which allows for drawing geographical conclusions. The linguistic variables are examined both separately and as a sum variable, and the visualizations presented as static images and as interactive Sigma.js visualizations. The variables represent different domains of language competence of scribes who learnt written Latin practically as a second-language. The results show that the network visualization of linguistic features helps in observing patterns which support linguistic-philological argumentation and which risk passing unnoticed with traditional methods. However, the approach is subject to the same limitations as all visualization techniques: the human eye can only perceive a certain, relatively small amount of information at a time.", "article_title": "Visualizing linguistic variation in a network of Latin documents and scribes", "authors": [{"given": "Timo", "family": "Korkiakangas", "affiliation": ["UiO - University of Oslo"]}, {"given": "Matti", "family": "Lassila", "affiliation": ["Open Science Centre [Jyväskylä]"]}], "publisher": null, "date": "2018-04-24", "keywords": ["network visualization", " Latin linguistics", " Early Middle Ages", " philology"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01466986v2", "identifier": {"string_id": "hal-01466986", "id_scheme": "hal"}, "abstract": "Digital humanities require IT Infrastructure and sophisticated analytical tools, including data visualization, data mining, statistics, text mining and information retrieval. Regarding funding, to build a local data center will necessitate substantial investments. Fortunately, there is another option that will help researchers take advantage of these IT services to access, use and share information easily. Cloud services ideally offer on-demand software and resources over the Internet to read and analyze ancient documents. More interestingly, billing system is completely flexible and based on resource usage and Quality of Service (QoS) level. In spite of its multiple advantages, outsourcing computations to an external provider arises several challenges. Specifically, security is the major factor hindering the widespread acceptance of this new concept. As a case study, we review the use of cloud computing to process digital images safely. Recently, various solutions have been suggested to secure data processing in cloud environement. Though, ensuring privacy and high performance needs more improvements to protect the organization's most sensitive data. To this end, we propose a framework based on segmentation and watermarking techniques to ensure data privacy. In this respect, segementation algorithm is used to to protect client's data against untauhorized access, while watermarking method determines and maintains ownership. Consequentely, this framework will increase the speed of development on ready-to-use digital humanities tools.", "article_title": "A Secured Data Processing Technique for Effective Utilization of Cloud Computing", "authors": [{"given": "Mbarek", "family": "Marwan", "affiliation": ["UCD - Université Chouaib Doukkali", "Département Télécommunications, Réseaux et Informatique"]}, {"given": "Ali", "family": "Kartit", "affiliation": ["Département Télécommunications, Réseaux et Informatique", "UCD - Université Chouaib Doukkali"]}, {"given": "Hassan", "family": "Ouahmane", "affiliation": ["Département Télécommunications, Réseaux et Informatique", "UCD - Université Chouaib Doukkali"]}], "publisher": null, "date": "2017-11-21", "keywords": ["data processing", "security", "digital humanities", "cloud computing"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Scientific and Technological Strategic Intelligence (2016)", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01458216v1", "identifier": {"string_id": "hal-01458216", "id_scheme": "hal"}, "abstract": "In this paper we present a system for offline recognition cursive Arabic handwritten text which is analytical without explicit segmentation based on Hidden Markov Models (HMMs). Extraction features preceded by baseline estimation are statistical and geometric to integrate both the peculiarities of the text and the pixel distribution characteristics in the word image. These features are modelled using hidden Markov models. The HMM-based classifiercontains a training module and a recognition module. The training module estimates the parameters of each of the character HMMs uses the Baum-Welchalgorithm. In the recognition phase, feature vectors extracted from an image are passed to a network of word lexicon entries formed of character models. The character sequence providing the maximumlikelihood identifies the recognized entry. If required, the recognition can generate N best output hypotheses rather than just the single best one. To determine the best output hypotheses, the Viterbi algorithm is used.The experiments on images of the benchmark IFN/ENIT database show that the proposed system improves recognition.", "article_title": "Cursive Arabic Handwriting Recognition System Without Explicit Segmentation Based on Hidden Markov Models", "authors": [{"given": "Mouhcine", "family": "Rabi", "affiliation": ["IRF-SIC - Laboratoire Image et Reconnaissance de Formes - Systèmes Intelligents et Communicants"]}, {"given": "Mustapha", "family": "Amrouch", "affiliation": ["IRF-SIC - Laboratoire Image et Reconnaissance de Formes - Systèmes Intelligents et Communicants"]}, {"given": "Zouhair", "family": "Mahani", "affiliation": ["IRF-SIC - Laboratoire Image et Reconnaissance de Formes - Systèmes Intelligents et Communicants"]}], "publisher": null, "date": "2017-02-09", "keywords": ["HMMs", "Arabic text", "handwriting", "Recognition"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Scientific and Technological Strategic Intelligence (2016)", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01294591v2", "identifier": {"string_id": "hal-01294591", "id_scheme": "hal"}, "abstract": "This paper discusses the word level alignment of lemmatised bitext consisting of the Oratio I of Gregory of Nazianzus in its Greek model and Georgian translation. This study shows how the direct and empirical observations offered by an aligned text enable an accurate analysis of techniques of translation and many philological parameters of the text.", "article_title": "Text Alignment in Ancient Greek and Georgian: A Case-Study on the First Homily of Gregory of Nazianzus", "authors": [{"given": "Tamara", "family": "Pataridze", "affiliation": ["Fonds de la Recherche Scientifique [FNRS]", "INCAL - Institut des Civilisations Arts et Lettres (INCAL)"]}, {"given": "Bastien", "family": "Kindt", "affiliation": ["INCAL - Institut des Civilisations Arts et Lettres (INCAL)"]}], "publisher": null, "date": "2017-12-22", "keywords": ["lexical tagging", "corpus", "bilingual dictionaries", "bitext", "Ancient Greek", "Ancient Georgian", "morphological tagging", "text alignment", "lemmatisation", " morphological tagging"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01762730v5", "identifier": {"string_id": "hal-01762730", "id_scheme": "hal"}, "abstract": "This work applies knowledge engineering’s techniques to medieval illuminations. Inside it, an illumination is considered as a knowledge graph which was used by some elites in the Middle Ages to represent themselves as a social group and exhibit the events in their lives, and their cultural values. That graph is based on combinations of symbolic elements linked each to others with semantic relations. Those combinations were used to encode visual metaphors and influential messages whose interpretations are sometimes tricky for not experts. Our work aims to describe the meaning of those elements through logical modelling using ontologies. To achieve that, we construct logical reasoning rules and simulate them using artificial intelligence mechanisms. The goal is to facilitate the interpretation of illuminations and provide, in a future evolution of current social media, logical formalisation of new encoding and information transmission services.", "article_title": "Causal reasoning and symbolic relationships in Medieval Illuminations", "authors": [{"given": "Djibril", "family": "Diarra", "affiliation": ["Checksem"]}, {"given": "Martine", "family": "Clouzot", "affiliation": ["ARTeHiS - Archéologie, Terre, Histoire, Sociétés [Dijon]"]}, {"given": "Christophe", "family": "Nicolle", "affiliation": ["Checksem"]}], "publisher": null, "date": "2019-06-03", "keywords": ["Semantic relation", "Symbolic relation", "Ontology", "Social network", "Medieval illumination"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Data Science and Digital Humanities @ EGC 2018", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/halshs-01557447v1", "identifier": {"string_id": "halshs-01557447", "id_scheme": "hal"}, "abstract": "A new method for grouping manuscripts in clusters is presented with the calculation of distances between readings, then between witnesses. A classification algorithm (\" Hierarchical Ascendant Clustering \"), achieved through computer-aided processing, enables the construction of trees illustrating the textual taxonomy obtained. This method is applied to the Old Latin witnesses of the Gospel of John, and, in order to provide a study of a reasonable size, to a chapter as a whole (chapter 14). The result basically confirms the text-types identified by Bonatius Fischer, founder of the Vetus Latina Institute, while it invalidates the classification adopted by the current edition of the Vetus Latina of the Gospel of John.", "article_title": "A Classification of Manuscripts Based on A New Quantitative Method. The Old Latin Witnesses of John's Gospel as Text Case", "authors": [{"given": "David", "family": "Pastorelli", "affiliation": ["AU - Avignon Université"]}], "publisher": null, "date": "2017-07-06", "keywords": ["Manuscripts", "Gospel of John", "latin witnesses"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01456090v2", "identifier": {"string_id": "hal-01456090", "id_scheme": "hal"}, "abstract": "This paper proposes an ontological integration model for credit risk management. It is based on three ontologies; one is global describing credit risk management process and two other locals, the first, describes the credit granting process, and the second presents the concepts necessary for the monitoring of credit system. This paper also presents the technique used for matching between global ontology and local ontologies. ", "article_title": "Applying ontologies to data integration systems for bank credit risk management", "authors": [{"given": "Jalil", "family": "Elhassouni", "affiliation": ["GSCM-LRIT - Laboratoire de Recherche en Informatique et Télécommunications [Rabat]"]}, {"given": "Mehdi", "family": "Bazzi", "affiliation": ["LIAD Laboratory, FSAC, Hassan II University, Morocco"]}, {"given": "Abderrahim", "family": "Qadi", "affiliation": ["Ecole Supérieure de Technologie de Meknès", "GSCM-LRIT - Laboratoire de Recherche en Informatique et Télécommunications [Rabat]"]}, {"given": "Mohamed", "family": "Haziti", "affiliation": ["GSCM-LRIT - Laboratoire de Recherche en Informatique et Télécommunications [Rabat]"]}], "publisher": null, "date": "2017-11-26", "keywords": ["credit risk management", " data integration", " ontologies alignment"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Scientific and Technological Strategic Intelligence (2016)", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01759191v2", "identifier": {"string_id": "hal-01759191", "id_scheme": "hal"}, "abstract": "The EUROPANGE project, involving both medievalists and computer scientists, aims to study the emergence of a corps of administrators in the Angevin controlled territories in the XIII–XV centuries. Our project attempts to analyze the officers' careers, shared relation networks and strategies based on the study of individual biographies. In this paper, we describe methods and tools designed to analyze these prosopographical data. These include OLAP analyzes and network analyzes associated with cartographic and chronological visualization tools.", "article_title": "Prosopographical data analysis. Application to the Angevin officers (XIII–XV centuries)", "authors": [{"given": "Anne", "family": "Tchounikine", "affiliation": ["BD - Base de Données"]}, {"given": "Maryvonne", "family": "Miquel", "affiliation": ["BD - Base de Données"]}, {"given": "Thierry", "family": "Pécout", "affiliation": ["LEM - Laboratoire d'Etudes sur les Monothéismes"]}, {"given": "Jean-Luc", "family": "Bonnaud", "affiliation": ["Universite de Moncton, Canada"]}], "publisher": null, "date": "2018-05-24", "keywords": [], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Data Science and Digital Humanities @ EGC 2018", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://arxiv.org/abs/1312.6675v2", "identifier": {"string_id": "1312.6675", "id_scheme": "arXiv"}, "abstract": "Social media and social networks have already woven themselves into the very fabric of everyday life. This results in a dramatic increase of social data capturing various relations between the users and their associated artifacts, both in online networks and the real world using ubiquitous devices. In this work, we consider social interaction networks from a data mining perspective - also with a special focus on real-world face-to-face contact networks: We combine data mining and social network analysis techniques for examining the networks in order to improve our understanding of the data, the modeled behavior, and its underlying emergent processes. Furthermore, we adapt, extend and apply known predictive data mining algorithms on social interaction networks. Additionally, we present novel methods for descriptive data mining for uncovering and extracting relations and patterns for hypothesis generation and exploration, in order to provide characteristic information about the data and networks. The presented approaches and methods aim at extracting valuable knowledge for enhancing the understanding of the respective data, and for supporting the users of the respective systems. We consider data from several social systems, like the social bookmarking system BibSonomy, the social resource sharing system flickr, and ubiquitous social systems: Specifically, we focus on data from the social conference guidance system Conferator and the social group interaction system MyGroup. This work first gives a short introduction into social interaction networks, before we describe several analysis results in the context of online social networks and real-world face-to-face contact networks. Next, we present predictive data mining methods, i.e., for localization, recommendation and link prediction. After that, we present novel descriptive data mining methods for mining communities and patterns.      ", "article_title": "Data Mining on Social Interaction Networks", "authors": [{"given": "Atzmueller", "family": "Martin", "affiliation": ["University of Kassel"]}], "publisher": null, "date": "2013-12-23", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01443713v1", "identifier": {"string_id": "hal-01443713", "id_scheme": "hal"}, "abstract": "Cloud Computing and Big Data are the upcoming Information Technology (IT) computing models. These groundbreaking paradigms are leading IT to a new set of rules that aims to change computing resources delivery and exploitation model, thus creating a novel business market that is exponentially growing and attracting more and more investments from both providers and end users that are looking forward to make profits from these innovative models of computing. In the same context, researchers and investigators are wrestling time in order to develop, test and optimize Cloud Computing and Big Data platforms, whereas several studies are ongoing to determine and enhance the essential aspects of these computing models especially compute resources allocation. The processing power scheduling is crucial when it comes to Cloud Computing and Big Data because of the data growth management and delivery design proposed by these new computing models, that requires faster responses from platforms and applications. Hence originates the importance of developing high efficient scheduling algorithms that are compliant with these computing models platforms and infrastructures requirement.", "article_title": "Smarter Round Robin Scheduling Algorithm for Cloud Computing and Big Data", "authors": [{"given": "Hicham", "family": "Gibet Tani", "affiliation": ["LIST - Laboratoire d'Informatique, Système et Télécommunications"]}, {"given": "Chaker", "family": "El Amrani", "affiliation": ["LIST - Laboratoire d'Informatique, Système et Télécommunications"]}], "publisher": null, "date": "2017-01-23", "keywords": ["Scheduling Algorithms", "Cloud Computing Simulation", "First Come First Served", "Cloud Computing", "Big Data", "Algorithme de planification", "Simulation Cloud Computing", "Round Robin", "Premier Arrivé Premier Servi"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Scientific and Technological Strategic Intelligence (2016)", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://arxiv.org/abs/1402.2003v2", "identifier": {"string_id": "1402.2003", "id_scheme": "arXiv"}, "abstract": "The project presents the strategy adopted by the Rough Cilicia Archaeological Survey team for publishing its primary data and reports via three potentially transformative strategies for digital humanities: Loose coupling of digital data curation and publishing platforms. In loosely coupled systems, components share only a limited set of simple assumptions, which enables systems to evolve dynamically. Collaborative creation of map based narrative content. Connecting print scholarship (book, reports, article) to online resources via two-dimensional barcodes (2D codes) that can be printed on paper and can call up hyperlinks when scanned with a Smartphone. The three strategies are made possible by loosely coupling two autonomous services: Visible Past, dedicated to web collaboration and digital-print publishing and Open Context, which is a geo-historical data archiving and publishing service. The Rough Cilicia Archaeological Survey, Visible Past, and Open Context work together to illustrate a new genre of scholarship, which combine qualitative narratives and quantitative representations of space and social phenomena. The project provides tools for collaborative creation of rich scholarly narratives that are spatially located and for connecting print publications to the digital realm. The project is a case study for utilizing the three new strategies for creating and publishing spatial humanities scholarship more broadly for ancient historians.      ", "article_title": "A New Approach to Reporting Archaeological Surveys: Connecting Rough Cilicia, Visible Past and Open Context through loose coupling and 3d codes", "authors": [{"given": "Sorin Adam", "family": "Matei", "affiliation": ["Purdue University"]}, {"given": "Nicholas K.", "family": "Rauh", "affiliation": ["Alexandria Institute, CA"]}, {"given": "Eric C.", "family": "Kansa", "affiliation": ["Purdue University"]}], "publisher": null, "date": "2014-02-09", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01466986v1", "identifier": {"string_id": "hal-01466986", "id_scheme": "hal"}, "abstract": "Digital humanities require IT Infrastructure and sophisticated analytical tools, including data visualization, data mining, statistics, text mining and information retrieval. Regarding funding, to build a local data center will necessitate substantial investments. Fortunately, there is another option that will help researchers take advantage of these IT services to access, use and share information easily. Cloud services ideally offer on-demand software and resources over the Internet to read and analyze ancient documents. More interestingly, billing system is completely flexible and based on resource usage and Quality of Service (QoS) level. In spite of its multiple advantages, outsourcing computations to an external provider arises several challenges. Specifically, security is the major factor hindering the widespread acceptance of this new concept. As a case study, we review the use of cloud computing to process digital images safely. Recently, various solutions have been suggested to secure data processing in cloud environement. Though, ensuring privacy and high performance needs more improvements to protect the organization's most sensitive data. To this end, we propose a framework based on segmentation and watermarking techniques to ensure data privacy. In this respect, segementation algorithm is used to to protect client's data against untauhorized access, while watermarking method determines and maintains ownership. Consequentely, this framework will increase the speed of development on ready-to-use digital humanities tools.", "article_title": "A Secured Data Processing Technique for Effective Utilization of Cloud Computing", "authors": [{"given": "Mbarek", "family": "Marwan", "affiliation": ["UCD - Université Chouaib Doukkali", "Département Télécommunications, Réseaux et Informatique"]}, {"given": "Ali", "family": "Kartit", "affiliation": ["Département Télécommunications, Réseaux et Informatique", "UCD - Université Chouaib Doukkali"]}, {"given": "Hassan", "family": "Ouahmane", "affiliation": ["Département Télécommunications, Réseaux et Informatique", "UCD - Université Chouaib Doukkali"]}], "publisher": null, "date": "2017-02-16", "keywords": ["cloud computing", "digital humanities", "security", "data processing"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Scientific and Technological Strategic Intelligence (2016)", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-00919370v3", "identifier": {"string_id": "hal-00919370", "id_scheme": "hal"}, "abstract": "This research study tested three different NLP technologies to analyze representative journalistic discourse used in the 2007 and 2012 presidential campaigns in France. The analysis focused on the discourse in relation to the candidate's gender and/ or political party. Our findings suggest that using specific software to examine a journalistic corpus can reveal linguistic patterns and choices made on the basis of political affiliation and/or gender stereotypes. These conclusions are drawn from quantitative and qualitative analysis carried out with three different software programs: SEMY, which semi-automatically provides semantic profiles; ANTCONC, which provides useful Keywords in Context (KWIC) or abstracts of texts, as well as collocations; TERMOSTAT, which reveals discourse specificities, frequencies and the most common morpho-syntactic patterns. Analysis of our data point to convergent asymmetries between female and male candidates in journalistic discourse (however conditionally) for the 2007 and the 2012 French presidential campaigns. We conclude that social gender (i.e., stereotypical expectations of who will be a typical member of a given category) and / or political favoritism may affect the representation of leadership in discourse, which, in turn, may influence the readership, hence the electorate. Thus the study recommends the use of corpus linguistic tools for the semi-automatic investigation of political texts.", "article_title": "ANALYSING JOURNALISTIC DISCOURSE AND FINDING OPINIONS SEMI-AUTOMATICALLY?: A CASE STUDY OF THE 2007 AND 2012 PRESIDENTIAL FRENCH CAMPAIGNS", "authors": [{"given": "Fabienne", "family": "Baider", "affiliation": ["UCY - University of Cyprus"]}], "publisher": null, "date": "2014-05-02", "keywords": ["semi-automatic reading", "gender stereotypes", "political favoritism", "journalistic discourse", "French presidential campaign", "lecture automatique", "stéréotypes de genre", "biais politique", "discours journalistique", "campagne présidentielle française"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "2014", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://arxiv.org/abs/1312.5817v3", "identifier": {"string_id": "1312.5817", "id_scheme": "arXiv"}, "abstract": "This study analyzed references and source papers of the Proceedings of 2009-2012 International Conference of Digital Archives and Digital Humanities (DADH), which was held annually in Taiwan. A total of 59 sources and 1,104 references were investigated, based on descriptive analysis and subject analysis of library practices on cataloguing. Preliminary results showed historical materials, events, bureaucracies, and people of Taiwan and China in the Qing Dynasty were the major subjects in the tempo-spatial dimensions. The subject-date figure depicted a long-low head and short-high tail curve, which demonstrated both characteristics of research of humanities and application of technology in digital humanities. The dates of publication of the references spanned over 360 years, which shows a long time span in research materials. A majority of the papers (61.41%) were single-authored, which is in line with the common research practice in the humanities. Books published by general publishers were the major type of references, and this was the same as that of established humanities research. The next step of this study will focus on the comparison of characteristics of both sources and references of international journals with those reported in this article.", "article_title": "Exploring Regional Development of Digital Humanities Research: A Case Study for Taiwan", "authors": [{"given": "Kuang-hua", "family": "Chen", "affiliation": ["National Taiwan University"]}, {"given": "Bi-Shin", "family": "Hsueh", "affiliation": ["National Taiwan University"]}], "publisher": null, "date": "2013-12-20", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01024985v4", "identifier": {"string_id": "hal-01024985", "id_scheme": "hal"}, "abstract": "The contribution of this article is twofold: the adaptation and application of models of deception from psychology, combined with data-mining techniques, to the text of speeches given by candidates in the 2008 U.S. presidential election; and the observation of both short-term and medium-term differences in the levels of deception. Rather than considering the effect of deception on voters, deception is used as a lens through which to observe the self-perceptions of candidates and campaigns. The method of analysis is fully automated and requires no human coding, and so can be applied to many other domains in a straightforward way. The authors posit explanations for the observed variation in terms of a dynamic tension between the goals of campaigns at each moment in time, for example gaps between their view of the candidate’s persona and the persona expected for the position; and the difficulties of crafting and sustaining a persona, for example, the cognitive cost and the need for apparent continuity with past actions and perceptions. The changes in the resulting balance provide a new channel by which to understand the drivers of political campaigning, a channel that is hard to manipulate because its markers are created subconsciously.", "article_title": "Deception in Speeches of Candidates for Public Office", "authors": [{"given": "David", "family": "Skillicorn", "affiliation": ["School of computing [Kingston]"]}, {"given": "Christian", "family": "Leuprecht", "affiliation": ["RMCC - Royal Military College of Canada"]}], "publisher": null, "date": "2015-08-16", "keywords": ["political discourse", "corpus analytics", "U.S. presidential elections", "deception", "singular value decomposition"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://arxiv.org/abs/1312.4617v2", "identifier": {"string_id": "1312.4617", "id_scheme": "arXiv"}, "abstract": "Social network has gained remarkable attention in the last decade. Accessing social network sites such as Twitter, Facebook LinkedIn and Google+ through the internet and the web 2.0 technologies has become more affordable. People are becoming more interested in and relying on social network for information, news and opinion of other users on diverse subject matters. The heavy reliance on social network sites causes them to generate massive data characterised by three computational issues namely; size, noise and dynamism. These issues often make social network data very complex to analyse manually, resulting in the pertinent use of computational means of analysing them. Data mining provides a wide range of techniques for detecting useful knowledge from massive datasets like trends, patterns and rules [44]. Data mining techniques are used for information retrieval, statistical modelling and machine learning. These techniques employ data pre-processing, data analysis, and data interpretation processes in the course of data analysis. This survey discusses different data mining techniques used in mining diverse aspects of the social network over decades going from the historical techniques to the up-to-date models, including our novel technique named TRCM. All the techniques covered in this survey are listed in the Table.1 including the tools employed as well as names of their authors.      ", "article_title": "A Survey of Data Mining Techniques for Social Media Analysis", "authors": [{"given": "Mariam", "family": "Adedoyin-Olowe", "affiliation": ["School of Computing Science and Digital Media, Robert Gordon University"]}, {"given": "Mohamed Medhat", "family": "Gaber", "affiliation": ["School of Computing Science and Digital Media, Robert Gordon University"]}, {"given": "Frederic", "family": "Stahl", "affiliation": ["School of Systems Engineering, University of Reading"]}], "publisher": null, "date": "2013-12-17", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://arxiv.org/abs/1311.5401v2", "identifier": {"string_id": "1311.5401", "id_scheme": "arXiv"}, "abstract": "Text data is often seen as \"take-away\" materials with little noise and easy to process information. Main questions are how to get data and transform them into a good document format. But data can be sensitive to noise oftenly called ambiguities. Ambiguities are aware from a long time, mainly because polysemy is obvious in language and context is required to remove uncertainty. I claim in this paper that syntactic context is not suffisant to improve interpretation. In this paper I try to explain that firstly noise can come from natural data themselves, even involving high technology, secondly texts, seen as verified but meaningless, can spoil content of a corpus; it may lead to contradictions and background noise.      ", "article_title": "Clustering and Relational Ambiguity: from Text Data to Natural Data", "authors": [{"given": "Nicolas", "family": "Turenne", "affiliation": ["Université Paris Est"]}], "publisher": null, "date": "2013-11-21", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://arxiv.org/abs/1010.0803v3", "identifier": {"string_id": "1010.0803", "id_scheme": "arXiv"}, "abstract": "How are people linked in a highly connected society? Since in many networks a power-law (scale-free) node-degree distribution can be observed, power-law might be seen as a universal characteristics of networks. But this study of communication in the Flickr social online network reveals that power-law node-degree distributions are restricted to only sparsely connected networks. More densely connected networks, by contrast, show an increasing divergence from power-law. This work shows that this observation is consistent with the classic idea from social sciences that similarity is the driving factor behind communication in social networks. The strong relation between communication strength and node similarity could be confirmed by analyzing the Flickr network. It also is shown that node similarity as a network formation model can reproduce the characteristics of different network densities and hence can be used as a model for describing the topological transition from weakly to strongly connected societies.      ", "article_title": "Node similarity as a basic principle behind connectivity in complex networks", "authors": [{"given": "Matthias", "family": "Scholz", "affiliation": ["Ernst-Moritz-Arndt-University, Greifswald, Germany", "University of Trento, Italy"]}], "publisher": null, "date": "2010-10-05", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://arxiv.org/abs/1405.3539v3", "identifier": {"string_id": "1405.3539", "id_scheme": "arXiv"}, "abstract": "Using geometric data analysis, our objective is the analysis of narrative, with narrative of emotion being the focus in this work. The following two principles for analysis of emotion inform our work. Firstly, emotion is revealed not as a quality in its own right but rather through interaction. We study the 2-way relationship of Ilsa and Rick in the movie Casablanca, and the 3-way relationship of Emma, Charles and Rodolphe in the novel Madame Bovary. Secondly, emotion, that is expression of states of mind of subjects, is formed and evolves within the narrative that expresses external events and (personal, social, physical) context. In addition to the analysis methodology with key aspects that are innovative, the input data used is crucial. We use, firstly, dialogue, and secondly, broad and general description that incorporates dialogue. In a follow-on study, we apply our unsupervised narrative mapping to data streams with very low emotional expression. We map the narrative of Twitter streams. Thus we demonstrate map analysis of general narratives.      ", "article_title": "Pattern Recognition in Narrative: Tracking Emotional Expression in Context", "authors": [{"given": "Fionn", "family": "Murtagh", "affiliation": ["University of Derby, UK", "Goldsmiths University of London, UK"]}, {"given": "Adam", "family": "Ganz", "affiliation": ["Royal Holloway, University of London, UK"]}], "publisher": null, "date": "2014-05-14", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-02324617v2", "identifier": {"string_id": "hal-02324617", "id_scheme": "hal"}, "abstract": "In this study, we address the interesting task of classifying historical texts by their assumed period of writ-ing. This task is useful in digital humanity studies where many texts have unidentified publication dates.For years, the typical approach for temporal text classification was supervised using machine-learningalgorithms.  These algorithms require careful feature engineering and considerable domain expertise todesign a feature extractor to transform the raw text into a feature vector from which the classifier couldlearn to classify any unseen valid input.  Recently, deep learning has produced extremely promising re-sults for various tasks in natural language processing (NLP). The primary advantage of deep learning isthat human engineers did not design the feature layers, but the features were extrapolated from data witha general-purpose learning procedure. We investigated deep learning models for period classification ofhistorical texts. We compared three common models: paragraph vectors, convolutional neural networks (CNN) and recurrent neural networks (RNN), and conventional machine-learning methods. We demon-strate that the CNN and RNN models outperformed the paragraph vector model and the conventionalsupervised machine-learning algorithms.  In addition, we constructed word embeddings for each timeperiod and analyzed semantic changes of word meanings over time.", "article_title": "Deep Learning for Period Classification of Historical Hebrew Texts", "authors": [{"given": "Chaya", "family": "Liebeskind", "affiliation": ["JCT - Jerusalem College of Technology"]}, {"given": "Shmuel", "family": "Liebeskind", "affiliation": ["JCT - Jerusalem College of Technology"]}], "publisher": null, "date": "2020-06-01", "keywords": ["Machine Learning", "Deep Learning", "Diachronic Corpus", "Period Classification"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "2020", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://arxiv.org/abs/1808.10685v3", "identifier": {"string_id": "1808.10685", "id_scheme": "arXiv"}, "abstract": "Open-ended survey data constitute an important basis in research as well as for making business decisions. Collecting and manually analysing free-text survey data is generally more costly than collecting and analysing survey data consisting of answers to multiple-choice questions. Yet free-text data allow for new content to be expressed beyond predefined categories and are a very valuable source of new insights into people's opinions. At the same time, surveys always make ontological assumptions about the nature of the entities that are researched, and this has vital ethical consequences. Human interpretations and opinions can only be properly ascertained in their richness using textual data sources; if these sources are analyzed appropriately, the essential linguistic nature of humans and social entities is safeguarded. Natural Language Processing (NLP) offers possibilities for meeting this ethical business challenge by automating the analysis of natural language and thus allowing for insightful investigations of human judgements. We present a computational pipeline for analysing large amounts of responses to open-ended questions in surveys and extract keywords that appropriately represent people's opinions. This pipeline addresses the need to perform such tasks outside the scope of both commercial software and bespoke analysis, exceeds the performance to state-of-the-art systems, and performs this task in a transparent way that allows for scrutinising and exposing potential biases in the analysis. Following the principle of Open Data Science, our code is open-source and generalizable to other datasets.      ", "article_title": "Extracting Keywords from Open-Ended Business Survey Questions", "authors": [{"given": "Barbara", "family": "McGillivray", "affiliation": ["The Alan Turing Institute", "University of Cambridge"]}, {"given": "Gard", "family": "Jenset", "affiliation": ["Independent Researcher"]}, {"given": "Dominik", "family": "Heil", "affiliation": ["University of the Witwatersrand, Johannesburg"]}], "publisher": null, "date": "2018-08-31", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-02154122v2", "identifier": {"string_id": "hal-02154122", "id_scheme": "hal"}, "abstract": "Tokenization of modern and old Western European languages seems to be fairly simple, as it stands on the presence mostly of markers such as spaces and punctuation. However, when dealing with old sources like manuscripts written in scripta continua, antiquity epigraphy or Middle Age manuscripts, (1) such markers are mostly absent, (2) spelling variation and rich morphology make dictionary based approaches difficult. Applying convolutional encoding to characters followed by linear categorization to word-boundary or in-word-sequence is shown to be effective at tokenizing such inputs. Additionally, the software is released with a simple interface for tokenizing a corpus or generating a training set.", "article_title": "Evaluating Deep Learning Methods for Word Segmentation of Scripta Continua Texts in Old French and Latin", "authors": [{"given": "Thibault", "family": "Clérice", "affiliation": ["PSL - Université Paris sciences et lettres", "ENC - École nationale des chartes", "CJM - Centre Jean Mabillon", "HiSoMA - Histoire et Sources des Mondes antiques"]}], "publisher": null, "date": "2020-04-05", "keywords": ["convolutional network", "scripta continua", "tokenization", "Old French", "word segmentation"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "2020", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://arxiv.org/abs/1807.04892v4", "identifier": {"string_id": "1807.04892", "id_scheme": "arXiv"}, "abstract": "In the past few years, computer vision and pattern recognition systems have been becoming increasingly more powerful, expanding the range of automatic tasks enabled by machine vision. Here we show that computer analysis of building images can perform quantitative analysis of architecture, and quantify similarities between city architectural styles in a quantitative fashion. Images of buildings from 18 cities and three countries were acquired using Google StreetView, and were used to train a machine vision system to automatically identify the location of the imaged building based on the image visual content. Experimental results show that the automatic computer analysis can automatically identify the geographical location of the StreetView image. More importantly, the algorithm was able to group the cities and countries and provide a phylogeny of the similarities between architectural styles as captured by StreetView images. These results demonstrate that computer vision and pattern recognition algorithms can perform the complex cognitive task of analyzing images of buildings, and can be used to measure and quantify visual similarities and differences between different styles of architectures. This experiment provides a new paradigm for studying architecture, based on a quantitative approach that can enhance the traditional manual observation and analysis. The source code used for the analysis is open and publicly available.      ", "article_title": "Computer Analysis of Architecture Using Automatic Image Understanding", "authors": [{"given": "Fan", "family": "Wei", "affiliation": ["Lawrence Technological University, USA"]}, {"given": "Yuan", "family": "Li", "affiliation": ["Lawrence Technological University, USA"]}, {"given": "Lior", "family": "Shamir", "affiliation": ["Lawrence Technological University, USA"]}], "publisher": null, "date": "2018-07-13", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://arxiv.org/abs/2001.01863v7", "identifier": {"string_id": "2001.01863", "id_scheme": "arXiv"}, "abstract": "The goal of this work is to build a classifier that can identify text complexity within the context of teaching reading to English as a Second Language (ESL) learners. To present language learners with texts that are suitable to their level of English, a set of features that can describe the phonological, morphological, lexical, syntactic, discursive, and psychological complexity of a given text were identified. Using a corpus of 6171 texts, which had already been classified into three different levels of difficulty by ESL experts, different experiments were conducted with five machine learning algorithms. The results showed that the adopted linguistic features provide a good overall classification performance (F-Score = 0.97). A scalability evaluation was conducted to test if such a classifier could be used within real applications, where it can be, for example, plugged into a search engine or a web-scraping module. In this evaluation, the texts in the test set are not only different from those from the training set but also of different types (ESL texts vs. children reading texts). Although the overall performance of the classifier decreased significantly (F-Score = 0.65), the confusion matrix shows that most of the classification errors are between the classes two and three (the middle-level classes) and that the system has a robust performance in categorizing texts of class one and four. This behavior can be explained by the difference in classification criteria between the two corpora. Hence, the observed results confirm the usability of such a classifier within a real-world application.      ", "article_title": "Text Complexity Classification Based on Linguistic Information: Application to Intelligent Tutoring of ESL", "authors": [{"given": "M. Zakaria", "family": "Kurdi", "affiliation": ["University of Lynchburg, USA"]}], "publisher": null, "date": "2020-01-07", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://hal.archives-ouvertes.fr/hal-01981922v3", "identifier": {"string_id": "hal-01981922", "id_scheme": "hal"}, "abstract": "This paper addresses the integration of a Named Entity Recognition and Disambiguation (NERD) service within a group of open access (OA) publishing digital platforms and considers its potential impact on both research and scholarly publishing. The software powering this service, called entity-fishing, was initially developed by Inria in the context of the EU FP7 project CENDARI and provides automatic entity recognition and disambiguation using the Wikipedia and Wikidata data sets. The application is distributed with an open-source licence, and it has been deployed as a web service in DARIAH's infrastructure hosted by the French HumaNum. In the paper, we focus on the specific issues related to its integration on five OA platforms specialized in the publication of scholarly monographs in the social sciences and humanities (SSH), as part of the work carried out within the EU H2020 project HIRMEOS (High Integration of Research Monographs in the European Open Science infrastructure). In the first section, we give a brief overview of the current status and evolution of OA publications, considering specifically the challenges that OA monographs are encountering. In the second part, we show how the HIRMEOS project aims to face these challenges by optimizing five OA digital platforms for the publication of monographs from the SSH and ensuring their interoperability. In sections three and four we give a comprehensive description of the entity-fishing service, focusing on its concrete applications in real use cases together with some further possible ideas on how to exploit the annotations generated. We show that entity-fishing annotations can improve both research and publishing process. In the last chapter, we briefly present further possible application scenarios that could be made available through infrastructural projects.", "article_title": "Leveraging Concepts in Open Access Publications", "authors": [{"given": "Andrea", "family": "Bertino", "affiliation": ["SUB Göttingen - Göttingen State and University Library"]}, {"given": "Luca", "family": "Foppiano", "affiliation": ["ALMAnaCH - Automatic Language Modelling and ANAlysis & Computational Humanities"]}, {"given": "Laurent", "family": "Romary", "affiliation": ["ALMAnaCH - Automatic Language Modelling and ANAlysis & Computational Humanities"]}, {"given": "Pierre", "family": "Mounier", "affiliation": ["CLEO - Centre pour l'édition électronique ouverte", "EHESS - École des hautes études en sciences sociales"]}], "publisher": null, "date": "2019-03-25", "keywords": ["Open Access", "Named Entity Recognition and Disambiguation (NERD)", "Entity-Fishing", "Monographs", "Digital Publishing Platforms"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "2019", "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": "https://arxiv.org/abs/1801.00912v3", "identifier": {"string_id": "1801.00912", "id_scheme": "arXiv"}, "abstract": "With the rapid evolution of cross-strait situation, \"Mainland China\" as a subject of social science study has evoked the voice of \"Rethinking China Study\" among intelligentsia recently. This essay tried to apply an automatic content analysis tool (CATAR) to the journal \"Mainland China Studies\" (1998-2015) in order to observe the research trends based on the clustering of text from the title and abstract of each paper in the journal. The results showed that the 473 articles published by the journal were clustered into seven salient topics. From the publication number of each topic over time (including \"volume of publications\", \"percentage of publications\"), there are two major topics of this journal while other topics varied over time widely. The contribution of this study includes: 1. We could group each \"independent\" study into a meaningful topic, as a small scale experiment verified that this topic clustering is feasible. 2. This essay reveals the salient research topics and their trends for the Taiwan journal \"Mainland China Studies\". 3. Various topical keywords were identified, providing easy access to the past study. 4. The yearly trends of the identified topics could be viewed as signature of future research directions.      ", "article_title": "How the Taiwanese Do China Studies: Applications of Text Mining", "authors": [{"given": "Hsuan-Lei", "family": "Shao", "affiliation": ["Department of East Asian Studies, National Taiwan Normal University"]}, {"given": "Sieh-Chuen", "family": "Huang", "affiliation": ["College of Law, National Taiwan University"]}, {"given": "Yun-Cheng", "family": "Tsai", "affiliation": ["Center for General Education, National Taiwan University"]}], "publisher": null, "date": "2018-01-03", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}]