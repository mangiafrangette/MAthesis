{"url": "http://dx.doi.org/10.3389/fdigh.2017.00005", "identifier": {"string_id": "10.3389/fdigh.2017.00005", "id_scheme": "DOI"}, "abstract": "In the area of Linked Open Data (LOD), meaningful and high-performance interlinking of different datasets has become an ongoing challenge. Necessary tasks are supported by established standards and software, e.g., for the transformation, storage, interlinking, and publication of data. Our use case Swissbib &lt;https://www.swissbib.ch/&gt; is a well-known provider for bibliographic data in Switzerland representing various libraries and library networks. In this article, a case study is presented from the project linked.swissbib.ch which focuses on the preparation and publication of the Swissbib data by means of LOD. Data available in Marc21 XML are extracted from the Swissbib system and transformed into an RDF/XML representation. From approximately 21 million monolithic records, the author information is extracted and interlinked with authority files from the Virtual International Authority File (VIAF) and DBpedia. The links are used to extract additional data from the counterpart corpora. Afterward, data are pushed into an Elasticsearch index to make the data accessible for other components. As a demonstrator, a search portal is developed which presents the additional data and the generated links to users. In addition to that, a REST interface is developed in order to enable also access by other applications. A main obstacle in this project is the amount of data and the necessity of day-to-day (partial) updates. In the current situation, the data in Swissbib and in the external corpora are too large to be processed by established linking tools. The arising memory footprint prevents the correct functioning of these tools. Also triple stores are unhandy by revealing a massive overhead for import and update operations. Hence, we have developed procedures for extracting and shaping the data into a more suitable form, e.g., data are reduced to the necessary properties and blocked. For this purpose, we used sorted N-Triples as an intermediate data format. This method proved to be very promising as our preliminary results show. Our approach could establish 30,773 links to DBpedia and 20,714 links to VIAF and both link sets show high precision values and could be generated in reasonable expenditures of time.", "article_title": "Interlinking Large-scale Library Data with Authority Records", "authors": [{"given": "Felix", "family": "Bensmann", "affiliation": [{"original_name": "GESIS – Leibniz Institute for the Social Sciences, Knowledge Technologies for the Social Sciences, Germany", "normalized_name": "Leibniz Institute for the Social Sciences", "country": "Germany", "identifiers": {"ror": "https://ror.org/018afyw53", "GRID": "grid.425053.5"}}]}, {"given": "Benjamin", "family": "Zapilko", "affiliation": [{"original_name": "GESIS – Leibniz Institute for the Social Sciences, Knowledge Technologies for the Social Sciences, Germany", "normalized_name": "Leibniz Institute for the Social Sciences", "country": "Germany", "identifiers": {"ror": "https://ror.org/018afyw53", "GRID": "grid.425053.5"}}]}, {"given": "Philipp", "family": "Mayr", "affiliation": [{"original_name": "GESIS – Leibniz Institute for the Social Sciences, Knowledge Technologies for the Social Sciences, Germany", "normalized_name": "Leibniz Institute for the Social Sciences", "country": "Germany", "identifiers": {"ror": "https://ror.org/018afyw53", "GRID": "grid.425053.5"}}]}], "publisher": "Frontiers Media SA", "date": "2017-03-15", "keywords": [], "journal_title": "Frontiers in Digital Humanities", "volume": "4", "issue": "", "ISSN": [{"value": "2297-2668", "type": "electronic"}]}