Selecting effective features from data sets is a particularly important part in text classification, data mining, pattern recognition, and artificial intelligence. Feature selection (FS) is capable of excluding irrelevant features for the classification task and reducing the dimensionality of data sets, which help us better understand data. Through FS selection, the performance of machine learning techniques is improved, and computation requirement is minimized. Thus far, a large number of FS methods have been proposed, whereas the most practically effective one has not been found. Although it is conceivable that different categories of FS methods follow different criteria for evaluating variables, rare studies have focused on evaluating various categories of FS methods. This article first lists thirteen superior FS methods under five different categories and focuses on evaluating and comparing the effectiveness and general versatility of these methods. The thirteen FS methods were ranked using rank aggregation method. Subsequently, the best five FS methods were elected to perform multi-class classifications. Support vector machine served as the classifier. Different languages, different numbers of selected features, and different performance measures were employed to measure the effectiveness and general versatility of these methods together. The analysis results suggest that Mahalanobis distance is the best method on the whole.