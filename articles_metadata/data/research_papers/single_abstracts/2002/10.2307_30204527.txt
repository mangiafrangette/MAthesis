Comprehensive computational lexicons are essential to practical natural language processing (NLP). To compile such computational lexicons by automatically acquiring lexical information, however, we previously require sufficiently large corpora. This study aims at predicting the ideal size of such automatic-lexical-acquisition oriented corpora, focusing on six specific factors: (1) specific versus general purpose prediction, (2) variation among corpora, (3) base forms versus inflected forms, (4) open class items, (5) homographs, and (6) unknown words. Another important and related issue with regard to predictability has something to do with data sparseness. Research using the TOTAL Corpus reveals serious data sparseness in this corpus. This, again, points towards the importance and necessity of reducing data sparseness to a satisfactory level for the automatic lexical acquisition and reliable corpus predictions. The functions of predicting the number of tokens and lemmas in a corpus are based on the piecewise curve-fitting algorithm. Unfortunately, the predicted size of a corpus for automatic lexical acquisition is too astronomical to compile it by using presently existing compiling strategies. Therefore, we suggest a practical and efficient alternative method. We are confident that this study will shed new light on issues such as corpus predictability, compiling strategies and linguistic comprehensiveness.