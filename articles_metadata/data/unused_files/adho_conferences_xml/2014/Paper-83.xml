<?xml version="1.0" encoding="utf-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:exsl="http://exslt.org/common" xml:id="Paper-83">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>Clustering Search to Navigate A Case Study of the Canadian World Wide Web as a Historical Resource</title>
        <author>
          <name>
            <surname>Milligan</surname>, <forename>Ian</forename>
          </name>
          <affiliation>University of Waterloo</affiliation>
          <email>i2milligan@uwaterloo.ca</email>
        </author>
      </titleStmt>
      <publicationStmt>
        <authority/>
        <publisher>EPFL, Switzerland</publisher>
        <distributor>
          <name>EPFL Digital Humanities Laboratory</name>
          <address>
            <addrLine>GC D2 386</addrLine>
            <addrLine>Station 18</addrLine>
            <addrLine>CH-1015 Lausanne</addrLine>
            <addrLine>frederic.kaplan@epfl.ch</addrLine>
          </address>
        </distributor>
        <pubPlace>Lausanne, Switzerland</pubPlace>
        <address>
          <addrLine>EPFL</addrLine>
          <addrLine>CH-1015 Lausanne</addrLine>
        </address>
        <availability>
          <p/>
        </availability>
      </publicationStmt>
      <notesStmt>
        <note type="abstract">Drawing on a novel dataset of nearly 5% of the top-level .ca domain preserved by the Internet Archive’s March 2011 Wide Web Scrape, this paper demonstrates how by using open-source software a researcher can move between the level of distant reading to the actual, preserved website. Mitigating the lack of a full-text search function in the WaybackMachine, this approach draws on clustering search engines to generate on-the-fly finding aids and search results.</note>
      </notesStmt>
      <sourceDesc>
        <p>No source: created in electronic format.</p>
        <p>
          <date when="20140710"/>
          <time when="11:00:00"/>
        </p>
		<p n="session">5</p>
		<p n="room">415 - Amphimax</p>
      </sourceDesc>
    </fileDesc>
    <profileDesc>
      <textClass>
        <keywords scheme="original" n="category">
          <term>Paper</term>
        </keywords>
        <keywords scheme="original" n="subcategory">
          <term>Short Paper</term>
        </keywords>
        <keywords scheme="original" n="keywords">
          <term></term>
        </keywords>
        <keywords scheme="original" n="topic"/>
      </textClass>
    </profileDesc>
  </teiHeader>
  <text type="paper">
    <front>
      
      <div>
        <p></p>
      </div>
    </front>
    <body>
      <div>
        <head>1. Introduction</head>
        <p>From a historian’s
perspective, I present an approach to navigate large amounts of Internet
Archive information, drawing on a case study of 4.7% of the top-level .ca
domains preserved in a scrape of the entire World Wide Web, the March 2011 Wide
Web Scrape. Every day, users record their thoughts, feelings, locations,
ratings, votes, reviews, jokes, and so forth; an assemblage of traces of the
past that historians will be able to mold into narratives. Here, I explain one
way to access them beyond the WaybackMachine using open-source tools such as WARC Tools, Apache Solr, and Carrot2 Workbench.</p>
        <head>

2. Literature Review and Project Rationale</head>
        <p>Information scholars and
digital archivists are having a conversation around digital preservation and
web archiving.<ref target="n1" rend="sup">1</ref> However, there is a need to approach these issues
from the perspective of a historian with an interest in using web archives. My
focus is on use rather than preservation.</p>
        <p>The current way to access
this material is through the WaybackMachine, run on the Internet Archive’s
server itself at <ref type="url" target="http://archive.org/web">archive.org/web</ref> or as a local installation.<ref target="n2" rend="sup">2</ref> The WaybackMachine is simple from a user
perspective: one enters a URL and then the available dates of various snapshots
are displayed across the top of the screen, and the web page is displayed if it
is available. </p>
        <p>I am concerned with the files that drive the
WaybackMachine: the WebARChive (WARC) files, the international
standard for preserving Web data.<ref target="n3" rend="sup">3</ref> Archiving web sites is difficult: a single page
is made up of hundreds of parts, with external calls for images or other code
hosted elsewhere. A WARC file provides a container for it all.</p>
        <p>There are good reasons for a historian to be interested
in these files rather than merely using the WaybackMachine. Chief among them is
the latter’s lack of a full-text search function. The WaybackMachine takes
a URL and generates the corresponding archived website. WARC files are the
system’s building blocks. They contain data with can be explored another way.</p>
        <head>3. The Canadian Internet Case Study</head>
        <p>I draw on the 80TB Wide
Web Scrape, released in October 2012 to celebrate the Internet Archive’s accumulation
of ten petabytes of data.<ref target="n4" rend="sup">4</ref> The WARC files in this collection are the
results of a crawl that began on 9 March 2011 and ended on 23 December 2011,
totalling 2,713,676,341 websites over 29,032,069 hosts. It is important to note the limitations of this scrape. First, we do not have multiple ones: if we had more scrapes, we could compare them and thus establish temporal changes. My hope is that if we can make cogent cases for what we can do with this sort of data, more might be released. Second, the sampling practices of the Internet Archive profoundly impact this sample and are beyond my control. The exact percentage of preserved websites is unknown, but it is probably only a little more than half.<ref target="n5" rend="sup">5</ref> Furthermore, they are not scraped and preserved on a temporally consistent basis. More work remains to be done to understand these processes, as they profoundly shape work done with it.</p>
        <p>Beginning by downloading all 111,690 index files (one
line per URL), I found the WARC files that contained the largest number of
websites within the .ca domain. I subsequently downloaded the top hundred WARCs
containing a total of 397,221 Canadian URLs. Based on Internet Archive
statistics, this dataset is 4.7% of the indexed .ca top-level domain.</p>
        <p>To
extract information, I was primarily interested in drawing on the large body of
text within this sample to analyze. Text analysis tools are most developed, and
this is my active area of research interest. In order to convert these files into
plain text, I relied upon the free and open-source WARC Tools collection. In short, this creates plain text files by
running each website through Lynx, a text-based browser. I subsequently selected
only those that had Canadian domain names, selected via regular expressions. Each
of those full-text files was then transformed into an XML document with fields
for the title (URL) and content (textual content of the website). This plain text conversion has also been extremely fruitful in exploring via
other text analysis tools, such as Named Entity Recognition.</p>
        <head>4. Findings</head>
        <p>The interplay between two
open-source tools, the Apache Solr
search engine and the Carrot2
clustering search engine, presents a fruitful way to explore these archives. Solr is a NoSQL search engine optimized
for working with millions of documents. Once data is ingested into Solr, which provides basic search, I
turn to Carrot2. It is useful because
of the clustering function, which takes objects and groups them into sets sharing
common characteristics. While Carrot2
offers several clustering algorithms, Lingo clustering is most fruitful. Its
goal is to "capture thematic threads in a search result, that is discover
groups of related documents and describe the subject of these groups in a way
meaningful to a human."<ref target="n6" rend="sup">6</ref></p>
        <p>I now want to provide an overview of what these
processes were able to do in my explorations of the Wide Web Scrape. My choices
of queries are necessarily limited, as this paper cannot do comprehensive
justice. In my other work, I am a historian of youth cultures; how could this
methodology help somebody with my research interests? Here, a query for
‘children’:</p>
        <figure>
          <graphic url="DH2014_71_1"/>
          <p>Fig. 1: The query ‘children’, demonstrating clusters within the collection</p>
        </figure>
        
        <p>At left is an input
panel, at right a list of clusters with the documents themselves. At lower left
we have visualization options. This visualization is akin to an archival
finding aid: we learn what these WARC files tell us about ‘Children,’ and
whether this is worth further investigation. We see files relating to children’s
health (Health Canada, Tylenol), research into children at various
universities, health and wellness services, as well as related topics such as
Youth Services, Family Services, and mental health. 
















</p>
        <p>Thus,
we have both an ad-hoc finding aid equivalent as well as a way to move beyond
distant and close reading levels. While future studies would need to expand the
amount of websites studied, we can see in this tranche of 5% of the Web that we
have a good amount of information pertaining to children’s health,
universities, and beyond. For some researchers, this would be a boon – for
others, an indication that they may need to look elsewhere. Some projects are
purely exploratory, however, and with a confident sample we could begin to make
overall statements concerning societal concerns towards children. Notably, this method helps us find relevant websites by putting them into easy-to-understand groups, allowing for both quantitative (number of sites) and qualitative (the sites themselves) findings.</p>
        
        <p>Clusters often contain more than one object, and the
relationship between clusters sheds light on the structure of a document
collection. Consider the following:</p>
        
        <figure>
          <graphic url="DH2014_71_2"/>
          <p>Fig. 2: The query ‘children’ visualized using the Aduna cluster visualization technique.</p>
        </figure>
        
        <p>Labels represent
clusters. If a document spans multiple clusters, it is represented by a dot
connected to both labels, which represent clusters. For example, “Christian
Education” appears in the middle left of the chart. There is one document to
the left of it (partially covered by the label), a document that belongs only
to it. Yet there is one to the right of it connected with “Early Learning,”
representing a website that falls into both categories. 

</p>
        <p>From this, we can learn quite a bit about the files that we can find in the Wide Web Scrape as well as suggest which might be most fruitful for exploration. In this chart, at the bottom we see  websites relating to children’s health, which connect to breastfeeding, which connect to timeframes, which actually then connect to employment (which often contains quite a bit of data and time information). We then also see that connect to early childhood workers, which in turn connects to early learning more generally. The structure of the web archive relating to children reveals itself. </p>
        <p>A
downside is that the individual files are plain text. However,
we can use the online WaybackMachine. Using an Automator script in OS X, a
service can be configured to prefix the WaybackMachine’s URL (<ref type="url" target="http://web.archive.org/web/">web.archive.org/web</ref>) to any URL. Through three steps (service
receives text, adds the prefix string, and displays the webpage), we retrieve
the archived version. See Fig 3, 4, and 5 below for this process:</p>
        <figure>
          <graphic url="DH2014_71_3"/>
          <p>Fig. 3: An example of the plain text files that power the search engine</p>
        </figure>
        <figure>
          <graphic url="DH2014_71_4"/>
          <p>Fig. 4: WaybackMachine Plugin in Action</p>
        </figure>
        <figure>
          <graphic url="DH2014_71_5"/>
          <p>Fig. 5: From distant to close reading: the above website in the WaybackMachine.</p>
        </figure>
        <head>5. Conclusions</head>
        <p>WARC files, and web
archives more generally, should be understood as key components of a future historian’s
professional training. Undertaking projects growing out of the 1990s or 2000s may
require access to such archives. Finding aids are generally unavailable for
this type of source, and would be impractical due to the sheer data quantity.
This approach, integrating on-the-fly finding aid generation and access to both
distant and close reading, should be considered for adoption by historians.  </p>
        <p>This
project shows that by drawing on a large dataset, 4.7% of the top-level .ca
domain, historians will be able to derive meaningful information and find
connections between disparate bodies of information. As history enters the web
era, new tools and resources will be necessary.</p>
      </div>
    </body>
    <back>
      <div type="References">
        <listBibl>
          <bibl>1. <hi rend="bold">Song, JaJa, J</hi>. (2008). <hi rend="italic">Fast browsing of archived Web contents in 8th International Web Archiving Workshop</hi>, Aarhus, Denmark, <ref type="url" target="http://www.umiacs.umd.edu/publications/fast-browsing-archived-web-contents">www.umiacs.umd.edu/publications/fast-browsing-archived-web-contents</ref>; <hi rend="bold">Toyoda, M., Kitsuregawa, M.</hi>, (2012). <hi rend="italic">The History of Web Archiving. Proceedings of the IEEE (Institute of Electrical and Electronics Engineers)</hi>, 100, 1441–1443; <hi rend="bold">Brügger, N. </hi>(2008). <hi rend="italic">The Archived Website and Website Philology:  A New Type of Historical Document</hi>. Nordicom Review, 29, 155–175; <hi rend="bold">Brügger, N</hi>. (2009). <hi rend="italic">Website history and the website as an object of study. New Media and Society</hi>, 11, 115–132; <hi rend="bold">Brügger, N.</hi> (2010). Web History. Bern: Peter Lang Publishing; <hi rend="bold">Brügger, N.</hi> (2012). <hi rend="italic">Web History and the Web as a Historical Source</hi>. Zeithistorische Forschungen/Studies in Contemporary History. 9; <hi rend="bold">Brügger, N.</hi>, Finnemann, N.O. (2013). <hi rend="italic">The Web and Digital Humanities: Theoretical and Methodological Concerns</hi>. Journal of Broadcasting and Electric Media 57, 66–80.
          </bibl>
          <bibl>2. <hi rend="bold">Internet Archive</hi> (n.d). WaybackMachine Github. GitHub. URL <ref type="url" target="http://github.com/internetarchive/wayback">github.com/internetarchive/wayback</ref> (accessed 5.29.13).
          </bibl>
          <bibl>3. <hi rend="bold">ISO</hi> (2009). ISO 28500:2009.
          </bibl>
          <bibl>4. <hi rend="bold">Internet Archive</hi> (2012). <hi rend="italic">80 terabytes of archived web crawl data available for research</hi>. Internet Archive Blog, <ref type="url" target="http://blog.archive.org/2012/10/26/80-terabytes-of-archived-web-crawl-data-available-for-research">blog.archive.org/2012/10/26/80-terabytes-of-archived-web-crawl-data-available-for-research</ref>.
          </bibl>
          <bibl>5. <hi rend="bold">Ed Summers</hi>, <hi rend="italic">The Web as a Preservation Medium</hi> Inkdroid.org, 26 November 2013, available online, <ref type="url" target="http://inkdroid.org/journal/2013/11/26/the-web-as-a-preservation-medium/">inkdroid.org/journal/2013/11/26/the-web-as-a-preservation-medium</ref>, accessed 20 February 2014.
          </bibl>
          <bibl>6. <hi rend="bold">Osiński, S., Stefanowski, J., Weiss, D</hi> (2004). <hi rend="italic">Lingo: Search Results Clustering Algorithm Based on Singular Value Decomposition</hi>, in: Advances in Soft Computing, Intelligent Information Processing and Web Mining, Proceedings of the International IIS: IIPWM ́04 Conference. Zakopane, Poland, pp. 359–368.
          </bibl>
        </listBibl>
      </div>
    </back>
  </text>
</TEI>
