<?xml version="1.0" encoding="utf-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:exsl="http://exslt.org/common" xml:id="Paper-163">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>The Telltale Hat: LDA and Classification Problems in a Large Folklore Corpus</title>
        <author>
          <name>
            <surname>Mimno</surname>, <forename>David</forename>
          </name>
          <affiliation>Cornell University Department of Information Science</affiliation>
          <email>mimno@cornell.edu</email>
        </author>
        <author>
          <name>
            <surname>Broadwell</surname>, <forename>Peter M.</forename>
          </name>
          <affiliation>UCLA Library</affiliation>
          <email>broadwell@library.ucla.edu</email>
        </author>
        <author>
          <name>
            <surname>Tangherlini</surname>, <forename>Timothy R.</forename>
          </name>
          <affiliation>UCLA Scandinavian Section and Department of Asian Languages and Cultures</affiliation>
          <email>tango@humnet.ucla.edu</email>
        </author>
      </titleStmt>
      <publicationStmt>
        <authority/>
        <publisher>EPFL, Switzerland</publisher>
        <distributor>
          <name>EPFL Digital Humanities Laboratory</name>
          <address>
            <addrLine>GC D2 386</addrLine>
            <addrLine>Station 18</addrLine>
            <addrLine>CH-1015 Lausanne</addrLine>
            <addrLine>frederic.kaplan@epfl.ch</addrLine>
          </address>
        </distributor>
        <pubPlace>Lausanne, Switzerland</pubPlace>
        <address>
          <addrLine>EPFL</addrLine>
          <addrLine>CH-1015 Lausanne</addrLine>
        </address>
        <availability>
          <p/>
        </availability>
      </publicationStmt>
      <notesStmt>
        <note type="abstract">This work addresses the problem of story classification in large folklore collections by combining human labeling and feedback with machine-learning techniques for identifying topics in text corpora. We describe a series of experiments conducted on a collection of approximately 35,000 Danish folklore records and explain our primary findings: 1) that computational cross-validation of traditional human indexes is effective at uncovering "liminal" stories that are mislabeled and overlooked in the original classification scheme but prove to be of great interest to present-day scholars; and 2) that automatically propagating multiple labeled topics from a modern research-oriented index across a large folklore collection performs best when combined with expert human feedback.</note>
      </notesStmt>
      <sourceDesc>
        <p>No source: created in electronic format.</p>
        <p>
          <date when="20140710"/>
          <time when="09:00:00"/>
        </p>
		<p n="session">4</p>
		<p n="room">414 - Amphimax</p>
      </sourceDesc>
    </fileDesc>
    <profileDesc>
      <textClass>
        <keywords scheme="original" n="category">
          <term>Paper</term>
        </keywords>
        <keywords scheme="original" n="subcategory">
          <term>Long Paper</term>
        </keywords>
        <keywords scheme="original" n="keywords">
          <term>topic modeling</term>
          <term>folklore</term>
          <term>classification</term>
          <term>story labeling</term>
          <term></term>
        </keywords>
        <keywords scheme="original" n="topic">
          <term>concording and indexing</term>
          <term>content analysis</term>
          <term>corpora and corpus activities</term>
          <term>data mining/text mining</term>
          <term>folklore and oral history</term>
          <term>ontologies</term>
          <term></term>
        </keywords>
      </textClass>
    </profileDesc>
  </teiHeader>
  <text type="paper">
    <front>

      <div>
        <p></p>
        <p></p>
        <p></p>
      </div>
    </front>
    <body>
      <div>
        <head>Introduction</head>
        <p>Classification is a vexing problem in folkloristics. Indexing collections that often include tens of thousands of records is essential, but neither fully manual nor fully automated methods are adequate. In this work, we combine human notions of genre and topic classification with computational classifiers and topic analysis to produce an indexing that is both appropriate for scholarly goals and robust in the presence of ambiguity.</p>
        <p>Traditional scholarly indexes have been limited by time and technology. Although broad genre classifications such as “ballad”, “folktale”, and “legend” are well established, these formal classifications are coarse and do little more than sort the materials into large, internally diverse groupings. Most standard classification schemes assign each record to a single classification and do not allow for cross-genre classification (e.g., a ballad and a legend about the same murder will be in different categories).<ref target="n1" rend="sup">1</ref><ref target="n2" rend="sup">2</ref><ref target="n3" rend="sup">3</ref><ref target="n4" rend="sup">4</ref> The inadequacy of these classification schemes has significantly constrained research on verbal folklore, particularly because such categorizations are often the only available topic index for any given collection.</p>
        <p>New unsupervised machine learning methods offer scalability but lack human intelligence. Clustering algorithms partition a corpus into groups of documents that are similar. Topic modeling is more flexible, allowing each document to express multiple automatically detected themes. But such methods usually rely on simple bag-of-words representations that miss aspects of a text that are clear to readers familiar with the corpus. In addition, patterns found by algorithms may be statistically valid but uninteresting to scholars. </p>
        <p>We explore the problem of classification in a large corpus (~35,000 records) of nineteenth-century Danish folklore and suggest possible solutions to these problems through classification and topic-modeling strategies that combine human labels with machine learning. We consider two classification schemes for the collection: in the first, each document receives one label, whereas the second assigns multiple labels to each document.</p>
        <head>One label per story</head>
        <p>The original collector assigned each story to exactly one of 36 labels, but we are most interested in “borderline” stories that could fit in many classes. These “liminal” stories not only reveal the challenges to classification that arise when a system can only accommodate a single label—as in the original index—but also help researchers to discover stories that are anomalous. </p>
        <p>An excellent example of such an anomalous story appears in our target corpus, <hi rend="italic">Danske sagn</hi> [Danish Legends]:<ref target="n5" rend="sup">5</ref></p>
        <p>
          <hi rend="bold">DS_I_056:</hi>
        </p>
        <p>
          <hi rend="italic">Per Overlade was out one evening shooting hares. It was up on Kræn Møller’s field. Kræn was in the process of moving his farm, and the old farm had not been completely disassembled yet, and Per intended to hide amid the old frame that was still standing and shoot a hare or two. But when he gets there, he sees an old man who is sitting in there with a red cap on who nods to him. Per gets scared and doesn’t dare go in there, and so he doesn’t catch any hares.</hi>
        </p>
        <p>Originally labeled as a story about “mound dwellers/hidden folk,” the story could just as easily be classified in several other categories: poaching, household guardian spirits (<hi rend="italic">nisse</hi>, suggested by the old man’s red hat), and law breaking, to name but three. The story also touches on shifting agricultural practices and the significant reorganization of the Danish landscape in the early 1800s, when farms were routinely dismantled and moved out onto the newly reapportioned fields. </p>
        <p>Where else could the editor/archivist have placed this story? To answer this question, we train a Naïve Bayes classifier by estimating a word-frequency histogram for each label. We then measure the similarity of a document to each of the resulting histograms, taking care to remove the word counts for the “query” document from the histogram for its original label. For many stories, the “true” label is the closest, but not in this case. Its top five labels in order are:</p>
        <table>
          <row>
            <cell role="label">ID</cell>
            <cell role="label">Story label</cell>
          </row>
          <row>
            <cell role="data"> 36</cell>
            <cell role="data">Our forbears' way of thinking and spiritual life</cell>
          </row>
          <row>
            <cell role="data"> 35</cell>
            <cell role="data">Outdoor life </cell>
          </row>
          <row>
            <cell role="data"> 29</cell>
            <cell role="data">Witches and their sport </cell>
          </row>
          <row>
            <cell role="data"> 27</cell>
            <cell role="data">Being in league with the Devil </cell>
          </row>
          <row>
            <cell role="data">
              <hi rend="bold"> 1</hi>
            </cell>
            <cell role="data">
              <hi rend="bold">Mound dwellers/hidden folk</hi>
            </cell>
          </row>
        </table>
        <p>Although the first assignment is so broad as to be of little use—emphasizing the inadequacy of the original index—the association of the story with topic 35 highlights its affinity to stories about hunting and poaching, while topic 29 indicates the story's connection with hares—animals most commonly associated with witches.</p>
        <p>Additionally, we can use this classification scheme to initialize a 36-topic model, creating one topic per original label. We assign each word token to the same topic as the label of its document. We then resample topic assignments for each word token in turn. Given the topic assignments of the tokens in a document, we can rank the topics for that document. After one sweep through the entire corpus, the “Mound dwellers” topic still accounts for more than 80% of the tokens in the story of Per Overlade, but after 10 sweeps, only 21% of the words remain in that topic. “Our forebears' way of thinking” and “Being in league with the Devil” instead account for a greater proportion, with the “Devil” topic triggered by words about shooting hares. Overall, the original topic class now accounts for the majority of tokens in 74% of the stories in the collection.</p>
        <p>As we increase the number of sweeps through the corpus, the relationship between the topics of the model and the original labels becomes attenuated. At 100 sweeps, the majority of tokens remains in the original class for only 39% of the stories. In our sample story, the prominent topics are “From the time of villeinage”, “Wiverns and small creepy-crawlies”, “Our forebears' way of thinking”, and “Death portents”. Words about shooting and hares are now assigned to the “Wiverns” topic, indicating that we should be careful in using these labels. The “Death portents” topic is represented by the words <hi rend="italic">forskrækket</hi> (scared) and <hi rend="italic">sidder</hi> (sitting). </p>
        <p>Finding anomalous stories is not simply a question of precision and recall: the very fact that a story is “missed” in a given classification makes it particularly interesting. One of the jobs of the folklorist is to reconstruct the imaginary boundaries of the belief world, so stories that question or test those boundaries are the ones that are most important. Computationally cross-validating a traditional human-generated index, as described above, is an effective way to discover such liminal cases.</p>
        <head>Multiple human-generated labels</head>
        <p>We can also construct computational story classifiers when editors assign more than one label to each document. Human experts have catalogued a subset of the documents in our target corpus by assigning multiple labels to each document from a modern ontology that includes aspects of stories such as people, locations, and events. We would like to know how these labels map to the words in the documents, but simply counting the words in every document assigned to a label may result in noisy histograms. To improve our ability to interpret the results, we use a labeled topic model to learn which words are associated with which labels.</p>
        <p>Multiple labels add complexity but allow us to make stronger assumptions. Since each document has more than one label, we cannot easily translate these labels into word-level assignments as in the previous experiment. On the other hand, we can be reasonably certain that the absence of a label implies that it is not relevant. Similar to LabeledLDA<ref target="n6" rend="sup">6</ref>, we can therefore estimate word-topic assignments under the constraint that words can only be assigned to one of the labels for the document, or to a “Background” label that can absorb frequent words not related to any label. We then re-estimate topic-word distributions given these assignments, and repeat the process as needed.</p>
        <p>To evaluate the resulting word distributions, the original creator of the ontology marked individual words that are highly relevant to each label. At each stage of the algorithm, we have a ranked list of words for each label. Given relevance assignments, we can compute mean average precision (MAP) for the model at each stage. Under the initial noisy distributions, MAP for precision up to rank 20 is .26. After the first iteration, MAP increases to .33, but then begins falling in subsequent iterations, indicating that the model may be overfitting.</p>
        <p>Consistent differences in ranking quality provide insight into labels. We are more successful at finding words related to concrete themes such as people, animals, and objects. More abstract labels, such as story resolutions and actions or events, were mostly unsuccessful. But there are exceptions: we identified no words related to the label “Farmer”, despite the fact that this is a very common label, while events such as “Disease” and “Death” identified many specific words.</p>
        <head>Conclusion</head>
        <p>We demonstrate that classification and topic modeling methods can be used to improve existing manual annotations in a collection of Danish folklore. We find that incorporating human labels into machine learning methods—even when the labels are noisy or incomplete—produces indexes that have the benefits of both scholarly domain expertise and data-driven analysis. We believe that these results are applicable for many corpora both in digital humanities and the wider document analysis community. </p>
      </div>
    </body>
    <back>
      <div type="References">
        <listBibl>
          <bibl>1. <hi rend="bold">Uther, Hans-Jörg</hi>. (2004). <hi rend="italic">The Types of International Folktales: A Classification and Bibliography, Based on the System of Antti Aarne and Stith Thompson</hi>. FF Communications. Helsinki: Suomalainen Tiedeakatemia.
          </bibl>
          <bibl>2. <hi rend="bold">Grundtvig, Svend, Axel Olrik, Hakon Grüner-Nielsen, Karl-Ivar Hildeman, Erik Dal, Iørn Piø, Thorkild Knudsen, Svend Nielsen, and Nils Schiørring</hi>, eds. 1966–1976 [1853–1976]. <hi rend="italic">Danmarks gamle Folkeviser</hi>. 12 volumes. Copenhagen: Universitets-Jubilæets Danske Samfund (Akademisk forlag).
          </bibl>
          <bibl>3. <hi rend="bold">Taylor, Archer</hi>. (1934). <hi rend="italic">An Index to "The Proverb"</hi>. FF Communications 113. Helsinki: Suomalainen Tiedeakatemia, 1934.
          </bibl>
          <bibl>4. <hi rend="bold">Christiansen, Reidar T</hi>. (1958). <hi rend="italic">The Migratory Legends</hi>. FF Communications 175. Helsinki: Suomalainen Tiedeakatemia.
          </bibl>
          <bibl>5. <hi rend="bold">Kristensen, Evald Tang</hi>. (1892). <hi rend="italic">Danske sagn, som de har lydt i folkemunde</hi>. Århus and Silkeborg: Århus Folkeblads Bogtrykkeri.
          </bibl>
          <bibl>6. <hi rend="bold">Ramage, Daniel, David Hall, Ramesh Nallapati, and Christopher D. Manning</hi>. (2009). <hi rend="italic">Labeled LDA: A Supervised Topic Model for Credit Attribution in Multi-Labeled Corpora</hi>. Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. 248-256.
          </bibl>
        </listBibl>
      </div>
    </back>
  </text>
</TEI>
