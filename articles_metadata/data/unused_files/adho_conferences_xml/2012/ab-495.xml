<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../schema/xmod_web.rnc" type="compact"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0"
     xmlns:xmt="http://www.cch.kcl.ac.uk/xmod/tei/1.0" 
     xml:id="ab-495">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Evaluating Unmasking for Cross-Genre Authorship Verification</title>
                <author>
                    <name>Kestemont, Mike</name>
                    <affiliation>CLiPS Computational Linguistics Group, University of Antwerp, Belgium</affiliation>
                    <email>mike.kestemont@ua.ac.be</email>
                </author>
                <author>
                    <name>Luyckx, Kim</name>
                    <affiliation>CLiPS Computational Linguistics Group, University of Antwerp, Belgium</affiliation>
                    <email>kim.luyckx@ua.ac.be</email>
                </author>
                <author>
                    <name>Daelemans, Walter</name>
                    <affiliation>CLiPS Computational Linguistics Group, University of Antwerp, Belgium</affiliation>
                    <email>walter.daelemans@ua.ac.be</email>
                </author>
                <author>
                    <name>Crombez, Thomas</name>
                    <affiliation>CLiPS Computational Linguistics Group, University of Antwerp, Belgium</affiliation>
                    <email>thomas.crombez@ua.ac.be</email>
                </author>
            </titleStmt>
            <publicationStmt>
                <publisher>Jan Christoph Meister, Universität Hamburg</publisher>
                <address>
                   <addrLine>Von-Melle-Park 6, 20146 Hamburg, Tel. +4940 428 38 2972</addrLine>
                   <addrLine>www.dh2012.uni-hamburg.de</addrLine>
              </address>
            </publicationStmt>
            <sourceDesc>
                <p>No source: created in electronic format.</p>
            </sourceDesc>
        </fileDesc>
        <revisionDesc>
            <change>
                <date>2012-04-15</date>
                <name>DH</name>
                <desc>generate TEI-template with data from ConfTool-Export</desc>
            </change>
            <change>
                <date>2012-04-13</date>
                <name>LS</name>
                <desc>provide metadata for publicationStmt</desc>
            </change>
        </revisionDesc>
    </teiHeader>
    <text type="paper">
        <body>
            <p>In this paper we will stress-test a recently proposed technique for computational
                authorship verification, ‘unmasking’ (Koppel et al<hi rend="italic">. </hi>2004,
                2007), which has been well received in the literature (Stein et al<hi rend="italic"
                    >. </hi>2010). The technique envisages an experimental set-up commonly referred
                to as ‘authorship verification’, a task generally deemed more difficult than
                so-called ‘authorship attribution’ (Koppel et al. 2007). We will apply the technique
                to authorship verification across genres, an extremely complex text categorization
                problem that so far has remained unexplored (Stamatatos 2009). We focus on five
                representative contemporary English-language authors. For each of them, the corpus
                under scrutiny contains several texts in two genres (literary prose and theatre
                plays).</p>
            <div>
                <head>Background: cross-genre authorship verification</head>
                <p> In authorship verification, the given text may have been written by one of the
                    candidate authors, but could also be written by none of them. Note that this <hi
                        rend="italic">open case</hi> scenario is typical of forensic applications:
                    the author of e.g. a bomb letter is not necessarily among the suspect candidate
                    authors. In the case of a suicide letter (potentially faked by a murderer), it
                    is highly likely that this is the only suicide letter the victim ever wrote. In
                    absence of similar material, it is difficult to extract reliable style markers
                    from pre-existing writings to determine authorship of the letter.</p>
                    <p>Authorship <hi rend="italic">across genres</hi> is an issue that is being
                    paid all too little attention in present-day research. The few remarks that have
                    been made on this issue agree that authorship attribution is difficult within a
                    single textual genre, even more difficult with several topics involved, and
                    likely to be extremely difficult with several genres involved (Luyckx &amp;
                    Daelemans 2011). Although it is generally assumed that an author will display
                    stable style characteristics throughout his oeuvre, irrespective of genre, this
                    remains speculative in the absence of systematic empirical investigation.
                    Consequently, cross-genre authorship verification deserves much more attention
                    than it has attracted so far. </p>
            </div>
            <div>
                <head>Unmasking</head>
                <p> Unmasking is a fairly complex meta-learning approach to authorship verification.
                    Koppel et al. (2007) observed in earlier experiments that a small number of
                    features had a lot of discriminatory power. It is indeed common for authors to
                    use ‘a small number of features in a consistently different way between works’.
                    Such features often relate to topic-related, narrative, or thematic differences.
                    As a result, a limited number of features can wrongfully maximize the
                    differences in writing style between two works of identical authorship.</p>
                    <p>The unmasking approach tests the <hi rend="italic">robustness</hi> of a
                    stylistic model by deliberately impairing it over a number of iterations, each
                    time removing those features that are most discriminative between the two texts.
                    The resulting ‘degradation curves’ display many sudden drops in accuracy: when
                    the most telling features are removed during each iteration, it becomes
                    increasingly difficult to differentiate between two texts. In the case of two
                    texts of non-identical authorship, however, a far larger number of features is
                    discriminative, causing less dramatic drops in accuracy during degradation.
                    Using training material in the form of a series of same-author and
                    different-author degradation curves, Koppel et al. (2007) try to verify whether
                    previously unseen degradation curves are of (non-)identical authorship.   </p>
                    <p>The unmasking technique is especially attractive for authorship verification
                        across genres, because of the interference between genre markers and
                        authorial style markers. It might help remedy genre-related artifacts in
                        that superficial genre-related differences between same-author texts in
                        different genres will be filtered out easily and removed from the model
                        early in the degradation process. After the removal of these non-essential
                        stylistic features, one could hypothesize that only features more relevant
                        for authorial identity will be preserved.</p></div>
                
                <div>
                    <head>Methodology and evaluation</head>
                <p>Our unmasking implementation closely adheres to the original description of the
                    procedure. In the experiments, we have used the same generic parameter settings
                    as tentatively adopted by Koppel et al. (2007): a <hi rend="italic">chunk
                        size</hi> of 500 tokens, <hi rend="italic">n</hi>=250, <hi rend="italic"
                        >m</hi>=10 and <hi rend="italic">k</hi>=3. The main difference is, that a
                    ‘leave-one-text- out validation’ is carried out on these curves for evaluation
                    purposes, whereas <hi rend="italic">k</hi>-fold cross-validation was applied in
                    the original paper. We train an SVM classifier on the training curves and have
                    it classify each of the test curves as a <hi rend="italic">same-author</hi> or
                        <hi rend="italic">different-author</hi> curve. When all predictions have
                    been collected, one can report on the overall classification accuracy and
                    macro-averaged F1-score.</p>
                </div>
            <div>
                <head>Corpus and selection of texts</head>
                <p>The corpus we collected for the experiments in cross-genre authorship
                    verification consists of published texts by five contemporary authors: Edward
                    Bond, David Mamet, Harold Pinter, Sam Shepard, and Arnold Wesker. The main
                    criterion for selecting an author was the availability of texts in more than one
                    literary genre. Theatre and prose were the genres these five authors were most
                    productive in, so these were chosen for the experiments. In our corpus, applying
                    a text length threshold of 10,000 words (cf. Sanderson &amp; Guenter 2006)
                    resulted in 11 prose works and 23 theatre plays. We experimented with a complete
                    matrix of authors and genres, allowing both intra-genre and cross-genre
                    experiments for all authors. Digitization of the material involved three steps:
                    scanning, OCR’ing, and manual post-correction.</p>
            </div>
            <div>
                <head>Intra-genre experiments</head>
                <p>Figure 1 shows degradation curves for an experiment on the eleven prose works in
                    the corpus. Solid lines represent <hi rend="italic">same-author</hi> curves,
                    whereas dotted lines represent <hi rend="italic">different-author</hi> curves.
                    All curves display downward slopes, with decreasing cross-validation accuracies,
                    as more predictive features get eliminated in each iteration. For <hi
                        rend="italic">same-author</hi> curves, however, it is clearly visible that
                    the effect of degradation generally sets off sooner and more dramatically. <hi
                        rend="italic">Different-author</hi> curves are more robust and yield higher
                    cross-validation accuracies, even when a large number of strongly discriminative
                    features is deleted. Intersections between both curve types are minimal. A
                    leave-one-text-out validation test on this set of curves confirms the success of
                    the approach: the overall accuracy amounts to 96%, which is only just over the
                    F1 score of 95%. This result confirms the potential of unmasking for authorship
                    verification in prose work collections.</p>
                <p><figure>
                        <graphic url="img495-1.jpg" rend="left" height="256px" width="341px"
                            mimeType="image/jpeg"/>
                        <head>Figure 1: Unmasking on prose texts by five authors</head>
                    </figure></p>
                <p>A second experiment has been carried out on the 23 theatrical works in the
                    corpus. Figure 2 displays a much less clear-cut differentiation of the <hi
                        rend="italic">same-author</hi> curves and their <hi rend="italic"
                        >different-author</hi> counterparts, suggesting that the unmasking approach
                    (with its default settings) is less effective for the theatrical section of the
                    corpus. The leave-one-text out validation confirms this, yielding an overall
                    accuracy of 84% and an F1 score of 62%.</p>
                <p><figure>
                        <graphic url="img495-2.jpg" rend="left" height="256px" width="341px"
                            mimeType="image/jpeg"/>
                        <head>Figure 2: Unmasking on theatre plays by five authors</head>
                    </figure></p>
            </div>
            <div>
                <head>Cross-genre experiments</head>
                <p>So far, the unmasking procedure has been mainly investigated for text pairs
                    within the same text variety, although Koppel <hi rend="italic">et al.</hi>
                    (2007) report on a successful application of the technique to Hebrew-Aramaic
                    texts across different topics. It is an interesting question whether the
                    degradation differences between <hi rend="italic">same- author</hi> and <hi
                        rend="italic">different-author</hi> curves would also hold for pairs of
                    texts that do not belong to the same genre. A leave-one-text-out validation,
                    however, shows poor performance of unmasking in this experiment, with an overall
                    accuracy of 77% and a macro-averaged F1 of 56%.</p>
            </div>
            <div>
                <head>Interpretation</head>
                <p>After unmasking has been applied, the individual degradation curves allow for
                    interpretation of results. Figure 3 visualizes the elimination process for
                    Pinter’s play <hi rend="italic">The Caretaker </hi>and Mamet’s prose text <hi
                        rend="italic">The Old Religion</hi>, who were personal friends. Mamet even
                    acknowledged Pinter as a key influence on his work. The limited degradation in
                    accuracy demonstrates that these Mamet and Pinter texts appear to adopt
                    well-distinguishable styles. Figure 3 shows early elimination of names of
                    principal characters (<hi rend="italic">davies</hi>, <hi rend="italic">mick
                    </hi>and <hi rend="italic">aston</hi> vs. <hi rend="italic">mark</hi> and <hi
                        rend="italic">pete</hi>), personal pronouns that relate to a text’s
                    narrative perspective (<hi rend="italic">i, you</hi>), and colloquial language
                        (<hi rend="italic">aint</hi>). Moreover, typical genre-features (e.g. the
                    director’s indication <hi rend="italic">pause</hi>) are deleted as
                    anticipated.</p>
                <p><figure>
                        <graphic url="img495-3.jpg" rend="left" height="256px" width="341px"
                            mimeType="image/jpeg"/>
                        <head>Figure 3: Visualization of the feature elimination process for
                            Pinter’s play The Caretaker and Mamet’s prose text The Old
                            Religion</head>
                    </figure></p>
            </div>
            <div><head>Conclusion</head>
                <p>The experiments reported on in this paper confirm that unmasking is an
                    interesting technique for computational authorship verification, especially
                    yielding reliable results within the genre of (larger) prose works in our
                    corpus. Authorship verification, however, proves much more difficult in the
                    theatrical part of our corpus. The original settings for the various parameters
                    often appear to be genre-specific or even author-specific, so that further
                    research on optimization is desirable. Finally, we have shown that
                    interpretability is an important asset of the unmasking technique.</p></div>
            <div><p><hi rend="bold">Funding and Acknowledgements</hi></p>
                <p>Kestemont is a Ph.D. fellow of the Research Foundation – Flanders (FWO). The
                    research of Luyckx and Daelemans is partially funded by the FWO project
                    ‘Computational Techniques for Stylometry for Dutch’. The research by Crombez is
                    partially funded by the FWO project ‘Mass Spectacle in Flanders’. The authors
                    would like to acknowledge Sarah Bekaert’s work on the digitization of the
                    corpus.</p></div>
        </body>
        <back>
            <div>
                
                    <head>References</head>
               <p><hi rend="bold">Koppel, M., J. Schler, and E. Bonchek-Dokow</hi> (2007).
                        Measuring Differentiability: Unmasking Pseudonymous Authors. <hi
                            rend="italic">Journal of Machine Learning Research </hi>8:
                        1261-1276.</p><p><hi rend="bold">Luyckx, K., and W. Daelemans</hi> (2011).
                        The Effect of Author Set Size and Data Size in Authorship Attribution. <hi
                            rend="italic">Literary and Linguistic Computing</hi> 26(1):
                            35-55.</p><p><hi rend="bold">Sanderson, C., and S. Guenter</hi> (2006).
                        Short Text Authorship Attribution via Sequence Kernels, Markov Chains and
                        Author Unmasking: An Investigation. <hi rend="italic">Proceedings of the
                            2006 Conference on Empirical Methods in Natural Language Processing
                            (EMNLP)</hi>, Sydney, Australia,<hi rend="italic"> </hi>pp.
                            482-491.</p><p><hi rend="bold">Stamatatos, E.</hi> (2009). A Survey of
                        Modern Authorship Attribution Methods. <hi rend="italic">Journal of the
                            American Society for Information Science and Technology </hi>60(3):
                        538-556.</p><p><hi rend="bold">Stein, B., N. Lipka, and P. Prettenhofer</hi>
                        (2011). Intrinsic Plagiarism Analysis. <hi rend="italic">Language Resources
                            and Evaluation </hi>45(1): 63-82.</p>
                    <p><hi rend="bold">Koppel, M., and J. Schler</hi> (2004). Authorship
                        Verification as a One-Class Classification Problem. <hi rend="italic"
                            >Proceedings of the 21st International Conference on Machine
                            Learning</hi>, Banff, Canada, </p>
                
            </div>
        </back>
    </text>
</TEI>