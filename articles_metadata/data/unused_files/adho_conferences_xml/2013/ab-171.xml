<?xml version="1.0" encoding="UTF-8"?>

<?oxygen RNGSchema="http://digitalhumanities.unl.edu/resources/schemas/tei/TEIP5.3.0.0/tei_all.rng" type="xml"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xml:id="ab-171">
    
<teiHeader>
<fileDesc>
<titleStmt>
<title>Digitizing Serialized Fiction</title>
<author>
<name><surname>Hess</surname>, <forename>Kirk</forename></name>
<affiliation>University of Illinois at Urbana-Champaign, United States of America</affiliation>
<email>kirkhess@illinois.edu</email>
</author>
</titleStmt>
<publicationStmt>
<authority></authority>
<publisher>University of Nebraska-Lincoln</publisher>
<distributor>
<name>Center for Digital Research in the Humanities</name>
<address>
<addrLine>319 Love Library</addrLine>
<addrLine>University of Nebraska&#8211;Lincoln</addrLine>
<addrLine>Lincoln, NE 68588-4100</addrLine>
<addrLine>cdrh@unlnotes.unl.edu</addrLine>
</address>
</distributor>
<pubPlace>Lincoln, Nebraska</pubPlace> 
<address>
<addrLine>University of Nebraska-Lincoln</addrLine>
<addrLine>Lincoln, NE 68588-4100</addrLine>
</address>
<availability>
<p></p>
</availability>
</publicationStmt>

<notesStmt><note type="abstract">One barrier to locating serialized fiction in a digital newspaper collection is the fact that the serials themselves are not indexed, and individual articles do not have metadata or tags associated with them that would identify them as fiction. As a result, specific article types are difficult to find unless the reader browses or simply hits upon a salutary keyword search. Finding the next installment is also a manual process, and there are always problems with the OCR quality. This paper will present the results of a research study which mined the University of Illinois&#039; digital newspaper collection for serialized fiction and evaluated multiple software approaches for solving these problems by traditional indexing and tagging, crowdsourcing to correct OCR problems, and automating the process with text analysis. Sample collection http://uller.grainger.uiuc.edu/omeka/</note></notesStmt>
    
<sourceDesc>
<p>No source: created in electronic format.</p>
<p>
<date when="20130717"></date>
<time when="08:30:00"></time>
</p>
<p n="session">SP02</p>
</sourceDesc>
</fileDesc>
    
<profileDesc>
<textClass>
<keywords scheme="original" n="category">
<term>Paper</term>
</keywords>
<keywords scheme="original" n="subcategory">
<term>Short Paper</term>
</keywords>
<keywords scheme="original" n="keywords">
<term>serialized fiction</term>
<term>crowdsourcing</term>
<term>text analysis</term>
                
</keywords>
<keywords scheme="original" n="topic">
<term>text analysis</term>
<term>crowdsourcing</term>
<term>digitisation &#8212; theory and practice</term>
<term>data mining/ text mining</term>  
                
                
</keywords>
</textClass>
</profileDesc>
    
<revisionDesc>
<change>
<date when="2013-03-29"></date>
<name>Erin Pedigo</name>
<desc>Initial encoding</desc>
</change>
</revisionDesc>
</teiHeader>
    
<text type="paper">
<body>
<div>

<div>
<head>Introduction</head>

    <p>One barrier to locating serialized fiction in a digital newspaper archive is the fact that the serialized fiction themselves are not indexed, and individual articles do not have subject terms or tags associated with them that would identify them as fiction. As a result, articles are difficult to find unless the reader browses a large volume of issues or simply hits upon a salutary keyword search. Keyword searching of the collection is more effective for articles on topics in farming or farm life than for works of fiction. Unless the reader is looking for stories by a specific author or for a known story title, keyword searching of fiction is highly ineffective.</p>
    <p>While the software used by most archives does a good job of connecting articles in a single issue, the reader does not know where to find the next installment in a serialized work of fiction, so he or she has to find it manually by browsing the collection or doing a keyword search. Finally, while the OCR scanning was done to the highest standard, this is an imperfect process, and much of the content cannot be adequately OCR’d due to background noise and broken letters, features of the original newspapers that impede scanning.</p>
    <p>This paper summarized the process of our project that was completed over 15 weeks in the summer of 2012. Our goal was to complete the manual indexing process that had already been started previously, display serialized fiction articles in a new repository, evaluate multiple software packages to see which ones were the most promising for use in the future, and evaluate any automated ways of finding serialized fiction.</p>
</div>

<div><head>Method and Results</head>

<div>
<head type="sub">Manual Indexing</head>

    <p>We experimented with three digital library systems, a Drupal/Fedora based repository (Islandora) <ref type="url" target="http://islandora.ca/">(http://islandora.ca/)</ref>, converting the fiction into TEI P5 and displaying it in the California Digital Library’s eXtensible Text Framework (XTF) <ref type="url" target="http://www.cdlib.org/services/publishing/tools/xtf/">(http://www.cdlib.org/services/publishing/tools/xtf/)</ref> and Omeka (<ref type="url" target="http://omeka.org">http://omeka.org</ref>) a PHP-based publishing platform for digital library objects. We were unable to get Islandora’s OCR correction module installed so we stopped using it in favor of Omeka. We used XSLT to transform the PrXML into very simple TEI5 files, which we were able to upload to XTF, but the lack of an editor and the intensely manual process of text encoding was also rejected in favor of Omeka. TEI Example: <ref type="url" target="http://uller.grainger.uiuc.edu:8080/xtf/search">http://uller.grainger.uiuc.edu:8080/xtf/search</ref></p>

    <p>Ultimately, we decided on using Omeka with the Scripto Plugin for correction.  Serialized fiction articles in one title, the Farmer’s Wife, was manually indexed in a spreadsheet, and graduate assistants converted those stories from PrXML into an exhibit, added Dublin Core metadata and links to the newspaper archive from the new serialized fiction collection. The end result was index of serialized fiction that would increase the accessibility of these articles. Omeka Exhibit: <ref type="url" target="http://uller.grainger.illinois.edu/omeka/">http://uller.grainger.illinois.edu/omeka/</ref></p>
</div>

<div>
<head>Crowdsourcing OCR correction</head>

    <p>The University of Illinois Digital Newspaper collections are in Olive Software ActivePaper Archive, which has a method for administrators to correct text but not users. Omeka provides a plugin called ‘Scripto’ for text correction that we were able to successfully use to correct the text in selected articles. We also evaluated Veridian <ref type="url" target="http://www.dlconsulting.com/veridian/">(http://www.dlconsulting.com/veridian/)</ref>, which is a commercial digital newspaper library solution used by Trove Digitised Newspapers (National Library of Australia), From the Page <ref type="url" target="http://beta.fromthepage.com/">(http://beta.fromthepage.com/)</ref> and Islandora <ref type="url" target="http://www.islandlives.ca/">(http://www.islandlives.ca/)</ref>. From the Page and Islandora were both very difficult to install and administer, and while not free we felt Veridian was a much better approach and we are evaluating it as part of our future newspaper digitization efforts.</p>
</div>

<div>
<head>Text Analysis</head>

    <p>How can we identify serialized fiction without having to have a human find it, index it in a spreadsheet and manually extract it from the archive? Certain n-grams are common within serialized fiction such as ‘chapter’, ‘the end’, ‘to be continued’ and could be used to simply search for keywords within documents; we could also calculate which words occur most frequently in fiction vs. other types of articles and use those terms to automatically tag articles.</p>
    <p>We also evaluated using topic analysis to find fiction. We evaluated the 580 articles we had already identified as serialized fiction using Mallet to find 25 topics with 25 words each. Figure 1. shows the top 25 topics modeled as a network using Gephi, while figure 2 shows the topic words ordered by frequency.</p>
        
<figure><graphic url="ab-171.001"/><head>Figure 1</head></figure>

<figure><graphic url="ab-171.002"/><head>Figure 2</head></figure>    

<p>Nodes were ranked by betweenness centrality and topic 14 had the highest at 51,321.01 and its component n-grams along with the other top topics could be used to find serialized fiction in other titles. </p>
    <p>One final text analysis technique that could be useful is identifying proper names is Named Entity Extraction.  While we made an effort to manually remove names from the topic analysis, as you can see they kept reappearing in the results. By using named entity extraction we could eliminate proper names from the topic analysis to make them more accurate, and to link fiction together by the character’s names. All three of these techniques (keyword frequency, topic analysis, named entity extraction) I plan on evaluating in a future study.</p></div>
</div>

<div>
<head>Conclusion</head>

    <p>Serialized fiction is an important component of historical newspapers and by making it more accessible to patrons and researchers we can expand the use and usefulness of our digital newspaper collections. The manual indexing approach was relatively inexpensive to accomplish but was time consuming and difficult to do over a large corpus of pages. Two promising approaches to find and digitize serialized fiction in our newspaper archive are adding a crowdsourcing feature to enable users to identify article types and correct mistakes, and utilizing text analysis techniques to identify fiction programmatically. We hope to report on our efforts at the latter at the DH 2013 conference.</p>
</div>

</div>
    
</body>
    
<back>
<div type="References">
<listBibl>

    <bibl><hi rend="bold">Bastian M., S. Heymann, and M. Jacomy</hi> (2009). <hi rend="italic">Gephi: an open source software for exploring and manipulating networks.</hi> International AAAI Conference on Weblogs and Social Media.</bibl>

    <bibl><hi rend="bold">Brandes, U.</hi> (2001). A Faster Algorithm for Betweenness Centrality. <hi rend="italic">Journal of Mathematical Sociology</hi><hi>25</hi> 163-177.</bibl>

    <bibl><hi rend="bold">Cohen, D.</hi> (2008). <hi rend="italic">Introducing Omeka.</hi> Dan Cohen's Digital Humanities Blog. <ref type="url" target="http://www.dancohen.org/2008/02/20/introducing-omeka/">http://www.dancohen.org/2008/02/20/introducing-omeka/</ref>.</bibl>

    <bibl><hi rend="bold">Islandora.</hi> <ref type="url" target="http://islandora.ca/">http://islandora.ca/</ref>.</bibl>

    <bibl><hi rend="bold">Island Lives.</hi> <ref type="url" target="http://www.islandlives.ca/">http://www.islandlives.ca/</ref></bibl>

    <bibl><hi rend="bold">McCallum, A. K.</hi> (2002). MALLET: A Machine Learning for Language Toolkit. <ref type="url" target="http://www.cs.umass.edu/~mccallum/mallet">http://www.cs.umass.edu/~mccallum/mallet</ref>.</bibl>

    <bibl><hi rend="bold">Veridian.</hi> <ref type="url" target="http://www.dlconsulting.com/veridian/">http://www.dlconsulting.com/veridian/</ref>.</bibl>

</listBibl>
</div>
</back>
</text>
</TEI>