<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><meta name="generator" content="ABBYY FineReader 14"/><title>Microsoft Word - 458. Király-Measuring completeness as metadata quality metric-458.docx</title><link rel="stylesheet" href="458_files/458.css" type="text/css"/>
</head>
<body><h1><a name="caption1"></a><a name="bookmark0"></a><span class="font6" style="font-weight:bold;">Measuring completeness as metadata quality metric in&nbsp;Europeana</span></h1>
<p><span class="font3" style="font-weight:bold;">Péter Kiraly</span></p>
<p><span class="font3"><a href="mailto:pkiraly@gwdg.de">pkiraly@gwdg.de</a></span></p>
<p><span class="font3">Gesellschaft für wissenschaftliche Datenverarbeitung</span></p>
<p><span class="font3">mbH Göttingen</span></p>
<p><span class="font3">Germany</span></p><h2><a name="bookmark1"></a><span class="font5" style="font-weight:bold;">Introduction</span></h2>
<p><span class="font3">The functionalities of an aggregated metadata collection are dependent on the quality of metadata records. Some examples from </span><span class="font3" style="text-decoration:underline;">Europeana</span><span class="font3">, the European digital library, to display the importance of metadata:&nbsp;(a) Several thousands records have the title „Photo”&nbsp;without further descriptions; how can a user find&nbsp;these objects?, (b) Several data providers listed in the&nbsp;„Institution” facet under multiple different names,&nbsp;should we expect that the user will select all name&nbsp;forms of an organization?, (c) Without formalized date&nbsp;value, we are not able to use the functionality of interactive date range selectors. The question is how can&nbsp;we determine which records should be improved, and&nbsp;which are good enough? The manual evaluation of&nbsp;each record is not affordable. This paper proposes a&nbsp;methodology and a software package, which can be&nbsp;used in Europeana and elsewhere in the domain of cultural heritage.</span></p><h2><a name="bookmark2"></a><span class="font5" style="font-weight:bold;">Background and foundations</span></h2>
<p><span class="font3">Europeana collects and presents cultural heritage metadata records. The database contains more than&nbsp;53 million records from more than 3200 institutions&nbsp;(figures extracted from the Europeana Search API) in&nbsp;the Europeana Data Model (EDM) schema. The organizations send their data in EDM or in another metadata&nbsp;standard. Due to the variety of original data formats,&nbsp;cataloging rules, languages and vocabularies, there are&nbsp;big differences in the quality of the individual records,&nbsp;which heavily affects the functionalities of Euro-peana's services.</span></p>
<p><span class="font3">In 2015 a Europeana task force investigating the problem of metadata quality published a report (Dan-gerfield et al.</span><span class="font3" style="font-weight:bold;">, </span><span class="font3">2015), however - as stated - „there was&nbsp;not enough scope ... to investigate ... metrics for&nbsp;metadata quality ...” In 2016 a wider </span><span class="font3" style="text-decoration:underline;">Data Quality&nbsp;Committee</span><span class="font3"> was founded. The current research is conducted in collaboration with it, having the purpose of&nbsp;finding methods, metrics and building </span><span class="font3" style="text-decoration:underline;">an open source&nbsp;tool</span><span class="font3"> (see also, the project’s </span><span class="font3" style="text-decoration:underline;">Github page</span><span class="font3">) to measure&nbsp;metadata quality.</span></p><h2><a name="bookmark3"></a><span class="font5" style="font-weight:bold;">State of the art</span></h2>
<p><span class="font3">The computational methods of metadata quality assessment emerged in the last decade in the domain (Bruce and Hillmann, 2004, Stvilia et al., 2007, Ochoa&nbsp;and Duval</span><span class="font3" style="font-weight:bold;">, </span><span class="font3">2009, Harper, 2016). Papers defined quality metrics and suggested computational implementations. They however mostly analyzed smaller volumes&nbsp;of records, metadata schemas which are less complex&nbsp;than EDM, and usually applied methods to more homogeneous data sets. The novelty of this research is&nbsp;that it increases the volume of records, introduces&nbsp;data visualizations, and provides open source implementation to use in other collections.</span></p><h2><a name="bookmark4"></a><span class="font5" style="font-weight:bold;">Methodology</span></h2>
<p><span class="font3">For every record, features were extracted or deducted which somehow related to the quality of the records. The main feature groups are:</span></p>
<p><span class="font4">• &nbsp;&nbsp;&nbsp;</span><span class="font3" style="font-weight:bold;">simple completeness </span><span class="font3">- ratio of filled fields,</span></p>
<p><span class="font4">• &nbsp;&nbsp;&nbsp;</span><span class="font3" style="font-weight:bold;">completeness of sub-dimensions </span><span class="font3">- fields&nbsp;groups support particular functions, such as&nbsp;searching, or accessibility,</span></p>
<p><span class="font4">• &nbsp;&nbsp;&nbsp;</span><span class="font3" style="font-weight:bold;">existence and cardinality of fields </span><span class="font3">- which&nbsp;fields are filled and how intensively.</span></p>
<p><span class="font3">The measurements happen on three levels: on individual records, on subsets (e.g. records of a data provider), and on the whole dataset. On second and third level we calculate aggregated metrics; the completeness of structural entities (such as the main descriptive part and the contextual entities - agent, concept,&nbsp;place, timespan - connecting the description to linked&nbsp;open data vocabularies).</span></p>
<p><span class="font3">The final completeness score is the combination of two approaches. In the first one the weighting reflects&nbsp;sub-dimensions. In the second one, the main factor is&nbsp;the normalized version of cardinality to prevent biasing effect of extreme values.</span></p>
<p><span class="font3">The tool - built on big data analytics software Apache Spark, the R statistical software and has a web&nbsp;front-end - is modular. There is a schema-independent core library and schema specific extensions. It is&nbsp;designed to be used in continuous integration for&nbsp;metadata quality assessment.</span></p><h2><a name="bookmark5"></a><span class="font5" style="font-weight:bold;">Results</span></h2>
<p><span class="font3">Comparison of the scores of the field importance and field cardinality approaches shows that they give&nbsp;different results (however they correlate by the Pearson's coefficient of 0.52.). Because of the nature of calculation the compound score is quite close to the first&nbsp;approach: the functionality based scores lie in the&nbsp;range of 0.186 and 0.76 and cardinality scores are in&nbsp;the range of 0.031 and 0.335, and it has smaller effect&nbsp;on the final score.</span></p>
<p><span class="font3">There are data providers, where all (in some cases more than ten thousand) records get the same scores:&nbsp;they have uniform structure. The field-level analysis&nbsp;shows (what one simple score is not able to testify)&nbsp;that in these collections all the records has the very&nbsp;same (Dublin Core based) field set. On the other end&nbsp;there are collections where both scores diverge a lot.&nbsp;For example in the identifying sub-dimension a data&nbsp;provider has five distinct values (from 0.4 to 0.8) almost evenly distributed while one of the best collection (of the category) is almost homogeneous: 99,7%&nbsp;or the records have the same value: 0.9 (even the rest&nbsp;0.3% has 0.8). It means that in the records of the first&nbsp;dataset the corresponding fields (dc:title, dcterms:al-ternative, dc:description, dc:type, dc:identifier,&nbsp;dc:date, dcterms:created and dcterms:issued in the&nbsp;ore:Proxy part and edm:provider and edm:dataPro-vider in the ore:Aggregation) are frequently not available, while they are almost always there in the second.&nbsp;The tool provides different graphs and tables to visualize the distribution of the scores.</span></p><img src="458_files/458-1.jpg" style="width:234pt;height:28pt;"/>
<p><span class="font0" style="font-weight:bold;">0.25 &nbsp;&nbsp;&nbsp;0.50&nbsp;&nbsp;&nbsp;&nbsp;0.75&nbsp;&nbsp;&nbsp;&nbsp;1.00 0.00&nbsp;&nbsp;&nbsp;&nbsp;0.25&nbsp;&nbsp;&nbsp;&nbsp;0.50&nbsp;&nbsp;&nbsp;&nbsp;0.75&nbsp;&nbsp;&nbsp;&nbsp;1.00 0.00&nbsp;&nbsp;&nbsp;&nbsp;0.25&nbsp;&nbsp;&nbsp;&nbsp;0.50&nbsp;&nbsp;&nbsp;&nbsp;0.75&nbsp;&nbsp;&nbsp;&nbsp;1.00</span></p>
<p><span class="font0" style="font-weight:bold;">weighted_completeness1 &nbsp;&nbsp;&nbsp;weighted_cardinality&nbsp;&nbsp;&nbsp;&nbsp;weighted_completeness2</span></p>
<p><span class="font1">Figure 1. Distribution of completeness scores in a dataset. We can see the differences between the functionality based&nbsp;(left),the cardinality based (center) and the combined&nbsp;method (right).</span></p>
<p><span class="font3">From the distribution of the fields the first conclusion is that lots of records miss contextual entities, and only a couple of data provider has 100% coverage (6%&nbsp;of the records has </span><span class="font3" style="font-style:italic;">agent</span><span class="font3">, 28% has </span><span class="font3" style="font-style:italic;">place</span><span class="font3">, 32% has&nbsp;</span><span class="font3" style="font-style:italic;">timespan</span><span class="font3"> and 40% has </span><span class="font3" style="font-style:italic;">concept</span><span class="font3"> entities). Only the mandatory technical elements appear in every records.&nbsp;There are fields, which are defined in the schema, but&nbsp;not filled in the records and there are overused fields&nbsp;- e. g. </span><span class="font3" style="font-style:italic;">dc:description</span><span class="font3"> is frequently used instead of more&nbsp;specific fields (such as table of contents, subject related fields or alternative title).</span></p>
<p><span class="font3">Users can check all the features on top, collection, and records level on the web interface. Data providers&nbsp;get a clear view of their data, and based on this analysis they can design a data cleaning or data improvement plan.</span></p>
<p><span class="font3">Europeana is working on its new ingestion system which integrates the tool. When a new record-set will&nbsp;arrive, the measuring will run automatically, and the&nbsp;Ingestion Officer can check the quality report.</span></p><h2><a name="bookmark6"></a><span class="font5" style="font-weight:bold;">Further work</span></h2>
<p><span class="font3">We will examine other metrics (e.g. multilinguality, accuracy, information content, timeliness), and check&nbsp;known metadata anti-patterns. We plan to compare&nbsp;the scores with experts' evaluation and with usage&nbsp;data and to implement related W3C standards: Shapes&nbsp;Constraint Language (Knublauch and Kontokostas,&nbsp;2016), and Data Quality Vocabulary (Albertoni and&nbsp;Isaac, 2016).</span></p><h2><a name="bookmark7"></a><span class="font5" style="font-weight:bold;">Conclusion</span></h2>
<p><span class="font3">In the research we re-thought the relationship between functionality and the metadata schema, implemented a framework which proved to be successful in measuring structural features which correlate with&nbsp;metadata issues, and we were able to select low and&nbsp;high quality records. We remarkably extended the volume of the analyzed records by introducing big data</span></p>
<p><span class="font3">tools, which were not mentioned previously in the literature.</span></p>
<p><span class="font3">I showed my research in case of a particular dataset and data schema but the method I follow based on generalized algorithms, so it is applicable to other data&nbsp;schema. Several DH researches based on schema defined cultural databases, and in those cases the research process could be improving by finding the weak&nbsp;points of the sources.</span></p><h2><a name="bookmark8"></a><span class="font5" style="font-weight:bold;">Acknowledgements</span></h2>
<p><span class="font3">I would like to thank all of the members of the Euro-peana Data Quality Committee.</span></p><h2><a name="bookmark9"></a><span class="font5" style="font-weight:bold;">Bibliography</span></h2>
<p><span class="font2" style="font-weight:bold;">Dangerfield, M-C. et al. </span><span class="font2">(2015). “Report and Recommendations from the Task Force on Metadata Quality.”</span></p>
<p><span class="font2">(</span><span class="font2" style="text-decoration:underline;"><a href="http://pro.europeana.eu/files/Europeana_Profes-">http://pro.europeana.eu/files/Europeana Profes-</a></span></p>
<p><span class="font2" style="text-decoration:underline;">sional/Publications/Metadata%20Quality%20Re-</span></p>
<p><span class="font2" style="text-decoration:underline;">port.pdf</span><span class="font2">)</span></p>
<p><span class="font2" style="font-weight:bold;">Bruce, T. R. and Hillmann, D. I. </span><span class="font2">(2004). “The Continuum of</span></p>
<p><span class="font2">Metadata Quality: Defining, Expressing, Exploiting.” In</span></p>
<p><span class="font2">Hillman, D. and Westbrooks E. (eds), </span><span class="font2" style="font-style:italic;">Metadata in Practice</span><span class="font2">, Chicago, ALA Editions, 2004.</span></p>
<p><span class="font2" style="font-weight:bold;">Stvilia, B., Gasser, L., Twidale, M. B. and Smith, L. C.</span></p>
<p><span class="font2">(2007). “A framework for information quality assessment.” </span><span class="font2" style="font-style:italic;">Journal of the American Society for Information Science and Technology</span><span class="font2">, 58(12): 1720-1733.</span></p>
<p><span class="font2" style="font-weight:bold;">Ochoa, X. and Duval, E. </span><span class="font2">(2009). “Automatic evaluation of metadata quality in digital repositories.” </span><span class="font2" style="font-style:italic;">International&nbsp;Journal of Digital Libraries</span><span class="font2">, 10: 67-91.</span></p>
<p><span class="font2" style="font-weight:bold;">Harper, C. </span><span class="font2">(2016). “Metadata Analytics, Visualization, and Optimization: Experiments in statistical analysis of the</span></p>
<p><span class="font2">Digital Public Library of America (DPLA).” </span><span class="font2" style="font-style:italic;">The</span></p>
<p><span class="font2" style="font-style:italic;">Code4Lib Journal,</span><span class="font2"> 33 (</span><span class="font2" style="text-decoration:underline;"><a href="http://joumal.code4lib.org/arti-">http://joumal.code4lib.org/arti-</a></span></p>
<p><span class="font2" style="text-decoration:underline;">cles/11752)</span></p>
<p><span class="font2" style="font-weight:bold;">Knublauch, H. and Kontokostas, D. </span><span class="font2">(eds.) (2016). “Shapes</span></p>
<p><span class="font2">Constraint Language (SHACL). W3C Working Draft 14 August 2016.” (</span><span class="font2" style="text-decoration:underline;"><a href="https://www.w3.org/TR/shacl/">https://www.w3.org/TR/shacl/</span><span class="font2"></a>)</span></p>
<p><span class="font2" style="font-weight:bold;">Albertoni, R. and Isaac, A. </span><span class="font2">(eds.) (2016). “Data on the Web</span></p>
<p><span class="font2">Best Practices: Data Quality Vocabulary. W3C Working Group Note 30 August 2016”&nbsp;(</span><span class="font2" style="text-decoration:underline;"><a href="https://www.w3.org/TR/vocab-dqv/">https://www.w3.org/TR/vocab-dqv/</span><span class="font2"></a>)</span></p>
</body>
</html>