<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><meta name="generator" content="ABBYY FineReader 14"/><title>Microsoft Word - 312. Lassner-Generative Model For Latent Reasons For Modifications-312.docx</title><link rel="stylesheet" href="312_files/312.css" type="text/css"/>
</head>
<body><h1><a name="caption1"></a><a name="bookmark0"></a><span class="font3" style="font-weight:bold;">Generative Model For Latent Reasons For Modifications</span></h1>
<p><span class="font7" style="font-weight:bold;">David Lassner</span></p>
<p><span class="font7"><a href="mailto:davidlassner@mailbox.tu-berlin.de">davidlassner@mailbox.tu-berlin.de</a> TU Berlin, Germany</span></p><h2><a name="bookmark1"></a><span class="font1" style="font-weight:bold;">Problem</span></h2>
<p><span class="font7">The idea that writing makes its way from the authors first draft manuscript to the intended reader&nbsp;without any detours or modifications is often&nbsp;inaccurate and oversimplified. In general, the author&nbsp;or a close person performs corrections and stylistic&nbsp;modifications in subsequent iterations. Additionally,&nbsp;there may be an editor or even an official censor who&nbsp;perform censorship of too private or too extreme parts&nbsp;of the document. The different versions of a document&nbsp;generated by these correction layers often become&nbsp;intransparent in printed versions of the document,&nbsp;while manuscripts are more likely to display traces of&nbsp;how the document has been modified to its current&nbsp;state. The digital scholarly edition “Letters and texts.&nbsp;Intellectual Berlin around 1800” (Baillot, 2016, IB in&nbsp;the following) combines genetic edition and entity&nbsp;annotation. The corpus encompasses literary and&nbsp;scholarly testimonies by a group of people, who&nbsp;influenced the intellectual Berlin between&nbsp;Enlightenment and Romanticism. The genetic&nbsp;encoding gives precise information regarding&nbsp;deletions and additions in the manuscript text.&nbsp;However, the reason for these modifications is not&nbsp;encoded. Three main domains for reasons why to&nbsp;modify such a document as a letter in the intellectual&nbsp;context of the time around 1800 have been identified:</span></p>
<p><span class="font7">1. &nbsp;&nbsp;&nbsp;Correction of mistakes</span></p>
<p><span class="font7">2. &nbsp;&nbsp;&nbsp;Stylistic modification</span></p>
<p><span class="font7">3. &nbsp;&nbsp;&nbsp;Moral censorship based on the topic</span></p>
<p><span class="font7">This paper proposes an unsupervised machine learning approach, which assigns the according reason&nbsp;to every modification. The proposed method focuses&nbsp;on dealing with stylistic modifications and moral&nbsp;censorships. I am aiming to increase the accessibility&nbsp;to manuscripts, by providing a structure for the&nbsp;modifications and to assist in evaluation of certain&nbsp;modifications. Furthermore the proposed method may&nbsp;be applied on different editorial problems, which I will&nbsp;discuss in the Outlook section.</span></p><h2><a name="bookmark2"></a><span class="font1" style="font-weight:bold;">Method</span></h2>
<p><span class="font7">As brought up in the Problem section, the proposed method focuses on stylistic and moral censorship&nbsp;reasons, based on the assumption that these two types&nbsp;of reasons relate to the topic of the modification. I&nbsp;convey a generative topic model, that is based on&nbsp;Latent Dirichlet Allocation (D. Blei, Ng, &amp; Jordan, 2003)&nbsp;and is able to take into account the structural&nbsp;information of modifications. There exists a wide&nbsp;range of topic models that customize LDA and many of&nbsp;these take into account additional structural&nbsp;information. To replace the Bag-of-words approach by&nbsp;introducing structural information about the word&nbsp;order is a major field of LDA research (Gruber, Rosen-Zvi, &amp; Weiss, 2007; Wallach, 2006). Moreover there&nbsp;exists a lot of research on topic hierarchies (D. M. Blei,&nbsp;Griffiths, &amp; Jordan, 2010; Paisley, Wang, Blei, &amp; Jordan,&nbsp;2015). LDA has also been modified to work with&nbsp;graph-structured documents (Xuan, Lu, Zhang, &amp; Luo,&nbsp;2015). However I am not aware of any literature that&nbsp;shows how to model modification reasons in a corpus&nbsp;of natural language.</span></p>
<p><span class="font7">Figure 1 illustrates the conceptual functioning of the method from left to right. As input on the left, a&nbsp;collection of documents is given. The documents have&nbsp;parts marked as modified. The generative model in the&nbsp;center infers reasons by taking into account all text,&nbsp;inside and outside the modifications. Every reason&nbsp;may stand for a stylistic, or a certain moral censorship&nbsp;reason (e.g. political, religious). On the right side, the&nbsp;model outputs a reason-modification assignment.</span></p><div><img src="312_files/312-1.jpg" style="width:295pt;height:93pt;"/>
<p><span class="font0">Figure 1: The generative model in the center receives input documents with modifications. It outputs reasons for&nbsp;modification and a reason assignment to each modification</span></p></div><br clear="all"/>
<p><span class="font7">In addition to the LDA latent variables, I introduce a topic-reason variable </span><span class="font7" style="font-variant:small-caps;">y,</span><span class="font7"> a word-reason-modification&nbsp;tendency X and a token-reason assignment r. The&nbsp;complete model in plate notation is shown in Figure 2.&nbsp;c (observed) models whether a token has been&nbsp;modified. For every topic, </span><span class="font7" style="font-variant:small-caps;">y</span><span class="font7"> holds a distribution over&nbsp;reasons, which may cause a modification. For most&nbsp;modifications this distribution should be sparse, for&nbsp;example if a censor crosses out a sentence that&nbsp;discusses the financial situation of the author, the the&nbsp;topic and the reason for censorship would be identical.&nbsp;On the contrary a stylistic modification wouldn't&nbsp;always have one or two clear corresponding topics.</span></p><img src="312_files/312-2.jpg" style="width:242pt;height:157pt;"/>
<p><span class="font0">Figure 2: Plate notation of the model. The left four circled variables represent LDA, the right ones the modification part</span></p>
<p><span class="font7">For every token and every reason, À holds a distribution over two states, namely whether the&nbsp;token tends to be modified for this reason. There may&nbsp;be token, that are representative for a topic, but they&nbsp;nonetheless do not tend to be modified. The&nbsp;categorical variable </span><span class="font7" style="font-style:italic;">r</span><span class="font7"> represents the reason&nbsp;assignment at that position.</span></p>
<p><span class="font7">The latent variables can be iteratively approximated using Variational Inference (Bishop,&nbsp;2006; Zhao, 2013).</span></p><h2><a name="bookmark3"></a><span class="font1" style="font-weight:bold;">Intermediate results</span></h2>
<p><span class="font7">In this section, evaluation methods on toy data are discussed and characteristics of the IB data set, as well&nbsp;as preparation steps and first intermediate results are&nbsp;presented.</span></p>
<p><span class="font1">Toy data</span></p>
<p><span class="font7">To evaluate the characteristics of this method, experiments with artificial toy data can be performed.&nbsp;The generative model described above can be&nbsp;employed for inference as well as for generating&nbsp;artificial documents with modifications. A typical&nbsp;experiment to evaluate a generative model is&nbsp;conceived as follows:</span></p>
<p><span class="font7">1. &nbsp;&nbsp;&nbsp;Initialize the latent variables of the model&nbsp;randomly</span></p>
<p><span class="font7">2. &nbsp;&nbsp;&nbsp;Generate documents with modifications</span></p>
<p><span class="font7">3. &nbsp;&nbsp;&nbsp;Re-initialize the latent variables</span></p>
<p><span class="font7">4. Try to infer the latent variables from the&nbsp;generated documents</span></p><img src="312_files/312-3.jpg" style="width:242pt;height:169pt;"/>
<p><span class="font0">Figure 3: Experiment with 585 generated corpora varying the size from 10.000 to 90.000 token and performing&nbsp;inference for z, r, 0 und y, leaving the remaining model</span></p>
<p><span class="font0">parameters fixed</span></p>
<p><span class="font7">I have performed a series of experiments to investigate the sensitivity of the model to corpus size.&nbsp;As expected, the accuracy of the model increases with&nbsp;an increasing amount of data. Figure 3 shows a&nbsp;decreasing distance between the true value and the&nbsp;expected one, when increasing the size of the data set.&nbsp;The accuracy does also largely depend on the sparsity&nbsp;of the concentration factors, which means that in order&nbsp;to predict the minimal size a real data set should have,&nbsp;one has to come up with according prior concentration&nbsp;factors for the Dirichlet variables.</span></p>
<p><span class="font1">IB data set</span></p>
<p><span class="font7">To apply this method on the IB data set, some preprocessing steps are necessary. Apart from&nbsp;standard natural language preprocessing, one has to&nbsp;filter out all corrections of mistakes.</span></p>
<p><span class="font7">Table 1 shows the change of data set characteristics that are caused by the pre-processing. A lot of&nbsp;modifications have been considered to be corrections&nbsp;of mistakes and thus have been filtered out.</span></p>
<p><span class="font7">The visualization in Figure 4 reveals a great variety in the structure of the modifications. The figure shows&nbsp;the state of all tokens of two letters from the IB data&nbsp;set. The upper letter contains a lot of small changes,&nbsp;where often a green (added part) and red (deleted&nbsp;part) occur as a combination. The letter below&nbsp;contains a lot of longer deleted parts, concluding, that&nbsp;the letter above contains corrections of mistakes,&nbsp;whereby the lower contains modifications related to&nbsp;the topic.</span></p><img src="312_files/312-4.jpg" style="width:243pt;height:69pt;"/>
<p><span class="font0">Figure 4: Above dark grey divider: Letter 4, Chamisso to de La Foye contains small corrections. Below: Letter 14,&nbsp;Dorothea Tieck to Uechtritz contains larger modifications.&nbsp;Deletions (red), additions (green)</span></p>
<table border="1">
<tr><td rowspan="2">
<p></p></td><td colspan="2">
<p></p></td><td>
<p></p></td><td>
<p></p></td></tr>
<tr><td colspan="2">
<p></p></td><td>
<p></p></td><td>
<p></p></td></tr>
<tr><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font5">H</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">J</span></p></td><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font4">1</span><span class="font2">II III 1.....II............[......II.....</span></p></td></tr>
</table>
<p><span class="font0">Figure 5: 93% of the modifications are shorter than 6 token. Two outliers of length 349 and 470 have not been included&nbsp;in the visualization</span></p>
<p><span class="font7">The size of the modifications seems to be a criterion to distinguish between corrections and topic related&nbsp;modifications. The distribution over the length of&nbsp;modifications however, reveals, that a lot of the&nbsp;modifications in the data set are small, thus likely to be&nbsp;corrections of mistakes (Figure 5).</span></p>
<p><span class="font7">The first preliminary experiments have been carried out with a binary setup: The model should&nbsp;distinguish between a) one particular moral&nbsp;censorship reason and b) everything else. To do so,&nbsp;prior knowledge about the topics has been introduced&nbsp;to the model in form of a keyword.</span></p>
<p><span class="font7">For example the topic sickness has been introduced to the model by the keyword “Krankheit”. The first&nbsp;results on this look very promising, as they reveal a&nbsp;precision = 1 and a recall = 0,67.</span></p><h2><a name="bookmark4"></a><span class="font1" style="font-weight:bold;">Outlook</span></h2>
<p><span class="font7">In the near future, I will undertake further experiments with the IB data set. To do so, I will&nbsp;incrementally increase the number of modification&nbsp;reasons. The results will be made accessible as part of&nbsp;the IB corpus, making the permeability between</span></p>
<p><span class="font7">editorial and algorithmic work more visible and accessible to all interested DH communities for reuse.</span></p>
<p><span class="font7">In a further step, I would like to look into different applications of this method. A promising idea would&nbsp;be, to look into different editions of the same text and&nbsp;consider each difference as a modification.&nbsp;</span><span class="font1" style="font-weight:bold;">Bibliography</span></p>
<p><span class="font6" style="font-weight:bold;">Baillot, A. </span><span class="font6">(Ed.). (2016). </span><span class="font6" style="font-style:italic;">Letters and texts. Intellectual Berlin around 1800.</span><span class="font6"> Berlin: Humboldt-Universität zu Berlin.&nbsp;Retrieved from <a href="http://www.berliner-intellektuelle.eu/">http://www.berliner-intellektuelle.eu/</a>.&nbsp;Please visit the web page for an up-to-date version.</span></p>
<p><span class="font6" style="font-weight:bold;">Bishop, C. M. </span><span class="font6">(2006). </span><span class="font6" style="font-style:italic;">Pattern Recognition and Machine Learning (Information Science and Statistics).</span><span class="font6"> Springer&nbsp;Science+Business Media, LLC.</span></p>
<p><span class="font6" style="font-weight:bold;">Blei, D. M., Griffiths, T. L., &amp; Jordan, M. I. </span><span class="font6">(2010). </span><span class="font6" style="font-style:italic;">The</span></p>
<p><span class="font6" style="font-style:italic;">Nested Chinese Restaurant Process and Bayesian Nonparametric Inference of Topic Hierarchies. J.</span><span class="font6"> ACM,&nbsp;57(2), 1-30.</span></p>
<p><span class="font6" style="font-weight:bold;">Blei, D., Ng, A., &amp; Jordan, M. </span><span class="font6">(2003). </span><span class="font6" style="font-style:italic;">Latent Dirichlet allocation.</span><span class="font6"> JMLR.</span></p>
<p><span class="font6" style="font-weight:bold;">Gruber, A., Rosen-Zvi, M., &amp; Weiss, Y. </span><span class="font6">(2007). Hidden Topic Markov Models. In </span><span class="font6" style="font-style:italic;">Proceedings of the Eleventh&nbsp;International Conference on Artificial Intelligence and&nbsp;Statistics.</span><span class="font6"> JMLR.</span></p>
<p><span class="font6" style="font-weight:bold;">Paisley, J., Wang, C., Blei, D. M., &amp; Jordan, M. I</span><span class="font6">. (2015). Nested hierarchical Dirichlet processes. </span><span class="font6" style="font-style:italic;">IEEE&nbsp;Transactions on Pattern Analysis and Machine&nbsp;Intelligence,</span><span class="font6"> 37(2).</span></p>
<p><span class="font6" style="font-weight:bold;">Wallach, H. </span><span class="font6">(2006). Topic Modeling: Beyond Bag-of-words. In </span><span class="font6" style="font-style:italic;">Proceedings of the 23rd International Conference on&nbsp;Machine Learning.</span><span class="font6"> New York: ACM.</span></p>
<p><span class="font6" style="font-weight:bold;">Xuan, J., Lu, J., Zhang, G., &amp; Luo, X. </span><span class="font6">(2015). Topic model for graph mining. </span><span class="font6" style="font-style:italic;">IEEE Transactions on Cybernetics,</span><span class="font6"> 45(12).</span></p>
<p><span class="font6" style="font-weight:bold;">Zhao, W. X. </span><span class="font6">(2013). </span><span class="font6" style="font-style:italic;">Varitional Methods for Latent Dirichlet Allocation.</span><span class="font6">&nbsp;&nbsp;&nbsp;&nbsp;Retrieved&nbsp;&nbsp;&nbsp;&nbsp;from</span></p>
<p><span class="font6"><a href="http://net.pku.edu.cn/~zhaoxin/vEMLDA.pdf">http://net.pku.edu.cn/~zhaoxin/vEMLDA.pdf</a> (accessed on 1st of November 2016)</span></p>
</body>
</html>