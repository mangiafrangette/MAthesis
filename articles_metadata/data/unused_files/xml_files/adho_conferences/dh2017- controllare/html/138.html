<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><meta name="generator" content="ABBYY FineReader 14"/><title>Microsoft Word - 138. Meunier-Computer-Assisted Conceptual Analysis of Textual Data as Applied-138.docx</title><link rel="stylesheet" href="138_files/138.css" type="text/css"/>
</head>
<body>
<p><span class="font2" style="font-weight:bold;">Computer-Assisted</span></p>
<p><span class="font2" style="font-weight:bold;">Conceptual Analysis of</span></p>
<p><span class="font2" style="font-weight:bold;">Textual Data as Applied to</span></p>
<p><span class="font2" style="font-weight:bold;">Philosophical Corpuses</span></p><h2><a name="caption1"></a><a name="bookmark0"></a><span class="font4" style="font-weight:bold;">Jean Guy Meunier</span></h2>
<p><span class="font4"><a href="mailto:meunier.jg@gmail.com">meunier.jg@gmail.com</a></span></p>
<p><span class="font4">Université du Québec à Montréal, Canada</span></p>
<p><span class="font4" style="font-weight:bold;">Louis Chartrand</span></p>
<p><span class="font4"><a href="mailto:lochartrand@gmail.com">lochartrand@gmail.com</a></span></p>
<p><span class="font4">Université du Québec à Montréal, Canada</span></p>
<p><span class="font4" style="font-weight:bold;">Mathieu Valette</span></p>
<p><span class="font4"><a href="mailto:mvalette@inalco.fr">mvalette@inalco.fr</a></span></p>
<p><span class="font4">National Institute for Oriental Languages &amp; Civilizations France</span></p>
<p><span class="font4" style="font-weight:bold;">Jackie Chi Kit Cheung</span></p>
<p><span class="font4"><a href="mailto:jcheung@cs.mcgill.ca">jcheung@cs.mcgill.ca</a></span></p>
<p><span class="font4">McGill University, Canada</span></p>
<p><span class="font4" style="font-weight:bold;">Marie-Noëlle Bayle</span></p>
<p><span class="font4"><a href="mailto:mrcal.bayle@gmail.com">mrcal.bayle@gmail.com</a></span></p>
<p><span class="font4">Université du Québec à Montréal, Canada</span></p><h2><a name="bookmark1"></a><span class="font0" style="font-weight:bold;">Overview</span></h2><h1><a name="caption2"></a><a name="bookmark2"></a><span class="font1" style="font-style:italic;">Jean Guy Meunier and Louis Chartrand</span></h1>
<p><span class="font4">Many DH projects call upon computer tools for descriptive and analytic purposes: lexicon statistics, concordances, parsers, descriptive statistics, classifications, topic modeling, annotations, automatic summarization, visualization tools, etc. They have been mainly applied to literary, political, journalistic, or otherwise mediatic corpuses. However, less work has&nbsp;been done on philosophical corpuses, or corpuses that&nbsp;have been tailored for the ends of philosophical investigation.</span></p>
<p><span class="font4">Computational approaches specialized for highly theoretical and abstract texts have proved their efficiency in various domains, particularly in those pertaining to the encoding, curation and presentation of&nbsp;textual data (e.g. digitization, web publishing, authorship analyses, etc.). These successes open avenues for&nbsp;more complex tools and methods to study linguistic&nbsp;features which are not directly observable, such as&nbsp;narratives, themes and concepts.</span></p>
<p><span class="font4">Concepts, in particular, constitute a key issue, given, on one hand, the polysemy of the concept of concept,&nbsp;and, on the other hand, its widespread use in philosophy and other disciplines. A conceptual analysis is a&nbsp;process through which we decompose and thus elucidate the meaning of a concept. While it is traditionally&nbsp;practiced from the armchair, concepts' meanings are&nbsp;reflected in the texts where they are expressed. As&nbsp;such, the development of methods and approaches to&nbsp;computer-assisted conceptual analysis of texts&nbsp;(CACAT) has the potential to make conceptual analysis&nbsp;more precise, more reliable, more exhaustive and&nbsp;more inclusive.</span></p>
<p><span class="font4">These new challenges call for an appropriation of modern computational tools which have demonstrated their potential at discovering such entities for&nbsp;natural language processing. The last two decades&nbsp;have seen important developments in computational&nbsp;linguistics and in machine learning which have enabled researchers to detect and manipulate various&nbsp;complex features of textual data. Complex objects such&nbsp;as entities, events, topics, arguments, or syntactic and&nbsp;discourse relations can now be detected and studied.&nbsp;Progress in modelling and learning approaches from&nbsp;fields like probabilistic modeling and neural networks&nbsp;have made it possible to represent complex representations between various latent and explicit textual features, and to learn them in efficient ways. Because they&nbsp;capture different aspects of concepts, including those&nbsp;which appear to be latent or implicit, the innovations&nbsp;could open new and exciting horizons for conceptual&nbsp;analysis.</span></p>
<p><span class="font4">In order to exploit this potential, digital humanists must participate in the conception of the aforementioned innovations. The humanities have problems&nbsp;and conceptual tools of their own, which differ from&nbsp;those of computer scientists, and, as such, ought to be&nbsp;enunciated and translated into tasks for algorithms to&nbsp;fulfill. On the other hand, computational approaches to&nbsp;conceptual analysis pose specific problems, which&nbsp;must be addressed as new methods are developed: indeterminacy of the nature of a concept and of the&nbsp;method of analysis, complexity in the relation between&nbsp;and among concepts, diversity of interpretations, incertitude in the evaluation schemes, limited set of&nbsp;computation tools to explore conceptual structures,&nbsp;shallowness of visualization tools, etc. These difficulties call for work from social scientists and humanists.</span></p>
<p><span class="font4">Our aim in this session is to give an overview of the challenges, avenues and opportunities that are shaping CACAT's development. Each paper plays a specific&nbsp;role within this session, so that information in one paper may serve as context for another. Jean-Guy&nbsp;Meunier's paper reviews the challenges facing CACAT,&nbsp;serving as context for the papers that follow. Mathieu&nbsp;Valette's paper (which will be moved to second place)&nbsp;expands on this topic. Drawing on the study of concept&nbsp;formation in philosophy of science, he offers a criticism on the concept of concept, proposes an alternative and draws the implications for the relationship between concepts and their concrete expressions in&nbsp;texts. In working to bridge the gap between the philosophical conceptual analysis in texts and modern techniques of computational linguistics, these two papers&nbsp;work from the conceptual analysis pole. The following&nbsp;two papers, by Louis Chartrand and Jackie Cheung,&nbsp;work from the computational linguistic pole, presenting models and techniques which have the potential of</span></p>
<p><span class="font4">addressing the challenges of CACAT. The final paper, by</span></p>
<p><span class="font4">Marie-Noelle Bayle, is a recent application of CACAT that exemplifies its potential in discovering implicit dimensions to a concept.</span></p>
<p><span class="font0" style="font-weight:bold;">Modelling Computer assisted conceptual analysis in text (CACAT)</span></p><h1><a name="bookmark3"></a><span class="font1" style="font-style:italic;">Jean-Guy Meunier</span></h1>
<p><span class="font4" style="font-weight:bold;">Conceptual analysis paradigms</span></p>
<p><span class="font4">In many fields of scientific research, be they social sciences, natural sciences or even professional practices, abstract or highly theoretical concepts are explored to discover their content and deepen the&nbsp;knowledge they embed. However, there is no consensus on the nature of a concept or on the methodology&nbsp;to analyze them. For example, how would one proceed&nbsp;in analyzing the concept of Evolution in Darwin's writings? Three radically different paradigms parameterize the methodology: philosophical, linguistic and cognitive.</span></p>
<p><span class="font4">In the philosophical paradigm, concepts are identified to the meaning of predicative words. For some, their analysis aims at finding the conditions (necessary, sufficient, fuzzy, etc.) under which these words&nbsp;refer to objects, events or actions in a possible or actual world. For others, analysis consists mainly of identifying the sense or intention of these words as related&nbsp;to the epistemic or metaphysical conditions for their&nbsp;understanding. Finally, for some others, an analysis&nbsp;should consider the use and context (linguistic, social&nbsp;or other) of these words. Hence, in this philosophical&nbsp;paradigm, conceptual analysis becomes a sort of&nbsp;logico-pragmatic analysis of the meaning of words. In&nbsp;our Darwin example, this paradigm would therefore&nbsp;ask what are the meaning conditions of the word evolution when Darwin uses it.</span></p>
<p><span class="font4">In the linguistic paradigm, concepts are also related to the meaning of words. For the Saussurian structuralists a concept is the core meaning embedded in the&nbsp;structure of the signified (le signifie) of words. For the&nbsp;neo-structuralists, the generativists and the cognitiv-ists, a concept is also equated to the semantic content&nbsp;of predicative linguistic expression, and meaning is understood as a complex set of semantic properties (features, relations, frames, nets, etc.,) underlying isolated&nbsp;words or their position in sentences and discourse.&nbsp;Here, conceptual analysis becomes identified with&nbsp;classical semantic analysis of words. In Darwin's&nbsp;works, the analysis would explore the semantics properties of the English word evolution: for instance, it&nbsp;would study its lexical content, its synonyms, its topics,&nbsp;is semantic nets, etc.</span></p>
<p><span class="font4">In the cognitive paradigm, concepts are the results of cognitive or mental operations. For psychology, they&nbsp;are seen as a sort of cognitive categorization. For the&nbsp;analytical and hermeneutic traditions of philosophy of&nbsp;mind, they are mental states or world representations.&nbsp;Conceptual analysis consists then in exploring how semiotic or linguistic forms embed categories, intentions, conceptual spaces, beliefs, mental states, Weltanschauung, etc. Hence conceptual analysis bears resemblance to an exploration of cognitive operations or&nbsp;states: representing, categorizing, reasoning, argu-menting, entailing, etc. In our analysis of Darwin, this&nbsp;cognitive paradigm would focus the analysis on the&nbsp;mental operations underlying the meaning of evolution. How is this category of mental representation acquired, built reasoned on, argued, etc.?</span></p>
<p><span class="font4">Choosing a paradigmatic methodology for analyzing concept is difficult, then, because not one of them is canonical. Conceptual analysis becomes an even&nbsp;more acute problem when computations are introduced in the methodology. The level of complexity of&nbsp;the task is so high that is not obvious how a computer&nbsp;assisted conceptual analysis of text (CACAT) project&nbsp;can be realized. Should it be computer-tool-driven or&nbsp;model-driven?</span></p>
<p><span class="font4" style="font-weight:bold;">Tool driven approaches</span></p>
<p><span class="font4">The first type of approach is tool-driven. Once a methodology inspired by one of the paradigms presented above is chosen, its practitioners use some&nbsp;computer programs already built and inserts them in&nbsp;appropriate moments of the analysis procedure. Many&nbsp;computer tools for this task actually exist.</span></p>
<p><span class="font4">A first set of tools focuses on the lexical expressions of a concept. The most classical ones are concordanc-ers, collocation and lexical analysers, taggers, etc.&nbsp;These tools explore the lexical properties and contexts&nbsp;of one or a few canonical predicates, expressing the&nbsp;specific concept to be analyzed. The limits of these&nbsp;types of tools lie in their underlying design hypothesis:&nbsp;a conceptual content is to be explored through specific&nbsp;canonical expressions. Such a hypothesis restricts the&nbsp;exploration of the conceptual content to one or a specific number of predicates. This is problematic, for as&nbsp;we know, concepts can be expressed in language in a&nbsp;myriad of ways. For example, it would be very problematic to restrict Darwin's concept of evolution to the&nbsp;analysis of the word evolution alone. Secondly, they&nbsp;may produce results that are larger than the original&nbsp;text. This is the case of the concordance of the concept&nbsp;of Esse in the Thomas Indexicus. Finally, sometimes,&nbsp;the opposite happens. These tools may deliver only a&nbsp;fraction of the overall textual segments or word collocations whose content is pertinent. For instance, in&nbsp;Darwin only uses a few dozen times the lexical form&nbsp;evolution. Hence concordance, collocation, etc. on such&nbsp;a small sample are not very fruitful for a conceptual&nbsp;analysis.</span></p>
<p><span class="font4">A second set of tools highly influenced by classical AI approaches focuses on natural language processing&nbsp;(NLP). These tools are sensitive to various meaning aspects of words, such as their semantic definition, their&nbsp;encyclopedic, pragmatic discursive content, etc. They&nbsp;promise to deliver finer results for a conceptual analysis. But these tools also have limits. Their underlying&nbsp;hypothesis is that these semantic, pragmatic and encyclopedic information added in the grammar and the&nbsp;lexicon will enhance the exploration of conceptual&nbsp;content. Unfortunately, the added information has often been collected from common and ordinary semantic knowledge of shared language usages. Such tools&nbsp;will then often tend to identify already known properties belonging to this common information about the&nbsp;lexical conceptual word under inquiry. And most of the&nbsp;time, it will ignore the properties that precisely are the&nbsp;one that are specific to the concepts analyzed mainly&nbsp;when they are original, and belong to a reflexive, creative literary or reflexive discursive process, etc. These&nbsp;semantic properties would not be part of the common&nbsp;doxastic conceptual content. For instance, a philosophy scholar using such types of tools would not be very&nbsp;satisfied in discovering that Darwin's concept of evolution is a name meaning an action of the type change&nbsp;and applied to the object: natural species.</span></p>
<p><span class="font4">Recently, a last set of tools that are more mathematically grounded, such as neural net and Bayesian classification, vector semantics, machine learning, deep learning, etc., have become appealing and are used in&nbsp;language processing, They can process large data and&nbsp;learn semantic information by themselves. But like the&nbsp;other set of tools they have their limits. First, they are&nbsp;nor readily usable. They are in fact very complex algorithms, and are not easily mastered by humanities&nbsp;scholars. Secondly, their lack of traceability becomes a&nbsp;major obstacle when applied to large and theoretical&nbsp;textual data where results become difficult to evaluate.&nbsp;Thirdly, they seem more successful for information retrieval applications than for digging into deep conceptual content. For the moment, we are not sure that how&nbsp;they can effectively assist conceptual analysis.</span></p>
<p><span class="font4">From these remarks, it does not seem to us that conceptual analysis can only be a serendipity tool-driven approach. The results produced by these tools&nbsp;have not yet convinced the scholarly community that&nbsp;practices expert conceptual analysis.</span></p>
<p><span class="font4" style="font-weight:bold;">Model driven approaches</span></p>
<p><span class="font4">The second type of approach is model-driven. Recent philosophers of science such as Morgan and Morrison (1999), Giere, (1999), Leonelli (2007) for instance, see science as a building models process where models are heuristic means for describing, explaining&nbsp;and understanding reality. And Mc Carthy (1999), has&nbsp;seen this modelling approach as a means of better understanding digital humanities interpretative projects.&nbsp;For our part, we explore this hypothesis and see&nbsp;CACAT as type of scientific inquiry where various models are used as intermediaries for understanding the&nbsp;analysis of highly theoretical and abstract concepts. In&nbsp;this perspective, we distinguish four types of models:&nbsp;conceptual, formal, computational and experimental.</span></p>
<p><span class="font4">A conceptual model defines parameters for identifying, explaining and understanding the properties and structure of linguistic items expressing conceptual&nbsp;content. A formal model translates certain aspects of a&nbsp;conceptual model in some controlled formal language&nbsp;that describes or identifies properties and relations of&nbsp;these conceptual expressions. A computational model&nbsp;translates some formal expressions of the formal&nbsp;model into algorithms and programs. Finally, an experimental model designs implementation of these formal&nbsp;models in a concrete computer where the analysis can&nbsp;be simulated and ultimately evaluated in correspondence to the other models.</span></p>
<p><span class="font4">In a concrete procedure, all these models interact and can be modified and adjusted. This allows the inquiry to be controllable and repeatable. It has been our&nbsp;own experience that, if a computer assisted conceptual&nbsp;analysis project is to be successful it must construct at&nbsp;least these four models. A CACAT project cannot bypass these models and their interactions.</span></p>
<p><span class="font4">Designing these models, their interactions and their experimentation to see CACAT as a scientific endeavour and not just computer gadget exploration. But&nbsp;each model is not built easily. And nothing comes&nbsp;smoothly. They are part of the research process. And&nbsp;much work must be done to clarify the conceptual, formal, computational and experimental models pertinent for a successful and pertinent conceptual analysis.</span></p>
<p><span class="font0" style="font-weight:bold;">Digital epistemology for concept analysis</span></p><h1><a name="bookmark4"></a><span class="font1" style="font-style:italic;">Mathieu Valette</span></h1>
<p><span class="font4">In the humanities, theory is most of the time outlined with texts: papers, books, conference presentations, lectures etc. we claim that the scientist is first a reader and a text producer. This textuality is so ordinary that it is almost invisible, and, as such, not considered as an object of science. Moreover, theories are&nbsp;read as synchronic systems, or even achronic systems,&nbsp;depending on their specific purposes (describing one&nbsp;fact, explaining one phenomenon...). Scientists appropriate models and concepts like tools; they have to&nbsp;know their function and how to manipulate them, but&nbsp;they do not care about knowing practical details of&nbsp;their enunciation. In fact, they ignore them, more or&nbsp;less. They find such details embarrassing, because they&nbsp;make concept borders fuzzy: lexicons, glossaries, and&nbsp;also handbooks, as they extract the concepts from&nbsp;their context, and standardise the definitions, creating&nbsp;an illusion of stability and tangibility. But concept tex-tuality necessarily has an incidence, not only on interpretation, but also on theorisation. If the scientist is a&nbsp;text producer, then theorisation is the construction of&nbsp;meaning. Theorisation is forced by enunciation, and&nbsp;scientific works, beyond their materiality, can be considered as text.</span></p>
<p><span class="font4">The textual aspect of scientific works had been noticed by those in Europe looking at epistemological culture. In this respect, French philosopher Michel&nbsp;Foucault's works, in the 1960s, must be acknowledged&nbsp;(see e.g. Foucault 1969). Foucault put in place a philological analysis of discourse centred on the combination and evolution on specific discursive structures.&nbsp;His purpose is, firstly, to recognize “discursive formations”, i.e. stabilized relations, regularities between&nbsp;objects, types of speech act, concepts and topics; and,&nbsp;secondly, to recognise breakpoints in idea system history. Foucault followed the example of some of his famous predecessors, such as Gaston Bachelard, Georges&nbsp;Canguilhem and Martial Gueroult. Bachelard's notion&nbsp;of Epistemological break, or Canguilhem's notion of&nbsp;concept shifts shows, for instance, that the history of a&nbsp;concept is not that of its increasing rationality and refinement, but that of the different fields in which they&nbsp;have been designed and validated. What we will call&nbsp;digital epistemology is a linguistic approach to this&nbsp;style of French epistemology.</span></p>
<p><span class="font4">Our topic is the study of scientific texts using, on the one hand, corpus linguistics tools which have been developed over the 40 last years and, on the other hand,&nbsp;a linguistic methodology (see Rastier 2009, Valette&nbsp;2003). Thus, our purpose is to develop tools and methodology Foucault did not have, among other reasons&nbsp;because some textual phenomena—as, for example,&nbsp;lexicon evolution, which depends on the reader's subjectivity—are invisible to a classical philological analysis. Concept emergence, concepts' individual and inter-related evolutions, the appropriation of a specific&nbsp;thematic, palinode, etc. constitute further examples.&nbsp;We do not adopt the logician's position, considering&nbsp;that conceptualization is a linguistic phenomenon&nbsp;with its own construction rules linked to a particular&nbsp;function of language. Neither do we ignore the psychological, social and interactional reasons of the development of concepts. Firstly, we consider that textuality—&nbsp;i.e. the constraints of the textual layout, formulations,&nbsp;be they constraints of syntactic, semantic, lexical or related discursive traditions (including genres and&nbsp;speech)—plays a major role in concept formation. Secondly, we do not consider texts only as resources to&nbsp;mine and extract terminological and conceptual material, but as archives, or, in other word, as the objective&nbsp;tracks of the process of creating concepts.</span></p>
<p><span class="font4">In essence, we focus here on concept emergence considered as the result of a slow and gradual stabilisation of contextual semantic feature. Drawing on recent critical readings of Saussure's semiology (see&nbsp;Rastier 2015), we propose to consider a concept as a&nbsp;stabilized semantic form; that is, as a combination of&nbsp;semantic features (or semes) mainly inherited from&nbsp;various contexts in which it has occurred. Eventually,&nbsp;we link concept design with text production rather&nbsp;than identification of items in a general ontology&nbsp;(Valette 2010).</span></p><h2><a name="bookmark5"></a><span class="font0" style="font-weight:bold;">Topic models for conceptual analysis</span></h2><h1><a name="bookmark6"></a><span class="font1" style="font-style:italic;">Louis Chartrand</span></h1>
<p><span class="font4">The last two decades have seen the rise of topic models in natural language processing (NLP). From&nbsp;the early successes of Latent Semantic Analysis, which&nbsp;decomposes datasets into “conceptual” dimensions,&nbsp;the introduction of probabilistic and generative models have enabled the discovery of underlying structures that condition the lexicon of a text. Those structures, in turn, are used to construct meaningful representations of corpuses and documents, and have&nbsp;proven fruitful in improving performances in many&nbsp;NLP tasks.</span></p>
<p><span class="font4">Those tools have interesting potential for the Digital Humanities, as they discover entities which are, on one hand, robust features of textual data, and, on the&nbsp;other hand, easily representable and interpretable by&nbsp;humans. For instance, topics may help in tasks such as&nbsp;categorizing documents or selecting a relevant subcorpus for analysis. However, once topics are represented using the words to which they are likely to be&nbsp;associated, they can also be used to make sense of&nbsp;what a set of textual segments are about, or to visualize&nbsp;the evolution of discourse in a corpus through time. As&nbsp;such, topics have interesting potential when it comes&nbsp;to representing textual data and improving our analyses of it.</span></p>
<p><span class="font4">In this presentation, some prominent topics mod-els—LSA, LDA and DTM—will be presented and contrasted, and their potential uses for Digital Humanities will be discussed.</span></p>
<p><span class="font4" style="font-weight:bold;">Latent Semantic Analysis (LSA)</span></p>
<p><span class="font4">Introduced by Deerwester et al. (1990), LSA used tools from linear algebra, in particular singular vector&nbsp;decomposition, to transform the representation of text&nbsp;segments in the form of word counts to a representation in the form of participation to “concepts”, or semantic dimensions.</span></p>
<p><span class="font4">As words give us a good idea of what a text is about, it is common practice in text mining to represent text&nbsp;segments by counting its words. A document containing “apple”, “orange” and “pear” a high number of times&nbsp;each is likely to talk about fruits. And if multiple documents share the same words, they are likely to share&nbsp;common topics. However, this approach does not fare&nbsp;well with synonyms, which it does not recognize.</span></p>
<p><span class="font4">The LSA uses co-occurrence in word uses to synthesize the word-count representations into more compact semantic dimensions. As synonyms tend to have the same cooccurents, they also tend to participate to&nbsp;the same semantic dimensions. As a result, the new&nbsp;representation is closer to a semantic representation&nbsp;than was the word-count representation, hence the&nbsp;name “Latent Semantic Analysis”.</span></p>
<p><span class="font4" style="font-weight:bold;">Latent Dirichlet Allocation (LDA)</span></p>
<p><span class="font4">While LSA still is part of every NLP reputable toolset, it falls short in at least two key aspects. Firstly, its semantic dimensions are hard to read for a human:&nbsp;from a list of its most prominent words, it is usually&nbsp;hard to give a satisfying interpretation of what a semantic dimension is about (Chang et al. 2009). Secondly, it has no clear hypothesis as to how text is structured. On one hand, this makes it harder to explain semantic dimensions in terms of linguistics, psychology&nbsp;or discourse analysis. On the other, it means that LSA&nbsp;gets only part of the picture, and better algorithms&nbsp;with additional assumptions might produce better semantic dimensions.</span></p>
<p><span class="font4">Latent Dirichlet Allocation (Blei et al. 2003) is a probabilistic models which attempts to address this&nbsp;latter issue, and ends up addressing the former as well.&nbsp;It supposes that in a corpus, there is a certain number&nbsp;of topics, which, when activated, make it more or less&nbsp;likely for specific words to be present. Thus, when&nbsp;someone writes a document, LDA assumes that she selects a certain restricted number of topics, which in&nbsp;term condition which words will be found in the document. Using this assumption and an arbitrary number&nbsp;of topics, the algorithm infers the most likely list of&nbsp;topics, and their most likely assignments to documents.</span></p>
<p><span class="font4">As such, it produces once again a representation of documents or text segments from word counts, but in&nbsp;terms of topics rather than more abstract conceptual&nbsp;dimensions. The words most likely to be present when&nbsp;a given topic is activated are often visibly related, either semantically or because they participate in a&nbsp;transparent narrative. As such, they are easily read by&nbsp;a human interpreter, and can be used to give a sense of&nbsp;what documents, sub-corpuses or textual segments&nbsp;are about.</span></p>
<p><span class="font4" style="font-weight:bold;">Dynamic Topic Models (DTM)</span></p>
<p><span class="font4">Another boon of LDA is that its probabilistic model can be modified account for particularities of the corpus, or to model features that we want to study in particular. For example, if we have a corpus that spans&nbsp;across decades, we might expect topics to evolve with&nbsp;time, as society and culture change.</span></p>
<p><span class="font4">To model this, the algorithm devised by Blei &amp; Lafferty (2006) uses a corpus split in time slices (say, per year) and topics are split accordingly, such that topic 1&nbsp;at time 1 is different from topic 1 at time 2. Then, a&nbsp;Markov assumption is enacted on the time series: topic&nbsp;1 at time 1 conditions topic 1 at time 2, which conditions topic 1 at time 3, etc. This gives topics the freedom to evolve, while enforcing a certain degree of conservatism.</span></p>
<p><span class="font4">Using this, one can not only track topics more efficiently, but also see the evolution of topics across time.</span></p>
<p><span class="font4" style="font-weight:bold;">What is a topic?</span></p>
<p><span class="font4">What is it, however, that we are talking about when we speak of LSA's conceptual dimensions or LDA's topics? Can it be equated with the notion of TOPIC that we&nbsp;encounter in discourse analysis, for instance?</span></p>
<p><span class="font4">While there are a variety of definitions for words such as “topic” and “theme”, most agree that a topic is&nbsp;what a text is about (Rimmon-Kenan, 1995). On this&nbsp;score, LDA's topic does seem to agree with the common&nbsp;notion of TOPIC: a word list representing a LDA topic&nbsp;is read as a representation of what the textual data is&nbsp;about (Blei, 2013). Furthermore, the information the&nbsp;probabilistic model captures is the one that is redundant across a number of text segments. As such, it highlights words and concepts which keep coming back as&nbsp;they put in relation with various entities in sentences.&nbsp;In other words, textual discourse and narratives are&nbsp;being sewn around them.</span></p>
<p><span class="font4">On the other hand, humans tend to make slightly different representations of topics compared to machines (Chang 2010), more readily constructing topics&nbsp;around concepts and thus providing sparser (more&nbsp;compact) representations. As Chang suggests, this&nbsp;might be because humans build these representations&nbsp;using general domain knowledge, whereas topic models try to infer this knowledge from word distributions.&nbsp;This seems to tell us that we should understand LDA&nbsp;topics as indicators, or reconstructed traces, of the topics that underlie a text, but not as true representation&nbsp;of topics themselves.</span></p>
<p><span class="font4" style="font-weight:bold;">Using topic models in conceptual analysis</span></p>
<p><span class="font4">As Chang's 2010 experiment suggest, topics entertain special relation with concepts, as a topic tends to be associated with a restricted number of concepts&nbsp;which are expressed very often in the text.</span></p>
<p><span class="font4">As such, topic models' potential in representing textual data can be exploited to discover associations that are likely to be useful to conceptual analysis and other&nbsp;philosophical analyses. For instance, it can help the analyst identify the most important parts of a corpus, and&nbsp;those that can be discarded. They can also be leveraged&nbsp;to build representations of the contexts in which the&nbsp;concept of interest appears, thus giving a sense of the&nbsp;topics with which it is associated. Using DTM, one can&nbsp;also get a sense of the evolution of a concept within a&nbsp;diachronic corpus. Beyond discovery of new informations concerning a concept's expression in a corpus,&nbsp;topic models can be useful to test some association, as&nbsp;the structures they uncover are relatively robust.</span></p>
<p><span class="font4">That said, as the DTM model shows, topic models can be used in a large variety of use cases, as their&nbsp;model can be expanded to take into account a corpus'&nbsp;metadata and thus open new and innovative avenues&nbsp;for conceptual analysis and the Digital Humanities in&nbsp;general.</span></p><h2><a name="bookmark7"></a><span class="font0" style="font-weight:bold;">Unsupervised natural language processing for conceptual analysis of events</span></h2><h1><a name="bookmark8"></a><span class="font1" style="font-style:italic;">Jackie Chi Kit Cheung</span></h1>
<p><span class="font4">In unsupervised machine learning, an algorithm is trained to discover regularities in data without access&nbsp;to human-provided labels. Such techniques can be useful in conceptual analysis of text, in cases where we do&nbsp;not have or want to impose a schema on the text corpus under analysis. The basic intuition behind unsupervised natural language processing techniques is&nbsp;that objects that appear in similar contexts in the data&nbsp;should be assigned similar representations, such that&nbsp;they can be grouped into clusters.</span></p>
<p><span class="font4">Unsupervised models differ according to several characteristics, from the type of information that is&nbsp;made available to the learner, to how similarity is defined between the different objects that are modeled,&nbsp;to the expected form of the output cluster that is&nbsp;learned. For example, the Latent Dirichlet Allocation&nbsp;(LDA) topic model (Blei et al., 2003) is a probabilistic&nbsp;model which is given access to multiple documents for&nbsp;training. The crucial assumptions behind the LDA&nbsp;model are that each document can be described as a&nbsp;mixture of multiple topics, and each word in a document is generated by one of the topics in that mixture.&nbsp;As a result of training an LDA model, multiple topics&nbsp;are learned, which correspond to clusters of words&nbsp;that tend to co-occur in the same documents.</span></p>
<p><span class="font4">More recently, there have been a number of unsupervised models that have been used to discover the structure of a sequence of entities and events that appear according to some narrative in the natural language processing literature (Chambers and Jurafsky,&nbsp;2008; Cheung and Penn, 2013). This is accomplished&nbsp;by explicitly modelling the sequential dependencies of&nbsp;events as they appear in a document. I will provide an&nbsp;overview of the assumptions of the event structure being learned by such models. For example, some methods produce discrete sequences of prototypical event&nbsp;and participant roles. In the work of Chambers and Ju-rafsky, (2008), narrative chains are learned corresponding to prototypical roles in a narrative. A chain&nbsp;such as _ accused X; X claimed _; X argued _; _ dismissed&nbsp;X might correspond to a defendant in a trial. Other&nbsp;work frame the problem as a task for probabilistic&nbsp;learning. Cheung and Penn (2013) define a probabilistic sequence model, in which the structure of an event&nbsp;and its participants are explicitly represented in the&nbsp;model as latent random variables. The nature of a&nbsp;learned cluster, then, would be how it influences the&nbsp;conditional probabilities of generating other cluster&nbsp;labels, as well as the word emission distributions from&nbsp;that latent topic (as in an LDA model).</span></p>
<p><span class="font4">I will discuss how such models can be used to discover templates of prototypical events, including how events and event participants are typically expressed&nbsp;in language. Such approaches can easily be applied to&nbsp;multiple domains, including texts in the legal or medical genres, because they make minimal assumptions&nbsp;about the structure of events, and do not require training data. I also discuss other applications of these models to information ordering, and automatic summarization, which may be of interest to researchers in conceptual analysis for the digital humanities.</span></p>
<p><span class="font0" style="font-weight:bold;">A computer-assisted analysis of SYMPTOM in psychiatry</span></p><h1><a name="bookmark9"></a><span class="font1" style="font-style:italic;">Marie-Noë</span><span class="font1" style="font-weight:bold;font-style:italic;">l</span><span class="font1" style="font-style:italic;">e Bayle</span></h1>
<p><span class="font4">The Diagnostic and Statistical Manual of Mental Disorders (DSM-5) is a general classification and diagnostic tool used in the rich and diverse universe of&nbsp;mental health. Being widely distributed and available&nbsp;online, it allows everyone to have a direct access on&nbsp;how to make a psychiatric diagnosis. To facilitate its&nbsp;reading, laypeople and professionals alike may consult&nbsp;definitions for important notions in the glossary section. However, these few lines will often fail to capture&nbsp;the complexity of a term. For instance, at the core of&nbsp;the clinical assessment of a disorder lie its signs and&nbsp;symptoms. Therefore, a proper understanding of what&nbsp;a symptom means, and how this concept relates to the&nbsp;disorder, is essential to the diagnostic approach.</span></p>
<p><span class="font4">In the DSM-5 glossary, symptom is defined as “A subjective manifestation of a pathological condition.”</span></p>
<p><span class="font4">In the practice, it is often treated as a necessary and/or sufficient condition for to seek diagnosis, or as a constraint on possible diagnoses. As such, non-doctors&nbsp;and patients often think of a symptom as a singular&nbsp;event, and conceive of it purely in terms of content and&nbsp;a means to a diagnosis.</span></p>
<p><span class="font4">However, as experience of mental disorders is often messier, we may wonder if such a notion of SYMPTOM&nbsp;overlooks important features of the way this notion is&nbsp;actually reflected in the DSM-5. Our hypothesis is that&nbsp;both the glossary definition for this concept and the&nbsp;common understanding of this notion fail to account&nbsp;for such essential aspects. To provide evidence for this&nbsp;claim, we performed a computer-assisted conceptual&nbsp;analysis of text (CACAT) for the concept SYMPTOM in&nbsp;the DSM-5. We find evidence that SYMPTOM is&nbsp;strongly associated to a dimension of temporality, and&nbsp;that it is expressed in relation not only with the disorder, but also remission. As such, it is proposed an improved definition would not only better reflect the&nbsp;content of the DSM-5, but could also contribute in a&nbsp;better understanding of assessment and practice of diagnostic.</span></p><h2><a name="bookmark10"></a><span class="font4" style="font-weight:bold;">Method</span></h2>
<p><span class="font4">The dataset consists in a small corpus composed with the most relevant chapters of DSM-5. Computational Text mining method and manual qualitative approach were used in a processing chain for the conceptual analysis of SYMPTOM. Firstly, extraction of the textual data from noisy sources and cleaning of the text&nbsp;were performed. Secondly, all sentences with the term&nbsp;“symptom” in it, plus one sentence before and after,&nbsp;were extracted, yielding a set of textual segments, on&nbsp;which stemming and lemmatisation were performed.&nbsp;Thirdly, the pretreated data was used to create a document-term matrix with the TF-IDF weighting scheme.&nbsp;Fourthly, from the matrix, textual segment clusters&nbsp;were produced with the k-means algorithm. Fifthly, the&nbsp;most salient words in each cluster and in the whole&nbsp;subcorpus were first represented using word clouds.&nbsp;Finally, relations of similarity between the most relevant words in each cluster and in the subcorpus were&nbsp;represented in a 3d space. All steps were performed&nbsp;using common R modules (tm, RWeka, qdap, cluster,&nbsp;knn, ggplot2, rgl). Each cluster was interpreted as a&nbsp;specific field in which a hypothetical conceptual property of SYMPTOM is expressed. Categorization was&nbsp;done by annotating manually the most typical textual&nbsp;segments in every cluster according to cosine distance&nbsp;to the centroid. Annotations consisted in the main conceptual property of SYMPTOM expressed in a segment.</span></p>
<p><span class="font4">Syntheses of these annotations were done for each cluster.</span></p>
<p><span class="font4" style="font-weight:bold;">Experimentation</span></p>
<p><span class="font4">From the subcorpus, 2036 sentences containing the term “symptom” were extracted which contained 5761&nbsp;different word types. The words most associated with&nbsp;symptom in this subset of the corpus are disord (disorder), criteria, presen (presence), sever (severity),&nbsp;medic (medical, medication). Using k-means, 30 clusters were extracted but 17 are deemed noisy, as they&nbsp;contain 10 textual segments or less. Most of the remaining clusters are located in the section II of the&nbsp;DSM (diagnostic criteria and codes), several having&nbsp;most of their segments in a specific disorder chapter.&nbsp;The converse, however, does not hold.</span></p>
<p><span class="font4" style="font-weight:bold;">Discussion</span></p>
<p><span class="font4">Analysing those clusters reveals that SYMPTOM has a transdiagnostic property, and is not only defined by&nbsp;its specific content. For example, let us examine cluster&nbsp;#8, which contains 98 segments, 90% of which fall in&nbsp;the three chapters about psychotic and mood disorders. Symptom in this cluster is linked with the words&nbsp;depress, hypomania, mania, but also with episode, period, full, meet. Furthermore, annotation of joined documents shows that temporality is a conceptual property of SYMPTOM. It modifies the pathological dimension of its content. SYMPTOM is not in direct causal relation with DISORDER; it is a dynamic sign whose presence needs to be situated in an episode, regardless of&nbsp;whether its content is depressive or manic. Therefore,&nbsp;the mere presence of a symptom is not sufficient for a&nbsp;diagnosis. Conversely, SYMPTOM is also linked with&nbsp;the concept of (partial) remission. SYMPTOM and DISORDER have a complex relationship on a continuum&nbsp;between negative (remission) and positive (disease)&nbsp;poles.</span></p>
<p><span class="font4">In conclusion, a mixed method, combining computational and manual processing and using quantitative and qualitative approaches, was applied in our conceptual analysis of the concept SYMPTOM. SYMPTOM appears to be a more complex and dynamic concept than&nbsp;patients and other non-doctors usually understand it&nbsp;to be. As a result, a better understanding of this complexity would likely profit assessment, diagnosis and&nbsp;treatment of mental disorders.</span></p><h2><a name="bookmark11"></a><span class="font0" style="font-weight:bold;">Bibliography</span></h2>
<p><span class="font3" style="font-weight:bold;">American Psychiatric Association, </span><span class="font3">(2013). Diagnostic and Statistical Manual of Mental Disorders, 5th ed. Washington, DC, American Psychiatric Association.</span></p>
<p><span class="font3" style="font-weight:bold;">Bachelard, G. </span><span class="font3">(1938) La formation de l'esprit scientifique.</span></p>
<p><span class="font3">Contribution a une psychanalyse de la connaissance objective. Vrin, Paris; The Formation of the Scientific Mind. Clinamen, Bolton, 2002.</span></p>
<p><span class="font3" style="font-weight:bold;">Blei, D. M. </span><span class="font3">(2012) Probabilistic Topic Models. Communications of the ACM 55, 4:77.</span></p>
<p><span class="font3" style="font-weight:bold;">Blei, D. M. </span><span class="font3">(2013) “Topic Modeling and Digital Humanities.” Journal of Digital Humanities, April 8, 2013.</span></p>
<p><span class="font3" style="font-weight:bold;">Blei, D. M., and Lafferty, J. D. </span><span class="font3">(2006) Dynamic Topic Models. In Proceedings of the 23rd International Conference on&nbsp;Machine Learning, 113-120. ACM.</span></p>
<p><span class="font3" style="font-weight:bold;">Blei, D. M., Ng, A. Y. and Jordan, M. I. </span><span class="font3">(2003) Latent Di-richlet Allocation. The Journal of Machine Learning Research 3: 993-1022.</span></p>
<p><span class="font3" style="font-weight:bold;">Chambers, N., and Jurafsky, D. </span><span class="font3">(2008) Unsupervised Learning of Narrative Event Chains. ACL, pp. 789-797.&nbsp;Cheung, J. C. K., and Penn, G. (2013) Probabilistic Domain</span></p>
<p><span class="font3">Modelling With Contextualized Distributional Semantic Vectors. ACL, pp. 392-491.</span></p>
<p><span class="font3" style="font-weight:bold;">Chang, J. </span><span class="font3">(2010) Not-so-Latent Dirichlet Allocation: Collapsed Gibbs Sampling Using Human Judgments.” In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical&nbsp;Turk, 131-138. CSLDAMT '10. Association for Computational Linguistics, Stroudsburg, PA, USA.</span></p>
<p><span class="font3" style="font-weight:bold;">Chang, J., Gerrish, S., Wang, C., Boyd-graber, J. L. and Blei, D.M. </span><span class="font3">(2009). “Reading Tea Leaves: How Humans Interpret Topic Models.” In Advances in Neural Information&nbsp;Processing Systems, pp. 288-296.</span></p>
<p><span class="font3" style="font-weight:bold;">Chartier, J. F., and Meunier, J. G. </span><span class="font3">(2011). Text Mining Methods for Social Representation Analysis in Large Corpora. Papers on Social Representations, 20: 37.1-37.46.</span></p>
<p><span class="font3" style="font-weight:bold;">Danis J. and Meunier, J. G. </span><span class="font3">(2012). CARCAT : Computer-Assisted Reading and Conceptual Analysis of Texts : An experiment applied to the concept of evolution in the work of Henri Bergson. Digital Studies/Le champ numerique,&nbsp;3(1).</span></p>
<p><span class="font3" style="font-weight:bold;">Deerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas, G. W., and Harshman, R.A. </span><span class="font3">(1990) Indexing by Latent Semantic Analysis. JASIS 41, 6: 391-407.</span></p>
<p><span class="font3" style="font-weight:bold;">Foucault, M. </span><span class="font3">(1969) L'archeologie du savoir. Gallimard, Paris; The Archaeology of Knowledge (1969), Routledge,&nbsp;1972.</span></p>
<p><span class="font3" style="font-weight:bold;">Giere, R., </span><span class="font3">(1999), Using Models to Represent Reality, in: L. Magnani, N. Nersessian and P. Thagard, (eds.), ModelBased Reasoning in Scientific Discovery. Plenum Publishers, New York, pp. 41-57.</span></p>
<p><span class="font3" style="font-weight:bold;">Hofmann, T. </span><span class="font3">(1999) Probabilistic Latent Semantic Analysis. In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, 289-296. Morgan Kaufmann Publishers Inc.</span></p>
<p><span class="font3" style="font-weight:bold;">Leonelli, S. </span><span class="font3">(2007) What is in a Model? Combining Theoretical and Material Models to Develop Intelligible, Modeling Biology: Structures, Behaviors, Evolution. In Manfred</span></p>
<p><span class="font3">Dietrich Laubichler, Gerd B. Mulle (eds.) Modeling Biology. MIT Press.</span></p>
<p><span class="font3" style="font-weight:bold;">McCarthy, W. </span><span class="font3">(2004). Humanities Computing. Palgrave MacMillan.</span></p>
<p><span class="font3" style="font-weight:bold;">Morgan, M.S., and Morrison, M. (eds.), </span><span class="font3">(1999), Models as mediators. Perspectives on natural and social science.&nbsp;Cambridge University Press.</span></p>
<p><span class="font3" style="font-weight:bold;">Rastier, F. </span><span class="font3">(2001) Arts et sciences du texte. PUF, Paris.</span></p>
<p><span class="font3" style="font-weight:bold;">Rastier, F. </span><span class="font3">(2015) Saussure au futur. Les belles lettres, Paris.</span></p>
<p><span class="font3" style="font-weight:bold;">Rockwell, G. </span><span class="font3">(2003). What is Text Analysis, Really? Literary and Linguistic Computing, 18(2).</span></p>
<p><span class="font3" style="font-weight:bold;">Salton, G., and McGill, M. </span><span class="font3">(1983). Introduction to Modern Information Retrieval. McGraw-Hill.</span></p>
<p><span class="font3" style="font-weight:bold;">Turenne, N. </span><span class="font3">(2016). Analyse de données textuelles sous R. ISTE Ewditions, coll. Sciences Cognitives. Londres.</span></p>
<p><span class="font3" style="font-weight:bold;">Valette, M. </span><span class="font3">(2003) Conceptualisation and Evolution of Concepts. The example of French Linguist Gustave Guillaume, Academic discourse - multidisciplinary approaches, Kj. Fl0ttum &amp; F. Rastier, eds., Novus Press, Oslo, pp. 55-74.</span></p>
<p><span class="font3" style="font-weight:bold;">Valette, M. </span><span class="font3">(2010) Des textes au concept. Propositions pour une approche textuelle de la conceptualisation », Actes&nbsp;des 21es Journees francophones d’Ingenierie des Connaissances (IC'2010) (8-11 juin 2010), Nîmes Sylvie Des-pres, ed., Publication de l’Ecole des Mines d’Ales, pp. 516.</span></p>
<p><span class="font3" style="font-weight:bold;">Widdows, D. </span><span class="font3">(2004). Geometry and Meaning. CSLI Publications, Center for the Study of Language and Information, Leland Stanford Junior University</span></p>
</body>
</html>