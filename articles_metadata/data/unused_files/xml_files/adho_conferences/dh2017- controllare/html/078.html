<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><meta name="generator" content="ABBYY FineReader 14"/><title>Microsoft Word - 078. Kestemont-Script Identification in Medieval Latin Manuscripts Using Convolutional Neural Networks-078.docx</title><link rel="stylesheet" href="078_files/078.css" type="text/css"/>
</head>
<body>
<p><span class="font4" style="font-weight:bold;">Script Identification in Medieval Latin Manuscripts&nbsp;Using Convolutional Neural&nbsp;Networks</span></p><h1><a name="caption1"></a><a name="bookmark0"></a><span class="font3" style="font-weight:bold;">Mike Kestemont</span></h1>
<p><span class="font3"><a href="mailto:mike.kestemont@gmail.com">mike.kestemont@gmail.com</a></span></p>
<p><span class="font3">University of Antwerp, Belgium</span></p><h1><a name="bookmark1"></a><span class="font3" style="font-weight:bold;">Dominique Stutzmann</span></h1>
<p><span class="font3"><a href="mailto:dominique.stutzmann@irht.cnrs.fr">dominique.stutzmann@irht.cnrs.fr</a></span></p>
<p><span class="font3">Centre National de la Recherche Scientifique, France</span></p><h1><a name="bookmark2"></a><span class="font1" style="font-weight:bold;">Introduction</span></h1>
<p><span class="font3">In paleography, scholars study the history of handwriting, a crucial aspect of book history and manuscript studies. Paleography has traditionally been dominated by expert-based approaches, driven by the&nbsp;opinions of a small group of highly trained individuals.&nbsp;These have acquired a set of expert skills through&nbsp;year-long training, e.g. the ability to date a handwriting or attribute it to specific individuals. This&nbsp;knowledge remains very hard to render explicit, in order to share it with others. Therefore, paleographers&nbsp;are increasingly interested in digital modelling techniques to enhance the creation and dissemination of&nbsp;paleographic knowledge (Stutzmann, 2015). An important task in paleography is the classification of&nbsp;script types, especially now that digital libraries&nbsp;(BVMM, Gallica, e-Codices, Manuscripta Mediaevalia,&nbsp;etc.) are amassing reproductions of medieval manuscripts, often with scarce metadata. Being able to recognize the script type of such historic artefacts is crucial to date, localize or (semi-)automatically transcribe&nbsp;them. This paper focuses on script identification for&nbsp;medieval Latin manuscripts (ca. 500 AD to 1600 AD)&nbsp;and demonstrates the feasibility of a fairly accurate,&nbsp;meaningful automated classification.</span></p>
<p><span class="font3">Medieval script classification was the focus of the recent CLaMM (Classification of Latin Medieval Manuscripts) competition. For this shared task, the organizers released a training data set of 2,000 photographic&nbsp;(greyscale, 300 dpi) reproductions of pages extracted&nbsp;from Latin manuscripts, which were classified into a&nbsp;12 script type classes, including uncial, caroline, textu-alis and humanistic script, but also more difficult to delineate classes such as the cursiva or (semi)hybrida.&nbsp;The participating teams had to submit a standalone&nbsp;application which was able to classify unseen images&nbsp;and estimate the distance between them. The organizers would then apply the submissions to 1,000 resp.&nbsp;2,000 test images (Stutzmann, 2016) and evaluate&nbsp;their performance using various evaluation schemes.&nbsp;Here, we discuss the </span><span class="font3" style="font-style:italic;">DeepScript</span><span class="font3"> submission to the&nbsp;CLaMM competition. The competition's results have&nbsp;been officially been released on 26 Oct. 2016. </span><span class="font3" style="font-style:italic;">Deep-Script</span><span class="font3"> was ranked first on task 2, i.e. the ‘crisp' classification of mixed script images (Cloppet et al., 2016). As&nbsp;the ground truth and results were released too recently, we limit this abstract to a general discussion of&nbsp;the approach; the final version and presentation of this&nbsp;paper will be supplemented with additional information and test results.</span></p>
<p><span class="font3">The </span><span class="font3" style="font-style:italic;">DeepScript</span><span class="font3"> submission builds upon recent advances in Computer Vision, where the use of so-called ‘deep' neural networks has recently led to dramatic&nbsp;breakthroughs in the state of the art of image classification (LeCun et al., 2015). The kind of neural networks applied in Computer Vision are typically convolutional: they slide small ‘filters' (feature detectors)&nbsp;across images to make the network robust to small&nbsp;translations of objects. The networks make use of&nbsp;many ‘layers' of such feature detectors, where the output of one feature detector always feeds into the next&nbsp;one. The use of such a stack of layers is beneficial, because this ‘deep architecture' allows algorithms to&nbsp;model features of an increasing complexity (Bengio et&nbsp;al., 2013): in the first layers of the network, very raw&nbsp;and primitive shapes are detected (‘edges'); it is only&nbsp;at the higher layers in the networks that these primitive features are combined into more complex, abstract visual patterns (e.g. entire faces). These neural&nbsp;networks lie at the basis of e.g. modern face verification algorithms on social media websites such as </span><span class="font3" style="font-style:italic;">Facebook</span><span class="font3">.</span></p>
<p><span class="font3">Neural networks are composed of millions of parameters which have be optimized. For this, the available training data is split out in a set of training images and a smaller set of development images (respectively&nbsp;ca. 1,800 and 200 images): the former is used to optimize the parameters of the network during training,&nbsp;the latter is used to monitor the performance of the&nbsp;network. The use of development data is necessary to&nbsp;avoid ‘overfitting': it is possible for a network to start&nbsp;‘memorizing' the training images, so that it produces&nbsp;perfect predictions for the training data, but is not able&nbsp;any more to generalize properly to new, unseen images. By using a development set, we can stop optimizing the network, if its predictions for the development&nbsp;data do not increase in quality anymore. Only at this&nbsp;stage, the algorithm is evaluated on the actual test images.</span></p>
<p><span class="font3">Modern neural networks are typically trained on hundred thousands of training images. In the field of&nbsp;Cultural Heritage data, a common challenge is that&nbsp;most data sets are much smaller, and CLaMM is no exception, so that the danger of overfitting is much&nbsp;larger. We therefore proceeded as follows: the generous resolution for each training image was downsized&nbsp;by one half. Next, we would select random square&nbsp;crops or patches from the image (150x150 pixels) and&nbsp;train the algorithms on batches of these crops. This approach is blunt, yet innovative, since we make no effort&nbsp;to extract more specific regions of interest from the&nbsp;images, such as individual lines, words or characters.&nbsp;To avoid overfitting, we also applied augmentation:&nbsp;each training crop would be 'distorted' through randomly varying the zoom level, rotation and translation. Introducing such noise in the input is a common&nbsp;strategy to combat overfitting. Below goes an example&nbsp;of such a set of augmented patches for a single manuscript page (Fig. 1).</span></p><img src="078_files/078-1.jpg" style="width:234pt;height:150pt;"/>
<p><span class="font0">Fig. 1: Example of augmented crops for a single manuscript page.</span></p>
<p><span class="font3">After each epoch, we evaluated the performance of the current state of the network through inspecting&nbsp;the classification accuracy on the development images: we randomly selected 30 crops from each image&nbsp;(without augmentation), and calculated the average&nbsp;probability for each output class. The full image was&nbsp;assigned to the class with the highest average probability. The best validation accuracy which we achieved&nbsp;was 91.17%, using a network architecture of 14 layers,&nbsp;inspired by the famous Oxford VGG net (Simonyan et&nbsp;al., 2015). The manual classification of CLaMM images&nbsp;was based on morphological differences and allographs, as defined in standard works on Latin scripts&nbsp;such as (Bischoff, 1986; Derolez, 2003). The confusion&nbsp;matrices in Fig. 2-4 for the actual and the predicted&nbsp;classes in the development and test data illustrate that&nbsp;the confusions generally make sense from a paleographic point of view (the normal textualis letter is for&nbsp;instance often mistaken for the Southern or semi-tex-tualis variant).</span></p><img src="078_files/078-2.jpg" style="width:180pt;height:190pt;"/>
<p><span class="font0">Fig. 2: Confusion matrix for the development data.</span></p><img src="078_files/078-3.jpg" style="width:243pt;height:337pt;"/>
<p><span class="font0">Fig. 3: Classifications for the test data as a confusion matrix (task 1). Horizontal lines: ground-truth; Vertical columns:&nbsp;predictions. Order: 1-Uncial; 2-Half-uncial; 3-Caroline; 4-</span></p>
<p><span class="font0">Humanistic; 5-Humanistic; Cursive; 6-Praegothica; 7-</span></p>
<p><span class="font0">Southern Textualis; 8-Semitextualis 9-Textualis; 10-Hybrida 11-Semihybrida 12-Cursiva.</span></p><img src="078_files/078-4.jpg" style="width:226pt;height:225pt;"/>
<p><span class="font0">Fig. 4: Membership Degree matrices for task 2, on the 999 test images where only one label is attributed to the image.</span></p>
<p><span class="font3">There exist interesting methods to visualize which patterns the trained network is sensitive to. Using the&nbsp;principle of gradient ascent, we start from a random&nbsp;noise image and feed it to one of the filters on the last&nbsp;convolutional layer: during 3,000 iterations we change&nbsp;the image so that it maximizes the activation of this&nbsp;particular filter. In Fig. 3, we show the 25 artificially&nbsp;generated images which yielded the strongest results;&nbsp;clearly, the network picks up relevant patterns. The&nbsp;third image from the left in the first line, for instance,&nbsp;clearly captures the presence of loops in the ascenders&nbsp;of individual characters (e.g. in the ‘b' or ‘h', which is&nbsp;crucial to differentiate between e.g. a textualis and a&nbsp;cursive letter). These visualizations directly tackle the&nbsp;issue of the computational ‘black box' in the Digital Humanities, and espsecially Digital Palaeography&nbsp;(Hassner et al., 2013; Stutzmann et al., 2014). In our&nbsp;paper, we will offer further interpretations and visualizations of our model and confront these with the results from other participants in the CLaMM competition to offer new perspectives on the graphic definition of script classes in traditional paleography.</span></p><img src="078_files/078-5.jpg" style="width:243pt;height:235pt;"/>
<p><span class="font0">Fig. 5: Artificially generated images that maximally activate filters in the final convolutional layer.</span></p><h1><a name="bookmark3"></a><span class="font1" style="font-weight:bold;">Bibliography</span></h1>
<p><span class="font2" style="font-weight:bold;">Bengio, Y., Courville, A. and Vincent, P. </span><span class="font2">(2013). Representation Learning: A Review and New Perspectives.</span></p>
<p><span class="font2" style="font-style:italic;">TPAMI</span><span class="font2" style="font-weight:bold;"> 35</span><span class="font2">:8, 1798-1828.</span></p>
<p><span class="font2" style="font-weight:bold;">Bischoff, B.</span><span class="font2">, </span><span class="font2" style="font-style:italic;">Paläographie des römischen Altertums und des abendländischen Mittelalters</span><span class="font2">, Berlin, 1986.</span></p>
<p><span class="font2" style="font-weight:bold;">Cloppet, F., Eglin, V., Kieu, V. C., Stutzmann, D. and Vincent, N. </span><span class="font2">(2016), ICFHR2016 Competition on the</span></p>
<p><span class="font2">Classification of Medieval Handwritings in Latin Script, </span><span class="font2" style="font-style:italic;">Proceedings of the ICFHR 2016</span><span class="font2">, 590-595.</span></p>
<p><span class="font2" style="font-weight:bold;">Derolez, A.</span><span class="font2">, </span><span class="font2" style="font-style:italic;">The Palaeography of Gothic Manuscript Books from the Twelfth to the Early Sixteenth Century</span><span class="font2">,&nbsp;Cambridge, 2003.</span></p>
<p><span class="font2" style="font-weight:bold;">Hassner, T., Rehbein, M., Stokes, P. and Wolf, L</span><span class="font2">. 2013.</span></p>
<p><span class="font2">Computation and palaeography: Potentials and limits. </span><span class="font2" style="font-style:italic;">Dagstuhl Manifestos</span><span class="font2" style="font-weight:bold;"> 2</span><span class="font2">: 14-35.</span></p>
<p><span class="font2" style="font-weight:bold;">LeCun, Y., Bengio, Y. and Hinton, G. </span><span class="font2">(2015). Deep learning. </span><span class="font2" style="font-style:italic;">Nature</span><span class="font2" style="font-weight:bold;"> 521</span><span class="font2">(7553): 436-44.</span></p>
<p><span class="font2" style="font-weight:bold;">Simonyan, K. and Zisserman, A. </span><span class="font2">(2015). Very Deep</span></p>
<p><span class="font2">Convolutional Networks for Large-Scale Image Recognition. </span><span class="font2" style="font-style:italic;">Proceedings of ICLR 2015.</span></p>
<p><span class="font2" style="text-decoration:underline;"><a href="https://arxiv.org/abs/1409.1556/">https://arxiv.org/abs/1409.1556/</span><span class="font2"></a>.</span></p>
<p><span class="font2" style="font-weight:bold;">Stutzmann, D. and Tarte, S</span><span class="font2">. (2014). Digital palaeogra-</span></p>
<p><span class="font2">phy: New machines and old texts: Executive summary. </span><span class="font2" style="font-style:italic;">Dagstuhl Reports</span><span class="font2"> 4.7: 112-34 (112-14, 132).</span></p>
<p><span class="font2" style="font-weight:bold;">Stutzmann, D. </span><span class="font2">(2015). Clustering of medieval scripts through computer image analysis: Towards an evaluation protocol. </span><span class="font2" style="font-style:italic;">Digital Medievalist</span><span class="font2" style="font-weight:bold;"> 10</span><span class="font2">, </span><span class="font2" style="text-decoration:underline;"><a href="http://digi-talmedievalist.org/journal/10/stutzmann/">http://digi-talmedievalist.org/journal/10/stutzmann/</span><span class="font2"></a>.</span></p>
<p><span class="font2" style="font-weight:bold;">Stutzmann, D</span><span class="font2">. (2016). </span><span class="font2" style="font-style:italic;">ICFHR2016 Competition on the Classification of Medieval Handwritings in Latin&nbsp;Script</span><span class="font2">. <a href="http://icfhr2016-clamm.irht.cnrs.fr/">http://icfhr2016-clamm.irht.cnrs.fr/</a>.</span></p>
</body>
</html>