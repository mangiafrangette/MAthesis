<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><meta name="generator" content="ABBYY FineReader 14"/><title>Microsoft Word - 309. van Bree-Iterative Data Modelling-309.docx</title><link rel="stylesheet" href="309_files/309.css" type="text/css"/>
</head>
<body><h1><a name="caption1"></a><a name="bookmark0"></a><span class="font1" style="font-weight:bold;">Iterative Data Modelling: from Teaching Practice to&nbsp;Research Method</span></h1>
<p><span class="font3" style="font-weight:bold;">Pim van Bree</span></p>
<p><span class="font3"><a href="mailto:pim@lab1100.com">pim@lab1100.com</a></span></p>
<p><span class="font3">LAB1100, The Netherlands</span></p>
<p><span class="font3" style="font-weight:bold;">Geert Kessels</span></p>
<p><span class="font3"><a href="mailto:geert@lab1100.com">geert@lab1100.com</a></span></p>
<p><span class="font3">LAB1100, The Netherlands</span></p><h2><a name="bookmark1"></a><span class="font0" style="font-weight:bold;">Introduction</span></h2>
<p><span class="font3">Data modelling is an essential process of almost any digital humanities project (Flanders and Jannidis,&nbsp;2015). Whether texts, images, or any other form of&nbsp;data is mapped or analysed, a model has to be&nbsp;conceptualised that describes the data and forms the&nbsp;bedrock of the application that contains or analyses&nbsp;the data.</span></p>
<p><span class="font3">Since data modelling in the humanities is largely perceived as an epistemological process, rather than&nbsp;an ontological process, there is a tension between the&nbsp;way in which material and knowledge presents itself&nbsp;and the manner in which material and knowledge can&nbsp;be described on a generalised or abstracted level. As&nbsp;Flanders and Jannidis (2015: 236) have pointed out:&nbsp;&quot;Some of the most fertile and urgent areas of digital&nbsp;humanities research involve the question of how to&nbsp;develop data modeling approaches that accommodate&nbsp;both the self-reflexivity required by humanities&nbsp;research and the actionability and computational&nbsp;clarity required by the digital domain.&quot;</span></p>
<p><span class="font3">In this paper we reflect on a data modelling approach that has proven to be an effective teaching&nbsp;practice as well as a useful research method. The&nbsp;iterative data modelling approach we put forward&nbsp;focuses on a continuous shift between three levels of&nbsp;data modelling: conceptual level, logical level, and&nbsp;interface level. We have found that this approach&nbsp;provides students and scholars in the humanities&nbsp;(&quot;scholars&quot;) with the skills they need to translate their&nbsp;body of data or research question into an operational&nbsp;process that produces rich (inclusive; fuzzy and&nbsp;uncertain) and complex (advocate divergent classes)&nbsp;actionable datasets. It is important to note that even&nbsp;though it is useful when a scholar can develop their&nbsp;own data model for computer-aided analytical&nbsp;purposes, we should not underestimate the learning&nbsp;curve this new skillset requires.</span></p>
<p><span class="font3">This paper focuses on experiences we gained from data modelling practices in the humanities aimed at&nbsp;developing a relational database. We draw on the&nbsp;results of over 20 courses and workshops for scholars&nbsp;we have held in the past three years on developing&nbsp;data models and using database applications. These&nbsp;insights are also informed by the continuous&nbsp;development of the research environment nodegoat,&nbsp;developed by the authors of this paper, and the&nbsp;scholarly collaborations resulting therefrom.&nbsp;</span><span class="font0" style="font-weight:bold;">Challenges</span></p>
<p><span class="font3">Most scholars do not perceive their material or knowledge as 'data' (Posner, 2015). Once a scholar has&nbsp;accepted that lists of people, statements, and ideas can&nbsp;also be seen as data and that we do not necessarily&nbsp;need to be able to count with them, it becomes clear&nbsp;that their material or knowledge can be modelled as&nbsp;well. Like the analog card catalog, a database helps to&nbsp;store data properly and sustainably. This allows us to&nbsp;filter and query the data. We can then also create&nbsp;networks and analyse relationships. It is important to&nbsp;note that vagueness, uncertainty, and incompleteness&nbsp;can be incorporated in a data model.</span></p>
<p><span class="font3">To allow scholars to operationalise the data modelling process, three levels of a data model have to&nbsp;be studied. The conceptual level, the logical level, and&nbsp;the interface level describe the data at hand, each in its&nbsp;own way. Here, it is necessary to reflect critically on&nbsp;hidden assumptions in the choice of entity types and&nbsp;classifications (Erickson, 2013). An iterative data&nbsp;modelling approach is largely research driven,&nbsp;although existing standards could be used as well. By&nbsp;asking scholars to operationalise their own data&nbsp;model, rather than using or implementing a preexisting model, they get acquainted with the&nbsp;complexities and granularities that operationalising a&nbsp;data model entails.</span></p>
<p><span class="font3">The interface challenge - how to operate a database application? - is very important. We see the translation&nbsp;of the data model into an actual database as a vital step&nbsp;to get a good understanding of the data modelling&nbsp;process. We prefer to do this with a database&nbsp;application that has a graphic user interface to be able&nbsp;to iterate quickly and to easily compare data models.&nbsp;</span><span class="font0" style="font-weight:bold;">Teaching Practice</span></p>
<p><span class="font3">The participants in our data modelling workshops ranged from undergraduates to established scholars.&nbsp;These workshops were either in the format of&nbsp;intensive one day workshops or stretched over&nbsp;multiple events in the course of months. During a&nbsp;workshop, we first addressed the aforementioned&nbsp;challenges to show that the challenges participants&nbsp;face are not new and that we can critically reflect on&nbsp;them. Secondly, we did collective exercises to give&nbsp;participants an understanding on how data models&nbsp;and database applications work.</span></p>
<p><span class="font3">When we then asked them to conceptualise a data model based on their own research question, most&nbsp;participants did not know where to start. The reason&nbsp;for this seemed to be twofold. First, they were unable&nbsp;to process new information regarding data modelling&nbsp;and the database application into an operational&nbsp;process. Since most of the participants were trained to&nbsp;conduct research with a syntagmatic dimension in&nbsp;mind, a linear text, it was hard to execute a research&nbsp;process that leads to a paradigmatic dimension, a&nbsp;database (Manovich, 1999). Secondly, since they were&nbsp;invested in the complex and unique aspects of their&nbsp;research project, they were unable to operationalise a&nbsp;coherent model while keeping relevant variables and&nbsp;complexities in mind (Beretta, 2016).</span></p>
<p><span class="font3">To overcome this, we introduced an iterative approach that took them back and forth from their&nbsp;research question to a partial conceptual data model,&nbsp;to a partial logical model, and to a partial functional&nbsp;database application. This process helped them to first&nbsp;understand how to translate one class of information&nbsp;to a single, non-relational, data table. Once they could&nbsp;process basic typed values (strings/numbers), they&nbsp;started to work with texts, images, dates, and&nbsp;locations. These steps informed participants on the&nbsp;transformation of a conceptual idea into a table with&nbsp;fields in a database application. They first focused on&nbsp;the basics, the finite, while leaving growing complexity,&nbsp;the infinite, to next iterations: creating additional data&nbsp;tables and constructing relationships between them.&nbsp;After these practical questions had been tackled,&nbsp;attention was shifted towards uncertain data, fuzzy&nbsp;data, and the question on using existing standards for&nbsp;a data model.</span></p>
<p><span class="font3">In literature on data modelling processes, a distinction is made between the conceptual/logical&nbsp;level and the level of the application. A data model&nbsp;should be portable and not dependant on one&nbsp;application (Flanders and Jannidis, 2015). However,&nbsp;this does not mean that the conceptual/logical level&nbsp;may not be informed by the application while teaching&nbsp;data modelling practices. The feedback loop between&nbsp;these different levels has proven to be an essential step&nbsp;in helping scholars understand how their own&nbsp;research project can be translated into a data model&nbsp;and a functional database application.</span></p><h2><a name="bookmark2"></a><span class="font0" style="font-weight:bold;">Research Method</span></h2>
<p><span class="font3">The iterative data modelling approach is also of value as a research method. The aforementioned&nbsp;distinction between the conceptual/logical level and&nbsp;the interface level works well when the data for a data&nbsp;model is complete and unambiguous and the process&nbsp;in which the data model plays a role is completely&nbsp;mapped out. Obviously, these variables rarely hold&nbsp;true for research projects in the humanities.</span></p>
<p><span class="font3">Oftentimes the data model does not correspond with data at hand. First, a data model may ask for data&nbsp;that is not there for the majority of data objects.&nbsp;Secondly, data may be too vague to fit typed fields&nbsp;defined in a data model. Thirdly, a data model may lead&nbsp;to a research process that is too time consuming due&nbsp;to its level of detail. In all these cases, revisions of the&nbsp;data model are needed in order to continue the&nbsp;research process.</span></p>
<p><span class="font3">Instead of smoothing out irregularities in the data by simplifying the data model, the model should be&nbsp;adjusted to reflect the existing complexities,&nbsp;vagueness, and uncertainties. As Rawson and Munoz&nbsp;(2016) have stated, scholars should &quot;see the messiness&nbsp;of data not as a block to scalability but as a vital feature&nbsp;of the world which our data represents and from&nbsp;which it emerges.&quot; We encourage scholars to include&nbsp;these data driven practices into their data model and&nbsp;have developed various strategies and features, such&nbsp;as 'reversed classification', to allow them to do this in&nbsp;nodegoat (van Bree and Kessels, 2014; van Bree and&nbsp;Kessels, 2017).</span></p>
<p><span class="font3">With the iterative methodology applied in nodegoat, we have facilitated research projects in the&nbsp;range of: disambiguation of Babylonian letters,&nbsp;questions of provenance and intertextuality in&nbsp;medieval manuscripts, creation of a multi-sourced&nbsp;19th century context of conference attendance on&nbsp;social issues, mapping structures of violence in 1965&nbsp;Indonesia, and documentation of an actor-network&nbsp;towards an encyclopedia of romantic nationalism.</span></p>
<p><span class="font3">An iterative data modelling approach allows scholars to enrich their data model during the research process.&nbsp;While a scholar may first want to use their own model,&nbsp;this can later be transformed or mapped to existing&nbsp;standards, like CIDOC-CRM, or semantic web&nbsp;standards. The data itself may be enriched by adding&nbsp;external identifiers such as VIAF identifiers or&nbsp;identifiers to other linked open data resources. This&nbsp;last point is important when data is published as an&nbsp;actionable dataset online (Berners-Lee, 2006).</span></p><h2><a name="bookmark3"></a><span class="font0" style="font-weight:bold;">Conclusion</span></h2>
<p><span class="font3">In this paper we have set out to describe an iterative data modelling approach that helps scholars&nbsp;become confident in modelling their data and that&nbsp;functions as a research method for database&nbsp;development in the humanities. We have argued how a&nbsp;continuous shift between three levels of data&nbsp;modelling helps to conceive actionable datasets and&nbsp;establishes a framework for dealing with the&nbsp;complexities associated with humanities research.</span></p><h2><a name="bookmark4"></a><span class="font0" style="font-weight:bold;">Bibliography</span></h2>
<p><span class="font2" style="font-weight:bold;">Beretta, F. </span><span class="font2">(2016). From Index Cards to a Digital</span></p>
<p><span class="font2">Information System: Teaching Data Modeling to Master's</span></p>
<p><span class="font2">Students in History. In </span><span class="font2" style="font-style:italic;">Digital Humanities 2016:</span></p>
<p><span class="font2" style="font-style:italic;">Conference Abstracts.</span><span class="font2"> Jagiellonian University &amp;</span></p>
<p><span class="font2">Pedagogical University, Krakow, pp. 132-135.</span></p>
<p><span class="font2" style="font-weight:bold;">Berners-Lee, T. </span><span class="font2">(2006) Linked Data. Retrieved from <a href="https://www.w3.org/DesignIssues/LinkedData.html">https://www.w3.org/DesignIssues/LinkedData.html</a>.</span></p>
<p><span class="font2" style="font-weight:bold;">Bree, P. van and Kessels, G. </span><span class="font2">(2014) Reversed Classification</span></p>
<p><span class="font2">[Blog &nbsp;&nbsp;&nbsp;Post].&nbsp;&nbsp;&nbsp;&nbsp;Retrieved&nbsp;&nbsp;&nbsp;&nbsp;from</span></p>
<p><span class="font2"><a href="https://nodegoat.net/blog.s/5/reversed-classification">https://nodegoat.net/blog.s/5/reversed-classification</a>.</span></p>
<p><span class="font2" style="font-weight:bold;">Bree, P. van and Kessels, G. </span><span class="font2">(2015) “Mapping memory landscapes in nodegoat” in: </span><span class="font2" style="font-style:italic;">Social Informatics,</span><span class="font2"> ed. L.M.&nbsp;Aiello and D. McFarland (Lecture Notes in Computer&nbsp;Science 8852), pp 274--278, New York : Springer&nbsp;International.</span></p>
<p><span class="font2" style="font-weight:bold;">Posner, M., &nbsp;&nbsp;&nbsp;</span><span class="font2">(2015) Humanities Data: A Necessary</span></p>
<p><span class="font2">Contradiction [Blog Post]. Retrieved from <a href="http://miriamposner.com/blog/humanities-data-a-necessary-contradiction/">http://miriamposner.com/blog/humanities-data-a-necessary-contradiction/</a>.</span></p>
<p><span class="font2" style="font-weight:bold;">Rawson, K. and Muñoz, T. </span><span class="font2">(2016) Against Cleaning. Retrieved&nbsp;&nbsp;&nbsp;&nbsp;from:</span></p>
<p><span class="font2"><a href="http://www.curatingmenus.org/articles/against-cleaning">http://www.curatingmenus.org/articles/against-cleaning</a>.</span></p>
<p><span class="font2" style="font-weight:bold;">Bree, P. van and Kessels, G. </span><span class="font2">(2017) Formulating Ambiguity in a Database [Blog Post]. Retrieved from&nbsp;<a href="https://nodegoat.net/blog.s/21/formulating-ambiguity-in-a-database">https://nodegoat.net/blog.s/21/formulating-ambiguity-in-a-database</a>.</span></p>
<p><span class="font2" style="font-weight:bold;">Erickson, A. T. </span><span class="font2">(2013). Historical Research and the Problem of Categories. In: Dougherty, J. and Nawrotzki, K. (eds),&nbsp;</span><span class="font2" style="font-style:italic;">Writing History in the Digital Age.</span><span class="font2"> Ann Arbor: University&nbsp;of Michigan Press, pp. 133-145.</span></p>
<p><span class="font2" style="font-weight:bold;">Flanders, J. and Jannidis, F</span><span class="font2">. (2015) Data Modeling, in </span><span class="font2" style="font-style:italic;">A New Companion to Digital Humanities</span><span class="font2"> (eds S.&nbsp;Schreibman, R. Siemens and J. Unsworth), John Wiley &amp;&nbsp;Sons,&nbsp;&nbsp;&nbsp;&nbsp;Ltd,&nbsp;&nbsp;&nbsp;&nbsp;Chichester,&nbsp;&nbsp;&nbsp;&nbsp;UK.&nbsp;&nbsp;&nbsp;&nbsp;doi:</span></p>
<p><span class="font2">10.1002/9781118680605.ch16</span></p>
<p><span class="font2" style="font-weight:bold;">Manovich, L. </span><span class="font2">(1999). Database as a symbolic form. </span><span class="font2" style="font-style:italic;">Millennium Film Journal,</span><span class="font2"> 34 (Fall)</span></p>
</body>
</html>