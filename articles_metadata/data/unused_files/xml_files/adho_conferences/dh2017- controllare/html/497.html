<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><meta name="generator" content="ABBYY FineReader 14"/><title>Microsoft Word - 497. Schmidt-Stable Random Projection-497.docx</title><link rel="stylesheet" href="497_files/497.css" type="text/css"/>
</head>
<body>
<p><span class="font0" style="font-weight:bold;">Stable Random Projection: Standardized universal&nbsp;dimensionality reduction for&nbsp;library-scale data</span></p>
<p><span class="font2" style="font-weight:bold;">Benjamin Schmidt</span></p>
<p><span class="font2"><a href="mailto:bmschmidt@gmail.com">bmschmidt@gmail.com</a></span></p>
<p><span class="font2">Northeastern University, United States of America</span></p>
<p><span class="font4" style="font-weight:bold;">Summary</span></p>
<p><span class="font2">This paper describes a new method for dimensionality reduction, “stable random projection,” (hereafter “SRP”) distinctly suited for large textual corpora like&nbsp;those used in the digital humanities. The method is&nbsp;computationally efficient and easily parallelizable;&nbsp;scales to the largest digital libraries; and creates a&nbsp;standard dimensionality reduction space for all texts&nbsp;so that corpora and models can be easily exchanged.&nbsp;The resulting space makes a wide variety of applications suitable to bag-of-words data, such as nearest&nbsp;neighbor searches, classification, and semantic querying possible with data sets an order of magnitude&nbsp;smaller in size than traditional feature counts.</span></p>
<p><span class="font2">SRP is a minimal, universal dimensionality reduction with two distinctive features:</span></p>
<p><span class="font2">1. It makes </span><span class="font2" style="font-style:italic;">no distinction between in- and out-ofdomain vocabularies.</span><span class="font2"> In particular, unlike standard dimensionality reduction it creates a single space that&nbsp;can hold documents of </span><span class="font2" style="font-style:italic;">any language</span><span class="font2">.</span></p>
<p><span class="font2">2. It is </span><span class="font2" style="font-style:italic;">trivially parallelizable</span><span class="font2">, both on a local machine and through web-based architectures because it&nbsp;relies only on code that can be easily transferred&nbsp;across servers, rather than requiring large matrices or&nbsp;model parameters.</span></p>
<p><span class="font2">These two features allow dimensionality reduction to be conceived of as a piece of infrastructure for digital humanities work, rather than just an ad-hoc convention used in a particular project. This method is&nbsp;particularly useful for provisioners and users of text&nbsp;data on extremely large and/or multilingual corpora.&nbsp;This creates a number of new applications for dimensionality reduction, both in scale and in type. SRP features could usefully be distributed by libraries as a&nbsp;(much smaller and easier to work with) supplement to&nbsp;feature counts. After a description of the method, some&nbsp;novel uses for dimensionality reduction on such libraries are shown using a sharable dataset of approximately 4,500,000 books projected into SRP-space&nbsp;from the Hathi Trust.</span></p>
<p><span class="font4" style="font-weight:bold;">Description of the method</span></p>
<p><span class="font2">The goal of SRP is to reduce of text of uncertain length to a much smaller fixed- length vector to which&nbsp;the many tools of textual analysis, machine learning,&nbsp;and linear algebra can be applied. The core technique&nbsp;used here for dimensionality reduction is </span><span class="font2" style="font-style:italic;">random projection.</span><span class="font2"> Random matrix theory has emerged in the past&nbsp;few decades as an useful alternative to more computationally complex forms of dimensionality reduc-tion.(Halko, Martinsson, and Tropp 2009) I make use&nbsp;here of the observation that it is possible to project&nbsp;into a space where points as determined purely by&nbsp;sampling randomly from the set [-1,1].(Achlioptas&nbsp;2003) A true random number generator is not suitable&nbsp;for reproduction. The other core element of SRP,&nbsp;therefore, is a quasi-random projection for every individual word created using cryptographic hashes (specifically, SHA-1).</span></p>
<p><span class="font2">This allows the method to be defined algorithmically, making it easy to apply to any text. I have written short code libraries to implement the transformation&nbsp;in the three most important language for DH tool development: Python, R, and Javascript. These include a&nbsp;few necessary additional conventions such as minimal&nbsp;tokenization rules, a method for expanding beyond the&nbsp;160 dimensions provided by SHA, and the byte-encoding of the Unicode character sets.</span></p>
<p><span class="font4" style="font-weight:bold;">Comparison to existing methods</span></p>
<p><span class="font2">The gold standard for dimensionality reduction are techniques that make use of co-occurrences in the&nbsp;term-document matrix such as latent semantic indexing and independent components analysis. More recent techniques such as semantic hashing can be even&nbsp;faster and more efficient at optimally organizing documents in various types of vector spaces designed especially for particular documents.(Salakhutdinov and&nbsp;Hinton 2009) Another strategy finding recent use in&nbsp;the digital humanities is using an LDA topic model as</span></p>
<p><span class="font2">dimensionality reduction, which produces neatly interpretable dimensions for analysis (Schoch, 2016; Fitzgerald,2016). In both the digital humanities and&nbsp;computer science, scholars frequently use “top-N”&nbsp;words as a good enough approximation of the textual&nbsp;footprint, limiting the dimensions to a few hundred of&nbsp;the most common words in the corpus, producing&nbsp;what Maciej Eder has called “endless discussions of&nbsp;how many frequent words or n-grams should be taken&nbsp;into account” for stylometry.(Underwood 2014, Eder&nbsp;(2015))</span></p>
<p><span class="font2">These methods suffer two problems that make</span></p>
<p><span class="font2">them problematic as a </span><span class="font2" style="font-style:italic;">general-use</span><span class="font2"> feature reduction. First, the better ones are computationally complex,&nbsp;and quite difficult to perform on a very large corpus.&nbsp;Second, it is difficult or impossible to project </span><span class="font2" style="font-style:italic;">out-of-domain</span><span class="font2"> documents into the space from a standard projection if they contain vocabulary different than the&nbsp;training corpus. This out-of-domain problem presents&nbsp;a particularly great problem for multilingual corpora,&nbsp;because texts that are missing or in sparsely-represented languages will behave erratically in the new environment.</span></p>
<p><span class="font2">Some other work in the digital humanities and computer science has used hashes, random projection,&nbsp;and other similar methods as an ad-hoc rather than infrastructural technique. SRP can be thought of as a particular species of </span><span class="font2" style="font-style:italic;">locality-sensitive hashing</span><span class="font2">, another&nbsp;version of which has been used by Douglas Duhaime&nbsp;to identify reuse in poetic texts based on three-letter&nbsp;phrases.(Duhaime 2016). Also related is the “hashing&nbsp;trick” in computer science(Weinberger et al. 2009),&nbsp;which is better than SRP in many ways for the short&nbsp;documents computer scientists frequently study, but&nbsp;takes significantly more memory to store for book-length documents (an edge case in the computer science literature, but among the most important for humanists).</span></p>
<p><span class="font4" style="font-weight:bold;">Applications</span></p>
<p><span class="font2">This reduced space can be put to many of the same uses as a standard bag-of- words model in considerably less space and with the potential for building web&nbsp;facing tools. Among those to be described are:</span></p>
<p><span class="font2" style="font-weight:bold;">1. &nbsp;&nbsp;&nbsp;Duplicate detection</span><span class="font2">. SRP is quite accurate&nbsp;at identifying duplicate books in a computationally tractable space using cosine similarity, both inside a corpus and across disparate&nbsp;corpora.</span></p>
<p><span class="font2" style="font-weight:bold;">2. &nbsp;&nbsp;&nbsp;Similarity Search</span><span class="font2">. A prototype web page allows any user to paste in any text; it will&nbsp;hashed on the client side into the standard&nbsp;space, and a server can return in a few seconds the most similar documents. The top&nbsp;entries can function for duplicate detection;</span></p>
<p><span class="font2">the lower ones presenting interesting opportunities for exploratory analysis. A search for </span><span class="font2" style="font-style:italic;">Huckleberry Finn</span><span class="font2">, for example, finds a large&nbsp;number of other American adventure novels&nbsp;about boys in the American west.</span></p>
<p><span class="font2" style="font-weight:bold;">3. Classification</span></p>
<p><span class="font3">• </span><span class="font2">SRP features perform approximately as well as top-n words (~77%) on a pre-existing&nbsp;task described by Ted Underwood, separating high- from low-prestige poetry.(Under-wood 2015)</span></p>
<p><span class="font3">• </span><span class="font2">A single hidden layer neural network trained with 640-dimensional SRP features can accurately classify a held-out sample of books&nbsp;into one of 225 Library of Congress Classification subclasses (for example, whether a&nbsp;work is PR: British Literature or PS: American Literature) with ~78% accuracy based&nbsp;on about 1 million training examples. A single classifier works in multiple languages&nbsp;simultaneously; its determinations on arbitrary pasted text are accessible for inspection through a </span><span class="font2" style="text-decoration:underline;">web site.</span></p>
<p><span class="font3">• </span><span class="font2">A different single hidden layer neural network trained with SRP features and a novel encoding scheme for years using Google's&nbsp;TensorFlow framework can accurately predict the years for withheld books with a median error of four years from the true publication date.</span></p>
<p><span class="font4" style="font-weight:bold;">SRP as Access</span></p>
<p><span class="font2">SRP fits in the DH2017's theme of “Access” in two ways.</span></p>
<p><span class="font2">First, it makes many forms of text analysis on huge digital libraries far more feasible for scholars without&nbsp;access to high performance computing resources. On&nbsp;large corpora, data storage and dimensionality reduction can be more resource- intensive than the actual&nbsp;analysis. The dimensionality-reduced dataset for the&nbsp;full Hathi Trust corpus can fit into 10 GB, easily storable on most computers; subsets are suitable for use in&nbsp;classroom or workshop settings.</span></p>
<p><span class="font2">Second, the ease with which it works with distributed web architectures, and its language agnosticism, can create new routes into neglected portions of large&nbsp;archives, particularly those with insufficient metadata.</span></p>
<p><span class="font4" style="font-weight:bold;">Bibliography</span></p>
<p><span class="font1" style="font-weight:bold;">Achlioptas, D. </span><span class="font1">(2003). “Database-Friendly Random Projections: Johnson- Lindenstrauss with Binary Coins.” </span><span class="font1" style="font-style:italic;">Journal</span></p>
<p><span class="font1" style="font-style:italic;">of Computer and System Sciences,</span><span class="font1"> Special issue on PODS</span></p>
<p><span class="font1">2001, &nbsp;&nbsp;&nbsp;66&nbsp;&nbsp;&nbsp;&nbsp;(4):&nbsp;&nbsp;&nbsp;&nbsp;671-87.&nbsp;&nbsp;&nbsp;&nbsp;doi:10.1016/S0022-</span></p>
<p><span class="font1">0000(03)00025-4.</span></p>
<p><span class="font1" style="font-weight:bold;">Duhaime, D. </span><span class="font1">(2016). “Plagiary Poets. Plagiary Poets.” <a href="http://plagiarypoets">http://plagiarypoets</a>. io/.</span></p>
<p><span class="font2" style="font-weight:bold;">Eder, M. </span><span class="font2">(2015). “Visualization in Stylometry: Cluster Analysis Using Networks.” </span><span class="font1" style="font-style:italic;">Digital Scholarship in the Humanities,</span><span class="font2"> November, fqv061. doi:10.1093/llc/fqv061.</span></p>
<p><span class="font1" style="font-weight:bold;">Fitzgerald, J. D. </span><span class="font1">(2016) “What Made the Front Page in the 19th Century?: Computationally Classifying Genre&nbsp;in ‘Viral Texts”. July 13 2016 <a href="http://jonathandfitzger-ald.com/blog/2016/07/13/keystone-paper.html">http://jonathandfitzger-ald.com/blog/2016/07/13/keystone-paper.html</a></span></p>
<p><span class="font1" style="font-weight:bold;">Halko, N., Martinsson,P.-G., and Tropp, J. A. </span><span class="font1">(2009). “Finding Structure with Randomness: Probabilistic Algorithms for Constructing Ap- proximate Matrix Decompositions.” arXiv:0909.4061 [Math], August. http:</span></p>
<p><span class="font1">//arxiv.org/abs/0909.4061.</span></p>
<p><span class="font2" style="font-weight:bold;">Salakhutdinov, R., and Hinton, G. </span><span class="font2">(2009). “Semantic</span></p>
<p><span class="font2">Hashing</span><span class="font2" style="font-style:italic;">.” </span><span class="font1" style="font-style:italic;">International Journal of Approximate Reasoning,</span><span class="font2"> Special section on graphical models </span><span class="font1">and information retrieval, 50 &nbsp;&nbsp;&nbsp;(7):&nbsp;&nbsp;&nbsp;&nbsp;969-78.</span></p>
<p><span class="font1">doi:10.1016/j.ijar.2008.11.006.</span></p>
<p><span class="font2" style="font-weight:bold;">Schoch, C. </span><span class="font2">(2016, pre-publication) “Topic Modeling Genre: An Exploration of French Classical and Enlightenment Drama”</span><span class="font2" style="font-style:italic;">. Digital Humanities Quarterly.</span></p>
<p><span class="font1" style="font-weight:bold;">Underwood, T. </span><span class="font1">(2014). “Understanding Genre in a Collection of a Million Volumes, Interim Report.” <a href="http://figshare.com/articles/Understanding_Genre_">http://figshare.com/articles/Understanding_Genre_</a>&nbsp;in_a</span><span class="font1" style="font-style:italic;">_Collection_of_a_Million_Volumes_Interim_Re-</span></p>
<p><span class="font1" style="font-style:italic;">port</span><span class="font1">/1281251.</span></p>
<p><span class="font1" style="font-weight:bold;">Underwood, T.</span><span class="font1">. (2015). “The Literary Uses of High-Dimensional Space.” </span><span class="font1" style="font-style:italic;">Big Data &amp; Society</span><span class="font1"> 2 (2): 2053951715602494. doi:10.1177/2053951715602494.</span></p>
<p><span class="font2" style="font-weight:bold;">Weinberger, K., Dasgupta, A., Langford, J., Smola, A., and Attenberg, J. </span><span class="font2">(2009). “Feature Hashing for Large&nbsp;Scale Multitask Learning.” In </span><span class="font1" style="font-style:italic;">Proceedings of the 26th Annual International Conference on Machine Learning</span><span class="font1">,&nbsp;</span><span class="font2">1113-20. ICML '09. New York, NY, USA: ACM.&nbsp;doi:10.1145/1553374.1553516.</span></p>
</body>
</html>