<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><meta name="generator" content="ABBYY FineReader 14"/><title>Microsoft Word - 590. Bermeitinger-Object Classification in Images of Neoclassical Artifacts Using Deep Learning-590.docx</title><link rel="stylesheet" href="590_files/590.css" type="text/css"/>
</head>
<body>
<p><span class="font2" style="font-weight:bold;">Object Classification in Images of Neoclassical&nbsp;Artifacts Using Deep&nbsp;Learning</span></p><h3><a name="caption1"></a><a name="bookmark0"></a><span class="font4" style="font-weight:bold;">Bernhard Bermeitinger</span></h3>
<p><span class="font4"><a href="mailto:bernhard.bermeitinger@uni-passau.de">bernhard.bermeitinger@uni-passau.de</a> Universität Passau, Germany</span></p><h3><a name="bookmark1"></a><span class="font4" style="font-weight:bold;">Simon Donig</span></h3>
<p><span class="font4"><a href="mailto:simon.donig@uni-passau.de">simon.donig@uni-passau.de</a></span></p>
<p><span class="font4">Universität Passau, Germany</span></p><h3><a name="bookmark2"></a><span class="font4" style="font-weight:bold;">Maria Christoforaki</span></h3>
<p><span class="font4"><a href="mailto:maria.christoforaki@uni-passau.de">maria.christoforaki@uni-passau.de</a> Universität Passau, Germany</span></p><h3><a name="bookmark3"></a><span class="font4" style="font-weight:bold;">André Freitas</span></h3>
<p><span class="font4"><a href="mailto:andre.freitas@uni-passau.de">andre.freitas@uni-passau.de</a></span></p>
<p><span class="font4">Universität Passau, Germany</span></p><h3><a name="bookmark4"></a><span class="font4" style="font-weight:bold;">Siegfried Handschuh</span></h3>
<p><span class="font4"><a href="mailto:siegfried.handschuh@uni-passau.de">siegfried.handschuh@uni-passau.de</a> Universität Passau, Germany</span></p><h1><a name="caption2"></a><a name="bookmark5"></a><span class="font1" style="font-weight:bold;">Classifying aesthetic forms - a methodology at the heart of art history</span></h1>
<p><span class="font4">The transformation of aesthetic styles has been at the heart of art history since its inception as a scholarly discipline in the late eighteenth century. Analyzing the single artifact and the carefully curated corpus&nbsp;have been the techniques for crafting hermeneutic understanding for such processes of change. Recently&nbsp;new instruments based on statistical techniques empower us for a fresh take on bodies of sources once disregarded as second tier complementary sources such&nbsp;as for instance very large corpora.</span></p><h1><a name="bookmark6"></a><span class="font1" style="font-weight:bold;">The Neoclassica research framework</span></h1>
<p><span class="font4">The </span><span class="font4" style="font-style:italic;">Neoclassica</span><span class="font4"> research framework (Donig et al., 2016) was conceived to provide scholars with new instruments and methods for analyzing and classifying</span></p>
<p><span class="font4">artifacts and aesthetic forms from the era of Classicism</span></p>
<p><span class="font4">(ca. 1760-1860). The neoclassic movement was of almost global scale—affecting architecture and design</span></p>
<p><span class="font4">from Sidney to New York, and from Athens to the outreach of the Russian Urals—while relating to a common reference in classical antiquity, therefore making it an ideal topic for studying processes of stylistic&nbsp;transformation.</span></p>
<p><span class="font4">It accommodates both traditional knowledge representation as a formal ontology and data-driven knowledge discovery, where cultural patterns will be&nbsp;identified by means of algorithms in statistical analysis and machine learning, having in particular the potential to uncover hitherto unknown patterns in the&nbsp;source data. The outcomes of the top-down and the&nbsp;bottom-up approach will be united in a consistent, unified formal knowledge representation.</span></p>
<p><span class="font4">Motivated by the need to combine object classification with domain knowledge representation, the ontology focuses at the moment on artifacts (in particular furniture and architecture) and their components. Following the preliminary hypotheses that aesthetic&nbsp;forms in furniture and architecture are in closest communication with each other due to constructional&nbsp;commonalities and their shared reference of the Classic, we decided to start developing the knowledge discovery module of </span><span class="font4" style="font-style:italic;">Neoclassica</span><span class="font4"> by classifying artifacts in&nbsp;digital images.</span></p><h1><a name="bookmark7"></a><span class="font1" style="font-weight:bold;">Knowledge discovery</span></h1>
<p><span class="font4">In this paper, we report on our efforts for using deep learning for classifying artifacts in digital visuals.&nbsp;We chose a deep learning approach for our classification method because of its current superiority over&nbsp;other methods and still rising accuracy over the last&nbsp;years in nearly all image classification and object detection challenges.</span></p>
<p><span class="font4">Initially, we compiled a body of images both from commercial sources such as auction houses, antique&nbsp;dealers and other public sources. Due to the complex&nbsp;copyright situation, this corpus can not be redistributed. In order to make our experiment reproducible&nbsp;and since the </span><span class="font4" style="font-style:italic;">Metropolitan Museum of Art</span><span class="font4"> (MET) has&nbsp;released 375,000 images in the public domain (The&nbsp;Metropolitan Museum of Art, 2017). we assembled a&nbsp;corpus of 379 artifacts relevant to our research. We&nbsp;processed this corpus with the same algorithm as the&nbsp;original proprietary corpus and released the data together with the source code (The Neoclassica Project,&nbsp;2017).</span></p><h2><a name="bookmark8"></a><span class="font1">Classifier description</span></h2>
<p><span class="font4">The main classifier for our experiments is a </span><span class="font4" style="font-style:italic;">Convolutional Neural Network</span><span class="font4"> (CNN). It classifies an input image as a whole.</span></p>
<p><span class="font4">In a first step, we applied a standard implementation of a CNN (namely </span><span class="font4" style="font-style:italic;">VGG19</span><span class="font0"> (Russakovsky et al.,</span></p>
<p><span class="font0">2015))</span><span class="font4">. The results were not satisfactory for our</span></p>
<p><span class="font4">needs. It classified the type of the object depicted in the image with an accuracy of 0.37.</span></p>
<p><span class="font4">In a second step, we opted to employ pre-training, a common technique for improving accuracy in neural&nbsp;networks. We experienced that available pre-trained&nbsp;classifiers for generic image classification proved ineffective in our case. Most of them are trained on a specific subset of </span><span class="font4" style="font-style:italic;">ImageNet</span><span class="font4"> (Deng et al., 2009), containing&nbsp;1000 classes. These classes are broadly spread around&nbsp;everyday objects like dogs and planes. This led us to&nbsp;assume that the amount of very different classes that&nbsp;don't occur in our corpora interfere with the classification. Following that hypothesis, we decided to train&nbsp;the algorithm on a specific subset compiled from&nbsp;</span><span class="font4" style="font-style:italic;">ImageNet</span><span class="font4"> mainly containing different furniture objects like tables, chairs, and cabinets. They sum up to&nbsp;35,000 images. The first training step with these images resulted in an accuracy of 0.54 of classifying the&nbsp;object correctly.</span></p><h2><a name="bookmark9"></a><span class="font1">First layout</span></h2>
<p><span class="font4">The first corpus contained 2,129 images representing 300 European period artifacts mostly in a colored format of highly diverging quality and resolution. They&nbsp;depict the objects fully, partially or are close-up shots&nbsp;of specific forms. We coarsely annotated these images&nbsp;by manually labeling them on the level of folders. The&nbsp;concepts applied during this labeling process are directly taken from the </span><span class="font4" style="font-style:italic;">Neoclassica</span><span class="font4"> ontology and describe concepts for types of artifacts. These concepts&nbsp;were derived from period sources.</span></p>
<p><span class="font4">The depth of the class hierarchy was partly reflected by the folder structure. The folder labeled “Chest of drawers” contains all instances of this class.&nbsp;Their labels in turn reflect the names of all the subclasses in the most extensive specification (e.g.&nbsp;semainier, Wellington chest, commode scriban).</span></p>
<p><span class="font4">After pre-training, the next step was fine-tuning with this corpus. The accuracy was 0.44, the F1 measure 0.44.</span></p><h2><a name="bookmark10"></a><span class="font1">Second layout</span></h2>
<p><span class="font4">The second corpus was assembled from open data released by the MET. It contains 1,246 images representing 379 European and American period artifacts&nbsp;ranging roughly from 1780-1840 including some&nbsp;transition pieces, drawings, and prints. They also depict the objects fully, partially, or are close-up shots of&nbsp;specific forms. We used the titles provided by the MET&nbsp;and manually aligned them with the </span><span class="font4" style="font-style:italic;">Neoclassica</span><span class="font4"> ontology.</span></p>
<p><span class="font4">The overall mean accuracy over all classes was 0.36, the F1 measure 0.21. For the computation of&nbsp;these numbers, all results that are non-computable&nbsp;(due to only having one image in either the train or test&nbsp;set) were removed. These low numbers result from&nbsp;the existence of two many artifacts represented by&nbsp;only one image, thus making a split in training and&nbsp;testing data meaningless. However, applying pretraining using same </span><span class="font4" style="font-style:italic;">ImageNet</span><span class="font4"> corpus as in the first layout yielded a mean accuracy over all classes of 0.59&nbsp;and a F1 measure of 0.58.</span></p>
<p><span class="font4">In order to achieve better results and since the classifier classifies the image as whole, we excluded all images that did not depict the whole artifact. We kept multiple copies of the same image if they were used to&nbsp;describe a different but similar object. We split the images depicting multiple objects so that the resulting&nbsp;images represent only one artifact. We also processed&nbsp;these images so that neighboring objects were covered&nbsp;with solid colors. The images that could not be split&nbsp;(e.g. room interiors) were excluded from the corpus.</span></p>
<p><span class="font4">Using the same settings with the curated corpus and with pre-training we achieved an accuracy of 0.77&nbsp;and an F1 measure of 0.76.</span></p><h2><a name="bookmark11"></a><span class="font1">Challenges</span></h2>
<p><span class="font4">While pre-training and improving the curation process helped us to raise the accuracy, we assume that there is room for improvement.</span></p>
<p><span class="font4">Parameters to be taken into consideration include the small size of corpora and how to overcome this&nbsp;limitation since this limits the effectiveness of a neural&nbsp;net. Additionally, since pre-training has been proven&nbsp;to enhance the results, it is rational to assume that a&nbsp;pre-training corpus better suited to period artifacts&nbsp;would improve the results further. Third, our experiment was affected by the limitation of the standard implementation of the CNN which classifies the image as&nbsp;a whole and not parts of it.</span></p>
<p><span class="font4">Outlining parts inside an image and classifying them is a difficult task for machine learning methods.&nbsp;Recently, a new type of neural net emerged that tackles this challenge: </span><span class="font4" style="font-style:italic;">Regional CNN</span><span class="font4"> (RCNN). It is implemented most prominently in an algorithm called </span><span class="font4" style="font-style:italic;">MultiPath Network</span><span class="font4"> (Zagoruyko et al., 2016).</span></p>
<p><span class="font4">4Current improvements: Using a Regional CNN</span></p>
<p><span class="font4">We are manually annotating regions within the images with classes from the ontology for training a RCNN that locates objects.</span></p>
<p><span class="font4">The implementation of the RCNN is divided into two steps. First, it detects objects in the image and&nbsp;draws their outline as a polygon. The second step is&nbsp;classifying the outlined objects using the included&nbsp;standard CNN.</span></p>
<p><span class="font4">Preliminary results of </span><span class="font4" style="font-style:italic;">MultiPath Network</span><span class="font4"> with default pre-trained settings show that the first step of the RCNN already outlines objects in our corpora within&nbsp;reasonable limits. The corpus for pre-training is </span><span class="font4" style="font-style:italic;">COCO&nbsp;</span><span class="font4">(Lin et al., 2014). Naturally, specific domain objects are&nbsp;not located and the class names are too generic. For&nbsp;our purpose, fine-tuning on a custom annotated corpus is essential. An RCNN requires a more detailed corpus. The exhaustive task of manually draw the objects'&nbsp;outlines within an image promises higher quality in locating objects (first step) and is necessary to classify&nbsp;the objects according to the ontology (second step).</span></p><h1><a name="bookmark12"></a><span class="font1" style="font-weight:bold;">Future work</span></h1>
<p><span class="font4">We decided to take two steps in the near future for improving our results.</span></p>
<p><span class="font4">First, we are compiling a new corpus to train the RCNN with, avoiding pitfalls like inconsistent quality,&nbsp;heterogeneous image rights and an inadequate distribution of image per class. Here we would like to go a&nbsp;dual approach. Together with domain experts, we intend to collate a corpus from the large repository of a&nbsp;major auction house, providing us not only with a selection of artifacts' images but also with texts to be&nbsp;used in multimodal analysis.</span></p>
<p><span class="font4">On the other hand, this kind of artifacts may exhibit provenance issues (e.g. heterogeneity or lack of provenance). We will thus compensate for such issues by&nbsp;digitizing a major corpus of neoclassical artifacts forming an ensemble and comprising artifacts in multiple&nbsp;modes having evolved in close reference to each other.&nbsp;Therefore, we have entered a partnership with the&nbsp;Dessau-Wörlitz UNESCO world-heritage site, an almost untouched complex of manor houses and their&nbsp;furnishings in early neoclassical style.</span></p>
<p><span class="font4">Regarding the annotations, we are developing our own semantic annotation and ontology population&nbsp;tool since January 2017. The tool will create an annotated corpus. The actual annotation process will be&nbsp;conducted in cooperation with emerging domain experts from the chair of Visual Culture and Art History&nbsp;at the University Passau.</span></p>
<p><span class="font3">(2009). ImageNet: A Large-Scale Hierarchical Image Database. </span><span class="font3" style="font-style:italic;">CVPR09</span><span class="font3">.</span></p>
<p><span class="font3" style="font-weight:bold;">Donig, S., Christoforaki, M. and Handschuh, S. </span><span class="font3">(2016). Ne-oclassica - A Multilingual Domain Ontology. In Bozic, Mendel-Gleason, Debruyne and O'Sullivan (eds), </span><span class="font3" style="font-style:italic;">2nd&nbsp;IFIP International Workshop on Computational History&nbsp;and Data-Driven Humanities</span><span class="font3">.</span></p>
<p><span class="font3" style="font-weight:bold;">Lin, T. Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-</span></p>
<p><span class="font3" style="font-weight:bold;">manan, D., Dollar, P. and Zitnick, C. L. </span><span class="font3">(2014). Microsoft COCO: Common objects in context. </span><span class="font3" style="font-style:italic;">Lecture Notes in Computer Science (Including Subseries Lecture Notes in</span></p>
<p><span class="font3" style="font-style:italic;">Artificial Intelligence and Lecture Notes in Bioinformatics),</span><span class="font3"> vol. 8693 LNCS. pp. 740-755 doi:10.1007/978-3-319-10602-1_48.</span></p>
<p><span class="font3" style="font-weight:bold;">Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., et al. </span><span class="font3">(2015). ImageNet Large Scale&nbsp;Visual Recognition Challenge. </span><span class="font3" style="font-style:italic;">International Journal of&nbsp;Computer Vision</span><span class="font3">, </span><span class="font3" style="font-weight:bold;">115</span><span class="font3">(3): 211-252 doi:10.1007/s11263-015-0816-y.</span></p>
<p><span class="font3" style="font-weight:bold;">The Metropolitan Museum of Art </span><span class="font3">(2017). The Met Makes</span></p>
<p><span class="font3">Its Images of Public-Domain Artworks Freely Available through New Open Access Policy <a href="http://www.met-museum.org/press/news/2017/open-access">http://www.met-museum.org/press/news/2017/open-access</a> (accessed</span></p>
<p><span class="font3">1 March 2017).</span></p>
<p><span class="font3" style="font-weight:bold;">The Neoclassica Project </span><span class="font3">(2017). Neoclassica - A Framework for Research in Neoclassicism <a href="http://www.neo-classica.network/resources">http://www.neo-classica.network/resources</a> (accessed 1 April 2017).</span></p>
<p><span class="font3" style="font-weight:bold;">Zagoruyko, S., Lerer, A., Lin, T.-Y., Pinheiro, P. O., Gross, S., Chintala, S. and Dollar, P. </span><span class="font3">(2016). A MultiPath Network for Object Detection. </span><span class="font3" style="font-style:italic;">BMVC&nbsp;</span><span class="font3"><a href="http://arxiv.org/abs/1604.02135">http://arxiv.org/abs/1604.02135</a>.</span></p><h1><a name="bookmark13"></a><span class="font1" style="font-weight:bold;">Bibliography</span></h1>
<p><span class="font3" style="font-weight:bold;">Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K. and Fei-Fei, L.</span></p>
</body>
</html>