<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><meta name="generator" content="ABBYY FineReader 14"/><title>Microsoft Word - 192. Reiter-A Shared Task for a Shared Goal-192.docx</title><link rel="stylesheet" href="192_files/192.css" type="text/css"/>
</head>
<body><h1><a name="caption1"></a><a name="bookmark0"></a><span class="font2" style="font-weight:bold;">A Shared Task for a Shared</span></h1><h1><a name="bookmark1"></a><span class="font2" style="font-weight:bold;">Goal: Systematic Annotation of Literary Texts&nbsp;</span><span class="font4" style="font-weight:bold;">Nils Reiter</span></h1>
<p><span class="font4"><a href="mailto:nils.reiter@ims.uni-stuttgart.de">nils.reiter@ims.uni-stuttgart.de</a></span></p>
<p><span class="font4">Stuttgart University, Germany</span></p>
<p><span class="font4" style="font-weight:bold;">Evelyn Gius</span></p>
<p><span class="font4"><a href="mailto:evelyn.gius@uni-hamburg.de">evelyn.gius@uni-hamburg.de</a></span></p>
<p><span class="font4">Hamburg University, Germany;</span></p>
<p><span class="font4" style="font-weight:bold;">Jannik Strotgen</span></p>
<p><span class="font4"><a href="mailto:jannik.stroetgen@mpi-inf.mpg.de">jannik.stroetgen@mpi-inf.mpg.de</a></span></p>
<p><span class="font4">Max Planck Institute for Informatics, Germany</span></p>
<p><span class="font4" style="font-weight:bold;">Marcus Willand</span></p>
<p><span class="font4"><a href="mailto:marcus.willand@ilw.uni-stuttgart.de">marcus.willand@ilw.uni-stuttgart.de</a></span></p>
<p><span class="font4">Stuttgart University, Germany</span></p>
<p><span class="font1" style="font-weight:bold;">Introduction</span></p>
<p><span class="font4">In this talk, we would like to outline a proposal for a shared task (ST) in and for the digital humanities.&nbsp;In general, shared tasks are highly productive frameworks for bringing together different research-ers/research groups and, if done in a sensible way,&nbsp;foster interdisciplinary collaboration. They have a&nbsp;tradition in natural language processing (NLP)&nbsp;where organizers define research tasks and settings.&nbsp;In order to cope for the specialties of DH research, we&nbsp;propose a ST that works in two phases, with two distinct target audiences and possible participants.</span></p>
<p><span class="font4">Generally, this setup allows both “sides” of the DH community to bring in what they do best: Humanities&nbsp;scholars focus on conceptual issues, their description&nbsp;and definition. Computer science researchers focus</span></p>
<p><span class="font4">on technical issues and work towards automatisation</span></p>
<p><span class="font4">(cf. Kuhn &amp; Reiter, 2015). The ideal scenario- that both “sides” of DH contribute to the work in both areas- is challenging to achieve in practice. The shared-task scenario takes this into account and encourages&nbsp;Humanities scholars without access to programming&nbsp;“resources” to contribute to the conceptual phase&nbsp;(Phase 1), while software engineers without interest&nbsp;in literature per se can contribute to the automatisation phase (Phase 2). We believe that this setup can&nbsp;actually lower the entry bar for DH research. Decoupling, however, does not imply strict, un-crossable&nbsp;boundaries: There needs to be interaction between&nbsp;the two phases, which is supported by our mixed organisation team. In particular, this setup allows&nbsp;mixed teams to participate in both phases (and it will&nbsp;be interesting to see how they fare).</span></p>
<p><span class="font4">In Phase 1 of a shared task, participants with a strong understanding of a specific literary phenomenon (literary studies scholars) work on the creation&nbsp;of annotation guidelines. This allows them to bring in&nbsp;their expertise without worrying about the feasibility of automatisation endeavours or struggling with&nbsp;technical issues. We will compare the different annotation guidelines both qualitatively: by having an indepth discussion during a workshop, and quantitatively: by measuring inter-annotator agreement. This&nbsp;will result in a community guided selection of annotation guidelines for a set of phenomena. The involvement of the research community in this process&nbsp;guarantees that heterogeneous points of view are&nbsp;taken into account.</span></p>
<p><span class="font4">The guidelines will then enter Phase 2 to actually make annotations on a semi-large scale. These annotations then enter a “classical” shared task as it is established in the NLP community: Various teams competitively contribute systems whose performances&nbsp;will be evaluated in a quantitative manner.</span></p>
<p><span class="font4">Given the complexity of many phenomena in literature, we expect the automatisation of such annotations to be an interesting challenge from an engineering perspective. On the other hand, it is an excellent opportunity to initiate the development of tools tailored to the detection of specific phenomena that are&nbsp;relevant for computational literary studies.</span></p>
<p><span class="font4">This talk has two purposes:</span></p>
<p><span class="font1">• &nbsp;&nbsp;&nbsp;</span><span class="font4">To discuss these ideas and collect&nbsp;feedback and propositions. This is also&nbsp;an explicit invitation to contribute in&nbsp;the setup of this initiative. We are also&nbsp;welcoming a discussion about the&nbsp;phenomena that should be included.</span></p>
<p><span class="font1">• &nbsp;&nbsp;&nbsp;</span><span class="font4">To advertise the idea of a shared task&nbsp;and to invite possible participants.</span></p>
<p><span class="font4">The success of STs relies on a certain number of participants. Given that this&nbsp;has never been organized in the DH&nbsp;community before, we want to spread&nbsp;this idea throughout the community to</span></p>
<p><span class="font4">gather estimates of potential participants.</span></p>
<p><span class="font1" style="font-weight:bold;">The Importance of Annotations</span></p>
<p><span class="font4">In computational literary studies, many phenomena cannot directly be detected from the text surface. To find and categorize such phenomena as, for example, the &quot;narrated time&quot; in a novel, it is first necessary&nbsp;to have an in-depth understanding of the text,&nbsp;knowledge about its author or literary conventions,&nbsp;or knowledge of the text's historical context. Therefore, instances of such phenomena need to be annotated either by human experts or software that is tailored to this task.</span></p>
<p><span class="font4">Unfortunately, many theories describing interesting phenomena are very difficult to apply to real texts. It has been shown numerous times (e.g., Reiter,&nbsp;2015, Musi et al., 2016) that annotating theories or&nbsp;concepts directly can lead to very poor inter-annotator agreement (IAA): Different annotators have different interpretions of not only the text, but also descriptions of the theoretical concepts. Although subjective annotations have their merit, studying annotations on large scale depends on their consistency,&nbsp;i.e., a high IAA. In addition, many theories are underspecified and provide examples for illustrations only.&nbsp;Creators of annotation guidelines often have to interpret what is meant by a certain statement and extend&nbsp;definitions to cover examples found in real texts.</span></p>
<p><span class="font4">Annotation guidelines serve as a mediator between the annotators and a theory (that may use specialised vocabulary). Additionally, such guidelines often contain re-appearing instance patterns and their modes of annotation and/or exceptions, as well as&nbsp;many examples from real texts (see below).</span></p>
<p><span class="font4">We see the creation of annotation guidelines as one of the cornerstones of large scale text analysis in&nbsp;computational literary studies. Additionally, the creation of annotiation guidelines supports systematic&nbsp;disciplinary discussions about concepts and thus&nbsp;may lead to additional findings relevant for the theoretical discourse (e.g., Meister, 1995; Gius and Jacke,&nbsp;forthcoming). Experts from the field literary studies&nbsp;are well-suited to work with annotation guidelines,&nbsp;as annotation of literary phenomena in literary texts&nbsp;can be seen as a special form of close reading.</span></p>
<p><span class="font1" style="font-weight:bold;">Phase One: Annotation Guidelines</span></p>
<p><span class="font1">The Shared Task</span></p>
<p><span class="font4">In theory, any phenomenon can be addressed in this fashion, as long as it can be defined inter-subjectively, is reasonably frequent, and is of interest in&nbsp;computational literary studies. As a starting point,&nbsp;we propose to address the issue of narrative levels&nbsp;(Pier, 2014). Narrative levels are a core concept in&nbsp;narrative theory (Genette, 1980; Bal, 1997) which in</span></p>
<p><span class="font4">turn has shown to be a promising foundation for automatisation in literary theory (Bogel et al. 2015). The first reason for choosing to examine narrative&nbsp;levels is their ubiquity: every narrative text necessarily consists at least one, most texts contain many&nbsp;narrative levels; each element of a text can be assigned to a specific level. The second reason is our intuition that a definition of &quot;level&quot; in guidelines is as&nbsp;achievable as the automated detection of levels by&nbsp;computers.</span></p>
<p><span class="font4">Concretely, participating teams are asked to create guidelines for the detection and annotation of a) narrative levels and b) the relation of the narrator(s)&nbsp;to the narrated world (i.e., is the narrator part of the&nbsp;narrated world or not?). Participants are not bound&nbsp;to adhere to a specific narratological theory. The result of this phase, however, will be a fixation on a set&nbsp;of guidelines (that instantiate a theory).</span></p>
<p><span class="font4">We will select a number of literary narrative texts and provide copyright-free digitized corpora. All “official” texts (development and test sets) will be English literary texts. Naturally, the second step will be&nbsp;to extend this framework to other languages and/or&nbsp;phenomena.</span></p>
<p><span class="font1">Evaluation</span></p>
<p><span class="font4">In NLP shared tasks, the predictions of the systems are compared against a fixed test set- the “gold standard”. Since there is no gold standard in Phase 1,&nbsp;we will evaluate the guidelines using an unseen data&nbsp;set. Each participating team annotates this set using&nbsp;their own guidelines before the guidelines are submitted. Submitted guidelines will be anonymized and&nbsp;re-distributed among the participants. Each participant is asked to annotate the evaluation data set using two other annotation guidelines. In addition, we&nbsp;will be collecting annotations from students.</span></p>
<p><span class="font4">The evaluation data set will thus be annotated according to each participant's guidelines four times (1x self, 1x student, 2x other participants).</span></p>
<p><span class="font4">This setup allows direct calculation of inter-annotator agreement. However, IAA should be only one aspect in evaluating the guidelines, but not the only&nbsp;one. Therefore, we will submit a workshop at DH&nbsp;2018 to discuss the submissions and select final&nbsp;ones. This setup also allows the merging of different&nbsp;annotation guidelines as well as adaptation according to the discussion during the workshop. Soon after&nbsp;the workshop, Phase 2 will commence.</span></p>
<p><span class="font1" style="font-weight:bold;">Phase Two: Automatic Prediction</span></p>
<p><span class="font4">In Phase 2 of this endeavour, the selected guidelines of Phase 1 will be annotated on a large scale by student assistants. Since we do not yet know how&nbsp;long, complex and involved the guidelines will be,&nbsp;there should be close communication between the&nbsp;organizers and the team responsible for the selected&nbsp;guidelines.</span></p>
<p><span class="font4">As soon as the texts have been annotated, they will be made accessible to participants. For the deadline in Phase 2, participants will be asked to process</span></p>
<p><span class="font4">a new data set - one that has not been released before - and return the predicted annotations to the organizers, who will then make evaluations using standard measures (e.g., accuracy). Finally, there will&nbsp;be a workshop in which each participating team presents its system. This workshop is projected to be coordinated with the LaTeCH workshop series, which&nbsp;has taken place at ACL conferences in the past years&nbsp;(one of the authors of this paper has been a workshop organiser in past years).</span></p>
<p><span class="font4">There will be no fixation on computational approaches, statistical models, programming languages or environments to tackle this problem. The main&nbsp;benefit of shared tasks towards automatisation is&nbsp;that different approaches can be compared directly.&nbsp;Restricting this possibility space would directly harm&nbsp;the goal.</span></p>
<p><span class="font1" style="font-weight:bold;">Technical Details and Timeline</span></p>
<p><span class="font4">This proposal is innovative in a number of ways: Shared tasks are a new kind of framework in the&nbsp;DH/H community, as such, focalisation have not been&nbsp;investigated in this way before. Last but not least, a&nbsp;shared task with the goal of creating annotation&nbsp;guidelines has not been organized before (to our&nbsp;knowledge). We believe it is more important to do&nbsp;this right than to do this fast, hence we are looking at&nbsp;a rather lengthy timeline.</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font0">Date</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">Event</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font0">August 2017</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">Announcement talks at DH and LaTeCH 2017</span></p></td></tr>
<tr><td>
<p><span class="font0">November 2017</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">Finalisation of Phase 1 details, submission for DH 2018, Call for participation (Phase 1)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">April 2018</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Submission deadline for guidelines</span></p></td></tr>
<tr><td>
<p><span class="font0">June 2018</span></p></td><td>
<p><span class="font0">Phase 1 workshop (DH)</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font0">August 2018</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">Announcement talk for Phase 2 (LaTeCH 2018)</span></p></td></tr>
<tr><td>
<p><span class="font0">October 2018</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">Finalisation of Phase 2 details, submission for ACL 2019, Call for participation (Phase 2)</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font0">May 2019</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">Submission deadline for systems for Phase 2</span></p></td></tr>
<tr><td>
<p><span class="font0">August 2019</span></p></td><td>
<p><span class="font0">Phase 2 workshop (ACL/LaTeCH)</span></p></td></tr>
</table>
<p><span class="font4">Attracting enough participants is the main challenge from the organiser's perspective. The main incentives we envision for contributors are excellent publication opportunities: All submitted and generated materials will be published online (open access)&nbsp;under the umbrella of the shared task. Each individual work will be citable. This includes the submitted&nbsp;annotation guidelines, the produced consensus&nbsp;guidelines, and explanation and commentary documents.</span></p>
<p><span class="font4">In addition to the publication incentive, we believe that our approach is an important contribution towards systematic text analysis in the DH realm. We&nbsp;count on the playfulness and curiosity (in the best&nbsp;sense!) of the DH community to take part in this experiment.</span></p>
<p><span class="font1" style="font-weight:bold;">Bibliography</span></p>
<p><span class="font3" style="font-weight:bold;">Bal, M. </span><span class="font3">(1997). Narratology: Introduction to the Theory of</span></p>
<p><span class="font3">Narrative. University of Toronto Press, 2<sup>nd</sup> edition.</span></p>
<p><span class="font3" style="font-weight:bold;">Bogel, T, Gertz M., Gius, E., Jacke, J., Meister, J. C., Petris,</span></p>
<p><span class="font3" style="font-weight:bold;">M., and Strotgen, J. </span><span class="font3">(2015). Collaborative Text Annotation Meets Machine Learning: heureCLEnA, a Digital Heuristic of Narrative. In DHCommons Journal, 2015.&nbsp;URL <a href="http://dhcommons.org/journal/issue-1/collabo-rative-text-annotation-meets-machine-learning-heu-recl%C3%A9-digital-heuristic">http://dhcommons.org/journal/issue-1/collabo-rative-text-annotation-meets-machine-learning-heu-recl%C3%A9-digital-heuristic</a>.</span></p>
<p><span class="font3" style="font-weight:bold;">Carlson, L., and Marcu, D </span><span class="font3">(2001). Discourse tagging reference manual. Annotation Manual, University of Southern California, 2001. URL <a href="https://www.isi.edu/">https://www.isi.edu/</a>&nbsp;marcu/discourse/tagging-ref-manual.pdf.</span></p><div>
<p><span class="font3" style="font-weight:bold;">Ferro, L., Gerber, L, Mani, I., , Sundheim, B, and Wilson,</span></p>
<p><span class="font3" style="font-weight:bold;">G. </span><span class="font3">(2005). TIDES 2005 Standard for the Annotation of Temporal Expressions. Technical report, The MITRE&nbsp;Corporation.</span></p></div><br clear="all"/>
<p><span class="font3" style="font-weight:bold;">Genette, G. </span><span class="font3">(1980). Narrative Discourse. An Essay in Method. Ithaca, N.Y: Cornell University Press.</span></p>
<p><span class="font3" style="font-weight:bold;">Gius, E., and Janina Jacke, J. </span><span class="font3">(2016). Zur Annotation nar-ratologischer Kategorien der zeit. Annotation Manual 2.0, Hamburg University, November 2016. URL&nbsp;<a href="http://heureclea.de/guidelines">http://heureclea.de/guidelines</a></span></p>
<p><span class="font3" style="font-weight:bold;">Gius, E., and Jacke, J. </span><span class="font3">(n.d.)The Hermeneutic Profit of Annotation. On preventing and fostering disagreement in literary text analysis. International Journal of Humanities and Arts Computing, forthcoming.</span></p>
<p><span class="font3" style="font-weight:bold;">Hovy, E., and Lavid. J. </span><span class="font3">(2010) Towards a ‘science' of corpus annotation: A new methodological challenge for corpus linguistics. International Journal of Translation&nbsp;Studies, 22(1).</span></p>
<p><span class="font3" style="font-weight:bold;">Kuhn, J., and Reiter, N. (</span><span class="font3">2015) A Plea for a Method-Driven Agenda in the Digital Humanities. In Proceedings of&nbsp;Digital Humanities 2015, Sydney, Australia, June 2015.</span></p>
<p><span class="font3" style="font-weight:bold;">Meister, J. C. </span><span class="font3">(1995). Consensus ex Machina? Consensus</span></p>
<p><span class="font3">qua Machina! Literary and Linguistic Computing,</span></p>
<p><span class="font3">10(4), pages 263-270.</span></p>
<p><span class="font3" style="font-weight:bold;">Musi, E., Ghosh, D., and Muresan, S. </span><span class="font3">(2016) Towards feasible guidelines for the annotation of argument schemes. In Proceedings of the Third Workshop on Argument Mining (ArgMining2016), pages 82-93, Berlin,</span></p>
<p><span class="font3">Germany, August 2016. Association for Computational Linguistics. URL <a href="http://www.aclweb.org/anthol-">http://www.aclweb.org/anthol-</a></span></p>
<p><span class="font3">ogy/W16-2810.</span></p>
<p><span class="font3" style="font-weight:bold;">Pier, J. </span><span class="font3">(2014). Narrative Levels (revised version; uploaded 23 April 2014). In Peter Huhn et al., editors, the living&nbsp;handbook of narratology. Hamburg: Hamburg University. URL <a href="http://www.lhn.uni-hamburg.de/arti-cle/narrative-levels-revised-version-uploaded-23-april-2014">http://www.lhn.uni-hamburg.de/arti-cle/narrative-levels-revised-version-uploaded-23-april-2014</a></span></p>
<p><span class="font3" style="font-weight:bold;">Reiter, N. </span><span class="font3">(2015). Towards annotating narrative segments. In Kalliopi Zervanou, Marieke van Erp, and Beatrice&nbsp;Alex, editors, Proceedings of the 9th SIGHUM Workshop on Language Technology for Cultural Heritage,&nbsp;Social Sciences, and Humanities (LaTeCH), pages 3438, Beijing,China, July 2015. Association for Computational Linguistics, Association for Computational Linguistics. URL <a href="http://www.aclweb.org/anthol-ogy/W15-3705">http://www.aclweb.org/anthol-ogy/W15-3705</a>.</span></p>
<p><span class="font3" style="font-weight:bold;">Santorini, B. </span><span class="font3">(1992) Penn treebank: Part-of-speech tagging. Annotation Manual 3, University of Pennsylvania, 1992. URL <a href="ftp://ftp.cis.upenn.edu/pub/tree-bank/doc/tagguide.ps.gz">ftp://ftp.cis.upenn.edu/pub/tree-bank/doc/tagguide.ps.gz</a>.</span></p>
<p><span class="font3" style="font-weight:bold;">Sauri, R., Littman, J., Knippen, B., Gaizauskas, R., Setzer, A., and Pustejovsky, J. </span><span class="font3">(n.d.) TimeML Annotation</span></p>
<p><span class="font3">Guidelines, Version 1.2.1. <a href="http://timeml.org/publica-tions/timeMLdocs/annguide_1.2.1.pdf">http://timeml.org/publica-tions/timeMLdocs/annguide_1.2.1.pdf</a></span></p>
<p><span class="font3" style="font-weight:bold;">Styler, W., Savova, G., Palmer, M., Pustejovsky, J., O'Gorman, T., and de Groen, P. C. </span><span class="font3">(2014) THYME Annotation Guidelines. Technical report. <a href="http://clear.colo-rado.edu/compsem/documents/THYME_guide-">http://clear.colo-rado.edu/compsem/documents/THYME_guide-</a></span></p>
<p><span class="font3">lines.pdf</span></p>
</body>
</html>