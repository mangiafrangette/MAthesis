<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><meta name="generator" content="ABBYY FineReader 14"/><title>Microsoft Word - 028. Sperberg-McQueen- Hapax Probabilistic part-of-speech tagging in XQuery and XForms-28.docx</title><link rel="stylesheet" href="028_files/028.css" type="text/css"/>
</head>
<body><h1><a name="caption1"></a><a name="bookmark0"></a><span class="font1" style="font-weight:bold;">Hapax: Probabilistic part-of-speech tagging in&nbsp;XQuery and XForms</span></h1>
<p><span class="font3" style="font-weight:bold;">C. M. Sperberg-McQueen</span></p>
<p><span class="font3"><a href="mailto:cmsmcq@blackmesatech.com">cmsmcq@blackmesatech.com</a></span></p>
<p><span class="font3">Black Mesa Technologies LLC, United States of America</span></p>
<p><span class="font3">Many programs perform part-of-speech (POS) tagging on texts [Leech et al. 1983, Booth 1985, Church 1988,&nbsp;DeRose 1988, Brill 1992, Leech et al. 1994, Schmidt&nbsp;1994, 1995, Toutanova et al. 2003]; although they use&nbsp;a variety of algorithms, their interfaces tend to be similar:</span></p>
<p><span class="font0">• &nbsp;&nbsp;&nbsp;</span><span class="font3">They work in batch mode, not interactively.</span></p>
<p><span class="font0">• &nbsp;&nbsp;&nbsp;</span><span class="font3">They generally model text as a flat sequence&nbsp;of characters; for most, XML markup must be&nbsp;removed before data are submitted to the&nbsp;tagger, and afterwards merged back into the&nbsp;output.</span></p>
<p><span class="font0">• &nbsp;&nbsp;&nbsp;</span><span class="font3">They are consequently unable to exploit information in XML markup — for example,&nbsp;that “Brown” is here a proper noun and “Essex” there a place name.</span></p>
<p><span class="font0">• &nbsp;&nbsp;&nbsp;</span><span class="font3">They tag each word token in the input with&nbsp;their best guess at the correct POS; by default, they do not distinguish low- and high-probability guesses.</span></p>
<p><span class="font0">• &nbsp;&nbsp;&nbsp;</span><span class="font3">They cannot accept partially tagged input. In&nbsp;consequence, the human annotator cannot&nbsp;help them by providing hints on some words.</span></p>
<p><span class="font0">• &nbsp;&nbsp;&nbsp;</span><span class="font3">They operate on words, not smaller segments.</span></p>
<p><span class="font3">This paper describes an XQuery-based POS tagger designed to differ from existing taggers in all of these ways. It works interactively one sentence at a time directly on XML (by default, TEI-encoded) text, exploits&nbsp;relevant markup, provides not just the most probable&nbsp;tagging of the input but several ranked alternatives, accepts partially tagged input, and can work on user-specified segments (e.g. TEI w [word] or m [morph] elements) instead of only on space-delimited tokens. Because the tagger described here is designed to support&nbsp;semi-automatic (or ‘half-automatic') POS annotation&nbsp;for XML data, it has been given the name Hapax.&nbsp;Hapax has been designed and implemented as part of&nbsp;the project “Annotated Turki Manuscripts from the Jarring Collection Online” (ATMO), supported by the&nbsp;Henry R. Luce Foundation; the author thanks both the&nbsp;Luce Foundation for their support and his colleagues&nbsp;in the ATMO project for their collaboration.</span></p>
<p><span class="font0" style="font-weight:bold;">Design considerations</span></p>
<p><span class="font3">Early POS taggers used morphological and other rules to assign POS tags to input; later experience&nbsp;showed that purely statistical methods like hidden&nbsp;Markov models (HMMs) could achieve better accuracy&nbsp;with less effort; for tutorial descriptions of HMMs see&nbsp;Rabiner 1989 and Charniak 1993.</span></p>
<p><span class="font3">For batch-mode POS tagging, accuracy and speed are obvious desiderata. Many modifications, refinements, and alternatives to HMMs have been proposed;&nbsp;these can improve accuracy by several percentage&nbsp;points. Larger training sets make a much larger difference. Schmid 1994 reports a comparison in which the&nbsp;least and most accurate taggers differ by two to four&nbsp;percentage points, while accuracy rates for small and&nbsp;large training sets (&lt; 10,000 and &gt; 1,000,000 words)&nbsp;differ by twelve to sixteen points.</span></p>
<p><span class="font3">For Hapax, intended to support human annotators working on under-resourced languages, raw speed is&nbsp;unimportant. For any tagger, the human annotator will&nbsp;need to correct many proposed taggings; the key to improving annotation speed is to make corrections faster.</span></p>
<p><span class="font3">Selecting the correct tag from a menu requires several interactions: the 80 tags in the Brown Corpus POS tag set do not fit into a single menu; many tag sets are&nbsp;larger. Accepting a proposed tagging for a word requires a single user-interface interaction (e.g. clicking&nbsp;“OK”).</span></p>
<p><span class="font3">So speed improves with accuracy: the fastest corrections are those not needed. But high accuracy requires large training sets, which under-resourced languages lack by definition. Some algorithms cope well with limited data. In the Brown Corpus, 92% of all tokens are tagged with the most frequent POS tag for&nbsp;their word type. A trivial 1-gram tagger, which just assigns the most frequent POS tag for each word form,&nbsp;will thus do almost as well on known words as more&nbsp;sophisticated algorithms. In reality, not all words are&nbsp;known, but a 1-gram tagger trained on as little as 2000&nbsp;words from the Brown Corpus will tag 60 to 70% of&nbsp;input tokens correctly. Larger training sets (8000,&nbsp;32000, 128000, 500000 words) again do better (6878%, 73-85%, 77-90%, 82-92%).</span></p>
<p><span class="font3">Also, we can make tagging errors less costly to fix. If the tagger provides one tag for each segment, every&nbsp;wrong guess costs a manual tag selection. If the tagger&nbsp;proposes several POS tags, then some errors will be as&nbsp;cheap as a correct tagging: one mouse-click. So the goal&nbsp;of Hapax's design is to minimize the need to select tags&nbsp;from menus, by proposing not one but several POS tags&nbsp;for each word.</span></p>
<p><span class="font3">If a 1-gram tagger for the Brown Corpus proposes not one but three POS tags, the correct tag will be&nbsp;among those proposed 71-80%, 79-86%, 84-93%, 8797%, or 90-98% of the time (for 2000-, 8000-, 32000, 128000, 500000-word training sets). If five tags are&nbsp;proposed, the correct tag will be proposed 79-88%,</span></p>
<p><span class="font3">87-92%, 91-95%, 92-97%, or 94-98% of the time.</span></p>
<p><span class="font3">If a single user interaction can accept a proposed tagging for the entire sentence, we will save one interaction for each word of the sentence. Hapax uses a&nbsp;standard bigram HMM to calculate the N most likely&nbsp;taggings for the entire sentence. The higher N is set,&nbsp;the greater the chances that only a single mouseclick&nbsp;will be required, but more time will be needed for&nbsp;reading and considering the proposals; it is likely that&nbsp;there is a point of diminishing returns.</span></p>
<p><span class="font0" style="font-weight:bold;">XQuery implementation</span></p>
<p><span class="font3">Hapax is implemented as a library of XQuery functions. One set of functions reads the training material and produces XML word- or POS-frequency lists from&nbsp;them. These list word types or POS tags by frequency,&nbsp;subdivided by POS tags or word types (or, for bigrams,&nbsp;POS of following segment). Additional functions calculate probability distributions for use with unknown&nbsp;words, using the technique of Charniak et al. 1993.</span></p>
<p><span class="font3">The 1-gram tagger consults the word/POS frequency list and returns the N most likely POS tags for the given word form. The bigram tagger consults the&nbsp;bigram and POS/word lists and uses the standard&nbsp;Viterbi algorithm to calculate the most likely path&nbsp;through the trellis of possible taggings for a sentence.&nbsp;A simple modification of the algorithm allows Hapax to&nbsp;calculate not one path but the best N paths, with time&nbsp;linear in the number of tags in the trellis.</span></p>
<p><span class="font3">Testing routines generate random test and training sets from a corpus stored as an XQuery database; in a&nbsp;project setting, the training sets are not created on the&nbsp;fly but prepared in advance and stored in a data-base.The primary interface for consumers of the Hapax library is the function hapax:tag(), which accepts&nbsp;as arguments:</span></p>
<p><span class="font0">• </span><span class="font3">An XML element representing a sentence</span></p>
<p><span class="font0">• </span><span class="font3">An indication of what frequency data to use</span></p>
<p><span class="font0">• </span><span class="font3">Optionally, a set of access functions The function calls the 1-gram and bigram taggers and&nbsp;returns an XML document describing possible POS taggings for the input. In the common case, the input sentence is a tei:s element, containing tei:w or tei:m elements to be tagged. Input elements may have type attributes; such a partial tagging of the sentence will affect the probabilities for the POS tags for other elements. The optional set of access functions allows Hapax to be used with non-TEI markup; the user-supplied&nbsp;functions are used to identify words in a sentence, detect POS tagging in the input, and add POS tags to the&nbsp;output.</span></p>
<p><span class="font3">The entire Hapax library is a few thousand lines of XQuery; the rich sets of data structures (including XML&nbsp;as a native type), higher-order functions, and grouping&nbsp;constructs in XQuery and XSLT make the implementation of POS-tagging algorithms remarkably straightforward.</span></p>
<p><span class="font0" style="font-weight:bold;">XForms interface</span></p>
<p><span class="font3">In the ATMO project, Hapax supports a browser-based user interface specified with XForms. The form displays a document, providing an Annotate button for&nbsp;each sentence. When the button fires, the form sends&nbsp;the sentence to the Hapax back end and uses the response to build a form for accepting or changing the&nbsp;annotation. The most likely taggings for the sentence&nbsp;are shown, each with an Accept button. A “Tag word-by-word” button is also shown; in word-by-word annotation, each segment in the sentence is displayed&nbsp;with several proposed tags: first those in the full-sentence taggings, then other common tags for the word&nbsp;type, and a worst-case “Tag manually” button which&nbsp;exposes the POS menus. The user can tag one or more&nbsp;words and activate a “Re-annotate” button, which resubmits the sentence to the back end. This allows the&nbsp;user to explore the effect of one POS assignment on&nbsp;POS probabilities for nearby words.</span></p>
<p><span class="font3">Within the ATMO project, data must also be segmented and spelling-regularized; those topics and their interaction with POS tagging are not discussed&nbsp;here.</span></p>
<p><span class="font0" style="font-weight:bold;">Further work</span></p>
<p><span class="font3">Hapax v1 uses standard 1- and 2-gram HMMs for POS tagging (Charniak et al. 1993). Future versions&nbsp;should implement Schmid's binary-decision-tree&nbsp;method (1994, 1995), which helps with sparse data.&nbsp;More challenging will be adapting the directed-graph&nbsp;model of Xuehelaiti et al. (2013) to probabilistic POS&nbsp;tagging. This two-level model would allow the probability of a given stem's POS tag to depend not only on&nbsp;the POS of the immediately preceding morpheme(s)&nbsp;but on the tag(s) of the preceding word stems, which&nbsp;may improve tagging accuracy for agglutinative languages.</span></p>
<p><span class="font0" style="font-weight:bold;">Bibliography</span></p>
<p><span class="font2" style="font-weight:bold;">Booth, B.M. </span><span class="font2">(1985), “Revising CLAWS,” </span><span class="font2" style="font-style:italic;">ICAME News</span><span class="font2"> 9: 2935.</span></p>
<p><span class="font2" style="font-weight:bold;">Brill, E. </span><span class="font2">(1992), “A simple rule-based part of speech tagger,” in </span><span class="font2" style="font-style:italic;">Proceedings of the Third conference on applied natural&nbsp;language processing</span><span class="font2">, Trento 31 March - 3 April 1992&nbsp;([n.p.]: Association for Computational Linguistics), pp.&nbsp;152-155.</span></p>
<p><span class="font2" style="font-weight:bold;">Charniak, E. </span><span class="font2">(1993), </span><span class="font2" style="font-style:italic;">Statistical language learning</span><span class="font2"> (Cambridge: MIT Press).</span></p>
<p><span class="font2" style="font-weight:bold;">Charniak, E., Hendrickson, C., Jacobson, N., and Parko-witz, M. </span><span class="font2">(1993), “Equations for Part-of-Speech Tagging,” in </span><span class="font2" style="font-style:italic;">Proceedings of the 11th National conference on artificial intelligence</span><span class="font2">, Washington DC July 11-15, 1993 ([n.p.]:&nbsp;The AAAI Press; Cambridge: MIT Press, 1993), pp. 784789. Web. <a href="http://www.aaai.org/Pa-pers/AAAI/1993AAAI93-117.pdf(Accessed">http://www.aaai.org/Pa-pers/AAAI/1993AAAI93-117.pdf(Accessed</a>: 1 October&nbsp;2016)</span></p>
<p><span class="font2" style="font-weight:bold;">Church, K. W. </span><span class="font2">(1988), “A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text,” in </span><span class="font2" style="font-style:italic;">Proceedings of&nbsp;the Second Conference on Applied Natural Language Processing</span><span class="font2">, Austin 9-12 February 1988 ([n.p.]: Association&nbsp;for Computational Linguistics), pp. 136-143. Web.&nbsp;<a href="http://www.aclweb.org/anthology/A/A88/A88-">http://www.aclweb.org/anthology/A/A88/A88-</a></span></p>
<p><span class="font2">1019.pdf. (Accessed: 1 October 2016)</span></p>
<p><span class="font2" style="font-weight:bold;">DeRose, S.J. </span><span class="font2">(1988), “Grammatical category disambiguation by statistical optimization,” </span><span class="font2" style="font-style:italic;">Computational Linguistics&nbsp;</span><span class="font2">14.1, pp. 31- 39.</span></p>
<p><span class="font2" style="font-weight:bold;">Rabiner, L. R. </span><span class="font2">(1989) “A Tutorial on Hidden Markov Models and Selected Applications in Speech Recogni-tion.”Markov Models and Selected Applications in Speech Recognition.” </span><span class="font2" style="font-style:italic;">Proceedings of the IEEE</span><span class="font2"> 77.2: 257286.</span></p>
<p><span class="font2" style="font-weight:bold;">Schmid, H. </span><span class="font2">(1995), “Improvements in part-of-speech tagging with an application to German,&quot; In </span><span class="font2" style="font-style:italic;">Proceedings of the ACL SIGDAT-Workshop. Dublin, Ireland</span><span class="font2">. Revised version available on the Web at <a href="http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/tree-tagger2.pdf">http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/tree-tagger2.pdf</a> (Accessed: 1 October 2016.)</span></p>
<p><span class="font2" style="font-weight:bold;">Schmid, H. </span><span class="font2">(1994), “Probabilistic part-of-speech tagging using decision trees,&quot; In </span><span class="font2" style="font-style:italic;">Proceedings of International Conference on New Methods in Language Processing, ManChester, &nbsp;&nbsp;&nbsp;UK.</span><span class="font2">&nbsp;&nbsp;&nbsp;&nbsp;Web.&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font2" style="text-decoration:underline;">http://www.cis.uni-</span></p>
<p><span class="font2" style="text-decoration:underline;">muenchen.de/~schmid/tools/TreeTagger/data/tree-</span></p>
<p><span class="font2" style="text-decoration:underline;">tagger1.pdf</span><span class="font2"> . (Accessed: 1 October 2016)</span></p>
<p><span class="font2" style="font-weight:bold;">Toutanova, K., Klein, D., Christopher Manning,, C. and Singer, Y. </span><span class="font2">(2003), “Feature-Rich Part-of-Speech Tagging&nbsp;with a Cyclic Dependency Network,&quot; In </span><span class="font2" style="font-style:italic;">Proceedings of</span></p>
<p><span class="font2" style="font-style:italic;">HLT-NAACL 2003,</span><span class="font2"> pp. 252-259.</span></p>
<p><span class="font2" style="font-weight:bold;">Xuehelaiti, M., Liu, K., Jiang, W., and Yibulayin, T.. </span><span class="font2">(2013). “Graphic Language Model for Agglutinative Languages:&nbsp;Uyghur as Study Case.&quot; </span><span class="font2" style="font-style:italic;">Chinese computational linguistics&nbsp;and natural language processing based on naturally annotated big data: 12th China National Conference, CCL&nbsp;2013 and First International Symposium, NLP-NABD&nbsp;2013</span><span class="font2">, Suzhou, China, October 10-12, 2013, Proceedings,&nbsp;ed. Maosong Sun. LNAI 8202. Berlin: Springer, pp. 268279.</span></p>
<p><span class="font2" style="font-weight:bold;">Leech, G. Garside, R., and Bryant, M. </span><span class="font2">(1994), “CLAWS4:</span></p>
<p><span class="font2">The tagging of the British National Corpus,” In </span><span class="font2" style="font-style:italic;">Proceedings of the 15th International conference on computational linguistics (COLING 94) Kyoto, Japan</span><span class="font2">, pp. 622-628. Web. <a href="http://ucrel.lancs.ac.uk/papers/coling.html">http://ucrel.lancs.ac.uk/papers/coling.html</a> . (Accessed: 1 October 2016)</span></p>
<p><span class="font2" style="font-weight:bold;">Leech, G., Garside, R., and Atwell, E. </span><span class="font2">(1983), “The automatic grammatical tagging of the LOB corpus,” </span><span class="font2" style="font-style:italic;">ICAME News</span><span class="font2"> 7: 13-33.</span></p>
</body>
</html>