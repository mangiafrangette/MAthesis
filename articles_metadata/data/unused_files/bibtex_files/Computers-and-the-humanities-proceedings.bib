@article{10.2307/30204878,
 ISSN = {00104817},
 URL = {http://www.jstor.org/stable/30204878},
 abstract = {Traditional discourses upon literature have been predicated upon the ability to refer to a text that others may consult (Landow, 1994, p. 33). Texts that involve elements of feedback and nontrivial decision-making on the part of the reader (Aarseth, 1997, p. 1) therefore present a challenge to readers and critics alike. Since a persuasive case has been made against a critical method that sets out to "identify the task of interpretation as a task of territorial exploration and territorial mastery" (Aarseth, p. 87), this paper proposes the use of readers in an empirically based approach to hypertext fiction. Meta-interpretation, a method that combines individual responses to a text, reading logs, screen recordings and limited qualitative/quantitative analysis, and critical interpretation is outlined. By analysing readers' responses it is possible to suggest both the ways that textual elements may have influenced or determined readers' choices and the ways that readers' choices "configure" the text. The method thus addresses Espen Aarseth's concerns and illuminates interesting features of interactive processes in fictional environments. The paper is divided into two parts: the first part sketches out meta-interpretation through consideration of the main problems confronting the literary critic; the second part describes reading research aimed at generating data for the literary critic.},
 author = {Colin Gardner},
 journal = {Computers and the Humanities},
 number = {1},
 pages = {33--56},
 publisher = {Springer},
 title = {Meta-Interpretation and Hypertext Fiction: A Critical Response},
 volume = {37},
 year = {2003}
}





@article{10.2307/30204879,
 ISSN = {00104817},
 URL = {http://www.jstor.org/stable/30204879},
 abstract = {Between August 2000 and August 2002, the Electronic Text Center at the University of Virginia distributed over seven million freely-available electronic books to users from more than 100 different countries. Delivered in a variety of formats, including .lit and .pdb, these ebooks have provided proof-of-concept for the adaptive uses of TEI standards beyond the World Wide Web standards that the Electronic Text Center has employed since its inception in 1992. The first half of this paper discusses the mechanics of ebook production at the Etext Center, the limits of the current technology, and the conversion workflow we hope to implement in the future. The second half discusses user response to our ebook collection, classroom applications of ebook technology, and the advantages and disadvantages that different formats offer to scholars and instructors in the humanities.},
 author = {Matthew Gibson and Christine Ruotolo},
 journal = {Computers and the Humanities},
 number = {1},
 pages = {57--63},
 publisher = {Springer},
 title = {Beyond the Web: TEI, the Digital Library, and the Ebook Revolution},
 volume = {37},
 year = {2003}
}





@article{10.2307/30204880,
 ISSN = {00104817},
 URL = {http://www.jstor.org/stable/30204880},
 abstract = {In this paper, we will present a publication system in which selected material from letter collections is presented as dialogues between two persons.},
 author = {Øyvind Eide},
 journal = {Computers and the Humanities},
 number = {1},
 pages = {65--75},
 publisher = {Springer},
 title = {Putting the Dialogue Back Together Re-Creating Structure in Letter Publishing},
 volume = {37},
 year = {2003}
}





@article{10.2307/30204881,
 ISSN = {00104817},
 URL = {http://www.jstor.org/stable/30204881},
 abstract = {This paper presents a numeric and information theoretic model for the measuring of language change, without specifying the particular type of change. It is shown that this measurement is intuitively plausible and that meaningful measurements can be made from as few as 1000 characters. This measurement technique is extended to the task of determining the "rate" of language change based on an examination of brief excerpts from the National Geographic Magazine and determining both their linguistic distance from one another as well as the number of years of temporal separation. A statistical analysis of these results shows, first, that language change can be measured, and second, that the rate of language change has not been uniform, and that in particular, the period 1939-1948 had particularly slow change, while 1949-1958 and 1959-1968 had particularly rapid changes.},
 author = {Patrick Juola},
 journal = {Computers and the Humanities},
 number = {1},
 pages = {77--96},
 publisher = {Springer},
 title = {The Time Course of Language Change},
 volume = {37},
 year = {2003}
}





@article{10.2307/30204882,
 ISSN = {00104817},
 URL = {http://www.jstor.org/stable/30204882},
 abstract = {Chaucer's Canterbury Tales consists of loosely-connected stories, appearing in many different orders in extant manuscripts. Differences in order result from rearrangements by scribes during copying, and may reveal relationships among manuscripts. Identifying these relationships is analogous to determining evolutionary relationships among organisms from the order of genes on a genome. We use gene order analysis to construct a stemma for the Canterbury Tales. This stemma shows relationships predicted by earlier scholars, reveals new relationships, and shares features with a word variation stemma. Our results support the idea that there was no established order when the first manuscripts were written.},
 author = {Matthew Spencer and Barbara Bordalejo and Li-San Wang and Adrian C. Barbrook and Linne R. Mooney and Peter Robinson and Tandy Warnow and Christopher J. Howe},
 journal = {Computers and the Humanities},
 number = {1},
 pages = {97--109},
 publisher = {Springer},
 title = {Analyzing the Order of Items in Manuscripts of "The Canterbury Tales"},
 volume = {37},
 year = {2003}
}





@article{10.2307/30204883,
 ISSN = {00104817},
 URL = {http://www.jstor.org/stable/30204883},
 abstract = {There are two important strategies in computer-assisted reading and analysis of text (CARAT). The first relates to the classification process, and the second pertains to the categorisation process. These two often-interrelated operations have been regularly recognised as essential components of text analysis. However, the two operations are highly time-consuming. A possible solution to this problem calls upon more inductive or bottom-up strategies that are numerical and statistical in nature. In our own research, we have been exploring a few of these techniques and their combination. We now know, through our own past research and others' work, that the classification methods allow a good empirical thematic exploration of a corpus. More specifically, in this paper we shall concentrate on the problem of assisting the automatic categorisation of small segments of a philosophical text into a set of thematic categories.},
 author = {Jean-Frédéric De Pasquale and Jean-Guy Meunier},
 journal = {Computers and the Humanities},
 number = {1},
 pages = {111--118},
 publisher = {Springer},
 title = {Categorisation Techniques in Computer-Assisted Reading and Analysis of Texts (CARAT) in the Humanities},
 volume = {37},
 year = {2003}
}


