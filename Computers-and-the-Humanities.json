{
    "10.2307/30204788": {
        "year": "2000",
        "volume": "34",
        "title": "Framework and Results for English SENSEVAL",
        "publisher": "Springer",
        "pages": "15--48",
        "number": "1/2",
        "journal": "Computers and the Humanities",
        "author": "A. Kilgarriff and J. Rosenzweig",
        "abstract": "SENSEVAL was the first open, community-based evaluation exercise for Word Sense Disambiguation programs. It adopted the quantitative approach to evaluation developed in MUC and other ARPA evaluation exercises. It took place in 1998. In this paper we describe the structure, organisation and results of the SENSEVAL exercise for English. We present and defend various design choices for the exercise, describe the data and gold-standard preparation, consider issues of scoring strategies and baselines, and present the results for the 18 participating systems. The exercise identifies the state-of-the-art for fine-grained word sense disambiguation, where training data is available, as 74-78% correct, with a number of algorithms approaching this level of performance. For systems that did not assume the availability of training data, performance was markedly lower and also more variable. Human inter-tagger agreement was high, with the gold standard taggings being around 95% replicable.",
        "url": "http://www.jstor.org/stable/30204788",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204788"
    },
    "10.2307/30204790": {
        "year": "2000",
        "volume": "34",
        "title": "Senseval/Romanseval: The Framework for Italian",
        "publisher": "Springer",
        "pages": "61--78",
        "number": "1/2",
        "journal": "Computers and the Humanities",
        "author": "Nicoletta Calzolari and Ornella Corazzari",
        "abstract": "In this paper we present some observations concerning an experiment of (manual/ automatic) semantic tagging of a small Italian corpus performed within the framework of the SENSEVAL/ROMANSEVAL initiative. The main goal of the initiative was to set up a framework for evaluation of Word Sense Disambiguation systems (WSDS) through the comparative analysis of their performance on the same type of data. In this experiment there are two aspects which are of relevance: first, the preparation of the reference annotated corpus, and, second, the evaluation of the systems against it. In both aspects we are mainly interested here in the analysis of the linguistic side which can lead to a better understanding of the problem of semantic annotation of a corpus, be it manual or automatic annotation. In particular, we will investigate, firstly, the reasons for disagreement between human annotators, secondly, some linguistically relevant aspects of the performance of the Italian WSDS and, finally, the lessons learned from the present experiment.",
        "url": "http://www.jstor.org/stable/30204790",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204790"
    },
    "10.2307/30204791": {
        "year": "2000",
        "volume": "34",
        "title": "Tagger Evaluation Given Hierarchical Tag Sets",
        "publisher": "Springer",
        "pages": "79--84",
        "number": "1/2",
        "journal": "Computers and the Humanities",
        "author": "I. Dan Melamed and Philip Resnik",
        "abstract": "We present methods for evaluating human and automatic taggers that extend current practice in three ways. First, we show how to evaluate taggers that assign multiple tags to each test instance, even if they do not assign probabilities. Second, we show how to accommodate a common property of manually constructed \"gold standards\" that are typically used for objective evaluation, namely that there is often more than one correct answer. Third, we show how to measure performance when the set of possible tags is tree-structured in an IS-A hierarchy. To illustrate how our methods can be used to measure inter-annotator agreement, we show how to compute the kappa coefficient over hierarchical tag sets.",
        "url": "http://www.jstor.org/stable/30204791",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204791"
    },
    "10.2307/30204792": {
        "year": "2000",
        "volume": "34",
        "title": "Peeling an Onion: The Lexicographer's Experience of Manual Sense-Tagging",
        "publisher": "Springer",
        "pages": "85--97",
        "number": "1/2",
        "journal": "Computers and the Humanities",
        "author": "Ramesh Krishnamurthy and Diane Nicholls",
        "abstract": "SENSEVAL set itself the task of evaluating automatic word sense disambiguation programs (see Kilgarriff and Rosenzweig, this volume, for an overview of the framework and results). In order to do this, it was necessary to provide a 'gold standard' dataset of 'correct' answers. This paper will describe the lexicographic part of the process involved in creating that dataset. The primary objective was for a group of lexicographers to manually examine keywords in a large number of corpus contexts, and assign to each context a sense-tag for the keyword, taken from the Hector dictionary. Corpus contexts also had to be manually part-of-speech (POS) tagged. Various observations made and insights gained by the lexicographers during this process will be presented, including a critique of the resources and the methodology.",
        "url": "http://www.jstor.org/stable/30204792",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204792"
    },
    "10.2307/30204794": {
        "year": "2000",
        "volume": "34",
        "title": "Combining Supervised and Unsupervised Lexical Knowledge Methods for Word Sense Disambiguation",
        "publisher": "Springer",
        "pages": "103--108",
        "number": "1/2",
        "journal": "Computers and the Humanities",
        "author": "E. Agirre and G. Rigau and L. Padró and J. Atserias",
        "abstract": "This work combines a set of available techniques - which could be further extended to perform noun sense disambiguation. We use several unsupervised techniques (Rigau et al., 1997) that draw knowledge from a variety of sources. In addition, we also apply a supervised technique in order to show that supervised and unsupervised methods can be combined to obtain better results. This paper tries to prove that using an appropriate method to combine those heuristics we can disambiguate words in free running text with reasonable precision.",
        "url": "http://www.jstor.org/stable/30204794",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204794"
    },
    "10.2307/30204795": {
        "year": "2000",
        "volume": "34",
        "title": "Word Sense Disambiguation Using Automatically Acquired Verbal Preferences",
        "publisher": "Springer",
        "pages": "109--114",
        "number": "1/2",
        "journal": "Computers and the Humanities",
        "author": "John Carroll and Diana McCarthy",
        "abstract": "The selectional preferences of verbal predicates are an important component of a computational lexicon. They have frequently been cited as being useful for WSD, alongside other sources of knowledge. We evaluate automatically acquired selectional preferences on the level playing field provided by SENSEVAL to examine to what extent they help in WSD.",
        "url": "http://www.jstor.org/stable/30204795",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204795"
    },
    "10.2307/30204796": {
        "year": "2000",
        "volume": "34",
        "title": "A Topical/Local Classifier for Word Sense Identification",
        "publisher": "Springer",
        "pages": "115--120",
        "number": "1/2",
        "journal": "Computers and the Humanities",
        "author": "Martin Chodorow and Claudia Leacock and George A. Miller",
        "abstract": "TLC is a supervised training (S) system that uses a Bayesian statistical model and features of a word's context to identify word sense. We describe the classifier's operation and how it can be configured to use only topical context cues, only local cues, or a combination of both. Our results on Senseval's final run are presented along with a comparison to the performance of the best S system and the average for S systems. We discuss ways to improve TLC by enriching its feature set and by substituting other decision procedures for the Bayesian model. Future development of supervised training classifiers will depend on the availability of tagged training data. TLC can assist in the hand-tagging effort by helping human taggers locate infrequent senses of polysemous words.",
        "url": "http://www.jstor.org/stable/30204796",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204796"
    },
    "10.2307/30204798": {
        "year": "2000",
        "volume": "34",
        "title": "Word Sense Disambiguation by Information Filtering and Extraction",
        "publisher": "Springer",
        "pages": "127--134",
        "number": "1/2",
        "journal": "Computers and the Humanities",
        "author": "Jeremy Ellman and Ian Klincke and John Tait",
        "abstract": "We describe a simple approach to word sense disambiguation using information filtering and extraction. The method fully exploits and extends the information available in the Hector dictionary. The algorithm proceeds by the application of several filters to prune the candidate set of word senses returning the most frequent if more than one remains. The experimental methodology and its implication are also discussed.",
        "url": "http://www.jstor.org/stable/30204798",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204798"
    },
    "10.2307/30204799": {
        "year": "2000",
        "volume": "34",
        "title": "Large Scale WSD Using Learning Applied to SENSEVAL",
        "publisher": "Springer",
        "pages": "135--140",
        "number": "1/2",
        "journal": "Computers and the Humanities",
        "author": "Paul Hawkins and David Nettleton",
        "abstract": "A word sense disambiguation system which is going to be used as part of a NLP system needs to be large scale, able to be optimised towards a specific task and above all accurate. This paper describes the knowledge sources used in a disambiguation system able to achieve all three of these criteria. It is a hybrid system combining sub-symbolic, stochastic and rule-based learning. The paper reports the results achieved in Senseval and analyses them to show the system's strengths and weaknesses relative to other similar systems.",
        "url": "http://www.jstor.org/stable/30204799",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204799"
    },
    "10.2307/30204800": {
        "year": "2000",
        "volume": "34",
        "title": "Word Sense Disambiguation Using the Classification Information Model: Experimental Results on the SENSEVAL Workshop",
        "publisher": "Springer",
        "pages": "141--146",
        "number": "1/2",
        "journal": "Computers and the Humanities",
        "author": "Ho Lee and Hae-Chang Rim and Jungyun Seo",
        "abstract": "A Classification Information Model is a pattern classification model. The model decides the proper class of an input instance by integrating individual decisions, each of which is made with each feature in the pattern. Each individual decision is weighted according to the distributional property of the feature deriving the decision. An individual decision and its weight are represented as classification information which is extracted from the training instances. In the word sense disambiguation based on the model, the proper sense of an input instance is determined by the weighted sum of whole individual decisions derived from the features contained in the instance.",
        "url": "http://www.jstor.org/stable/30204800",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204800"
    },
    "10.2307/30204802": {
        "year": "2000",
        "volume": "34",
        "title": "Senseval: The CL Research Experience",
        "publisher": "Springer",
        "pages": "153--158",
        "number": "1/2",
        "journal": "Computers and the Humanities",
        "author": "Kenneth C. Litkowski",
        "abstract": "The CL Research Senseval system was the highest performing system among the \"Allwords\" systems, with an overall fine-grained score of 61.6 percent for precision and 60.5 percent for recall on 98 percent of the 8,448 texts on the revised submission (up by almost 6 and 9 percent from the first). The results were achieved with an almost complete reliance on syntactic behavior, using (1) a robust and fast ATN-style parser producing parse trees with annotations on nodes, (2) DIMAP dictionary creation and maintenance software (after conversion of the Hector dictionary files) to hold dictionary entries, and (3) a strategy for analyzing the parse trees in concert with the dictionary data. Further considerable improvements are possible in the parser, exploitation of the Hector data (and representation of dictionary entries), and the analysis strategy, still with syntactic and collocational data. The Senseval data (the dictionary entries and the corpora) provide an excellent testbed for understanding the sources of failures and for evaluating changes in the CL Research system.",
        "url": "http://www.jstor.org/stable/30204802",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204802"
    },
    "10.2307/30204803": {
        "year": "2000",
        "volume": "34",
        "title": "Selecting Decomposable Models for Word-Sense Disambiguation: The \"Grling-Sdm\" System",
        "publisher": "Springer",
        "pages": "159--164",
        "number": "1/2",
        "journal": "Computers and the Humanities",
        "author": "Tom O'Hara and Janyce Wiebe and Rebecca Bruce",
        "abstract": "This paper describes the grling-sdm system, which is a supervised probabilistic classifier that participated in the 1998 SENSEVAL competition for word-sense disambiguation. This system uses model search to select decomposable probability models describing the dependencies among the feature variables. These types of models have been found to be advantageous in terms of efficiency and representational power. Performance on the SENSEVAL evaluation data is discussed.",
        "url": "http://www.jstor.org/stable/30204803",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204803"
    },
    "10.2307/30204804": {
        "year": "2000",
        "volume": "34",
        "title": "Simple Word Sense Discrimination: Towards Reduced Complexity",
        "publisher": "Springer",
        "pages": "165--170",
        "number": "1/2",
        "journal": "Computers and the Humanities",
        "author": "Keith Suderman",
        "abstract": "Wisdom is a system for performing word sense disambiguation (WSD) using a limited number of linguistic features and a simple supervised learning algorithm. The most likely sense tag for a word is determined by calculating co-occurrence statistics for words appearing within a small window. This paper gives a brief description of the components in the Wisdom system and the algorithm used to predict the correct sense tag. Some results for Wisdom from the Senseval competition are presented, and directions for future work are also explored.",
        "url": "http://www.jstor.org/stable/30204804",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204804"
    },
    "10.2307/30204805": {
        "year": "2000",
        "volume": "34",
        "title": "Memory-Based Word Sense Disambiguation",
        "publisher": "Springer",
        "pages": "171--177",
        "number": "1/2",
        "journal": "Computers and the Humanities",
        "author": "Jorn Veenstra and Antal Van den Bosch and Sabine Buchholz and Walter Daelemans and Jakub Zavrel",
        "abstract": "We describe a memory-based classification architecture for word sense disambiguation and its application to the SENSEVAL evaluation task. For each ambiguous word, a semantic word expert is automatically trained using a memory-based approach. In each expert, selecting the correct sense of a word in a new context is achieved by finding the closest match to stored examples of this task. Advantages of the approach include (i) fast development time for word experts, (ii) easy and elegant automatic integration of information sources, (iii) use of all available data for training the experts, and (iv) relatively high accuracy with minimal linguistic engineering.",
        "url": "http://www.jstor.org/stable/30204805",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204805"
    },
    "10.2307/30204806": {
        "year": "2000",
        "volume": "34",
        "title": "Hierarchical Decision Lists for Word Sense Disambiguation",
        "publisher": "Springer",
        "pages": "179--186",
        "number": "1/2",
        "journal": "Computers and the Humanities",
        "author": "David Yarowsky",
        "abstract": "This paper describes a supervised algorithm for word sense disambiguation based on hierarchies of decision lists. This algorithm supports a useful degree of conditional branching while minimizing the training data fragmentation typical of decision trees. Classifications are based on a rich set of collocational, morphological and syntactic contextual features, extracted automatically from training data and weighted sensitive to the nature of the feature and feature class. The algorithm is evaluated comprehensively in the SENSEVAL framework, achieving the top performance of all participating supervised systems on the 36 test words where training data is available.",
        "url": "http://www.jstor.org/stable/30204806",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204806"
    },
    "10.2307/30204807": {
        "year": "2000",
        "volume": "34",
        "title": "Using Semantic Classification Trees for WSD",
        "publisher": "Springer",
        "pages": "187--192",
        "number": "1/2",
        "journal": "Computers and the Humanities",
        "author": "C. de Loupy and M. El-Bèze and P. -F. Marteau",
        "abstract": "This paper describes the evaluation of a WSD method within SENSEVAL. This method is based on Semantic Classification Trees (SCTs) and short context dependencies between nouns and verbs. The training procedure creates a binary tree for each word to be disambiguated. SCTs are easy to implement and yield some promising results. The integration of linguistic knowledge could lead to substantial improvement.",
        "url": "http://www.jstor.org/stable/30204807",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204807"
    },
    "10.2307/30204809": {
        "year": "2000",
        "volume": "34",
        "title": "ROMANSEVAL: Results for Italian by SENSE",
        "publisher": "Springer",
        "pages": "199--204",
        "number": "1/2",
        "journal": "Computers and the Humanities",
        "author": "Stefano Federici and Simonetta Montemagni and Vito Pirrelli",
        "abstract": "The paper describes SENSE, a word sense disambiguation system that makes use of different types of cues to infer the most likely sense of a word given its context. Architecture and functioning of the system are briefly illustrated. Results are given for the ROMANSEVAL Italian test corpus of verbs.",
        "url": "http://www.jstor.org/stable/30204809",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204809"
    },
    "10.2307/30204811": {
        "year": "2000",
        "volume": "34",
        "title": "Consistent Criteria for Sense Distinctions",
        "publisher": "Springer",
        "pages": "217--222",
        "number": "1/2",
        "journal": "Computers and the Humanities",
        "author": "Martha Palmer",
        "abstract": "This paper specifically addresses the question of polysemy with respect to verbs, and whether or not the sense distinctions that are made in on-line lexical resources such as WordNet are appropriate for computational lexicons. The use of sets of related syntactic frames and verb classes are examined as a means of simplifying the task of defining different senses, and the importance of concrete criteria such as different predicate argument structures, semantic class constraints and lexical co-occurrences is emphasized.",
        "url": "http://www.jstor.org/stable/30204811",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204811"
    },
    "10.2307/30204812": {
        "year": "2000",
        "volume": "34",
        "title": "Cross-Lingual Sense Determination: Can It Work?",
        "publisher": "Springer",
        "pages": "223--234",
        "number": "1/2",
        "journal": "Computers and the Humanities",
        "author": "Nancy Ide",
        "abstract": "This article reports the results of a preliminary analysis of translation equivalents in four languages from different language families, extracted from an on-line parallel corpus of George Orwell's Nineteen Eighty-Four. The goal of the study is to determine the degree to which translation equivalents for different meanings of a polysemous word in English are lexicalized differently across a variety of languages, and to determine whether this information can be used to structure or create a set of sense distinctions useful in natural language processing applications. A coherence index is computed that measures the tendency for different senses of the same English word to be lexicalized differently, and from this data a clustering algorithm is used to create sense hierarchies.",
        "url": "http://www.jstor.org/stable/30204812",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204812"
    },
    "10.2307/30204813": {
        "year": "2000",
        "volume": "34",
        "title": "Is Word Sense Disambiguation Just One More NLP Task?",
        "publisher": "Springer",
        "pages": "235--243",
        "number": "1/2",
        "journal": "Computers and the Humanities",
        "author": "Yorick Wilks",
        "abstract": "The paper examines the task of Word Sense Disambiguation (WSD) critically and compares it with Part of Speech (POS) tagging, arguing that the ability of a writer to create new senses distinguishes the tasks and makes it more problematic to test WSD by the mark-up-and-model paradigm, because new senses cannot be marked up against dictionaries. This serves to set WSD apart and puts limits on its effectiveness as an independent NLP task. Moreover, it is argued that current WSD methods based on very small word samples are also potentially misleading because they may or may not scale up. Since all-word WSD methods are now available and are producing figures comparable to the smaller scale tasks, it is argued that we should concentrate on the former and find ways of bootstrapping test materials for such tests in the future.",
        "url": "http://www.jstor.org/stable/30204813",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204813"
    },
    "10.2307/30204818": {
        "year": "2000",
        "volume": "34",
        "title": "Tailor-Made or Off-the-Peg? Virtual Courses in the Humanities",
        "publisher": "Springer",
        "pages": "255--264",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Geoffrey Channon",
        "abstract": "The article describes how the British Government sees the electronic campus as a \"quick fix\" for delivering its idea of the learning society and mass higher (and further) education. It suggests that this solution poses a number of major difficulties, especially for the humanities. The issues are located in a global context in which it is argued that so-called \"mega-universities\" will come to dominate course production and distribution. This development will have profound implications for notions of institutional autonomy and the autonomy of individual teachers, and may undermine academic pluralism. The article concludes by arguing that, in the humanities, interventions are needed to secure an appropriate meld between the existing \"face-to-face\" methods of teaching and learning, and the new technology.",
        "url": "http://www.jstor.org/stable/30204818",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204818"
    },
    "10.2307/30204819": {
        "year": "2000",
        "volume": "34",
        "title": "From Concordances to Subject Portals: Supporting the Text-Centred Humanities Community",
        "publisher": "Springer",
        "pages": "265--278",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Michael Fraser",
        "abstract": "This paper discusses selected aspects of the work of the CTI Centre for Textual Studies, a Centre which has its roots in a 1984 initiative and ceased to operate in 1999. The work of the Centre was grounded in humanities computing, a subject area which itself has developed over time. The article compares earlier observations made by Joseph Raben and Susan Hockey about the integration of resources within humanities teaching and learning, to current realities. Its focus is the development of access to distributed resources, beginning with an interface between the early PC and the mainframe and ending with a vision of a humanities portal to distributed resources.",
        "url": "http://www.jstor.org/stable/30204819",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204819"
    },
    "10.2307/30204820": {
        "year": "2000",
        "volume": "34",
        "title": "Electrifying the Canon: The Impact of Computing on Classical Studies",
        "publisher": "Springer",
        "pages": "279--295",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Lorna Hardwick",
        "abstract": "The article offers a case study of the relationship between current developments in Classical Studies and the impact of computing and IT. The first section summarises the main features of the Classical Studies environment, especially the deep seated changes which have been taking place. These changes are then related to specific initiatives in Research, Teaching and Learning. The discussion is framed by a statement of micro-criteria for the evaluation of new developments and by reference to the macro-climate of debate about the nature of cyberspace, especially the dichotomy between conceptions of post-modern diversity and of Enlightenment images of rational structures. It is suggested that these debates mirror those with which the discipline itself engages.",
        "url": "http://www.jstor.org/stable/30204820",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204820"
    },
    "10.2307/30204821": {
        "year": "2000",
        "volume": "34",
        "title": "Wag the Dog? Online Conferencing and Teaching",
        "publisher": "Springer",
        "pages": "297--309",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Charles Ess",
        "abstract": "Web-accessible conferencing software and \"conversational ethics\" drawn from Habermas and Rawls have successfully brought together on-line participants separated by geography and viewpoint, and occasionally resulted in consensus regarding otherwise divisive issues such as abortion. The author describes successes, limitations, and costs of incorporating these technologies and discourse ethics in a religious studies class. Results are striking, but the pedagogical benefits involve technical risks and high labor and time costs. This experience, coupled with recent research, suggests that electronic pedagogies, like other teaching strategies, work for some, but not all students: this argues that we take up electronic teaching as one approach among many.",
        "url": "http://www.jstor.org/stable/30204821",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204821"
    },
    "10.2307/30204823": {
        "year": "2000",
        "volume": "34",
        "title": "Computers and Resource-Based History Teaching: A UK Perspective",
        "publisher": "Springer",
        "pages": "325--343",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Donald A. Spaeth and Sonja Cameron",
        "abstract": "This article presents an overview of developments in computer-aided history teaching in higher education in the UK and the US, which have focused primarily on providing students with access to primary sources in order to enhance their understanding of historical methods and content. From an initial emphasis on research training for postgraduates, which taught quantitative methods, or the use of drill-style question-and-answer programs, advances in hardware capacity and software provision have led to more varied methods of analysis. Computer-assisted learning lends itself particularly to the growing emphasis on students' use of primary sources, as more texts become available in electronic format on CD-ROM or the World Wide Web. Hypermedia can provide a unique learning environment in which students are exposed to different genres of sources such as images, texts and numerical data, encouraging them to discover interconnections and complexities, while learning at their own pace. Students can be expected to develop critical skills by comparing primary sources and forming their own historical interpretations. The various problems and methods of locating and assessing relevant information in cyberspace also foster critical thinking and a spirit of investigation. A computer-assisted course taught in Glasgow showed that students value the ease of access to relevant source materials offered by customised resource packs, which left them with more time to evaluate their contents critically.",
        "url": "http://www.jstor.org/stable/30204823",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204823"
    },
    "10.2307/30204829": {
        "year": "2000",
        "volume": "34",
        "title": "Spatial Distribution of Rural Social Strata: Using Digitised Maps in Historical Research",
        "publisher": "Springer",
        "pages": "359--375",
        "number": "4",
        "journal": "Computers and the Humanities",
        "author": "Siegfried Gruber",
        "abstract": "This article deals with using digitised maps in historical research and their possible contributions to it. The use of cartographic data is especially useful in research dealing with the spatial distribution of various phenomena. The spatial distribution of rural social strata is one such phenomenon that has not yet received much attention. The case study of Pischelsdorf, an Austrian settlement, in the beginning of the nineteenth century shall serve as an example for such research. Pischelsdorf is situated in the Austrian province of Styria, about 150 km from Vienna, and served as a small centre for trade and handicraft for the surrounding villages. This study is based on the land register of 1822 which has been digitised.",
        "url": "http://www.jstor.org/stable/30204829",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204829"
    },
    "10.2307/30204830": {
        "year": "2000",
        "volume": "34",
        "title": "Using Constraint Logic Programming to Analyze the Chronology in \"A Rose for Emily\"",
        "publisher": "Springer",
        "pages": "377--392",
        "number": "4",
        "journal": "Computers and the Humanities",
        "author": "Jennifer Burg and Anne Boyle and Sheau-Dong Lang",
        "abstract": "William Faulkner's non-chronological story telling style has long been a challenge to critics and a puzzle to beginning literature students. \"A Rose for Emily,\" one of Faulkner's most frequently anthologized stories, exemplifies the complexity of Faulkner's treatment of time. In this paper, we apply a constraint-based problem solving method to an analysis of the chronology of \"A Rose for Emily.\" Constraint logic programming is a declarative programming language paradigm that solves problems by enforcing constraints among variables. CLP's ability to sort numeric variables that do not yet have definite values makes it possible to sort the events of \"A Rose for Emily\" with only fragmented and relative time information. In attempting to sort the events of the story, we find an inconsistency in the temporal references scattered throughout the narrative. After removing this inconsistency, we are able to compare our chronology with earlier ones and discuss the thematic relevance of Faulkner's nonlinear plots.",
        "url": "http://www.jstor.org/stable/30204830",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204830"
    },
    "10.2307/30204831": {
        "year": "2000",
        "volume": "34",
        "title": "An Architecture and Query Language for a Federation of Heterogeneous Dictionary Databases",
        "publisher": "Springer",
        "pages": "393--407",
        "number": "4",
        "journal": "Computers and the Humanities",
        "author": "Jon Patrick and Jun Zhang and Xabier Artola-Zubillaga",
        "abstract": "An architecture for federating heterogeneous dictionary databases is described. It proposes a common description language and query language to provide for the exchange of information between databases with different organizations, on different platforms and in different DBMSs. The common query language has an SQL like structure. The first version of the description language follows the TEI standard tag definitions for dictionaries with the expectation that the description language will be expanded in the future. A practical implementation of the proposals using WWW technology for two multi-lingual dictionaries is described.",
        "url": "http://www.jstor.org/stable/30204831",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204831"
    },
    "10.2307/30204837": {
        "year": "2001",
        "volume": "35",
        "title": "Pattern Processing in Melodic Sequences: Challenges, Caveats and Prospects",
        "publisher": "Springer",
        "pages": "9--21",
        "number": "1",
        "journal": "Computers and the Humanities",
        "author": "Emilios Cambouropoulos and Tim Crawford and Costas S. Iliopoulos",
        "abstract": "In this paper a number of issues relating to the application of string processing techniques on musical sequences are discussed. A brief survey of some musical string processing algorithms is given and some issues of melodic representation, abstraction, segmentation and categorisation are presented. This paper is not intended to provide solutions to string processing problems but rather to highlight possible stumbling-block areas and raise awareness of primarily music-related particularities that can cause problems in matching applications.",
        "url": "http://www.jstor.org/stable/30204837",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204837"
    },
    "10.2307/30204838": {
        "year": "2001",
        "volume": "35",
        "title": "Perceptual Issues in Music Pattern Recognition: Complexity of Rhythm and Key Finding",
        "publisher": "Springer",
        "pages": "23--35",
        "number": "1",
        "journal": "Computers and the Humanities",
        "author": "Ilya Shmulevich and Olli Yli-Harja and Edward Coyle and Dirk-Jan Povel and Kjell Lemström",
        "abstract": "We consider several perceptual issues in the context of machine recognition of music patterns. It is argued that a successful implementation of a music recognition system must incorporate perceptual information and error criteria. We discuss several measures of rhythm complexity which are used for determining relative weights of pitch and rhythm errors. Then, a new method for determining a localized tonal context is proposed. This method is based on empirically derived key distances. The generated key assignments are then used to construct the perceptual pitch error criterion which is based on note relatedness ratings obtained from experiments with human listeners.",
        "url": "http://www.jstor.org/stable/30204838",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204838"
    },
    "10.2307/30204839": {
        "year": "2001",
        "volume": "35",
        "title": "Representing Melodic Patterns as Networks of Elaborations",
        "publisher": "Springer",
        "pages": "37--54",
        "number": "1",
        "journal": "Computers and the Humanities",
        "author": "Alan Marsden",
        "abstract": "Previous discussions of musical pattern have underlined difficulties in seeking pattern as a sequence of pitches, or of intervals or of other local and atomic features. This paper describes a manner of representing melodies through a hierarchical structure of elaboration, derived from concepts common in music theory (in particular, the concept of reduction found in the work of Schenker and of Lerdahl & Jackendoff). The fundamental structure is a planar directed acyclic graph, each node of which represents a musical note (not necessarily as it is present in the actual melody) and an elaboration which generates that note on the basis of two parents. These graph structures can be converted to trees, aiding processing and comparison, in two ways. Firstly, any graph can be transformed into a set of binary trees in which each node represents an interval between two notes and an elaboration of that interval. Secondly, in the planar graph, the link of a node to one of its parents often provides no useful information and can be disregarded, resulting in a reduction of the graph tending towards a set of trees. From this arises a new approach to the question of melodic segmentation. Examples of melodic fragments represented in this manner demonstrate how the representation makes explicit similarities between fragments which would not be found by an approach using sequences of features.",
        "url": "http://www.jstor.org/stable/30204839",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204839"
    },
    "10.2307/30204840": {
        "year": "2001",
        "volume": "35",
        "title": "Approximate Musical Evolution",
        "publisher": "Springer",
        "pages": "55--64",
        "number": "1",
        "journal": "Computers and the Humanities",
        "author": "Tim Crawford and Costas S. Iliopoulos and Russel Winder and Haifeng Yu",
        "abstract": "Musical patterns that recur in approximate, rather than identical, form within a composition (or body of musical work) are considered to be of considerable importance in music analysis. Here we consider the \"evolutionary chain problem\": this is the problem of computing a chain of all \"motif\" recurrences, each of which is a transformation of (\"similar\" to) the original motif, but each of which is progressively further from the original. Here we consider several variants of the evolutionary chain problem and we present efficient algorithms for solving them.",
        "url": "http://www.jstor.org/stable/30204840",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204840"
    },
    "10.2307/30204841": {
        "year": "2001",
        "volume": "35",
        "title": "Investigating the Influence of Representations and Algorithms in Music Classification",
        "publisher": "Springer",
        "pages": "65--79",
        "number": "1",
        "journal": "Computers and the Humanities",
        "author": "Karin Höthker and Dominik Hörnel and Christina Anagnostopoulou",
        "abstract": "Classification in music analysis involves the segmentation of a music piece and the categorisation of the segments depending on similarity-based criteria. In this paper we investigate, based on a formal approach, how variations in the representation of the musical segments and in the categorisation algorithm influence the outcome of the classification. More specifically, we vary the choice of features describing each segment, the way these features are represented, and the categorisation algorithm. At the same time, we keep the other parameters, that is the overall model architecture, the music pieces, and the segmentation, fixed. We show that the choice and representation of the features, but not the specific categorisation algorithm, have a strong impact on the obtained analysis. We introduce a distance function to compare the results of algorithmic and human classification, and we show that an appropriate choice of features can yield results that are very similar to a human classification. These results allow an objective evaluation of different approaches to music classification in a uniform setting.",
        "url": "http://www.jstor.org/stable/30204841",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204841"
    },
    "10.2307/30204845": {
        "year": "2001",
        "volume": "35",
        "title": "Finding Syntactic Structure in Unparsed Corpora: \"The Gsearch Corpus Query System\"",
        "publisher": "Springer",
        "pages": "81--94",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "Steffan Corley and Martin Corley and Frank Keller and Matthew W. Crocker and Shari Trewin",
        "abstract": "The Gsearch system allows the selection of sentences by syntactic criteria from text corpora, even when these corpora contain no prior syntactic markup. This is achieved by means of a fast chart parser, which takes as input a grammar and a search expression specified by the user. Gsearch features a modular architecture that can be extended straightforwardly to give access to new corpora. The Gsearch architecture also allows interfacing with external linguistic resources (such as taggers and lexical databases). Gsearch can be used with graphical tools for visualizing the results of a query.",
        "url": "http://www.jstor.org/stable/30204845",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204845"
    },
    "10.2307/30204846": {
        "year": "2001",
        "volume": "35",
        "title": "The Challenge of Optical Music Recognition",
        "publisher": "Springer",
        "pages": "95--121",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "David Bainbridge and Tim Bell",
        "abstract": "This article describes the challenges posed by optical music recognition - a topic in computer science that aims to convert scanned pages of music into an on-line format. First, the problem is described; then a generalised framework for software is presented that emphasises key stages that must be solved: staff line identification, musical object location, musical feature classification, and musical semantics. Next, significant research projects in the area are reviewed, showing how each fits the generalised framework. The article concludes by discussing perhaps the most open question in the field: how to compare the accuracy and success of rival systems, highlighting certain steps that help ease the task.",
        "url": "http://www.jstor.org/stable/30204846",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204846"
    },
    "10.2307/30204847": {
        "year": "2001",
        "volume": "35",
        "title": "Archaeological Data Models and Web Publication Using XML",
        "publisher": "Springer",
        "pages": "123--152",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "J. David Schloen",
        "abstract": "An appropriate standardized data model is necessary to facilitate electronic publication and analysis of archaeological data on the World Wide Web. A hierarchical \"item-based\" model is proposed which can be readily implemented as an Extensible Markup Language (XML) tagging scheme that can represent any kind of archaeological data and deliver it in a cross-platform, standardized fashion to any Web browser. This tagging scheme and the data model it implements permit seamless integration and joint querying of archaeological datasets derived from many different sources.",
        "url": "http://www.jstor.org/stable/30204847",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204847"
    },
    "10.2307/30204848": {
        "year": "2001",
        "volume": "35",
        "title": "Sentential Count Rules for Arabic Language",
        "publisher": "Springer",
        "pages": "153--166",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "Fawaz S. Al-Anzi",
        "abstract": "This paper presents two grammars for reading numbers of classical and modern Arabic language. The grammars make use of the structured Arabic counting system to present an accurate and compact grammar that can be easily implemented in different platforms. Automating the process of reading numbers from its numerical representation to its sentential form has many applications. Inquiring about your bank balance over the phone, automatically writing the amount of checks (from numerical form to letter form), and reading for the blind people are some of the fields that automated reading of numbers can be of service. The parsing problem of sentential representation of numbers in the Arabic language is also addressed. A grammar to convert from sentential representation to the numerical representation is also presented. Grammars presented can be used to translate from the sentential Arabic numbers to sentential English numbers, and vice versa, by using the common numerical representation as an intermediate code. Such methodology can be used to aid the automatic translation between the two natural languages. All grammars described in this paper have been implemented on a UNIX system. Examples of different number representations and the output of the implementation of the grammars are given as part of the paper.",
        "url": "http://www.jstor.org/stable/30204848",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204848"
    },
    "10.2307/30204849": {
        "year": "2001",
        "volume": "35",
        "title": "Change-Point Analysis: Elision in Euripides' \"Orestes\"",
        "publisher": "Springer",
        "pages": "167--191",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "Jan G. de Gooijer and Nancy M. Laan",
        "abstract": "This paper studies the problem of detecting multiple changes at unknown times in the mean level of elision in the trimeter sequences of the Orestes, a play written by the Ancient Greek dramatist Euripides (485-406 B.C.). Change-detection statistics proposed by MacNeill (1978) and Jandhayala and MacNeill (1991) are adopted for this purpose. Analysis of the trimeter sequences yields several points of change. A general explanation for their occurrence appears to be that Euripides varies his use of elision according to the emotional content of his text, i.e., he seems to change the form to support the content and, thus, seems to use elision frequency as a dramatic instrument.",
        "url": "http://www.jstor.org/stable/30204849",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204849"
    },
    "10.2307/30204850": {
        "year": "2001",
        "volume": "35",
        "title": "Computer-Based Authorship Attribution without Lexical Measures",
        "publisher": "Springer",
        "pages": "193--214",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "E. Stamatatos and N. Fakotakis and G. Kokkinakis",
        "abstract": "The most important approaches to computer-assisted authorship attribution are exclusively based on lexical measures that either represent the vocabulary richness of the author or simply comprise frequencies of occurrence of common words. In this paper we present a fully-automated approach to the identification of the authorship of unrestricted text that excludes any lexical measure. Instead we adapt a set of style markers to the analysis of the text performed by an already existing natural language processing tool using three stylometric levels, i.e., token-level, phrase-level, and analysis-level measures. The latter represent the way in which the text has been analyzed. The presented experiments on a Modern Greek newspaper corpus show that the proposed set of style markers is able to distinguish reliably the authors of a randomly-chosen group and performs better than a lexically-based approach. However, the combination of these two approaches provides the most accurate solution (i.e., 87% accuracy). Moreover, we describe experiments on various sizes of the training data as well as tests dealing with the significance of the proposed set of style markers.",
        "url": "http://www.jstor.org/stable/30204850",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204850"
    },
    "10.2307/30204851": {
        "year": "2001",
        "volume": "35",
        "title": "Integrating Linguistic Resources in TC through WSD",
        "publisher": "Springer",
        "pages": "215--230",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "L. Alfonso Ureña-López and Manuel Buenaga and José M. Gómez",
        "abstract": "Information access methods must be improved to overcome the information overload that most professionals face nowadays. Text classification tasks, like Text Categorization, help the users to access to the great amount of text they find in the Internet and their organizations. TC is the classification of documents into a predefined set of categories. Most approaches to automatic TC are based on the utilization of a training collection, which is a set of manually classified documents. Other linguistic resources that are emerging, like lexical databases, can also be used for classification tasks. This article describes an approach to TC based on the integration of a training collection (Reuters-21578) and a lexical database (WORDNET 1.6) as knowledge sources. Lexical databases accumulate information on the lexical items of one or several languages. This information must be filtered in order to make an effective use of it in our model of TC. This filtering process is a Word Sense Disambiguation task. WSD is the identification of the sense of words in context. This task is an intermediate process in many natural language processing tasks like machine translation or multilingual information retrieval. We present the utilization of WSD as an aid for TC. Our approach to WSD is also based on the integration of two linguistic resources: a training collection (SEMCOR and Reuters-21578) and a lexical database (WORDNET 1.6). We have developed a series of experiments that show that: TC and WSD based on the integration of linguistic resources are very effective; and, WSD is necessary to effectively integrate linguistic resources in TC.",
        "url": "http://www.jstor.org/stable/30204851",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204851"
    },
    "10.2307/30204852": {
        "year": "2001",
        "volume": "35",
        "title": "Spanish Word Frequency: A Historical Surprise",
        "publisher": "Springer",
        "pages": "231--236",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "M. J. Woods",
        "abstract": "This article compares the word frequencies of the few most common words in Spanish as revealed by a modern corpus of over five thousand words with a corpus of Golden-Age Spanish texts of over a million words, and finds that although de is by far the most common word in contemporary Spanish, in the 16th and 17th Centuries it was considerably less frequent, and in many texts was less frequent than y, or que for which shared very similar frequency figures. It is argued that this significant change in the Spanish language comes about in the 20th Century.",
        "url": "http://www.jstor.org/stable/30204852",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204852"
    },
    "10.2307/30204856": {
        "year": "2001",
        "volume": "35",
        "title": "Computing Historical Consciousness. A Quantitative Inquiry into the Presence of the Past in Newspaper Texts",
        "publisher": "Springer",
        "pages": "237--253",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Thijs Pollmann and R. Harald Baayen",
        "abstract": "In this paper, some electronically gathered data are presented and analyzed about the presence of the past in newspaper texts. In ten large text corpora of six different languages, all dates in the form of years between 1930 and 1990 were counted. For six of these corpora this was done for all the years between 1200 and 1993. Depicting these frequencies on the timeline, we find an underlying regularly declining curve, deviations at regular places and culturally determined peaks at irregular points. These three phenomena are analyzed. Mathematically spoken, all the underlying curves have the same form. Whether a newspaper gives much or little attention to the past, the distribution of this attention over time turns out to be inversely proportional to the distance between past and present. It is shown that this distribution is largely independent of the total number of years in a corpus, the culture in which it is published, the language and the date of origin of the corpus. The phenomenon is explained as a kind of forgetting: the larger the distance between past and present, the more difficult it is to connect something of the past to an item in the present day. A more detailed analysis of the data shows a breakpoint in the frequency vs. distance from the publication date of the texts. References to events older than approximately 50 years are the result of a forgetting process that is distinctively different from the forgetting speed of more recent events. Pandel's classification of the dimensions of historical consciousness is used to answer the question how these investigations elucidate the historical consciousness of the cultures in which the newspapers are written and read.",
        "url": "http://www.jstor.org/stable/30204856",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204856"
    },
    "10.2307/30204857": {
        "year": "2001",
        "volume": "35",
        "title": "The Times and the Man as Predictors of Emotion and Style in the Inaugural Addresses of U.S. Presidents",
        "publisher": "Springer",
        "pages": "255--272",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Cynthia Whissell and Lee Sigelman",
        "abstract": "Intercorrelations among stylistic and emotional variables and construct validity deduced from relationships to other ratings of U.S. presidents suggest that power language (language that is linguistically simple, emotionally evocative, highly imaged, and rich in references to American values) is an important descriptor of inaugural addresses. Attempts to predict the use of power language in inaugural addresses from variables representing the times (year, media, economic factors) and the man (presidential personality) lead to the conclusion that time-based factors are the best predictors of the use of such language (81% prediction of variance in the criterion) while presidential personality adds at most a small amount of prediction to the model. Changes in power language are discussed as the outcome of a tendency to opt for breadth of communication over depth.",
        "url": "http://www.jstor.org/stable/30204857",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204857"
    },
    "10.2307/30204858": {
        "year": "2001",
        "volume": "35",
        "title": "Automatic Extraction of Collocations from Korean Text",
        "publisher": "Springer",
        "pages": "273--297",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Seonho Kim and Juntae Yoon and Mansuk Song",
        "abstract": "In this paper, we propose a statistical method to automatically extract collocations from Korean POS-tagged corpus. Since a large portion of language is represented by collocation patterns, the collocational knowledge provides a valuable resource for NLP applications. One difficulty of collocation extraction is that Korean has a partially free word order, which also appears in collocations. In this work, we exploit four statistics, 'frequency', 'randomness', 'convergence', and 'correlation' in order to take into account the flexible word order of Korean collocations. We separate meaningful bigrams using an evaluation function based on the four statistics and extend the bigrams to n-gram collocations using a fuzzy relation. Experiments show that this method works well for Korean collocations.",
        "url": "http://www.jstor.org/stable/30204858",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204858"
    },
    "10.2307/30204859": {
        "year": "2001",
        "volume": "35",
        "title": "Data Mining and Serial Documents",
        "publisher": "Springer",
        "pages": "299--314",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Rachid Anane",
        "abstract": "This paper is concerned with the investigation of the relevance and suitability of the data mining approach to serial documents. Conceptually the paper is divided into three parts. The first part presents the salient features of data mining and its symbiotic relationship to data warehousing. In the second part of the paper, historical serial documents are introduced, and the Ottoman Tax Registers (Defters) are taken as a case study. Their conformance to the data mining approach is established in terms of structure, analysis and results. A high-level conceptual model for the Defters is also presented. The final part concludes with a brief consideration of the implication of data mining for historical research.",
        "url": "http://www.jstor.org/stable/30204859",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204859"
    },
    "10.2307/30204860": {
        "year": "2001",
        "volume": "35",
        "title": "Stephen Crane and the \"New-York Tribune\": A Case Study in Traditional and Non-Traditional Authorship Attribution",
        "publisher": "Springer",
        "pages": "315--331",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "David I. Holmes and Michael Robertson and Roxanna Paez",
        "abstract": "This paper describes how traditional and non-traditional methods were used to identify seventeen previously unknown articles that we believe to be by Stephen Crane, published in the New-York Tribune between 1889 and 1892. The articles, printed without byline in what was at the time New York City's most prestigious newspaper, report on activities in a string of summer resort towns on New Jersey's northern shore. Scholars had previously identified fourteen shore reports as Crane's; these possible attributions more than double that corpus. The seventeen articles confirm how remarkably early Stephen Crane set his distinctive writing style and artistic agenda. In addition, the sheer quantity of the articles from the summer of 1892 reveals how vigorously the twenty-year-old Crane sought to establish himself in the role of professional writer. Finally, our discovery of an article about the New Jersey National Guard's summer encampment reveals another way in which Crane immersed himself in nineteenth-century military culture and help to explain how a young man who had never seen a battle could write so convincingly of war in his soon-to-come masterpiece, The Red Badge of Courage. We argue that the joint interdisciplinary approach employed in this paper should be the way in which attributional research is conducted.",
        "url": "http://www.jstor.org/stable/30204860",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204860"
    },
    "10.2307/30204861": {
        "year": "2001",
        "volume": "35",
        "title": "Identifying Syntactic Ambiguities in Single-Parse Arabic Sentence",
        "publisher": "Springer",
        "pages": "333--349",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Kevin Daimi",
        "abstract": "The aim of this paper is to describe a technique for identifying the sources of several types of syntactic ambiguity in Arabic Sentences with a single parse only. Normally, any sentence with two or more structural representations is said to be syntactically ambiguous. However, Arabic sentences with only one structural representation may be ambiguous. Our technique for identifying Syntactic Ambiguity in Single-Parse Arabic Sentences (SASPAS) analyzes each sentence and verifies the conditions that govern the existence of certain types of syntactic ambiguities in Arabic sentences. SASPAS is integrated with the syntactic parser, which is based on Definite Clause Grammar (DCG) formalism. The system accepts Arabic sentences in their original script.",
        "url": "http://www.jstor.org/stable/30204861",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204861"
    },
    "10.2307/30204868": {
        "year": "2001",
        "volume": "35",
        "title": "A Framework for Cross-Language Information Access: Application to English and Japanese",
        "publisher": "Springer",
        "pages": "371--388",
        "number": "4",
        "journal": "Computers and the Humanities",
        "author": "Gareth Jones and Nigel Collier and Tetsuya Sakai and Kazuo Sumita and Hideki Hirakawa",
        "abstract": "Internet search engines allow access to online information from all over the world. However, there is currently a general assumption that users are fluent in the languages of all documents that they might search for. This has for historical reasons usually been a choice between English and the locally supported language. Given the rapidly growing size of the Internet, it is likely that future users will need to access information in languages in which they are not fluent or have no knowledge of at all. This paper shows how information retrieval and machine translation can be combined in a cross-language information access framework to help overcome the language barrier. We present encouraging preliminary experimental results using English queries to retrieve documents from the standard Japanese language BMIR-J2 retrieval test collection. We outline the scope and purpose of cross-language information access and provide an example application to suggest that technology already exists to provide effective and potentially useful applications.",
        "url": "http://www.jstor.org/stable/30204868",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204868"
    },
    "10.2307/30204869": {
        "year": "2001",
        "volume": "35",
        "title": "Japanese/English Cross-Language Information Retrieval: Exploration of Query Translation and Transliteration",
        "publisher": "Springer",
        "pages": "389--420",
        "number": "4",
        "journal": "Computers and the Humanities",
        "author": "Atsushi Fujii and Tetsuya Ishikawa",
        "abstract": "Cross-language information retrieval (CLIR), where queries and documents are in different languages, has of late become one of the major topics within the information retrieval community. This paper proposes a Japanese/English CLIR system, where we combine a query translation and retrieval modules. We currently target the retrieval of technical documents, and therefore the performance of our system is highly dependent on the quality of the translation of technical terms. However, the technical term translation is still problematic in that technical terms are often compound words, and thus new terms are progressively created by combining existing base words. In addition, Japanese often represents loanwords based on its special phonogram. Consequently, existing dictionaries find it difficult to achieve sufficient coverage. To counter the first problem, we produce a Japanese/English dictionary for base words, and translate compound words on a word-by-word basis. We also use a probabilistic method to resolve translation ambiguity. For the second problem, we use a transliteration method, which corresponds words unlisted in the base word dictionary to their phonetic equivalents in the target language. We evaluate our system using a test collection for CLIR, and show that both the compound word translation and transliteration methods improve the system performance.",
        "url": "http://www.jstor.org/stable/30204869",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204869"
    },
    "10.2307/30204870": {
        "year": "2001",
        "volume": "35",
        "title": "A Method for Supporting Document Selection in Cross-Language Information Retrieval and Its Evaluation",
        "publisher": "Springer",
        "pages": "421--438",
        "number": "4",
        "journal": "Computers and the Humanities",
        "author": "Masami Suzuki and Naomi Inoue and Kazuo Hashimoto",
        "abstract": "It is important to give useful clues for selecting desired content from a number of retrieval results obtained (usually) from a vague search request. Compared with monolingual retrieval, such a support framework is inevitable and much more significant for filtering given translingual retrieval results. This paper describes an attempt to provide appropriate translation of major keywords in each document in a cross-language information retrieval (CLIR) result, as a browsing support for users. Our idea of determining appropriate translation of major keywords is based on word cooccurrence distribution in the translation target language, considering the actual situation of WWW content where it is difficult to obtain aligned parallel (multilingual) corpora. The proposed method provides higher quality of keyword translation to yield a more effective support in identifying the target documents in the retrieval result. We report the advantage of this browsing support technique through evaluation experiments including comparison with conditions of referring to a translated document summary, and discuss related issues to be examined towards more effective cross-language information extraction.",
        "url": "http://www.jstor.org/stable/30204870",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204870"
    },
    "10.2307/30204696": {
        "year": "2002",
        "volume": "36",
        "title": "Text-Image Coupling for Editing Literary Sources",
        "publisher": "Springer",
        "pages": "49--73",
        "number": "1",
        "journal": "Computers and the Humanities",
        "author": "Eric Lecolinet and Laurent Robert and François Role",
        "abstract": "Users need more sophisticated tools to handle the growing number of image-based documents available in databases. In this paper, we present a system devoted to the editing and browsing of complex literary hypermedia including original manuscript documents and other handwritten sources. Editing capabilities allow the user to transcribe manuscript images in an interactive way and to encode the resulting textual representation by means of a logical markup language (based on the XML/TEI specification). Both representations (image and structured text) are tightly linked to facilitate the reading and the interpretation of documents. This text/image coupling scheme is an attempt to unify several layers of information in order to provide the user with a global vision of the work. Our system also supplies tools capable of processing and relating information stored both in images and structured texts. Finally, application-specific visualization techniques have been developed in order to provide users with a way to identify relationships between source documents and help them to navigate.",
        "url": "http://www.jstor.org/stable/30204696",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204696"
    },
    "10.2307/30204526": {
        "year": "2002",
        "volume": "36",
        "title": "Profil: An Iconographic Database for Modern Watermarked Papers",
        "publisher": "Springer",
        "pages": "143--169",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "Brigitte de la Passardière and Claire Bustarret",
        "abstract": "The database Profil has been set up to offer readers studying modern literary manuscripts a reference tool to identify watermarked papers. In the study of writers' drafts as in artists' sketches, the different kinds of papers used provide valuable information on the genesis of a work of art and watermarks, when they exist, are the best visible hint allowing us to identify paper. A multimedia database, with digitized images more precise than usual traced design, seems to be appropriate to register, visualize, and compare modern watermarked papers. Besides its usefulness for specialists, such a database bearing on modern manuscripts should also be conceived in a didactic perspective, as it is oriented towards literary scholars who are not particularly familiar with the history of modern paper. In this paper we present the database Profil which includes a set of digitized images from a collection of betagraphies made by the reproduction service of the National French Library. Then we explain problems of database normalization when human sciences are involved.",
        "url": "http://www.jstor.org/stable/30204526",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204526"
    },
    "10.2307/30204527": {
        "year": "2002",
        "volume": "36",
        "title": "On the Corpus Size Needed for Compiling a Comprehensive Computational Lexicon by Automatic Lexical Acquisition",
        "publisher": "Springer",
        "pages": "171--190",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "Dan-Hee Yang and Ik-Hwan Lee and Pascual Cantos",
        "abstract": "Comprehensive computational lexicons are essential to practical natural language processing (NLP). To compile such computational lexicons by automatically acquiring lexical information, however, we previously require sufficiently large corpora. This study aims at predicting the ideal size of such automatic-lexical-acquisition oriented corpora, focusing on six specific factors: (1) specific versus general purpose prediction, (2) variation among corpora, (3) base forms versus inflected forms, (4) open class items, (5) homographs, and (6) unknown words. Another important and related issue with regard to predictability has something to do with data sparseness. Research using the TOTAL Corpus reveals serious data sparseness in this corpus. This, again, points towards the importance and necessity of reducing data sparseness to a satisfactory level for the automatic lexical acquisition and reliable corpus predictions. The functions of predicting the number of tokens and lemmas in a corpus are based on the piecewise curve-fitting algorithm. Unfortunately, the predicted size of a corpus for automatic lexical acquisition is too astronomical to compile it by using presently existing compiling strategies. Therefore, we suggest a practical and efficient alternative method. We are confident that this study will shed new light on issues such as corpus predictability, compiling strategies and linguistic comprehensiveness.",
        "url": "http://www.jstor.org/stable/30204527",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204527"
    },
    "10.2307/30204528": {
        "year": "2002",
        "volume": "36",
        "title": "Extracting an Arabic Lexicon from Arabic Newspaper Text",
        "publisher": "Springer",
        "pages": "191--221",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "Saleem Abuleil and Martha Evens",
        "abstract": "We describe how to build a large comprehensive, integrated Arabic lexicon by automatic parsing of newspaper text. We have built a parser system to read Arabic newspaper articles, isolate the tokens from them, find the part of speech, and the features for each token. To achieve this goal we designed a set of algorithms, we generated several sets of rules, and we developed a set of techniques, and a set of components to carry out these techniques. As each sentence is processed, new words and features are added to the lexicon, so that it grows continuously as the system runs. To test the system we have used 100 articles (80,444 words) from the Al-Raya newspaper. The system consists of several modules: the tokenizer module to isolate the tokens, the type finder system to find the part of speech of each token, the proper noun phrase parser module to mark the proper nouns and to discover some information about them and the feature finder module to find the features of the words.",
        "url": "http://www.jstor.org/stable/30204528",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204528"
    },
    "10.2307/30204529": {
        "year": "2002",
        "volume": "36",
        "title": "GATE, a General Architecture for Text Engineering",
        "publisher": "Springer",
        "pages": "223--254",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "Hamish Cunningham",
        "abstract": "This paper presents the design, implementation and evaluation of GATE, a General Architecture for Text Engineering. GATE lies at the intersection of human language computation and software engineering, and constitutes an infrastructural system supporting research and development of language processing software.",
        "url": "http://www.jstor.org/stable/30204529",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204529"
    },
    "10.2307/30200526": {
        "year": "2002",
        "volume": "36",
        "title": "A New Computer-Assisted Literary Criticism?",
        "publisher": "Springer",
        "pages": "259--267",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Raymond G. Siemens",
        "abstract": "If there is such a thing as a new computer-assisted literary criticism, its expression lies in a model that is as broad-based as that presented in John Smith's seminal article, \"Computer Criticism,\" and is as encompassing of the discipline of literary studies as it is tied to the evolving nature of the electronic literary text that lies at the heart of its intersection with computing. It is the desire to establish the parameters of such a model for the interaction between literary studies and humanities computing - for a model of the new computer-assisted literary criticism - that gave rise to the papers in this collection and to the several conference panel-presentations and discussions that, in their print form, these papers represent.",
        "url": "http://www.jstor.org/stable/30200526",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30200526"
    },
    "10.2307/30200527": {
        "year": "2002",
        "volume": "36",
        "title": "The Text of Performance and the Performance of Text in the Electronic Edition",
        "publisher": "Springer",
        "pages": "269--282",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Michael Best",
        "abstract": "The surviving texts of many of Shakespeare's plays include what might be termed a \"performance crux\": a moment that is puzzling to the director and actors, and which calls for some kind of stage business to justify or explain action. Sometimes the director's response is simply to remove the passage. This paper will look at a crux of this kind, and discuss how a modern, multimedia electronic edition can provide tools for the reader or actor to explore the possibilities both of the basic text and the performance that grows from it. In Romeo and Juliet there is an awkward moment when Friar Lawrence flees the tomb and deserts Juliet as he hears people approach. The plot requires that he be present as she awakens and absent as she commits suicide, but Shakespeare is not often as arbitrary as the received text (Quarto 2) makes him: \"Come go good Iuliet, I dare no longer stay. Exit.\" By comparing this moment with Quarto 1 (the first published version of the text), Shakespeare's source, later adaptations, and some modern performances, the paper will discuss the mutual illumination of text and performance.",
        "url": "http://www.jstor.org/stable/30200527",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30200527"
    },
    "10.2307/30200528": {
        "year": "2002",
        "volume": "36",
        "title": "Computer-Mediated Texts and Textuality: Theory and Practice",
        "publisher": "Springer",
        "pages": "283--293",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Susan Schreibman",
        "abstract": "The majority of humanities computing projects within the discipline of literature have been conceived more as digital libraries than monographs which utilise the medium as a site of interpretation. The impetus to conceive electronic research in this way comes from the underlying philosophy of texts and textuality implicit in SGML and its instantiation for the humanities, the TEI, which was conceived as \"a markup system intended for representing already existing literary texts\". This article explores the most common theories used to conceive electronic research in literature, such as hypertext theory, OCHO (Ordered Hierarchy of Content Objects), and Jerome J. McGann's \"noninformational\" forms of textuality. It also argues that as our understanding of electronic texts and textuality deepens, and as advances in technology progresses, other theories, such as Reception Theory and Versioning, may well be adapted to serve as a theoretical basis for conceiving research more akin to an electronic monograph than a digital library.",
        "url": "http://www.jstor.org/stable/30200528",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30200528"
    },
    "10.2307/30200529": {
        "year": "2002",
        "volume": "36",
        "title": "Industrial Text and French Neo-Structuralism",
        "publisher": "Springer",
        "pages": "295--306",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "William Winder",
        "abstract": "Parallel to, and to some degree in reaction to French poststructuralist theorization (as championed by Derrida, Foucault, and Lacan, among others) is a French neo-structuralism built directly on the achievements of structuralism using electronic means. This paper examines some exemplary approaches to text analysis in this neo-structuralist vein: SATOR's topoi dictionary, the WinBrill POS tagger and Frangois Rastier's interpretative semantics. I consider how a computerassisted \"Wissenschaft\" accumulation of expertise complements the neo-structuralist approach. Ultimately, electronic critical studies will be defined by their strategic position at the intersection of the two chief technologies shaping our society: the new information processing technology of computers and the representational techniques that have accumulated for centuries in texts. Understanding how these two information management paradigms complement each other is a key issue for the humanities, for computer science, and vital to industry, even beyond the narrow realm of the language industries. The direction of critical studies, a small planet long orbiting in only rarefied academic circles, will be radically altered by the sheer size of the economic stakes implied by a new kind of text, the industrial text, the technological heart of an information society.",
        "url": "http://www.jstor.org/stable/30200529",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30200529"
    },
    "10.2307/30200530": {
        "year": "2002",
        "volume": "36",
        "title": "The Question concerning Theory: Humanism, Subjectivity, and Computing",
        "publisher": "Springer",
        "pages": "307--318",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Tamise Van Pelt",
        "abstract": "Deeply held humanist ideas constitute a residual discourse in our late age of print, a discourse that has not been effaced by the rise of technology or by widespread computing practices. Against humanism's Enlightenment idea of the rational individual, mid-twentieth century schools of anti-humanism postulated a \"subject\" constructed by, rather than controlling, its language, culture, and technologies. The contemporary notion of the subject comes from Lacanian psychoanalytic theory, where Lacan draws upon cybernetics and computing as evidence of a symbolic order constructive of subjectivity. This computational symbolic, in turn, owes much to Heidegger's post-war notions of technology. Today, however, work in new media suggests that the dominant discourse on the subject - the discourse underwriting contemporary theory - is being challenged by an emergent discourse of the posthuman. Consequently, theory itself is thrown into question. The question of theory now arises since we must ask whether the posthuman subject of technology is rewriting the anti-humanist subject of theory in new and unanticipated ways. This article thus offers an overview of the shift from humanist, to anti-humanist, to posthumanist assumptions. My goal is to help readers decide whether today's computing environments can still be approached through late twentieth century anti-humanist theories or whether e-texts demand new, media-specific analyses.",
        "url": "http://www.jstor.org/stable/30200530",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30200530"
    },
    "10.2307/30200531": {
        "year": "2002",
        "volume": "36",
        "title": "Animating the Language Machine: Computers and Performance",
        "publisher": "Springer",
        "pages": "319--344",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Marshall Soules",
        "abstract": "A range of inter-disciplinary discourses consider the computer-mediated writing space as a unique performance medium with characteristic protocols. Drawing on contemporary performance theory, literary criticism, and communication theory, the author proposes that technologists, academics, and artists are developing idiomatic rhetorics - a lingua franca - to explore the technical and expressive properties of the new \"language machines\" and their hypertextual environments. The role of improvisation - and its cross-disciplinary protocols - provides a further focus in the discussion of computing practice and performance.",
        "url": "http://www.jstor.org/stable/30200531",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30200531"
    },
    "10.2307/30200532": {
        "year": "2002",
        "volume": "36",
        "title": "Gore Galore: Literary Theory and Computer Games",
        "publisher": "Springer",
        "pages": "345--358",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Geoffrey Rockwell",
        "abstract": "Computer games have not been adequately theorized within the humanities. In this paper a brief history of computer games is presented as a starting point for developing a topology of games and a theory of computer games as rhetorical artifacts suitable for critical study. The paper addresses the question of why games should be treated seriously and suggests a theoretical approach based on Bakhtin's poetics of the novel where the experience of time and space (the chronotope) provides a framework of questions for discussing computer games.",
        "url": "http://www.jstor.org/stable/30200532",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30200532"
    },
    "10.2307/30200533": {
        "year": "2002",
        "volume": "36",
        "title": "Mutability, Medium, and Character",
        "publisher": "Springer",
        "pages": "359--378",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Dene Grigar",
        "abstract": "Looking specifically at the genre of adaptive narrative, this article explores the future of literature created for and with computer technology, focusing primarily on the trope of mutability as it is played out with new media. Some of the questions asked are: What can the medium of a work of literature, that is its material aspect, tell us about the text? About character? What can it possibly matter if narrative is recounted on papyrus, retold on parchment and rag, and then remediated in pixels? Isn't it the message carried by the medium we are most concerned with, stable or unstable throughout the process of inscription, reinscription, encoding and decoding, translation and remediation? This paper speculates about possibilities rather than attempts to answer these questions, but the structuring and mean-making components considered here stand as examples of some we may want to think about when developing future theories about literature - and all types of writing generated by and for electronic environments.",
        "url": "http://www.jstor.org/stable/30200533",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30200533"
    },
    "10.2307/30204685": {
        "year": "2002",
        "volume": "36",
        "title": "Statistical Morphological Disambiguation for Agglutinative Languages",
        "publisher": "Springer",
        "pages": "381--410",
        "number": "4",
        "journal": "Computers and the Humanities",
        "author": "Dilek Z. Hakkani-Tür and Kemal Oflazer and Gökhan Tür",
        "abstract": "We present statistical models for morphological disambiguation in agglutinative languages, with a specific application to Turkish. Turkish presents an interesting problem for statistical models as the potential tag set size is very large because of the productive derivational morphology. We propose to handle this by breaking up the morhosyntactic tags into inflectional groups, each of which contains the inflectional features for each (intermediate) derived form. Our statistical models score the probability of each morhosyntactic tag by considering statistics over the individual inflectional groups and surface roots in trigram models. Among the four models that we have developed and tested, the simplest model ignoring the local morphotactics within words performs the best. Our best trigram model performs with 93.95% accuracy on our test data getting all the morhosyntactic and semantic features correct. If we are just interested in syntactically relevant features and ignore a very small set of semantic features, then the accuracy increases to 95.07%.",
        "url": "http://www.jstor.org/stable/30204685",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204685"
    },
    "10.2307/30204686": {
        "year": "2002",
        "volume": "36",
        "title": "Stylistic Constancy and Change across Literary Corpora: Using Measures of Lexical Richness to Date Works",
        "publisher": "Springer",
        "pages": "411--430",
        "number": "4",
        "journal": "Computers and the Humanities",
        "author": "J. A. Smith and C. Kelly",
        "abstract": "The measure of the lexical richness of literary texts as a tool in the comparative analysis of literary style has been hampered by the problem of the inequality of text lengths within and between literary corpora. This paper proposes an empirical method of description of lexical richness by averaging measures on multiple chunks of text of a standard length within a literary work or corpus. A work's average vocabulary richness, average portion of hapax legomena of the corpus from which it derives, and average repetition of frequently appearing vocabulary may then characterize that work relative to other works partitioned along with it. This method reveals the possibility of significant variance of these measures of vocabulary among works of a single author's corpus and warns against the notion of some absolute authorial stylistic character. We apply this method of vocabulary averaging to the corpora of three playwrights from classical antiquity whose works are chronologically rankable: Euripides, Aristophanes, and Terence. We look for trends in vocabulary richness over time, which we posit functions as an indicator of progressively changing authorial ability or inclination. This method then holds the potential of predicting dates for undateable or tenuously dated works within a corpus of otherwise securely dated texts. From the results derived, a relatively late date for the composition of the redrafted version of Aristophanes' Clouds appears likely; we predict an early composition date for the redraft of Terence's Hecyra (and thus are inclined to think that the playwright did very little redrafting); and finally we find Euripides' Electra and Supplices exhibiting vocabulary characteristics of extremely late composition and we predict dates much later than those assigned based on metrical considerations.",
        "url": "http://www.jstor.org/stable/30204686",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204686"
    },
    "10.2307/30204687": {
        "year": "2002",
        "volume": "36",
        "title": "Korean Combinatory Categorial Grammar and Statistical Parsing",
        "publisher": "Springer",
        "pages": "431--453",
        "number": "4",
        "journal": "Computers and the Humanities",
        "author": "Jeongwon Cha and Geunbae Lee and Jonghyeok Lee",
        "abstract": "Korean Combinatory Categorial Grammar (KCCG) is an extended combinatory categorial grammar formalism to capture the syntax and interpretation of a \"relative free\" word order, long distance scrambling, and other specific characteristics of Korean. KCCG formalism can uniformly handle word order variations among arguments and adjuncts within a clause, as well as in complex clauses and across clause boundaries, i.e. long distance scrambling. The approach we develop takes advantage of the ability of CCG for type raising and composition along with the ability of variable categories and unordered argument modeling for relatively free word order treatment (Lee et al., 1994; Lee et al., 1997). We apply a probability model and heuristics using Korean characteristics to our KCCG parser. Results of the experiments on various text genre show that the KCCG parser performs at 87.67/87.03% constituent precision/recall.",
        "url": "http://www.jstor.org/stable/30204687",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204687"
    },
    "10.2307/30204890": {
        "year": "2003",
        "volume": "37",
        "title": "Another Perspective on Vocabulary Richness",
        "publisher": "Springer",
        "pages": "151--178",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "David L. Hoover",
        "abstract": "This article examines the usefulness of vocabulary richness for authorship attribution and tests the assumption that appropriate measures of vocabulary richness can capture an author's distinctive style or identity. After briefly discussing perceived and actual vocabulary richness, I show that doubling and combining texts affects some measures in computationally predictable but conceptually surprising ways. I discuss some theoretical and empirical problems with some measures and develop simple methods to test how well vocabulary richness distinguishes texts by different authors. These methods show that vocabulary richness is ineffective for large groups of texts because of the extreme variability within and among them. I conclude that vocabulary richness is of marginal value in stylistic and authorship studies because the basic assumption that it constitutes a wordprint for authors is false.",
        "url": "http://www.jstor.org/stable/30204890",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204890"
    },
    "10.2307/30204891": {
        "year": "2003",
        "volume": "37",
        "title": "Vocabulary in Interviews as Related to Respondent Characteristics",
        "publisher": "Springer",
        "pages": "179--204",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "Kjell Härnqvist and Ulf Christianson and Daniel Ridings and Jan-Gunnar Tingsell",
        "abstract": "Responses in personal interviews about education and career with 415 Swedish men and women (age 34) forms the basis of a speech corpus with 1.8 million words. The vocabulary is described by means of two sets of variables. One is based on the number of tokens and types, word length and sectioning of the running text. The other set divides the corpus into grammatical categories. Both sets of variables are related to a number of background variables such as gender, socioeconomic background, education, and indicators of verbal proficiency at age 13 and 32. This possibility to study the relationship between vocabulary and a broad set of respondent characteristics is a unique feature of this corpus.",
        "url": "http://www.jstor.org/stable/30204891",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204891"
    },
    "10.2307/30204893": {
        "year": "2003",
        "volume": "37",
        "title": "Locating the Eureka Stockade: Use of a Geographical Information System (GIS) in a Historiographical Research Context",
        "publisher": "Springer",
        "pages": "229--234",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "J. T. Harvey",
        "abstract": "GIS methodology was used for the purpose of locating the disputed site of a historically significant battle, which took place in 1854 when miners on an Australian gold field staged an armed uprising against government forces. The route of the first survey of the area (1854) and the earliest known contour map (1856-1857) were overlaid on a modem street grid. Other features such as the vantage points of illustrators and the authors of eyewitness accounts were also incorporated. The resulting composite map was used as the key reference framework for comparing and critically evaluating a large body of primary and secondary written accounts, and for reaching a conclusion concerning the site.",
        "url": "http://www.jstor.org/stable/30204893",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204893"
    },
    "10.2307/30204894": {
        "year": "2003",
        "volume": "37",
        "title": "Chronological Distribution of Information in Historical Texts",
        "publisher": "Springer",
        "pages": "235--240",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "Jordan Tabov",
        "abstract": "In their papers, Kalashnikov et al. (1986), Rachev et al. (1989) and Fomenko et al. (1990) introduced the so-called \"volume function\" describing the chronological distribution of information in historical texts. Here we give another approach to constructing similar functions.",
        "url": "http://www.jstor.org/stable/30204894",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204894"
    },
    "10.2307/30204900": {
        "year": "2003",
        "volume": "37",
        "title": "Introducing Computational Techniques in Dialectometry",
        "publisher": "Springer",
        "pages": "245--255",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "John Nerbonne and William Kretzschmar",
        "abstract": "Dialectology is the study of dialects, and dialectometry is the measurement of dialect differences, i.e. linguistic differences whose distribution is determined primarily by geography. The earliest works in dialectology showed that language variation is complex both geographically and linguistically and cannot be reduced to simple characterizations. There has thus always been a perceived need for techniques which can deal with large amounts of data in a controlled means, i.e. computational techniques. This special issue of Computers and the Humanities presents a range of recent work on this topic.",
        "url": "http://www.jstor.org/stable/30204900",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204900"
    },
    "10.2307/30204901": {
        "year": "2003",
        "volume": "37",
        "title": "The Use of the Almeida-Braun System in the Measurement of Dutch Dialect Distances",
        "publisher": "Springer",
        "pages": "257--271",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Wilbert Heeringa and Angelika Braun",
        "abstract": "Measuring dialect distances can be based on the comparison of words, and the comparison words should be based on the comparison of sounds. In this research we used an adjusted version of an articulation-based system, developed by Almeida and Braun (1986) for finding sound distances, using the IPA system. For comparison of two pronunciations of a word corresponding with two different varieties, we used the Levenshtein algorithm, which finds the easiest way in which one word can be changed into the other by inserting, deleting or substituting sounds. As operations weights of these three operations we used distances as found with the Almeida & Braun system. The dialect distance is now equal to the average of a range of word distances. We applied the technique to 360 Dutch dialects. The transcriptions of 125 words for each dialect are taken from the Reeks Nederlandse Dialectatlassen (Blancquaert and Peé, 1925-1982). We get a division with clear similarities to traditional dialect maps when classifying dialects. Using logarithmic sound distances improves results compared to results based on constant sound distances.",
        "url": "http://www.jstor.org/stable/30204901",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204901"
    },
    "10.2307/30204902": {
        "year": "2003",
        "volume": "37",
        "title": "Phonetic Alignment and Similarity",
        "publisher": "Springer",
        "pages": "273--291",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Grzegorz Kondrak",
        "abstract": "The computation of the optimal phonetic alignment and the phonetic similarity between words is an important step in many applications in computational phonology, including dialectometry. After discussing several related algorithms, I present a novel approach to the problem that employs a scoring scheme for computing phonetic similarity between phonetic segments on the basis of multivalued articulatory phonetic features. The scheme incorporates the key concept of feature salience, which is necessary to properly balance the importance of various features. The new algorithm combines several techniques developed for sequence comparison: an extended set of edit operations, local and semiglobal modes of alignment, and the capability of retrieving a set of near-optimal alignments. On a set of 82 cognate pairs, it performs better than comparable algorithms reported in the literature.",
        "url": "http://www.jstor.org/stable/30204902",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204902"
    },
    "10.2307/30204903": {
        "year": "2003",
        "volume": "37",
        "title": "Norwegian Dialects Examined Perceptually and Acoustically",
        "publisher": "Springer",
        "pages": "293--315",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Wilbert Heeringa and Charlotte Gooskens",
        "abstract": "Gooskens (2003) described an experiment which determined linguistic distances between 15 Norwegian dialects as perceived by Norwegian listeners. The results are compared to Levenshtein distances, calculated on the basis of transcriptions (of the words) of the same recordings as used in the perception experiment. The Levenshtein distance is equal to the sum of the weights of the insertions, deletions and substitutions needed to change one pronunciation into another. The success of the method depends on the reliability of the transcriber. The aim of this paper is to find an acoustic distance measure between dialects which approximates perceptual distance measure. We use and compare different representations of the acoustic signal: Barkfilter spectrograms, cochleagrams and formant tracks. We now apply the Levenshtein algorithm to spectra or formant value bundles instead of transcription segments. From these acoustic representations we got the best results using the formant track representation. However the transcription-based Levenshtein distances correlate still more closely. In the acoustic signal the speaker-dependent influence is kept to some extent, while a transcriber abstracts from voice quality. Using more samples per dialect word (instead of only one as in our research) should improve the accuracy of the measurements.",
        "url": "http://www.jstor.org/stable/30204903",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204903"
    },
    "10.2307/30204904": {
        "year": "2003",
        "volume": "37",
        "title": "Profile-Based Linguistic Uniformity as a Generic Method for Comparing Language Varieties",
        "publisher": "Springer",
        "pages": "317--337",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Dirk Speelman and Stefan Grondelaers and Dirk Geeraerts",
        "abstract": "In this text we present \"profile-based linguistic uniformity\", a method designed to compare language varieties on the basis of a wide range of potentially heterogeneous linguistic variables. In many respects a parallel can be drawn with current methods in dialectometry (for an overview, see, Nerbonne and Heeringa, 2001; Heeringa, Nerbonne and Kleiweg, 2002): in both cases dissimilarities between varieties on the basis of individual variables are summarized in global dissimilarities, and a series of language varieties are subsequently clustered or charted using multivariate techniques such as cluster analysis or multidimensional scaling. This global similarity between the methods makes it possible to compare them and to investigate the implications of notable differences. In this text we specifically focus on, and defend one characteristic of our methodology, its profile-based nature.",
        "url": "http://www.jstor.org/stable/30204904",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204904"
    },
    "10.2307/30204905": {
        "year": "2003",
        "volume": "37",
        "title": "Lexical Distance in LAMSAS",
        "publisher": "Springer",
        "pages": "339--357",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "John Nerbonne and Peter Kleiweg",
        "abstract": "The Linguistic Atlas of the Middle and South Atlantic States (LAMSAS) is admirably accessible for reanalysis. The present paper applies a lexical distance measure to assess the lexical relatedness of LAMSAS's sites, a popular focus of investigation in the past (Kurath, 1949; Carver, 1989; McDavid, 1994). Several conclusions are noteworthy: First, and least controversially, we note that LAMSAS is dialectometrically challenging at least due to the range of field workers and questionnaires employed. Second, on the issue of which areas ought to be recognized, we note that our investigations tend to support a three-way North/South/Midlands division rather than a two-way North/South division, i.e. they tend to support Kurath and McDavid rather than Carver, but this tendency is not conclusive. Third, we extend dialectometric technique in suggesting means of dealing with alternate forms and multiple responses.",
        "url": "http://www.jstor.org/stable/30204905",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204905"
    },
    "10.2307/30204906": {
        "year": "2003",
        "volume": "37",
        "title": "Neighbours or Enemies? Competing Variants Causing Differences in Transitional Dialects",
        "publisher": "Springer",
        "pages": "359--372",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Marjatta Palander and Lisa Lena Opas-Hänninen and Fiona Tweedie",
        "abstract": "The aim of this study is to show how cluster analysis can shed light on very complex variation in a transitional dialect zone in eastern Finland. In the course of history this area has been on the border between Sweden and Russia and the population has clearly been of two kinds: the Savo people and the Karelians. It is a well-known fact that there is variation among these dialects, but the spread and extent of the variation has not been demonstrated previously. The idiolects of the area were studied in the light of ten phonological and morphological features. The material consisted of recordings of 198 idiolects, totalling around 195 hours and representing 19 parishes. The variation was analysed using hierarchical cluster analysis. While the analysis showed the extent of the variation between idiolects and parishes, it also demonstrated how the effects of the old parishes, borders and settlements are still visible in the dialects. On the parish level, the data formed clear clusters that correspond with the main dialects in the area and its surroundings. On the idiolect level, however, the speakers from the surrounding areas formed fairly homogenous clusters but the idiolects from the Savonlinna area were spread across almost all clusters.",
        "url": "http://www.jstor.org/stable/30204906",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204906"
    },
    "10.2307/30204912": {
        "year": "2003",
        "volume": "37",
        "title": "Extending Dublin Core Metadata to Support the Description and Discovery of Language Resources",
        "publisher": "Springer",
        "pages": "375--388",
        "number": "4",
        "journal": "Computers and the Humanities",
        "author": "Steven Bird and Gary Simons",
        "abstract": "As language data and associated technologies proliferate and as the language resources community expands, it is becoming increasingly difficult to locate and reuse existing resources. Are there any lexical resources for such-and-such a language? What tool works with transcripts in this particular format? What is a good format to use for linguistic data of this type? Questions like these dominate many mailing lists, since web search engines are an unreliable way to find language resources. This paper reports on a new digital infrastructure for discovering language resources being developed by the Open Language Archives Community (OLAC). At the core of OLAC is its metadata format, which is designed to facilitate description and discovery of all kinds of language resources, including data, tools, or advice. The paper describes OLAC metadata, its relationship to Dublin Core metadata, and its dissemination using the metadata harvesting protocol of the Open Archives Initiative.",
        "url": "http://www.jstor.org/stable/30204912",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204912"
    },
    "10.2307/30204913": {
        "year": "2003",
        "volume": "37",
        "title": "C-rater: Automated Scoring of Short-Answer Questions",
        "publisher": "Springer",
        "pages": "389--405",
        "number": "4",
        "journal": "Computers and the Humanities",
        "author": "Claudia Leacock and Martin Chodorow",
        "abstract": "C-rater is an automated scoring engine that has been developed to score responses to content-based short answer questions. It is not simply a string matching program - instead it uses predicate argument structure, pronominal reference, morphological analysis and synonyms to assign full or partial credit to a short answer question. C-rater has been used in two studies: National Assessment for Educational Progress (NAEP) and a statewide assessment in Indiana. In both studies, c-rater agreed with human graders about 84% of the time.",
        "url": "http://www.jstor.org/stable/30204913",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204913"
    },
    "10.2307/30204914": {
        "year": "2003",
        "volume": "37",
        "title": "Authorship Attribution and Pastiche",
        "publisher": "Springer",
        "pages": "407--429",
        "number": "4",
        "journal": "Computers and the Humanities",
        "author": "Harold Somers and Fiona Tweedie",
        "abstract": "This paper considers the question of authorship attribution techniques when faced with a pastiche. We ask whether the techniques can distinguish the real thing from the fake, or can the author fool the computer? If the latter, is this because the pastiche is good, or because the technique is faulty? Using a number of mainly vocabulary-based techniques, Gilbert Adair's pastiche of Lewis Carroll, Alice Through the Needle's Eye, is compared with the original 'Alice' books. Standard measures of lexical richness, Yule's K and Orlov's Z both distinguish Adair from Carroll, though Z also distinguishes the two originals. A principal component analysis based on word frequencies finds that the main differences are not due to authorship. A discriminant analysis based on word usage and lexical richness successfully distinguishes the pastiche from the originals. Weighted cusum tests were also unable to distinguish the two authors in a majority of cases. As a cross-validation, we made similar comparisons with control texts: another children's story from the same era, and other work by Carroll and Adair. The implications of these findings are discussed.",
        "url": "http://www.jstor.org/stable/30204914",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204914"
    },
    "10.2307/30204915": {
        "year": "2003",
        "volume": "37",
        "title": "Modeling Task-Oriented Dialogue",
        "publisher": "Springer",
        "pages": "431--454",
        "number": "4",
        "journal": "Computers and the Humanities",
        "author": "Maite Taboada",
        "abstract": "A common tool for improving the performance quality of natural language processing systems is the use of contextual information for disambiguation. Here I describe the use of a finite state machine (FSM) to disambiguate speech acts in a machine translation system. The FSM has two layers that model, respectively, the global and local structures found in naturally-occurring conversations. The FSM has been modeled on a corpus of task-oriented dialogues in a travel planning situation. In the dialogues, one of the interactants is a travel agent or hotel clerk, and the other a client requesting information or services. A discourse processor based on the FSM was implemented in order to process contextual information in a machine translation system. Evaluation results show that the discourse processor is able to disambiguate and improve the quality of the dialogue translation. Other applications include human-computer interaction and computer-assisted language learning.",
        "url": "http://www.jstor.org/stable/30204915",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204915"
    },
    "10.2307/30204916": {
        "year": "2003",
        "volume": "37",
        "title": "A Machine Learning Approach for Identification of Thesis and Conclusion Statements in Student Essays",
        "publisher": "Springer",
        "pages": "455--467",
        "number": "4",
        "journal": "Computers and the Humanities",
        "author": "Jill Burstein and Daniel Marcu",
        "abstract": "This study describes and evaluates two essay-based discourse analysis systems that identify thesis and conclusion statements from student essays written on six different essay topics. Essays used to train and evaluate the systems were annotated by two human judges, according to a discourse annotation protocol. Using a machine learning approach, a number of discourse-related features were automatically extracted from a set of annotated training data. Using these features, two discourse analysis models were built using C5.0 with boosting: a topic-dependent and a topic-independent model. Both systems outperformed a positional algorithm. While the topic-dependent system showed somewhat higher performance, the topic-independent system showed similar results, indicating that a system can generalize to unseen data - that is, essay responses on topics that the system has not seen in training.",
        "url": "http://www.jstor.org/stable/30204916",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204916"
    },
    "10.2307/30204917": {
        "year": "2003",
        "volume": "37",
        "title": "Talking about Meter in SGML",
        "publisher": "Springer",
        "pages": "469--473",
        "number": "4",
        "journal": "Computers and the Humanities",
        "author": "Anne Mahoney",
        "abstract": "This paper describes an encoding for representing quantitative metrical analyses in TEI SGML or XML documents, using only characters from the standard keyboard set, and a system for converting this encoding to other forms for display.",
        "url": "http://www.jstor.org/stable/30204917",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204917"
    },
    "10.2307/30204922": {
        "year": "2004",
        "volume": "38",
        "title": "Representing Multiple Pathways of Textual Flow in the Greek Manuscripts of the Letter of James Using Reduced Median Networks",
        "publisher": "Springer",
        "pages": "1--14",
        "number": "1",
        "journal": "Computers and the Humanities",
        "author": "Matthew Spencer and Klaus Wachtel and Christopher J. Howe",
        "abstract": "Many manuscripts of the Greek New Testament are influenced by multiple pathways of textual flow. This makes it difficult to reconstruct a stemma using traditional methods. We describe a novel application of the reduced median algorithm (developed in evolutionary biology) to reconstructing a stemma for selected Greek manuscripts of the Letter of James. This stemma is a network in which contamination is explicitly represented. It is consistent with the ideas that most variants arose early in the history of the Greek New Testament, that early manuscripts were often influenced by both oral and written traditions, and that later copies introduced fewer variants. Since similar problems of contamination occur in many text traditions, we expect methods of this kind to be widely useful.",
        "url": "http://www.jstor.org/stable/30204922",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204922"
    },
    "10.2307/30204923": {
        "year": "2004",
        "volume": "38",
        "title": "Detecting Collaborations in Text: Comparing the Authors' Rhetorical Language Choices in the Federalist Papers",
        "publisher": "Springer",
        "pages": "15--36",
        "number": "1",
        "journal": "Computers and the Humanities",
        "author": "Jeff Collins and David Kaufer and Pantelis Vlachos and Brian Butler and Suguru Ishizaki",
        "abstract": "In author attribution studies function words or lexical measures are often used to differentiate the authors' textual fingerprints. These studies can be thought of as quantifying the texts, representing the text with measured variables that stand for specific textual features. The resulting quantifications, while proven useful for statistically differentiating among the texts, bear no resemblance to the understanding a human reader-even an astute one-would develop while reading the texts. In this paper we present an attribution study that, instead, characterizes the texts according to the representational language choices of the authors, similar to a way we believe close human readers come to know a text and distinguish its rhetorical purpose. From our automated quantification of The Federalist papers, it is clear why human readers find it impossible to distinguish the authorship of the disputed papers. Our findings suggest that changes occur in the processes of rhetorical invention when undertaken in collaborative situations. This points to a need to re-evaluate the premise of autonomous authorship that has informed attribution studies of The Federalist case.",
        "url": "http://www.jstor.org/stable/30204923",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204923"
    },
    "10.2307/30204924": {
        "year": "2004",
        "volume": "38",
        "title": "An Analysis of Verb Subcategorization Frames in Three Special Language Corpora with a View towards Automatic Term Recognition",
        "publisher": "Springer",
        "pages": "37--60",
        "number": "1",
        "journal": "Computers and the Humanities",
        "author": "Eugenia Eumeridou and Blaise Nkwenti-Azeh and John McNaught",
        "abstract": "Current term recognition algorithms have centred mostly on the notion of term based on the assumption that terms are monoreferential and as such independent of context. The characteristics and behaviour of terms in real texts are however far removed from this ideal because factors such as text type or communicative situation greatly influence the linguistic realisation of a concept. Context, therefore, is important for the correct identification of terms (Dubuc and Lauriston, 1997). Based on this assumption, we have shifted our emphasis from terms towards surrounding linguistic context, namely verbs, as verbs are considered the central elements in the sentence. More specifically, we have set out to examine whether verbs and verbal syntax in particular, could help us towards the task of automatic term recognition. Our findings suggest that term occurrence varies significantly in different argument structures and different syntactic positions. Additionally, deviant grammatical structures have proved rich environments for terms. The analysis was carried out in three different specialised subcorpora in order to explore how the effectiveness of verbal syntax as a potential indicator of term occurrence can be constrained by factors such as subject matter and text type.",
        "url": "http://www.jstor.org/stable/30204924",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204924"
    },
    "10.2307/30204925": {
        "year": "2004",
        "volume": "38",
        "title": "Change of Writing Style with Time",
        "publisher": "Springer",
        "pages": "61--82",
        "number": "1",
        "journal": "Computers and the Humanities",
        "author": "Fazli Can and Jon M. Patton",
        "abstract": "This study investigates the writing style change of two Turkish authors, Çetin Altan and Yaşar Kemal, in their old and new works using respectively their newspaper columns and novels. The style markers are the frequencies of word lengths in both text and vocabulary, and the rate of usage of most frequent words. For both authors, t-tests and logistic regressions show that the length of the words in new works is significantly longer than that of the old. The principal component analyses graphically illustrate the separation between old and new texts. The works are correctly categorized as old or new with 75 to 100% accuracy and 92% average accuracy using discriminant analysisbased cross validation. The results imply higher time gap may have positive impact in separation and categorization. For Altan a regression analysis demonstrates a decrease in average word length as the age of his column increases. One interesting observation is that for one word each author has similar preference changes over time.",
        "url": "http://www.jstor.org/stable/30204925",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204925"
    },
    "10.2307/30204926": {
        "year": "2004",
        "volume": "38",
        "title": "Hypertext Writing Profiles and Visualisation",
        "publisher": "Springer",
        "pages": "83--105",
        "number": "1",
        "journal": "Computers and the Humanities",
        "author": "Margit Pohl and Peter Purgathofer",
        "abstract": "University students increasingly use hypertext to write their assignments. To employ hypertext effectively, more information about the hypertext authoring process is needed. There are features of hypertext which are not reflected in traditional theories of composition, especially the possibility to structure information visually. Our study indicates that graphical overview maps which can be edited are a rather attractive feature of hypertext authoring systems. Nevertheless, not all students profit from such features. Students employ different writing styles when they create hypertext documents. The majority of students experiments with hypertext's new features but others are still influenced by the model of the book.",
        "url": "http://www.jstor.org/stable/30204926",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204926"
    },
    "10.2307/30204930": {
        "year": "2004",
        "volume": "38",
        "title": "On the Ownership of Text",
        "publisher": "Springer",
        "pages": "115--127",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "Yorick Wilks",
        "abstract": "The paper explores the notions of text ownership and its partial inverse, plagiarism, and asks how close or different they are from a procedural point of view that might seek to establish either of these properties. The emphasis is on procedures rather than on the conventional subject division of authorship studies, plagiarism detection etc. We use, as a particular example, our research on the notion of computational detection of text rewriting, in the benign sense of a standard journalist's adaptation of the Press Association newsfeed. The conclusion is that, whatever may be the case in copyright law, procedural detection and establishment of the ownership is a complex and vexed matter. Behind the paper is an unspoken appeal to return to an earlier historical phase, one where texts were normally rewritten and rewritten again and the ownership of text by an individual was a less clear matter than in historically recent times.",
        "url": "http://www.jstor.org/stable/30204930",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204930"
    },
    "10.2307/30204931": {
        "year": "2004",
        "volume": "38",
        "title": "Semantic Roles as Slots in OIL Ontologies",
        "publisher": "Springer",
        "pages": "129--147",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "Jolanta Cybulka and Jacek Martinek",
        "abstract": "The purpose of our research is to consider how the paradigms of EuroWordNet and SIMPLE linguistic projects on the one hand and the OIL methodology on the other hand may affect each other. OIL (Ontology Inference Layer) aims at implementing the \"semantic\" Web idea and is based on the notion of ontology, which is also employed in EuroWordNet and SIMPLE. In both latter projects the meanings of words are partially described by means of the finite sets of relations to other meanings of words, whereas in OIL the user is free to define the arbitrary relations of this kind. The relations considered in EuroWordNet and SIMPLE were defined on the basis of a careful observation of the large linguistic area, and they aim at reflecting the meaning as precisely as possible, therefore it seems useful to merge them with OIL. Moreover, the valuable feature of OIL is its formal language with precisely defined semantics. All things considered, we suggest how certain EuroWordNet and SIMPLE definitions may be expressed in OIL.",
        "url": "http://www.jstor.org/stable/30204931",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204931"
    },
    "10.2307/30204932": {
        "year": "2004",
        "volume": "38",
        "title": "The Development of Early Computer-Assisted Writing Instruction (1960-1978): The Double Logic of Media and Tools",
        "publisher": "Springer",
        "pages": "149--162",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "Carl Whithaus",
        "abstract": "This essay traces a distinction between computer-mediated writing environments that are tools for correcting student prose and those that are media for communication. This distinction has its roots in the influence of behavioral science on teaching machines and computer-aided writing instruction during the 1960s and 1970s. By looking at the development of the time-shared, interactive, computer-controlled, information television (TICCIT) and early human-computer interaction (HCI) research, this essay demonstrates that hardware and software systems had the potential to work as both tools and media. The influence of this double logic is not only historical but also has implications for post-secondary writing instruction in the age of Microsoft Word, ETS's e-rater, and the \"reading/assessment\" software tools being developed by Knowledge Analysis Technologies (KAT). This essay challenges composition researchers and computational linguists to develop pedagogies and software systems that acknowledge writing environments as situated within the logic of both tools for correction and media for communication.",
        "url": "http://www.jstor.org/stable/30204932",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204932"
    },
    "10.2307/30204933": {
        "year": "2004",
        "volume": "38",
        "title": "Extracting Multilingual Lexicons from Parallel Corpora",
        "publisher": "Springer",
        "pages": "163--189",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "Dan Tufiş and Ana Maria Barbu and Radu Ion",
        "abstract": "The paper describes our recent developments in automatic extraction of translation equivalents from parallel corpora. We describe three increasingly complex algorithms: a simple baseline iterative method, and two non-iterative more elaborated versions. While the baseline algorithm is mainly described for illustrative purposes, the non-iterative algorithms outline the use of different working hypotheses which may be motivated by different kinds of applications and to some extent by the languages concerned. The first two algorithms rely on cross-lingual POS preservation, while with the third one POS invariance is not an extraction condition. The evaluation of the algorithms was conducted on three different corpora and several pairs of languages.",
        "url": "http://www.jstor.org/stable/30204933",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204933"
    },
    "10.2307/30204934": {
        "year": "2004",
        "volume": "38",
        "title": "Intertextual Encoding in the Writing of Women's Literary History",
        "publisher": "Springer",
        "pages": "191--206",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "Susan Brown and Isobel Grundy and Patricia Clements and Renée Elio and Sharon Balazs and Rebecca Cameron",
        "abstract": "This paper explores theoretical and practical aspects of intertextuality, in relation to the highly interpretative  tag within the SGML tagset developed by the Orlando Project for its history of women's writing in the British Isles. Arguing that the concept of intertextuality is both crucial to and poses particular challenges to the creation of an encoding scheme for literary historical text, it outlines the ways in which the project's tags address broader issues of intertextuality. The paper then describes the specific tag in detail, and argues on the basis of provisional results drawn from the Orlando Project's textbase that despite the impossibility of tracking intertextuality exhaustively or devising a tagset that completely disambiguates the concept, this tag provides useful pathways through the textbase and valuable departure points for further inquiry. Finally, the paper argues that the challenges to notions of rigour posed by the concept of intertextuality can help us fruitfully to examine some of the suppositions (gendered and other) that we bring to electronic text markup.",
        "url": "http://www.jstor.org/stable/30204934",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204934"
    },
    "10.2307/30204935": {
        "year": "2004",
        "volume": "38",
        "title": "Semantic Variation in Idiolect and Sociolect: Corpus Linguistic Evidence from Literary Texts",
        "publisher": "Springer",
        "pages": "207--221",
        "number": "2",
        "journal": "Computers and the Humanities",
        "author": "Max M. Louwerse",
        "abstract": "Idiolects are person-dependent similarities in language use. They imply that texts by one author show more similarities in language use than texts between authors. Sociolects, on the other hand, are group-dependent similarities in language use. They imply that texts by a group of authors, for instance in terms of gender or time period, share more similarities within a group than between groups. Although idiolects and sociolects are commonly used terms in the humanities, they have not been investigated a great deal from corpus and computational linguistic points of view. To test several idiolect and sociolect hypotheses a factorial combination was used of time period (Modernism, Realism), gender of author (male, female) and author (Eliot, Dickens, Woolf, Joyce) totaling 16 corresponding literary texts. In a series of corpus linguistic studies using Boolean and vector models, no conclusive evidence was found for the selected idiolect and sociolect hypotheses. In final analyses testing the semantics within each literary text, this lack of evidence was explained by the low homogeneity within a literary text.",
        "url": "http://www.jstor.org/stable/30204935",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204935"
    },
    "10.2307/30204939": {
        "year": "2004",
        "volume": "38",
        "title": "Bitext Generation through Rich Markup",
        "publisher": "Springer",
        "pages": "223--251",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Arantza Casillas and Raquel Martínez",
        "abstract": "This paper reports on a method for exploiting a bitext as the primary linguistic information source for the design of a generation environment for specialized bilingual documentation. The paper discusses such issues as Text Encoding Initiative (TEI), proposals for specialized corpus tagging, text segmentation and alignment of translation units and their allocation into translation memories, Document Type Definition (DTD), abstraction from tagged texts, and DTD deployment for bilingual text generation. The parallel corpus used for experimentation has two main features: - It contains bilingual documents from a dedicated domain of legal and administrative publications rich in specialized jargon. - It involves two languages, Spanish and Basque, which are typologically very distinct (both lexically and morpho-syntactically). Starting from an annotated bitext we show how Standard Generalized Markup Language (SGML) elements can be recycled to produce complementary language resources. Several translation memory databases are produced. Furthermore, DTDs for source and target documents are derived and put into correspondence. This paper discusses how these resources are automatically generated and applied to an interactive bilingual authoring system.",
        "url": "http://www.jstor.org/stable/30204939",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204939"
    },
    "10.2307/30204940": {
        "year": "2004",
        "volume": "38",
        "title": "Article: Collating Texts Using Progressive Multiple Alignment",
        "publisher": "Springer",
        "pages": "253--270",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Matthew Spencer and Christopher J. Howe",
        "abstract": "To reconstruct a stemma or do any other kind of statistical analysis of a text tradition, one needs accurate data on the variants occurring at each location in each witness. These data are usually obtained from computer collation programs. Existing programs either collate every witness against a base text or divide all texts up into segments as long as the longest variant phrase at each point. These methods do not give ideal data for stemma reconstruction. We describe a better collation algorithm (progressive multiple alignment) that collates all witnesses word by word without a base text, adding groups of witnesses one at a time, starting with the most closely related pair.",
        "url": "http://www.jstor.org/stable/30204940",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204940"
    },
    "10.2307/30204941": {
        "year": "2004",
        "volume": "38",
        "title": "Uncovering Text-Music Connections with a Relational Database: Towards an Objective Measurement of Melodic Pitch Diversity in Relation to Literary Themes in Bach's Church Cantata Recitatives",
        "publisher": "Springer",
        "pages": "271--297",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Melvin Unger",
        "abstract": "Bach's cantatas are particularly rich in text imagery, and they typically employ chromatic melodies to accentuate the more piquant literary images, especially in recitatives. Heretofore theories about the intentionality of Bach's compositional choices in this regard have necessarily remained conjectural. In the following study, an objective measurement of pitch diversity in the vocal lines of Bach's church cantata recitatives in relation to literary themes was made possible with specially designed computer software allowing pertinent information to be entered efficiently into a relational database. Because the software tracked not only the 90,000 pitches constituting the vocal lines of these movements but also other attributes (e.g., overall length, presence or absence of accompaniment, opening and closing keys, chronological position, among others), interrelationships among the various attributes could be examined. Findings demonstrate clear correlation between pitch diversity and the degree of affective tension implied by particular textual subjects. While the findings do not prove exclusive causation (other factors such as tonal and structural considerations, social occasion, and evolution of style can also play a role), they do link the two elements, especially in light of Bach's method of composition as documented by Robert Marshall. This study is important for its systematic and comprehensive approach, its findings giving definition and clarity to commonly held generalizations about the relationships between melodic chromaticism (of which pitch diversity is an important aspect and indicator) and textual content. Furthermore, the software holds promise for additional studies of Bach's pitch materials and for studies in other stylistic contexts.",
        "url": "http://www.jstor.org/stable/30204941",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204941"
    },
    "10.2307/30204942": {
        "year": "2004",
        "volume": "38",
        "title": "Networked Collaborative Learning in the Study of Modern History and Literature",
        "publisher": "Springer",
        "pages": "299--315",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Guglielmo Trentin",
        "abstract": "Many teachers adopt networked collaborative learning strategies even though these approaches systematically increase the time needed to deal with a given subject. \"But who's making them do it?\". Probably there has to be a return on investment, in terms of time and obviously in terms of educational results, which justifies that commitment. After surveying the particular features of two experimental projects based on networked collaborative learning, the paper will then offer a series of thoughts triggered by observation of the results and the dynamics generated by this specific approach. The purpose of these thoughts is to identify some key factors that make it possible to measure the real added value produced by network collaboration in terms of the acquisition of skills, knowledge, methods and attitudes that go beyond the \"mere\" learning of contents (however fundamental this may be). And it is precisely on the basis of these considerations that teachers usually answer the above question, explaining \"who (or what) made them do it!\".",
        "url": "http://www.jstor.org/stable/30204942",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204942"
    },
    "10.2307/30204943": {
        "year": "2004",
        "volume": "38",
        "title": "Film as \"Explicador\" for Hypertext",
        "publisher": "Springer",
        "pages": "317--333",
        "number": "3",
        "journal": "Computers and the Humanities",
        "author": "Joe Essid",
        "abstract": "A few ideas from film theory, most notably Eisenstein's concept of montage, can improve students' understanding of hypertexts and lessen their resistance to open-ended, nonlinear narratives. These structural characteristics, so frustrating to many new readers of hypertext, can also be found in popular and experimental films. In particular, Godfrey Reggio's (1983) documentary Koyaanisqatsi provides a good starting point for merging hypertext and film theory. Koyaanisqatsi not only broke new ground for documentary film; its structure also resembles Landow's model for an axial hypertext. At the same time, techniques pioneered by Landow, Joyce, Guyer, and others involved in creating and critiquing hypertext can be used to examine film. Having students look closely at Koyaanisqatsi's composition allows them to become amateur cinematographers, who now possess software for breaking a film down and examining its composition, montage, transitions, subliminal messages, and motifs - a process that may then be applied to hypertext.",
        "url": "http://www.jstor.org/stable/30204943",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204943"
    },
    "10.2307/30204951": {
        "year": "2004",
        "volume": "38",
        "title": "Pitfalls in Corpus Research",
        "publisher": "Springer",
        "pages": "343--362",
        "number": "4",
        "journal": "Computers and the Humanities",
        "author": "Toni Rietveld and Roeland van Hout and Mirjam Ernestus",
        "abstract": "This paper discusses some pitfalls in corpus research and suggests solutions on the basis of examples and computer simulations. We first address reliability problems in language transcriptions, agreement between transcribers, and how disagreements can be dealt with. We then show that the frequencies of occurrence obtained from a corpus cannot always be analyzed with the traditional χ² test, as corpus data are often not sequentially independent and unit independent. Next, we stress the relevance of the power of statistical tests, and the sizes of statistically significant effects. Finally, we point out that a t-test based on log odds often provides a better alternative to a χ² analysis based on frequency counts.",
        "url": "http://www.jstor.org/stable/30204951",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204951"
    },
    "10.2307/30204952": {
        "year": "2004",
        "volume": "38",
        "title": "Automatic Acquisition and Expansion of Hypernym Links",
        "publisher": "Springer",
        "pages": "363--396",
        "number": "4",
        "journal": "Computers and the Humanities",
        "author": "Emmanuel Morin and Christian Jacquemin",
        "abstract": "Recent developments in computational terminology call for the design of multiple and complementary tools for the acquisition, the structuring and the exploitation of terminological data. This paper proposes to bridge the gap between term acquisition and thesaurus construction by offering a framework for automatic structuring of multi-word candidate terms with the help of corpus-based links between single-word terms. First, we present a system for corpus-based acquisition of terminological relationships through discursive patterns. This system is built on previous work on automatic extraction of hyponymy links through shallow parsing. Second, we show how hypernym links between single-word terms can be extended to semantic links between multi-word terms through corpus-based extraction of semantic variants. The induced hierarchy is incomplete but provides an automatic generalization of singleword terms relations to multi-word terms that are pervasive in technical thesauri and corpora.",
        "url": "http://www.jstor.org/stable/30204952",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204952"
    },
    "10.2307/30204953": {
        "year": "2004",
        "volume": "38",
        "title": "Experimenting with a Question Answering System for the Arabic Language",
        "publisher": "Springer",
        "pages": "397--415",
        "number": "4",
        "journal": "Computers and the Humanities",
        "author": "Bassam Hammo and Saleem Abuleil and Steven Lytinen and Martha Evens",
        "abstract": "The World Wide Web (WWW) today is so vast that it has become more and more difficult to find answers to questions using standard search engines. Current search engines can return ranked lists of documents, but they do not deliver direct answers to the user. The goal of Open Domain Question Answering (QA) systems is to take a natural language question, understand the meaning of the question, and present a short answer as a response based on a repository of information. In this paper we present QARAB, a QA system that combines techniques from Information Retrieval and Natural Language Processing. This combination enables domain independence. The system takes natural language questions expressed in the Arabic language and attempts to provide short answers in Arabic. To do so, it attempts to discover what the user wants by analyzing the question and a variety of candidate answers from a linguistic point of view.",
        "url": "http://www.jstor.org/stable/30204953",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204953"
    },
    "10.2307/30204954": {
        "year": "2004",
        "volume": "38",
        "title": "Evaluation of Linguistic Features for Word Sense Disambiguation with Self-Organized Document Maps",
        "publisher": "Springer",
        "pages": "417--435",
        "number": "4",
        "journal": "Computers and the Humanities",
        "author": "Krister Lindén",
        "abstract": "Word sense disambiguation automatically determines the appropriate senses of a word in context. We have previously shown that self-organized document maps have properties similar to a large-scale semantic structure that is useful for word sense disambiguation. This work evaluates the impact of different linguistic features on self-organized document maps for word sense disambiguation. The features evaluated are various qualitative features, e.g. part-of-speech and syntactic labels, and quantitative features, e.g. cut-off levels for word frequency. It is shown that linguistic features help make contextual information explicit. If the training corpus is large even contextually weak features, such as base forms, will act in concert to produce sense distinctions in a statistically significant way. However, the most important features are syntactic dependency relations and base forms annotated with part of speech or syntactic labels. We achieve 62.9% ± 0.73% correct results on the fine grained lexical task of the English SENSEVAL-2 data. On the 96.7% of the test cases which need no back-off to the most frequent sense we achieve 65.7% correct results.",
        "url": "http://www.jstor.org/stable/30204954",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204954"
    },
    "10.2307/30204955": {
        "year": "2004",
        "volume": "38",
        "title": "Multiple Heuristics and Their Combination for Automatic WordNet Mapping",
        "publisher": "Springer",
        "pages": "437--455",
        "number": "4",
        "journal": "Computers and the Humanities",
        "author": "Changki Lee and Gary Geunbae Lee and Jungyun Seo",
        "abstract": "This paper presents an automatic construction of Korean WordNet from preexisting lexical resources. We develop a set of automatic word sense disambiguation techniques to link a Korean word sense collected from a bilingual machine-readable dictionary to a single corresponding English WordNet synset. We show how individual links provided by each word sense disambiguation method can be non-linearly combined to produce a Korean WordNet from existing English WordNet for nouns.",
        "url": "http://www.jstor.org/stable/30204955",
        "issn": "00104817",
        "ENTRYTYPE": "article",
        "ID": "10.2307/30204955"
    }
}